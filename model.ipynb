{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZQr_2AoWc4x"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6KUID3e8a9O",
        "outputId": "2563c973-35fc-42a5-ccb7-10614378a869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "472N6GvK8b-2"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import random\n",
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stop_words\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Dropout, Activation, concatenate, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers, layers, optimizers, losses, metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcIToB_H8gDX"
      },
      "source": [
        "from IPython.display import SVG \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcCUrrC58q1x"
      },
      "source": [
        "pd.options.display.max_colwidth = 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbPzx63D8yxC"
      },
      "source": [
        "\n",
        "def tokenize(sentences, VOCAB_SIZE, filters='!\"#%&()*,+-/:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
        "             lower=True, split=' ', char_level=False, oov_token=\"<unk>\", verbose=True):\n",
        "    \"\"\"Convert the sentences (strings) into sequences of integers and toks.\n",
        "    When using deafult filter; words maybe include the `'` character \n",
        "    \n",
        "    '0' is a reserved index that won't be assigned to any word\n",
        "    If oov_token is passed: it will have index 1 and will be added to word_index, index_word dict\n",
        "    \n",
        "    Even though converted sequences has only VOCAB_SIZE words but all dicts of tokenizer (word_index, word_counts, word_docs) contains all the words.\n",
        "    All the words whose idx > VOCAB_SIZE will be treated as <unk> words\n",
        "    \"\"\"\n",
        "    \n",
        "    # no pruning of words\n",
        "    if VOCAB_SIZE == 0:\n",
        "        VOCAB_SIZE = None\n",
        "    \n",
        "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, filters = filters,\n",
        "                                                   lower=lower, split=split, char_level=char_level,\n",
        "                                                   oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    idx = tokenizer.texts_to_sequences(sentences)\n",
        "    tok = tokenizer.sequences_to_texts(idx)\n",
        "\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"tokenizer details:\")\n",
        "        \n",
        "        keys1 = ['char_level', 'filters', 'lower',  'split', 'oov_token', \n",
        "                 'document_count', 'num_words']\n",
        "        keys2 = ['index_docs', 'word_docs', 'index_word',  'word_index', 'word_counts']\n",
        "\n",
        "        for key in keys1:\n",
        "            print(f\"\\t{key:<20}: {tokenizer.__getattribute__(key)}\")\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        for key in keys2:\n",
        "            print(f\"\\tlen {key:<20}: {len(tokenizer.__getattribute__(key))}\")\n",
        "        print(\"\\n\")\n",
        "        \n",
        "\n",
        "    return tokenizer, idx, tok\n",
        "\n",
        "\n",
        "def build_word_index(tokenizer, VOCAB_SIZE, pad_idx=0, verbose=True):\n",
        "    \"\"\"Adds pad token to toknizer and returns word2idx, idx2word till VOCAB_SIZE\"\"\"\n",
        "    \n",
        "    # we need to add <pad> token to tokenizer.word_index if plan to use inbuilt sequences_to_texts\n",
        "    # else sequences_to_texts on padded data will replace pad_idx (by default 0) by <unk> rather than <pad>\n",
        "    tokenizer.word_index.update({'<pad>':pad_idx})\n",
        "    tokenizer.index_word.update({pad_idx:'<pad>'})\n",
        "    tokenizer.word_index.update({'<marker>':1})\n",
        "    tokenizer.index_word.update({1:'<marker>'})\n",
        "\n",
        "\n",
        "    # since tokenizer.word_index has all words > we get only what's under VOCAB_SIZE\n",
        "    word2idx = {w:idx for w,idx in tokenizer.word_index.items() if idx < VOCAB_SIZE}\n",
        "    idx2word = {idx:w for w,idx in word2idx.items()}\n",
        "\n",
        "    # if our vocab was less than VOCAB_SIZE > we should adjust it\n",
        "    VOCAB_SIZE = len(word2idx)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"len word2idx: \", len(word2idx))\n",
        "        print(\"len idx2word: \", len(idx2word))\n",
        "        print(\"vocab size: \", VOCAB_SIZE)\n",
        "        print(\"\")\n",
        "    \n",
        "    return tokenizer, word2idx, idx2word\n",
        "    \n",
        "\n",
        "def built_embed_matrix(embed, word2idx, VOCAB_SIZE, EMBED_DIM, oov_vec = \"mean\"):\n",
        "    \"\"\"Will build embedding matrix using pre-trained word vector\n",
        "    Returns embedding_matrix, unk_words (words that aren't found in pre-trained word embeddings)\n",
        "    \n",
        "    oov_vec: param decides how we want to handle OOV word that aren't present in pre-trained word vectors\n",
        "      \"mean\": average embedding of all words for replacing OOV words\n",
        "      \"norm\": randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "      \n",
        "      todo: optimize the oov_vec part.\n",
        "    \"\"\"\n",
        "    \n",
        "    # stack all pre-trained word embeddings\n",
        "    emb_all = np.stack(list(embed.values()))\n",
        "    \n",
        "    if oov_vec == \"mean\":\n",
        "        # average embedding of all words \n",
        "        emb_oov = np.mean(emb_all, axis=0)\n",
        "        print(f\"embed_all shape: {emb_all.shape} | embed_mean_vec shape: {emb_oov.shape}\")\n",
        "    \n",
        "    \n",
        "    if oov_vec == \"rand\":\n",
        "        # mean and std of all values in embed \n",
        "        # this can be used to randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "        emb_mean, emb_std = emb_all.mean(), emb_all.std()\n",
        "        emb_oov = np.random.normal(emb_mean, emb_std, (EMBED_DIM))\n",
        "        \n",
        "        print(f\"embed_all shape: {emb_all.shape}\")\n",
        "        print(f\"embed mean: {emb_mean} | embed std: {emb_std} | embed rand_vec shape: {emb_oov.shape}\")\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    print('\\nprepare embedding matrix...')\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
        "    embed_cnt = 0\n",
        "    unk_words = []\n",
        "\n",
        "    for word, idx in word2idx.items():\n",
        "        embedding_vector = embed.get(word)\n",
        "\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "            embed_cnt += 1\n",
        "\n",
        "        else:\n",
        "            # embedding for <pad> token will be 0's as initialized\n",
        "            if word != '<pad>':\n",
        "                if oov_vec == \"mean\":\n",
        "                    # average embedding of all words \n",
        "                    embedding_matrix[idx] = emb_oov\n",
        "                \n",
        "                if oov_vec == \"rand\":\n",
        "                    # randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "                    emb_oov = np.random.normal(emb_mean, emb_std, (EMBED_DIM))\n",
        "                    embedding_matrix[idx] = emb_oov\n",
        "                    \n",
        "                unk_words.append((word, idx))     \n",
        "\n",
        "    print(f\"\\tembedding matrix shape: {embedding_matrix.shape}\")\n",
        "    print(f\"\\tword embedding found: {embed_cnt}\")\n",
        "    print(f\"\\tword embedding not found: {len(unk_words)}\")\n",
        "    print(\"\")\n",
        "    \n",
        "    return embedding_matrix, unk_words\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-pmOfwa81DZ",
        "outputId": "5be56935-ac49-4df5-c625-aa2dbff487ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvWB6UXA8_o7"
      },
      "source": [
        "data = open(\"/content/gdrive/My Drive/train.csv\")\n",
        "sents = data.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGMA_pD39NRg",
        "outputId": "7602adc0-038c-4e7f-bf93-aae1e18607ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fin = []\n",
        "for sent in sents:\n",
        "  if sent != '\\n':\n",
        "    fin.append(sent)\n",
        "len(fin)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbmVKtFP9VvP"
      },
      "source": [
        "out = [1 for i in range(40)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQtDknbX9put"
      },
      "source": [
        "lens = []\n",
        "for sen in fin:\n",
        "  lens.append(len(sen.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA5kJTQeBi3s",
        "outputId": "bd2044df-d5cf-4ade-d96a-00406553e46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "for q in [.5, .6, .8, .9, .95, 1.0]:\n",
        "    print(f\"\\tquantile: {q:<5} | {np.quantile(lens, q)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tquantile: 0.5   | 42.5\n",
            "\tquantile: 0.6   | 45.0\n",
            "\tquantile: 0.8   | 53.20000000000002\n",
            "\tquantile: 0.9   | 68.80000000000001\n",
            "\tquantile: 0.95  | 79.94999999999995\n",
            "\tquantile: 1.0   | 106.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtsALaSzIiGF"
      },
      "source": [
        "c=0\n",
        "for sent in fin:\n",
        "  words = len(set(sent.split()))\n",
        "  c = c+words\n",
        "c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MjHrr8B04c",
        "outputId": "b2956d82-8130-4920-8f39-d53ab25402ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "VOCAB_SIZE = 1500\n",
        "tokenizer, idx, tok = tokenize(fin, VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizer details:\n",
            "\tchar_level          : False\n",
            "\tfilters             : !\"#%&()*,+-/:;<=>?@[\\]^_`{|}~\t\n",
            "\n",
            "\tlower               : True\n",
            "\tsplit               :  \n",
            "\toov_token           : <unk>\n",
            "\tdocument_count      : 40\n",
            "\tnum_words           : 1500\n",
            "\n",
            "\n",
            "\tlen index_docs          : 451\n",
            "\tlen word_docs           : 451\n",
            "\tlen index_word          : 452\n",
            "\tlen word_index          : 452\n",
            "\tlen word_counts         : 451\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97nVVZSPJnKF",
        "outputId": "d53c966a-5a82-438d-e185-fda1f81a1abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "tok[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for pos tagging and lemmatization we combine genia with its built in occasionally deviant to kenizer and tnt brants 2000 which operates on pre tokenized inputs but in its default models trained on financial news from the penn tree bank. $$$$$ tnt uses second order markov models for part ofspeech tagging.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbMU5-hJt8Y",
        "outputId": "72b73692-2112-48b4-dea9-f521aa5e5a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "tokenizer, word2idx, idx2word = build_word_index(tokenizer, VOCAB_SIZE, pad_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len word2idx:  454\n",
            "len idx2word:  453\n",
            "vocab size:  454\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NdDCi81OyBQ"
      },
      "source": [
        "def convert2seq():\n",
        "  data_idx = []\n",
        "  for sent in tok:\n",
        "    row=[]\n",
        "    for word in sent.split():\n",
        "      row.append(word2idx[word])\n",
        "    data_idx.append(row)\n",
        "  return data_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXLNyCmvPVJJ"
      },
      "source": [
        "data_idx = convert2seq()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy3ZPGDFPmnN",
        "outputId": "5c9b2ae5-f5ac-44e3-8095-d007593faa75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word2idx['$$$$$']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMNYm7zxPhXe",
        "outputId": "0e0610a7-7cda-4166-ed9b-5503ad049024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_SEQ_LEN = 90\n",
        "data_padded = keras.preprocessing.sequence.pad_sequences(data_idx, maxlen=MAX_SEQ_LEN, \n",
        "                                                         padding='post', truncating='post', value=0)\n",
        "data_padded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 90)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_fxTQj7QCXY"
      },
      "source": [
        "EMBED_DIM = 50\n",
        "embed_path = \"/content/gdrive/My Drive/glove.6B/glove.6B.%sd.txt\" % EMBED_DIM \n",
        "glove_embed = {}\n",
        "f = open(embed_path)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    if len(coefs) == 50:\n",
        "      glove_embed[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN_evn_nQGie",
        "outputId": "0064a8bd-1f6d-4aa1-fd8e-c20ed83595d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "embedding_matrix, unk_words = built_embed_matrix(glove_embed, word2idx, VOCAB_SIZE, \n",
        "                                                 EMBED_DIM, oov_vec='mean')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed_all shape: (400000, 50) | embed_mean_vec shape: (50,)\n",
            "\n",
            "prepare embedding matrix...\n",
            "\tembedding matrix shape: (1500, 50)\n",
            "\tword embedding found: 407\n",
            "\tword embedding not found: 46\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y263CAfbUvre",
        "outputId": "071d9917-0e17-4aaf-ba7b-9674458f76e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(np.array(data_padded), np.array(out), random_state=10, test_size=0.2, shuffle=True)\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 90), (32,), (8, 90), (8,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKQ_rlSiU9sp",
        "outputId": "86d40188-104a-4a51-beda-2ace85b7ecd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "NUM_CLASSES = 1\n",
        "input_ = Input(shape=(MAX_SEQ_LEN,), name=\"input\")\n",
        "\n",
        "embed_layer = Embedding(input_dim= VOCAB_SIZE, weights=[embedding_matrix], \n",
        "                        output_dim= EMBED_DIM, input_length = MAX_SEQ_LEN, \n",
        "                        trainable= False, mask_zero=True, name=\"embed\")(input_)\n",
        "\n",
        "lstm_layer = Bidirectional(LSTM(units= 128, dropout=.25, \n",
        "                                recurrent_dropout=.25, kernel_regularizer = regularizers.l2(0.01), return_sequences=True, return_state=True,\n",
        "                               ), name=\"lstm\")(embed_layer)\n",
        "\n",
        "dense_layer = Dense(64, activation=\"relu\", name=\"dense\")(lstm_layer)\n",
        "dense_layer = Dropout(.25)(dense_layer)\n",
        "out = Dense(NUM_CLASSES, activation= \"sigmoid\", name=\"output\")(dense_layer) \n",
        "\n",
        "model_lstm = Model(input_, out)\n",
        "\n",
        "model_lstm.compile(optimizer= \"adam\", \n",
        "                   loss= \"binary_crossentropy\", \n",
        "                   metrics= [\"accuracy\"]) \n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 90)]              0         \n",
            "_________________________________________________________________\n",
            "embed (Embedding)            (None, 90, 50)            75000     \n",
            "_________________________________________________________________\n",
            "lstm (Bidirectional)         (None, 256)               183296    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 274,809\n",
            "Trainable params: 199,809\n",
            "Non-trainable params: 75,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "305baZKpWAHt"
      },
      "source": [
        "class_weights = compute_class_weight('balanced', np.unique(train_y), train_y)\n",
        "class_weights = {i : class_weights[i] for i in range(2)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceUf9pLWPyM"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model_lstm.fit(train_x, train_y,\n",
        "                         batch_size = BATCH_SIZE,\n",
        "                         epochs = EPOCHS,\n",
        "                         shuffle = True,\n",
        "                         class_weight = class_weights,\n",
        "                         validation_data = (test_x, test_y)\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTCpYivOWivn"
      },
      "source": [
        "pred = model_lstm.predict(test_x)\n",
        "pred = (pred>=.70).astype(float)\n",
        "\n",
        "count = 0\n",
        "for i in pred:\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "\n",
        "print(count)\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(test_y, pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI0433tKL5TA",
        "outputId": "54cdd037-dfa1-49b3-8cf7-1445a571c78f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "model_lstm.save(\"model_lstm.h5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8e2eb13a40d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_lstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_lstm.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_lstm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1cSF83mWn89"
      },
      "source": [
        "model_lstm = load_model('lstm_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvp28SpWqey"
      },
      "source": [
        "for layer in model.layers:\n",
        "        if \"lstm\" in str(layer):\n",
        "            weightLSTM = layer.get_weights()\n",
        "warr,uarr, barr = weightLSTM\n",
        "warr.shape,uarr.shape,barr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn36axInYytF"
      },
      "source": [
        "input_ = Input(shape=(MAX_SEQ_LEN,), name=\"input\")\n",
        "embed_layer = Embedding(input_dim= VOCAB_SIZE, weights=[embedding_matrix], \n",
        "                        output_dim= EMBED_DIM, input_length = MAX_SEQ_LEN, \n",
        "                        trainable= False, mask_zero=True, name=\"embed\")(input_)\n",
        "out, h, c = Bidirectional(LSTM(units= 128, dropout=.25, \n",
        "                                recurrent_dropout=.25, kernel_regularizer = regularizers.l2(0.01), return_sequences=True, return_state=True,\n",
        "                               ), name=\"lstm\")(embed_layer)\n",
        "model1 = Model(input_, [h, c, out])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRuMV87-bDKc"
      },
      "source": [
        "for layer in model1.layers:\n",
        "  for layer1 in model_lstm.layers:\n",
        "    if layer.name == layer1.name:\n",
        "      layer.set_weights(layer1.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCO5mINbTeR"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "test1 = pd.read_csv(\"/content/gdrive/My Drive/test1.csv\")\n",
        "sentences = {}\n",
        "list1 = list(df['col1'])\n",
        "list2 = list(df['col2'])\n",
        "flist = list1+list2\n",
        "flist = set(flist)\n",
        "flist = list(flist)\n",
        "for sent in flist:\n",
        "  h_t_keras, c_t_keras, lstm = model1.predict(sent)\n",
        "  sentences[sent]=h_t_keras\n",
        "\n",
        "for sent in set(list(df['col1'])):\n",
        "  inp = sentences[sent]\n",
        "  opts = list(df[df['col1']==sent].col2)\n",
        "\n",
        "for opt in opts:\n",
        "  dict1[opt] = distance.cosine(inp, sentences[opt])\n",
        "\n",
        "dict1 = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1])}\n",
        "print(dict1.keys()[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVkx5bMSWxgj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}