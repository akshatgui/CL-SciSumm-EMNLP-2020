col1,col2,col3
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",1
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Notice that a similar formula can be defined to integrate vertex weights.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The results are evaluated using precision, recall, and F-measure.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The learning system is a rule induction system with bagging.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The table also lists precision, recall, and F-measure.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Discussion.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Discussion.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",1
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",1
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Notice that a similar formula can be defined to integrate vertex weights.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The results are evaluated using precision, recall, and F-measure.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The learning system is a rule induction system with bagging.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The table also lists precision, recall, and F-measure.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Discussion.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",1
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Discussion.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
,"In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
,"In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
,"Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
,"Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
,"In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
,"Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
,"Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
,"In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
,"We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
,"Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
,The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
,"When one vertex links to another one, it is basically casting a vote for that other vertex.",0
,"The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
,"Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
,"Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
,"Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
,"For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
,"The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
,"In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
,"The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
,"Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
,"After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
,"Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
,"It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
,"HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
,"Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
,"For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
,"Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
,"As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
,"In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
,"However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
,It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
,"Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
,Notice that a similar formula can be defined to integrate vertex weights.,0
,"Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
,"While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
,"To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
,"Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
,"Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
,"Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
,The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
,"Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
,"Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
,The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
,"However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
,"The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
,"This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
,"A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
,"Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
,"More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
,"As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
,"In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
,"In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
,"Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
,"Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
,The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
,"The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
,Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
,"We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
,"Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
,"The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
,"One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
,"We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
,"The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
,"First, Compatibility of systems of linear constraints over the set of natural numbers.",0
,"Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
,Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
,"These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
,"To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
,"Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
,"After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
,"Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
,"While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
,"(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
,"For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
,"During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
,"For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
,Figure 2 shows a sample graph built for an abstract from our test collection.,0
,"While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
,"For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
,Notice that this ranking is different than the one rendered by simple word frequencies.,0
,"For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
,"All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
,"The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
,"This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
,The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
,"Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
,"We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
,"In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
,"Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
,"The results are evaluated using precision, recall, and F-measure.",0
,"Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
,"For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
,"Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
,"These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
,The learning system is a rule induction system with bagging.,0
,"Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
,"Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
,"For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
,"The table also lists precision, recall, and F-measure.",0
,Discussion.,0
,"TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
,"A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
,"Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
,"We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
,"The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
,"Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
,"We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
,Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
,"Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
,"Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
,"Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
,The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
,"In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
,"In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
,"TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
,"To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
,"For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
,"The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
,"Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
,"Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
,"The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
,"Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
,"The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
,"The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
,"Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
,"Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
,Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
,By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
,Flooding is expected in Puerto Rico and in the Virgin Islands.,0
,"The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
,Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
,The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
,It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
,"The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
,"Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
,"The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
,"The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
,"After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
,"Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
,"The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
,The sentences with the highest rank are selected for inclusion in the abstract.,0
,"For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
,"We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
,"For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
,"For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
,"Two manually produced reference summaries are provided, and used in the evaluation process5.",0
,"Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
,"Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
,"TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
,"Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
,Discussion.,0
,TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
,"Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
,Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
,"For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
,"Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
,"Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
,We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
,"Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
,"Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
,"Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
,"A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
,"For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
,"In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
,"The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
,"An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
,"In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
,"This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
,"Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
,"The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
,"The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
,"In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
,"In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",1
,"An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",1
,"In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
,"In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
,"Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
,"Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
,"In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
,"Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
,"Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
,"In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
,"We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
,"Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
,The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
,"When one vertex links to another one, it is basically casting a vote for that other vertex.",0
,"The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
,"Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
,"Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
,"Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
,"For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
,"The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
,"In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
,"The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
,"Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
,"After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
,"Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
,"It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
,"HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
,"Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
,"For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
,"Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
,"As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
,"In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
,"However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
,It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
,"Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
,Notice that a similar formula can be defined to integrate vertex weights.,0
,"Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
,"While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
,"To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
,"Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
,"Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
,"Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
,The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
,"Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
,"Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
,The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
,"However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
,"The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
,"This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
,"A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
,"Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
,"More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
,"As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
,"In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
,"In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
,"Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
,"Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
,The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
,"The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
,Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
,"We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
,"Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
,"The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
,"One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
,"We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
,"The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
,"First, Compatibility of systems of linear constraints over the set of natural numbers.",0
,"Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
,Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
,"These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
,"To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
,"Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
,"After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
,"Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
,"While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
,"(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
,"For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
,"During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
,"For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
,Figure 2 shows a sample graph built for an abstract from our test collection.,0
,"While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
,"For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
,Notice that this ranking is different than the one rendered by simple word frequencies.,0
,"For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
,"All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
,"The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
,"This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
,The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
,"Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
,"We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
,"In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
,"Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
,"The results are evaluated using precision, recall, and F-measure.",0
,"Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
,"For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
,"Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
,"These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
,The learning system is a rule induction system with bagging.,0
,"Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
,"Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
,"For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
,"The table also lists precision, recall, and F-measure.",0
,Discussion.,0
,"TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
,"A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
,"Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
,"We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
,"The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
,"Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
,"We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
,Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
,"Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
,"Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
,"Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
,The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
,"In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
,"In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
,"TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
,"To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
,"For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
,"The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
,"Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
,"Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
,"The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
,"Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
,"The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
,"The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
,"Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
,"Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
,Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
,By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
,Flooding is expected in Puerto Rico and in the Virgin Islands.,0
,"The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
,Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
,The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
,It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
,"The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
,"Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
,"The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
,"The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
,"After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
,"Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
,"The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
,The sentences with the highest rank are selected for inclusion in the abstract.,0
,"For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
,"We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
,"For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
,"For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
,"Two manually produced reference summaries are provided, and used in the evaluation process5.",0
,"Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
,"Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
,"TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
,"Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
,Discussion.,0
,TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
,"Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
,Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
,"For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
,"Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
,"Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
,We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
,"Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
,"Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
,"Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
,"A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
,"For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
,"In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
,"The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
,"An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
,"In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
,"This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
,"Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
,"The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
,"The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
,"In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
,"In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",1
,"An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",1
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",1
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Notice that a similar formula can be defined to integrate vertex weights.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The results are evaluated using precision, recall, and F-measure.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The learning system is a rule induction system with bagging.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The table also lists precision, recall, and F-measure.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Discussion.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",1
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Discussion.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Text Rank (TR) This is a graph-based sum mariser method (Mihalcea and Tarau, 2004) where each word is a vertex.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",1
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Notice that a similar formula can be defined to integrate vertex weights.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The results are evaluated using precision, recall, and F-measure.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The learning system is a rule induction system with bagging.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The table also lists precision, recall, and F-measure.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Discussion.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Discussion.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",1
"TextRank (Mihalcea and Tarau, 2004) is one of the most well-known graph based approaches to key phrase extraction.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Notice that a similar formula can be defined to integrate vertex weights.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The results are evaluated using precision, recall, and F-measure.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The learning system is a rule induction system with bagging.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The table also lists precision, recall, and F-measure.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Discussion.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",1
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",1
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Discussion.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"PageRank score: as described in (Mihalcea and Tarau, 2004), each sentence in Dq is a node in the graph and the cosine similarity between a pair of sentences is used as edge weight.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",1
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",1
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Notice that a similar formula can be defined to integrate vertex weights.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The results are evaluated using precision, recall, and F-measure.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The learning system is a rule induction system with bagging.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The table also lists precision, recall, and F-measure.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Discussion.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Discussion.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Note, by the way, that although our graphs are non-weighted and directed, like a graph of web pages and hyper links (and unlike the text graphs in Mihalcea and Tarau (2004), for example), several pairs of nodes may be connected by multiple edges, making a transition between them more probable.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",1
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Notice that a similar formula can be defined to integrate vertex weights.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The results are evaluated using precision, recall, and F-measure.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The learning system is a rule induction system with bagging.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The table also lists precision, recall, and F-measure.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Discussion.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Discussion.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",1
"Unsupervised approaches have also been proposed ,e.g. by Mihalcea and Tarau (2004) and Liu et al (2009).","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",1
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Notice that a similar formula can be defined to integrate vertex weights.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The results are evaluated using precision, recall, and F-measure.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The learning system is a rule induction system with bagging.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The table also lists precision, recall, and F-measure.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Discussion.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Discussion.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",1
"In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004), a graph based ranking model for key word extraction which achieves state-of-the-art accuracy.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Notice that a similar formula can be defined to integrate vertex weights.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The results are evaluated using precision, recall, and F-measure.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The learning system is a rule induction system with bagging.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",1
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The table also lists precision, recall, and F-measure.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Discussion.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",1
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Discussion.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank key words based on the co-occurrence links between words.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",1
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",1
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Notice that a similar formula can be defined to integrate vertex weights.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The results are evaluated using precision, recall, and F-measure.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The learning system is a rule induction system with bagging.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The table also lists precision, recall, and F-measure.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Discussion.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Discussion.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"If substep 1) is performed on each single document without considering the cluster context, the approach is degenerated into the simple Tex t Rank model (Mihalcea and Tarau, 2004), which is denoted as SingleRank in this paper.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",1
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Notice that a similar formula can be defined to integrate vertex weights.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The results are evaluated using precision, recall, and F-measure.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The learning system is a rule induction system with bagging.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The table also lists precision, recall, and F-measure.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Discussion.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",1
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Discussion.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"As in Mihalcea and Tarau (2004), the documents are tagged by a 2 The original words are used without stemming.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",1
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",1
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Notice that a similar formula can be defined to integrate vertex weights.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The results are evaluated using precision, recall, and F-measure.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The learning system is a rule induction system with bagging.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The table also lists precision, recall, and F-measure.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Discussion.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Discussion.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Meanwhile, Mihalcea and Tarau (2004) presented their PageRank variation, called TextRank, in the same year.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",1
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Notice that a similar formula can be defined to integrate vertex weights.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The results are evaluated using precision, recall, and F-measure.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The learning system is a rule induction system with bagging.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The table also lists precision, recall, and F-measure.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Discussion.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",1
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Discussion.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"As an example of an unsupervised keyphrase extraction approach, the graph-based ranking (Mihalcea and Tarau, 2004) regards key phrase extraction as a ranking task, where a document is represented by a term graph based on term relatedness, and then a graph-based ranking algorithm is used to assign importance scores to each term.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Notice that a similar formula can be defined to integrate vertex weights.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,1
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",1
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The results are evaluated using precision, recall, and F-measure.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The learning system is a rule induction system with bagging.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The table also lists precision, recall, and F-measure.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Discussion.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Discussion.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Existing methods usually use term co occurrences within a specified window size in the given document as an approximation of term relatedness (Mihalcea and Tarau, 2004).","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",1
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",1
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Notice that a similar formula can be defined to integrate vertex weights.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The results are evaluated using precision, recall, and F-measure.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The learning system is a rule induction system with bagging.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The table also lists precision, recall, and F-measure.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Discussion.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Discussion.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"Starting with TextRank (Mihalcea and Tarau,2004), graph-based ranking methods are becoming the most widely used unsupervised approach for key phrase extraction.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Notice that a similar formula can be defined to integrate vertex weights.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",1
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",1
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The results are evaluated using precision, recall, and F-measure.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The learning system is a rule induction system with bagging.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The table also lists precision, recall, and F-measure.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Discussion.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",The sentences with the highest rank are selected for inclusion in the abstract.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Discussion.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"The dataset is used in both (Hulth, 2003) and (Mihalcea and Tarau, 2004).","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Notice that a similar formula can be defined to integrate vertex weights.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",1
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",1
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The results are evaluated using precision, recall, and F-measure.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The learning system is a rule induction system with bagging.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The table also lists precision, recall, and F-measure.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Discussion.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",The sentences with the highest rank are selected for inclusion in the abstract.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Discussion.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"We use the uncontrolled key phrases for evaluation as proposed in (Hulth, 2003) and followed by (Mihalcea and Tarau, 2004).","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In this paper, we introduce TextRank – a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Graph-based ranking algorithms like Kleinberg’s HITS algorithm (Kleinberg, 1999) or Google’s PageRank (Brin and Page, 1998) have been successfully used in citation analysis, social networks, and the analysis of the link-structure of the World Wide Web.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Arguably, these algorithms can be singled out as key elements of the paradigm-shift triggered in the field of Web search technology, by providing a Web page ranking mechanism that relies on the collective knowledge of Web architects rather than individual content analysis of Web pages.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In short, a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph, by taking into account global information recursively computed from the entire graph, rather than relying only on local vertex-specific information.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Applying a similar line of thinking to lexical or semantic graphs extracted from natural language documents, results in a graph-based ranking model that can be applied to a variety of natural language processing applications, where knowledge drawn from an entire text is used in making local ranking/selection decisions.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases, to extractive summarization and word sense disambiguation (Mihalcea et al., 2004).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In this paper, we introduce the TextRank graphbased ranking model for graphs extracted from natural language texts.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We investigate and evaluate the application of TextRank to two language processing tasks consisting of unsupervised keyword and sentence extraction, and show that the results obtained with TextRank are competitive with state-of-the-art systems developed in these areas.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph, based on global information recursively drawn from the entire graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The basic idea implemented by a graph-based ranking model is that of “voting” or “recommendation”.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","When one vertex links to another one, it is basically casting a vote for that other vertex.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The higher the number of votes that are cast for a vertex, the higher the importance of the vertex.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Moreover, the importance of the vertex casting the vote determines how important the vote itself is, and this information is also taken into account by the ranking model.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Hence, the score associated with a vertex is determined based on the votes that are cast for it, and the score of the vertices casting these votes.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Formally, let be a directed graph with the set of vertices and set of edges , where is a subset of .",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For a given vertex , let be the set of vertices that point to it (predecessors), and let be the set of vertices that vertex points to (successors).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The score of a vertex is defined as follows (Brin and Page, 1998): where is a damping factor that can be set between 0 and 1, which has the role of integrating into the model the probability of jumping from a given vertex to another random vertex in the graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In the context of Web surfing, this graph-based ranking algorithm implements the “random surfer model”, where a user clicks on links at random with a probability , and jumps to a completely new page with probability .",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The factor is usually set to 0.85 (Brin and Page, 1998), and this is the value we are also using in our implementation.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Starting from arbitrary values assigned to each node in the graph, the computation iterates until convergence below a given threshold is achieved 1.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","After running the algorithm, a score is associated with each vertex, which represents the “importance” of the vertex within the graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Notice that the final values obtained after TextRank runs to completion are not affected by the choice of the initial value, only the number of iterations to convergence may be different.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","It is important to notice that although the TextRank applications described in this paper rely on an algorithm derived from Google’s PageRank (Brin and Page, 1998), other graph-based ranking algorithms such as e.g.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","HITS (Kleinberg, 1999) or Positional Function (Herings et al., 2001) can be easily integrated into the TextRank model (Mihalcea, 2004).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Although traditionally applied on directed graphs, a recursive graph-based ranking algorithm can be also applied to undirected graphs, in which case the outdegree of a vertex is equal to the in-degree of the vertex.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For loosely connected graphs, with the number of edges proportional with the number of vertices, undirected graphs tend to have more gradual convergence curves.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Figure 1 plots the convergence curves for a randomly generated graph with 250 vertices and 250 edges, for a convergence threshold of 0.0001.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","As the connectivity of the graph increases (i.e. larger number of edges), convergence is usually achieved after fewer iterations, and the convergence curves for directed and undirected graphs practically overlap.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In the context of Web surfing, it is unusual for a page to include multiple or partial links to another page, and hence the original PageRank definition for graph-based ranking is assuming unweighted graphs.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","However, in our model the graphs are build from natural language texts, and may include multiple or partial links between the units (vertices) that are extracted from text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices and as a weight added to the corresponding edge that connects the two vertices.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Consequently, we introduce a new formula for graph-based ranking that takes into account edge weights when computing the score associated with a vertex in the graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Notice that a similar formula can be defined to integrate vertex weights.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Figure 1 plots the convergence curves for the same sample graph from section 2.1, with random weights in the interval 0–10 added to the edges.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","While the final vertex scores (and therefore rankings) differ significantly as compared to their unweighted alternatives, the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","To enable the application of graph-based ranking algorithms to natural language texts, we have to build a graph that represents the text, and interconnects words or other text entities with meaningful relations.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Depending on the application at hand, text units of various sizes and characteristics can be added as vertices in the graph, e.g. words, collocations, entire sentences, or others.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Similarly, it is the application that dictates the type of relations that are used to draw connections between any two such vertices, e.g. lexical or semantic relations, contextual overlap, etc.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Regardless of the type and characteristics of the elements added to the graph, the application of graphbased ranking algorithms to natural language texts consists of the following main steps: In the following, we investigate and evaluate the application of TextRank to two natural language processing tasks involving ranking of text units: (1) A keyword extraction task, consisting of the selection of keyphrases representative for a given text; and (2) A sentence extraction task, consisting of the identification of the most “important” sentences in a text, which can be used to build extractive summaries.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The task of a keyword extraction application is to automatically identify in a text a set of terms that best describe the document.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Such keywords may constitute useful entries for building an automatic index for a document collection, can be used to classify a text, or may serve as a concise summary for a given document.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Moreover, a system for automatic identification of important terms in a text can be used for the problem of terminology extraction, and construction of domain-specific dictionaries.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The simplest possible approach is perhaps to use a frequency criterion to select the “important” keywords in a document.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","However, this method was generally found to lead to poor results, and consequently other methods were explored.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The state-ofthe-art in this area is currently represented by supervised learning methods, where a system is trained to recognize keywords in a text, based on lexical and syntactic features.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","This approach was first suggested in (Turney, 1999), where parametrized heuristic rules are combined with a genetic algorithm into a system for keyphrase extraction - GenEx - that automatically identifies keywords in a document.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","A different learning algorithm was used in (Frank et al., 1999), where a Naive Bayes learning scheme is applied on the document collection, with improved results observed on the same data set as used in (Turney, 1999).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Neither Turney nor Frank report on the recall of their systems, but only on precision: a 29.0% precision is achieved with GenEx (Turney, 1999) for five keyphrases extracted per document, and 18.3% precision achieved with Kea (Frank et al., 1999) for fifteen keyphrases per document.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","More recently, (Hulth, 2003) applies a supervised learning system to keyword extraction from abstracts, using a combination of lexical and syntactic features, proved to improve significantly over previously published results.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","As Hulth suggests, keyword extraction from abstracts is more widely applicable than from full texts, since many documents on the Internet are not available as full-texts, but only as abstracts.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In her work, Hulth experiments with the approach proposed in (Turney, 1999), and a new approach that integrates part of speech information into the learning process, and shows that the accuracy of the system is almost doubled by adding linguistic knowledge to the term representation.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In this section, we report on our experiments in keyword extraction using TextRank, and show that the graph-based ranking model outperforms the best published results in this problem.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Similar to (Hulth, 2003), we are evaluating our algorithm on keyword extraction from abstracts, mainly for the purpose of allowing for a direct comparison with the results she reports with her keyphrase extraction system.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Notice that the size of the text is not a limitation imposed by our system, and similar results are expected with TextRank applied on full-texts.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The expected end result for this application is a set of words or phrases that are representative for a given natural language text.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The units to be ranked are therefore sequences of one or more lexical units extracted from text, and these represent the vertices that are added to the text graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Any relation that can be defined between two lexical units is a potentially useful connection (edge) that can be added between two such vertices.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We are using a co-occurrence relation, controlled by the distance between word occurrences: two vertices are connected if their corresponding lexical units co-occur within a window of maximum words, where can be set anywhere from 2 to 10 words.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Co-occurrence links express relations between syntactic elements, and similar to the semantic links found useful for the task of word sense disambiguation (Mihalcea et al., 2004), they represent cohesion indicators for a given text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The vertices added to the graph can be restricted with syntactic filters, which select only lexical units of a certain part of speech.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","One can for instance consider only nouns and verbs for addition to the graph, and consequently draw potential edges based only on relations that can be established between nouns and verbs.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We experimented with various syntactic filters, including: all open class words, nouns and verbs only, etc., with best results observed for nouns and adjectives only, as detailed in section 3.2.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","First, Compatibility of systems of linear constraints over the set of natural numbers.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","These criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types systems and systems of mixed types. the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","To avoid excessive growth of the graph size by adding all possible combinations of sequences consisting of more than one lexical unit (ngrams), we consider only single words as candidates for addition to the graph, with multi-word keywords being eventually reconstructed in the post-processing phase.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of words.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","After the graph is constructed (undirected unweighted graph), the score associated with each vertex is set to an initial value of 1, and the ranking algorithm described in section 2 is run on the graph for several iterations until it converges – usually for 20-30 iterations, at a threshold of 0.0001.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","While may be set to any fixed value, usually ranging from 5 to 20 keywords (e.g.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","(Turney, 1999) limits the number of keywords extracted with his GenEx system to five), we are using a more flexible approach, which decides the number of keywords based on the size of the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For the data used in our experiments, which consists of relatively short abstracts, is set to a third of the number of vertices in the graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","During post-processing, all lexical units selected as potential keywords by the TextRank algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For instance, in the text Matlab code for plotting ambiguity functions, if both Matlab and code are selected as potential keywords by TextRank, since they are adjacent, they are collapsed into one single keyword Matlab code.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Figure 2 shows a sample graph built for an abstract from our test collection.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","While the size of the abstracts ranges from 50 to 350 words, with an average size of 120 words, we have deliberately selected a very small abstract for the purpose of illustration.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For this example, the lexical units found to have higher “importance” by the TextRank algorithm are (with the TextRank score indicated in parenthesis): numbers (1.46), inequations (1.45), linear (1.29), diophantine (1.28), upper (0.99), bounds (0.99), strict (0.77).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Notice that this ranking is different than the one rendered by simple word frequencies.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For the same text, a frequency approach provides the following top-ranked lexical units: systems (4), types (3), solutions (3), minimal (3), linear (2), inequations (2), algorithms (2).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","All other lexical units have a frequency of 1, and therefore cannot be ranked, but only listed.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The data set used in the experiments is a collection of 500 abstracts from the Inspec database, and the corresponding manually assigned keywords.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","This is the same test data set as used in the keyword extraction experiments reported in (Hulth, 2003).",1
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The Inspec abstracts are from journal papers from Computer Science and Information Technology.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Each abstract comes with two sets of keywords assigned by professional indexers: controlled keywords, restricted to a given thesaurus, and uncontrolled keywords, freely assigned by the indexers.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We follow the evaluation approach from (Hulth, 2003), and use the uncontrolled set of keywords.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In her experiments, Hulth is using a total of 2000 abstracts, divided into 1000 for training, 500 for development, and 500 for test2.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Since our approach is completely unsupervised, no training/development data is required, and we are only using the test docu2Many thanks to Anette Hulth for allowing us to run our algorithm on the data set used in her keyword extraction experiments, and for making available the training/test/development data split. ments for evaluation purposes.",1
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The results are evaluated using precision, recall, and F-measure.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Notice that the maximum recall that can be achieved on this collection is less than 100%, since indexers were not limited to keyword extraction – as our system is – but they were also allowed to perform keyword generation, which eventually results in keywords that do not explicitly appear in the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For comparison purposes, we are using the results of the state-of-the-art keyword extraction system reported in (Hulth, 2003).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Shortly, her system consists of a supervised learning scheme that attempts to learn how to best extract keywords from a document, by looking at a set of four features that are determined for each “candidate” keyword: (1) within-document frequency, (2) collection frequency, (3) relative position of the first occurrence, (4) sequence of part of speech tags.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","These features are extracted from both training and test data for all “candidate” keywords, where a candidate keyword can be: Ngrams (unigrams, bigrams, or trigrams extracted from the abstracts), NP-chunks (noun phrases), patterns (a set of part of speech patterns detected from the keywords attached to the training abstracts).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The learning system is a rule induction system with bagging.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Our system consists of the TextRank approach described in Section 3.1, with a co-occurrence windowsize set to two, three, five, or ten words.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Table 1 lists the results obtained with TextRank, and the best results reported in (Hulth, 2003).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For each method, the table lists the total number of keywords assigned, the mean number of keywords per abstract, the total number of correct keywords, as evaluated against the set of keywords assigned by professional indexers, and the mean number of correct keywords.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The table also lists precision, recall, and F-measure.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Discussion.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","TextRank achieves the highest precision and F-measure across all systems, although the recall is not as high as in supervised methods – possibly due the limitation imposed by our approach on the number of keywords selected, which is not made in the supervised systema.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","A larger window does not seem to help – on the contrary, the larger the window, the lower the precision, probably explained by the fact that a relation between words that are further apart is not strong enough to define a connection in the text graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Experiments were performed with various syntactic filters, including: all open class words, nouns and adjectives, and nouns only, and the best performance was achieved with the filter that selects nouns and adjectives only.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We have also experimented with a setting where no part of speech information was added to the text, and all words - except a predefined list of stopwords - were added to the graph.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The results with this setting were significantly lower than the systems that consider part of speech information, which corroborates with previous observations that linguistic information helps the process of keyword extraction (Hulth, 2003).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Experiments were also performed with directed graphs, where a direction was set following the natural flow of the text, e.g. one candidate keyword “recommends” (and therefore has a directed arc to) the candidate keyword that follows in the text, keeping the restraint imposed by the co-occurrence relation.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We have also tried the reversed direction, where a lexical unit points to a previous token in the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Table 1 includes the results obtained with directed graphs for a co-occurrence window of 2.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Regardless of the direction chosen for the arcs, results obtained with directed graphs are worse than results obtained with undirected graphs, which suggests that despite a natural flow in running text, there is no natural “direction” that can be established between cooccurring words.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Overall, our TextRank system leads to an Fmeasure higher than any of the previously proposed systems.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Notice that TextRank is completely unsupervised, and unlike other supervised systems, it relies exclusively on information drawn from the text itself, which makes it easily portable to other text collections, domains, and languages.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The other TextRank application that we investigate consists of sentence extraction for automatic summarization.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In a way, the problem of sentence extraction can be regarded as similar to keyword extraction, since both applications aim at identifying sequences that are more “representative” for the given text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In keyword extraction, the candidate text units consist of words or phrases, whereas in sentence extraction, we deal with entire sentences.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","TextRank turns out to be well suited for this type of applications, since it allows for a ranking over text units that is recursively computed based on information drawn from the entire text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","To apply TextRank, we first need to build a graph associated with the text, where the graph vertices are representative for the units to be ranked.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For the task of sentence extraction, the goal is to rank entire sentences, and therefore a vertex is added to the graph for each sentence in the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The co-occurrence relation used for keyword extraction cannot be applied here, since the text units in consideration are significantly larger than one or few words, and “co-occurrence” is not a meaningful relation for such large contexts.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Instead, we are defining a different relation, which determines a connection between two sentences if there is a “similarity” relation between them, where “similarity” is measured as a function of their content overlap.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Such a relation between two sentences can be seen as a process of “recommendation”: a sentence that addresses certain concepts in a text, gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts, and therefore a link can be drawn between any two such sentences that share common content.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences, or it can be run through syntactic filters, which only count words of a certain syntactic category, e.g. all open class words, nouns and verbs, etc.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Moreover, to avoid promoting long sentences, we are using a normalization factor, and divide the content overlap TextRank extractive summary Hurricane Gilbert swept toward the Dominican Republic Sunday, and the Civil De− fense alerted its heavily populated south coast to prepare for high winds, heavy rains and high seas.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The National Hurricane Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north, longitude 67.5 west, about 140 miles south of Ponce, Puerto Rico, and 200 miles southeast of Santo Domingo.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The National Weather Service in San Juan, Puerto Rico, said Gilbert was moving westward at 15 mph with a &quot;broad area of cloudiness and heavy weather&quot; rotating around the center of the storm.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Strong winds associated with Gilbert brought coastal flooding, strong southeast winds and up to 12 feet to Puerto Rico’s south coast.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Manual abstract I Hurricane Gilbert is moving toward the Dominican Republic, where the residents of the south coast, especially the Barahona Province, have been alerted to prepare for heavy rains, and high wind and seas.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Tropical storm Gilbert formed in the eastern Carribean and became a hurricane on Saturday night.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",By 2 a.m. Sunday it was about 200 miles southeast of Santo Domingo and moving westward at 15 mph with winds of 75 mph.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Flooding is expected in Puerto Rico and in the Virgin Islands.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The second hurricane of the season, Florence, is now over the southern United States and down− graded to a tropical storm.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Manual abstract II Tropical storm Gilbert in the eastern Carribean strenghtened into a hurricane Saturday night.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The National Hurricane Center in Miami reported its position at 2 a.m. Sunday to be about 140 miles south of Puerto Rico and 200 miles southeast of Santo Domingo.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",It is moving westward at 15 mph with a broad area of cloudiness and heavy weather with sustained winds of 75 mph gusting to 92 mph.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The Dominican Republic’s Civil Defense alerted that country’s heavily populated south coast and the National Weather Service in San Juan, Puerto Rico issued a flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday. of two sentences with the length of each sentence.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Formally, given two sentences and ,with a sentence being represented by the set of words that appear in the sentence: , the similarity of and is defined as: Other sentence similarity measures, such as string kernels, cosine similarity, longest common subsequence, etc. are also possible, and we are currently evaluating their impact on the summarization performance.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The resulting graph is highly connected, with a weight associated with each edge, indicating the strength of the connections established between various sentence pairs in the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The text is therefore represented as a weighted graph, and consequently we are using the weighted graph-based ranking formula introduced in Section 2.2.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","After the ranking algorithm is run on the graph, sentences are sorted in reversed order of their score, and the top ranked sentences are selected for inclusion in the summary.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Figure 3 shows a text sample, and the associated weighted graph constructed for this text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The figure also shows sample weights attached to the edges connected to vertex 94, and the final TextRank score computed for each sentence.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",The sentences with the highest rank are selected for inclusion in the abstract.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For this sample article, the sentences with id-s 9, 15, 16, 18 are extracted, resulting in a summary of about 100 words, which according to automatic evaluation measures, is ranked the second among summaries produced by 15 other systems (see Section 4.2 for evaluation methodology).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","We evaluate the TextRank sentence extraction algorithm on a single-document summarization task, using 567 news articles provided during the Document Understanding Evaluations 2002 (DUC, 2002).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For each article, TextRank generates an 100-words summary — the task undertaken by other systems participating in this single document summarization task.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Two manually produced reference summaries are provided, and used in the evaluation process5.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Fifteen different systems participated in this task, and we compare the performance of TextRank with the top five performing systems, as well as with the baseline proposed by the DUC evaluators – consisting of a 100-word summary constructed by taking the first sentences in each article.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Table 2 shows the results obtained on this data set of 567 news articles, including the results for TextRank (shown in bold), the baseline, and the results of the top five performing systems in the DUC 2002 single document summarization task (DUC, 2002).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","TextRank, top 5 (out of 15) DUC 2002 systems, and baseline.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Evaluation takes into account (a) all words; (b) stemmed words; (c) stemmed words, and no stopwords.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Discussion.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",TextRank succeeds in identifying the most important sentences in a text based on information exclusively drawn from the text itself.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Unlike other supervised systems, which attempt to learn what makes a good summary by training on collections of summaries built for other articles, TextRank is fully unsupervised, and relies only on the given text to derive an extractive summary, which represents a summarization model closer to what humans are doing when producing an abstract for a given document.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",Notice that TextRank goes beyond the sentence “connectivity” in a text.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For instance, sentence 15 in the example provided in Figure 3 would not be identified as “important” based on the number of connections it has with other vertices in the graph, but it is identified as “important” by TextRank (and by humans – see the reference summaries displayed in the same figure).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Another important aspect of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries (headlines consisting of one The evaluation is done using the Ngram(1,1) setting of ROUGE, which was found to have the highest correlation with human judgments, at a confidence level of 95%.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Only the first 100 words in each summary are considered. sentence), or longer more explicative summaries, consisting of more than 100 words.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.",We are also investigating combinations of keyphrase and sentence extraction techniques as a method for building short/long summaries.,0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Finally, another advantage of TextRank over previously proposed methods for building extractive summaries is the fact that it does not require training corpora, which makes it easily adaptable to other languages or domains.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Intuitively, TextRank works well because it does not only rely on the local context of a text unit (vertex), but rather it takes into account information recursively drawn from the entire text (graph).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Through the graphs it builds on texts, TextRank identifies connections between various entities in a text, and implements the concept of recommendation.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","A text unit recommends other related text units, and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","For instance, in the keyphrase extraction application, co-occurring words recommend each other as important, and it is the common context that enables the identification of connections between words in text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In the process of identifying important sentences in a text, a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The sentences that are highly recommended by other sentences in the text are likely to be more informative for the given text, and will be therefore given a higher score.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","An analogy can be also drawn with PageRank’s “random surfer model”, where a user surfs the Web by following links from any given Web page.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In the context of text modeling, TextRank implements what we refer to as “text surfing”, which relates to the concept of text cohesion (Halliday and Hasan, 1976): from a certain concept in a text, we are likely to “follow” links to connected concepts – that is, concepts that have a relation with the current concept (be that a lexical or semantic relation).",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","This also relates to the “knitting” phenomenon (Hobbs, 1974): facts associated with words are shared in different parts of the discourse, and such relationships serve to “knit the discourse together”.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","Through its iterative mechanism, TextRank goes beyond simple graph connectivity, and it is able to score text units based also on the “importance” of other text units they link to.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The text units selected by TextRank for a given application are the ones most recommended by related text units in the text, with preference given to the recommendations made by most influential ones, i.e. the ones that are in turn highly recommended by other related units.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","The underlying hypothesis is that in a cohesive text fragment, related text units tend to form a “Web” of connections that approximates the model humans build about a given context in the process of discourse understanding.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In this paper, we introduced TextRank – a graphbased ranking model for text processing, and show how it can be successfully used for natural language applications.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","In particular, we proposed and evaluated two innovative unsupervised approaches for keyword and sentence extraction, and showed that the accuracy achieved by TextRank in these applications is competitive with that of previously proposed state-of-the-art algorithms.",0
"In (Mihalcea and Tarau, 2004), due to the unsupervised method, only the test set was used for comparing the performance of Text Rank and Hulth' s method.","An important aspect of TextRank is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres, or languages.",0
