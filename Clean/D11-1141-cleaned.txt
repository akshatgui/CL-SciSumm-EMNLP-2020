We employ the method of Ritter et al (2011) to tokenise messages, and use token unigrams as features, including any hash tags, but ignoring twitter mentions, URLs and purely numeric tokens. $$$$$ For instance, tweets often start with a verb (where the subject ‘I’ is implied), as in

 $$$$$ On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.
 $$$$$ Us1532

Our second component - chunker - is taken from (Ritter et al, 2011), which also comes with a model trained on Twitter data and shown to perform better on noisy data such as user comments. $$$$$ The performance of off-the-shelf news-trained POS taggers also suffers on Twitter data.
Our second component - chunker - is taken from (Ritter et al, 2011), which also comes with a model trained on Twitter data and shown to perform better on noisy data such as user comments. $$$$$ We now discuss our approach to named entity recognition on Twitter data.

The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker. $$$$$ LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.
The chunker from (Ritter et al., 2011) relies on its own POS tagger, however, in our structural representations we favor the POS tags from the CMU Twitter tagger and take only the chunk tags from the chunker. $$$$$ T-POS and T-CHUNK in segmenting Named Entities.

For example, Ritter et al (2011) develop a system that exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. $$$$$ To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al., 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity’s context across its mentions.
For example, Ritter et al (2011) develop a system that exploits a CRF model to segment named entities and then uses a distantly supervised approach based on LabeledLDA to classify named entities. $$$$$ For example, we are able to use discriminative methods for named entity segmentation and distantly supervised approaches for classification.

We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets. $$$$$ All tools in §2 are used as features for named entity segmentation in §3.1.
We have also used a named entity tagger trained specifically on the Twitter data (Ritter et al, 2011) to directly extract named entities from tweets. $$$$$ We now discuss our approach to named entity recognition on Twitter data.

Firstly, a named entity recognizer (Ritter et al, 2011) is employed to identify named entities. $$$$$ Compared with the state-of-the-art newstrained Stanford Named Entity Recognizer (Finkel et al., 2005), T-SEG obtains a 52% increase in F1 score.
Firstly, a named entity recognizer (Ritter et al, 2011) is employed to identify named entities. $$$$$ In addition we take a dis- identify the types of the entities they contain.

The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al, 2011), retrieval (Subramaniam et al, 2009), summarization (Liu et al, 2011a), sentiment analysis (Celikyilmaz et al, 2010), etc. $$$$$ Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.
The social text serves as a very valuable information source for many NLP applications, such as the information extraction (Ritter et al, 2011), retrieval (Subramaniam et al, 2009), summarization (Liu et al, 2011a), sentiment analysis (Celikyilmaz et al, 2010), etc. $$$$$ In addition Finin et. al.

We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al, 2011). $$$$$ Generate the word we,i from Mult(o,;e,i).
We generate part-of-speech information over the original raw text using a Twitter part-of-speech tagger (Ritter et al, 2011). $$$$$ LabeledLDA) for classifying developed in parallel, Gimpell et al. (2011) build a named entities has a similar effect, in that informaPOS tagger for tweets using 20 coarse-grained tags. tion about an entity’s distribution of possible types Benson et. al.

Using the UW Twitter NLP tools (Ritter et al, 2011). $$$$$ NLP tools are available at

To study the diversity of named entities (NEs) in retweets, we used UW Twitter NLP Tools (Ritter et al., 2011) to extract NEs from RT-data. $$$$$ NLP tools are available at

We then standardized variants (i.e. "fb" as a variant of "Facebook"), and manually categorized them against the 10-class schema defined by Ritter et al. (2011). $$$$$ Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.
We then standardized variants (i.e. "fb" as a variant of "Facebook"), and manually categorized them against the 10-class schema defined by Ritter et al. (2011). $$$$$ In addition Finin et. al.

Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets $$$$$ Following Collins and Singer (1999), Downey et al. (2007) and Elsner et al.
Leaving out the dedicated test set to avoid in-sample bias, we evaluate our models across three data sets $$$$$ In addition Finin et. al.

 $$$$$ On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.
 $$$$$ Us1532

 $$$$$ On a 4-fold cross validation over 800 tweets, T-POS outperforms the Stanford tagger, obtaining a 26% reduction in error.
 $$$$$ Us1532
