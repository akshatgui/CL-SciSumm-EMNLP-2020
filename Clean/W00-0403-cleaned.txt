Obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (Radev et al, 2000). $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.
Obviously, intrinsic evaluation is more reliable, but it remains an extremely laborious process, where inter-judge disagreement is still an issue, see (Radev et al, 2000). $$$$$ We are in the process of running experiments with other SCORE formulas.

The first of these, relative utility (RU) (Radev et al,2000) allows model summaries to consist of sentences with variable ranking. $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.
The first of these, relative utility (RU) (Radev et al,2000) allows model summaries to consist of sentences with variable ranking. $$$$$ MEAD decides which sentences to include in the extract by ranking them according to a set of parameters.

For details see (Radev et al, 2000). $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
For details see (Radev et al, 2000). $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.

Three judges have assessed the sentences in each cluster and have provided a score on a scale from 0 to 10 (i.e. utility judgement), expressing how important the sentence is for the topic of the cluster (Radev et al, 2000). $$$$$ Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (from 0 to 10) of a ' particular sentence to the general topic of the entire cluster (for a dis cussion of what is a topic, see [Allan et al. 1998]).
Three judges have assessed the sentences in each cluster and have provided a score on a scale from 0 to 10 (i.e. utility judgement), expressing how important the sentence is for the topic of the cluster (Radev et al, 2000). $$$$$ First, six judges were each given six clusters and asked to ascribe an importance score from 0 to 10 to each sentence within a particular cluster.

However, this experiment differs from prior work in that we use judges to determine the relevance of sentences to sub-events rather than to evaluate summaries (Radev et al, 2000). $$$$$ MEAD is significantly different from previous work on multi-document summarization [Radev & McKeown, 1998; Carbonell and Goldstein, 1998; Mani and Bloedorn, 1999; McKeown et al., 1999], which use techniques such as graph matching, maximal marginal relevance, or language generation.
However, this experiment differs from prior work in that we use judges to determine the relevance of sentences to sub-events rather than to evaluate summaries (Radev et al, 2000). $$$$$ Overall, out of 80 In conclusion, we found very high interjudge agreement in the first experiment and moderately low agreement in the second experiment.

 $$$$$ 2 in the second article, while sentence 9 from the former article is later repeated in sentences 3 and 4 of the latter article.
 $$$$$ We would also like to thank Carl Sable, Min-Yen Kan, Dave Evans, Adam Budzikowski, and Veronika Horvath for their help with the evaluation.

Following (Radev et al, 2000), we used relative utility as our metric. $$$$$ Their metric is used as an enhancement to a query-based summary whereas CSIS is designed for query-independent (a.k.a., generic) summaries.
Following (Radev et al, 2000), we used relative utility as our metric. $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.

MEAD (Radev et al, 2000) $$$$$ Centroid-Based Summarization Of Multiple Documents

As in (Radev et al, 2000), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length. $$$$$ MEAD decides which sentences to include in the extract by ranking them according to a set of parameters.
As in (Radev et al, 2000), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length. $$$$$ Sentences are laid in the same order as they appear in the original documents with documents ordered chronologically.

MEAD (Radev et al, 2000) $$$$$ Centroid-Based Summarization Of Multiple Documents

3.1.3 Relative Utility Relative Utility (RU) (Radev et al, 2000) is tested on a large corpus for the first time in this project. $$$$$ A large body of research in TDT has been created over the past two years [Allan et al., 98].
3.1.3 Relative Utility Relative Utility (RU) (Radev et al, 2000) is tested on a large corpus for the first time in this project. $$$$$ Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (from 0 to 10) of a ' particular sentence to the general topic of the entire cluster (for a dis cussion of what is a topic, see [Allan et al. 1998]).

This is in contrast with a method proposed by Radev (Radev et al, 2000), where the centroid of a cluster is selected as the representative one. $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
This is in contrast with a method proposed by Radev (Radev et al, 2000), where the centroid of a cluster is selected as the representative one. $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.

A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
A number of techniques for choosing the right sentences to extract have been proposed in the literature, ranging from word counts (Luhn, 1958), key phrases (Edmundson, 1969), naive Bayesian classification (Kupiec et al, 1995), lexical chains (Barzilay and Elhadad, 1997), topic signatures (Hovy and Lin, 1999) and cluster centroids (Radev et al, 2000). $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.

We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries. $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
We use the Relative Utility (RU) method (Radev et al, 2000) to compare our various summaries. $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.

These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al, 2000) .Both summarizers rely on information extraction from the corpus. $$$$$ In the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm.
These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al, 2000) .Both summarizers rely on information extraction from the corpus. $$$$$ We used a new utility-based technique, CBSU, for the evaluation of MEAD and of summarizers in general.

The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). $$$$$ We now describe the corpus used for the evaluation of MEAD, and later in this section we present MEAD's algorithm.

First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events (Radev et al, 2000). $$$$$ Event clusters range from2 to 10 documents from which MEAD produces summaries in the form of sentence extracts.
First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events (Radev et al, 2000). $$$$$ Sentences are laid in the same order as they appear in the original documents with documents ordered chronologically.

Finally, MEAD is a widely used MDS and evaluation platform (Radev et al, 2000). $$$$$ There is not yet a widely accepted evaluation scheme.
Finally, MEAD is a widely used MDS and evaluation platform (Radev et al, 2000). $$$$$ We now describe the corpus used for the evaluation of MEAD, and later in this section we present MEAD's algorithm.

We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 (Radev et al, 2000). $$$$$ We have implemented CBS in a system, named MEAD.
We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 (Radev et al, 2000). $$$$$ We presented a new multi-document summarizer, MEAD.

Radev et al (2000) use it in their MDS system MEAD. $$$$$ We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.
Radev et al (2000) use it in their MDS system MEAD. $$$$$ Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.
