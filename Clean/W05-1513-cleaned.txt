This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. $$$$$ Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003).
This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. $$$$$ Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.

Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003).
Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.

Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Training the parser is accomplished by training its classifier.
Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.

We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ As mentioned before, our parser shares similarities with the dependency parsers of Yamada and Matsumoto (2003) and Nivre and Scholz (2004) in that it uses a classifier to guide the parsing process in deterministic fashion.

A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string.

Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. $$$$$ While in many natural language processing tasks different classifiers perform at similar levels of accuracy, we have observed a dramatic difference between using support vector machines and a memory-based learner.
Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. $$$$$ Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags.

Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ This set of features and corresponding actions is then used to train a classifier, resulting in a complete parser.
Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ Future work includes the investigation of the effects of individual features, the use of additional classification features, and the use of different classifiers.

Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ The total number of training instances was about 1.5 million.
Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.

For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ Training the parser is accomplished by training its classifier.
For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ This greatly reduced the time required to train the SVMs, but even with the splitting of the training set, total training time was about 62 hours.

More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ Using decision trees and fewer features, Kalt’s parser has significantly faster training and parsing times, but its accuracy is much lower than that of our parser.
More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ The classifier in the SVM-based parser (denoted by SVMpar) uses the polynomial kernel with degree 2, following the work of Yamada and Matsumoto (2003) on SVM-based deterministic dependency parsing, and a one-against-all scheme for multi-class classification.

Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).

One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ Our parser only pursues one path per sentence, but it is very fast and of comparable accuracy (see section 4).
One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ Additionally, it is considerably faster than lexicalized PCFG-based parsers, and offers a good alternative for when fast parsing is needed.

 $$$$$ This set of features and corresponding actions is then used to train a classifier, resulting in a complete parser.
 $$$$$ Additionally, we plan to investigate the use of the beam strategy of Ratnaparkhi (1997) to pursue multiple parses while keeping the run-time linear.

In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002).
In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ An example of transformation/detransformation is shown in figure 1.

Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).

This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ The transformed (or “binarized”) trees may then be used for training.

Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. $$$$$ When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).
Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. $$$$$ We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.

In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.
In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ The parser in Kalt (2004) uses a similar algorithm to the one described here, but the classification task is framed differently.

A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ In order to use trees with arbitrary branching for training, or generating them with the parser, we employ an instance of the transformation/detransformation process described in (Johnson, 1998).
A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ This involves the removal of non-terminals introduced in the transformation process, producing trees with arbitrary branching.

Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ A Classifier-Based Parser With Linear Run-Time Complexity
Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ Thus, the parser runs in linear time, assuming that classifying a parser action is done in constant time.
