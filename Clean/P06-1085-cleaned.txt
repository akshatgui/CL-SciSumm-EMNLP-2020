Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ Hierarchical Dirichlet processes.

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

USM $$$$$ The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).
USM $$$$$ 2006.

We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ In our experiments, we used the same corpus that NGS and MBDP were tested on.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Contextual Dependencies In Unsupervised Word Segmentation
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ 2001.

We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ 2006.

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.
We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ This richget-richer process creates a power-law distribution on word frequencies (Goldwater et al., 2006), the same sort of distribution found empirically in natural language.

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.
We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ The only way the model can capture these dependencies is by assuming that these collocations are in fact words themselves.

 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.
 $$$$$ Computational Linguistics, 27(3)

Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).

Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).

Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Hierarchical Dirichlet processes.

While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ It is also one of the key problems that human language learners must solve as they are learning language.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ Utterance boundaries are given in the input to the system; other word boundaries are not.

It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ Contextual Dependencies In Unsupervised Word Segmentation
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ Specifically, this paper demonstrates the importance of contextual dependencies for word segmentation by comparing two probabilistic models that differ only in that the first assumes that the probability of a word is independent of its local context, while the second incorporates bigram dependencies between adjacent words.
