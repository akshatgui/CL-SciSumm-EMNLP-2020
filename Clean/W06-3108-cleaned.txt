Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). $$$$$ This evaluation consists of two parts

Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models

Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. $$$$$ Discriminative Reordering Models For Statistical Machine Translation
Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.

Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ We adopt the idea of predicting the orientation, but we propose to use a maximum-entropy based model.
Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ The idea of predicting the orientation is adopted from (Tillmann and Zhang, 2005) and (Koehn et al., 2005).

Figure 2 $$$$$ In (Koehn et al., 2005) several variants of the orientation model have been tried.
Figure 2 $$$$$ This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).

Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ One reason is that most phrase-based systems use a very simple reordering model.
Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ The word-class based features are not used for the translation experiments.

In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models

Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. $$$$$ Usually, the costs for phrase movements are linear in the distance, e.g. see (Och et al., 1999; Koehn, 2004; Zens et al., 2005).
Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. $$$$$ This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).

In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models

 $$$$$ The latter two models are used for both directions

The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ Then, we will present the discriminative reordering model in Section 4.
The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models

Following models were applied $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models

Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). $$$$$ Then, we will present the discriminative reordering model in Section 4.
Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). $$$$$ In this section, we will describe the proposed discriminative reordering model.

The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ The models are trained using the maximum entropy principle.
The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ This model is trained on the word aligned bilingual corpus using the maximum entropy principle.

Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. $$$$$ We use the source position j which is aligned to the last word of the target phrase in target position i.
Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. $$$$$ The illustration in Figure 1 contains such an example.

j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. $$$$$ Now, the model has to predict if the start position of the next phrase jâ€² is to the left or to the right of the current phrase.
j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. $$$$$ We use the source position j which is aligned to the last word of the target phrase in target position i.

Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. $$$$$ Adding the features based on word classes, the classification error rate can be further improved to 2.1%.
Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. $$$$$ We have shown that the model is able to predict the orientation very well, e.g. for Arabic-English the classification error rate is only 2.1%.

One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ Discriminative Reordering Models For Statistical Machine Translation
One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.

Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. $$$$$ We use the Arabic-English, the Chinese-English and the Japanese-English data.
Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. $$$$$ Again, using more features always helps to improve the performance.

To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. $$$$$ We use a state-of-the-art phrase-based translation system (Zens and Ney, 2004; Zens et al., 2005) including the following models
