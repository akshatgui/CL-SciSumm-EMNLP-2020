This alternate decoding path model was developed by Birch et al (2007). $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
This alternate decoding path model was developed by Birch et al (2007). $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

However, Birch et al (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.
However, Birch et al (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. $$$$$ This approach was used as the second way in which 14 we combined our models.

Birch et al (2007) then investigated source-side CCG super tag features, but did not show an improvement for Dutch-English. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.
Birch et al (2007) then investigated source-side CCG super tag features, but did not show an improvement for Dutch-English. $$$$$ 4.7 CCG Supertags on Source.

Lastly, Koehn and Schroeder (2007) reported improvements from using multiple decoding paths (Birch et al, 2007) to pass both tables to the Moses SMT decoder (Koehn et al, 2003), instead of directly combining the phrase tables to perform domain adaptation. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
Lastly, Koehn and Schroeder (2007) reported improvements from using multiple decoding paths (Birch et al, 2007) to pass both tables to the Moses SMT decoder (Koehn et al, 2003), instead of directly combining the phrase tables to perform domain adaptation. $$$$$ Inspired by work on factored language models, Koehn et al (2006) extend phrase-based models to incorporate multiple levels of linguistic knowledgeas factors.

We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al, 2007). $$$$$ Inspired by work on factored language models, Koehn et al (2006) extend phrase-based models to incorporate multiple levels of linguistic knowledgeas factors.
We have also shown in passing that the linear interpolation of translation models may work less well for translation model adaptation than the multiple paths decoding technique of (Birch et al, 2007). $$$$$ This is the first work suggesting the application of LOPs to decoding in ma chine translation.

Finally, Birch et al (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. $$$$$ CCG contains most of the structure ofthe grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al, 2006).
Finally, Birch et al (2007) exploit factored phrase-based translation models to associate each word with a supertag, which contains most of the information needed to build a full parse. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).

Birch et al (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al, 2007). $$$$$ CCG contains most of the structure ofthe grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al, 2006).
Birch et al (2007) also reported a significant improvement for Dutch-English translation by applying CCG supertags at a word level to a factorized SMT system (Koehn et al, 2007). $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

We built two separate phrase tables for the two bi-texts, and we used them in the alter native decoding path model of Birch et al (2007). $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
We built two separate phrase tables for the two bi-texts, and we used them in the alter native decoding path model of Birch et al (2007). $$$$$ This is similar to syntax-directed translation originally proposed for compiling (Aho and Ullman, 1969), and also used in machine translation (Quirk et al., 2005; Huang et al, 2006).

Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side ,translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
Birch et al (2007) and Hassan et al (2007) have shown the effectiveness of adding supertags on the target side, and Avramidis and Koehn (2008) have focused on the source side ,translating a morphologically-poor language (English) to a morphologically-rich language (Greek). $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the super tags on the target language side, English. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the super tags on the target language side, English. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

Probabilistic models for using only source tags were investigated by Birch et al (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) super tags as factors on the input words, but in this case English was the target language. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.
Probabilistic models for using only source tags were investigated by Birch et al (2007), who attached syntax hints in factored SMT models by having Combinatorial Categorial Grammar (CCG) super tags as factors on the input words, but in this case English was the target language. $$$$$ Factors are (w)ords, (p)os tags, (c)cg su pertags on the source s or the target tTable 1 shows that sequence models over CCG su pertags in the target (model sw, twc) improves over the baseline (model sw, tw) which has no supertags.

In order to reduce these problems, decoding needed to consider alternative paths to translation tables trained with less or no factors (as Birch et al (2007) suggested), so as to cover instances where a word appears with a factor which it has not been trained with. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
In order to reduce these problems, decoding needed to consider alternative paths to translation tables trained with less or no factors (as Birch et al (2007) suggested), so as to cover instances where a word appears with a factor which it has not been trained with. $$$$$ The language models and thesequence models were trained on the Europarl train ing data.

Supertagging encapsulates more contextual information than POS tags and Birch et al (2007) report improvements when comparing a super tag language model to a baseline using a word language model only. $$$$$ Sequence models over POS tags or supertags are smaller than language modelsbecause they have restricted lexicons.
Supertagging encapsulates more contextual information than POS tags and Birch et al (2007) report improvements when comparing a super tag language model to a baseline using a word language model only. $$$$$ 4.5 Language Model vs. Supertags.

Hassan et al (2007) and Birch et al (2007) use super tag n-gram LMs. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
Hassan et al (2007) and Birch et al (2007) use super tag n-gram LMs. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ This is a simple way of including syn tactic information in a phrase-based model, and has also been suggested by Hassan et al (2007).
Our approach is slightly different from (Birch et al, 2007) and (Hassan et al, 2007), who mainly used the supertags on the target language side, English. $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.

Factored translation models have also been used for the integration of CCG super tags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ CCG contains most of the structure ofthe grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al, 2006).
Factored translation models have also been used for the integration of CCG super tags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ For both Arabic-English (Hassan et al, 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.
