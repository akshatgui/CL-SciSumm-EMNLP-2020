Our approach to QC follows that of (Li and Roth, 2002). $$$$$ The paper is organized as follows: Sec.
Our approach to QC follows that of (Li and Roth, 2002). $$$$$ 3 discusses the learning issues involved in QC and presents ourlearning approach; Sec.

We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ The first classifies questions into coarse classes (Coarse Classifier) and the second into fineclasses (Fine Classifier).
We will use the TREC dataset provided by Li and Roth (2002), which assigns 6000 questions with both a coarse and a fine-grained label. $$$$$ Then each coarse class label in C 1 is expanded to a fixed set of fine classesdetermined by the class hierarchy.

This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. $$$$$ The intension is that this 1We do not address questions like ?Do you have a light??, which calls for an action, but rather only factual Wh-questions.
This scheme is more suitable here than other common answer-typing schemata such as the one in Li and Roth (2002), which tend to focus on questions asking for factual knowledge. $$$$$ Our feature sensors fail to determine that the focus of the question is ?speed?.

This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). $$$$$ of the sought after answer.
This is important because while large sets of existing questions can be obtained (Li and Roth, 2002), there are many fewer questions with available answers. Our experiments demonstrate that how-question specific unit lists consistently achieve higher answer identification performance than fixed-type, general purpose answer typing (which propose all numerical entities as answer candidates). $$$$$ Moreover, determin ing the specific semantic type of the answer couldalso be beneficial in locating the answer and veri fying it.

For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. $$$$$ To avoid this problem,we allow our classifiers to assign multiple class la bels for a single question.
For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. $$$$$ A question can be mapped to one of 50 pos sible classes (We call the set of all possible class labels for a given question a confusion set (Golding and Roth, 1999)).

(Li and Roth, 2002) propose a system based on SNoW. $$$$$ Pos tags are extracted using a SNoW-based pos tagger (Even-Zohar and Roth, 2001).
(Li and Roth, 2002) propose a system based on SNoW. $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).

The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. $$$$$ All 500 TREC 10 questions are used as the test set.
The same dataset has been used in other investigations, such as in (Li and Roth, 2002). The distribution of these 5500 training questions, with respect to its interrogative pronoun or the initial word is showed in Table 1. $$$$$ Training is done on 5,500 questions.

(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. $$$$$ Our original hope was that the hierarchical classifier would have a better performance, given that its fine classifier only needs to deal with a smaller confusion set.
(Li and Roth, 2002) obtain a better performance for English, around a 92.5% in terms of accuracy. $$$$$ As the results show, there is no perfor mance advantage for using a level of coarse classes, and the semantically appealing coarse classes do not contribute to better performance.

It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. $$$$$ This method is better than only allowing one label because we can apply all the classes in the later precessing steps without any loss.
It could be also interested to test the combination between a better QC system, the current one by Li and Roth's for instance (Li and Roth, 2002), and our machine translation method. $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).

Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. $$$$$ This paper presents a machine learning approach toquestion classification.
Li and Roth (2002) have developed a machine learning approach which uses the SNoW learning architecture. $$$$$ This paper presents a machine learning approach to question classification.

Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. $$$$$ 3.2 Feature Space.
Compared to the over feature size of 200000 in Li and Roth (2002), our feature space is much more compact, yet turned out to be more informative as suggested by the experiments. $$$$$ Overall, there are about 200; 000 features in the feature space of RelWorddue to the generation of complex features over sim ple feature types.

With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. $$$$$ 3 Learning a Question Classifier.
With the increasing popularity of statistical NLP, Li and Roth (2002), Hacioglu and Ward (2003) and Zhang and Lee (2003) used supervised learning for question classification on a data set from UIUC that is now standard1. $$$$$ 4.1 Data.

Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). $$$$$ Open-domain question answering (Lehnert, 1986; Harabagiu et al, 2001; Light et al, 2001) and storycomprehension (Hirschman et al, 1999) have become important directions in natural language pro cessing.
Li and Roth (2002) used a Sparse Network of Winnows (SNoW) (Khardon et al, 1999). $$$$$ How ever, in the context of factual questions that are of interest to us here, conceptual categories do notseem to be helpful; instead, our goal is to se mantically classify questions, as in earlier work on TREC (Singhal et al, 2000; Hovy et al, 2001; Harabagiu et al, 2001; Ittycheriah et al, 2001).

Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. $$$$$ One difficulty in the question classification task is that there is no completely clear boundary between classes.
Our findings corroborate Li and Roth (2002), who report little benefit from adding head chunk features for the fine classification task. $$$$$ Among the 6 primitive feature types, pos tags, chunks and head chunks are syntactic features while named entities and semantically related words are semantic features.

in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. $$$$$ of the sought after answer.
in (Li and Roth, 2002) to our basic QA system, YourQA (Quarteroni and Manandhar, 2008) and by gathering the top 20 answer paragraphs. $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).

 $$$$$ Therefore, the classification of a specific question can be quite ambiguous.
 $$$$$ In future work we plan to investigate further the application of deeper semantic analysis (including better named entity and semantic categorization) to feature extraction, automate the generation of thesemantic features and develop a better understand ing to some of the learning issues involved in thedifference between a flat and a hierarchical classi fier.

Answer types are determined using classification rules similar to Li and Roth (2002). $$$$$ These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.
Answer types are determined using classification rules similar to Li and Roth (2002). $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).

The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ The motiva tion behind adding a level of coarse classes is that of compatibility with previous work?s definitions, andcomprehensibility.
The classification scheme we propose is based on one dynamic 1 and one static layer, contrasting with previous work that uses static taxonomies (Li and Roth, 2002). $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).

1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ Data are collected from four sources: 4,500 English questions published by USC (Hovy et al, 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set3.These questions were manually labeled accord ing to our question hierarchy.
1500 of those questions come from the Li and Roth corpus (Li and Roth, 2002), 500 questions were taken from the TREC-10 questions and 100 questions were asked over the Italian Opera topic map. $$$$$ All 500 TREC 10 questions are used as the test set.

We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ What do bats eat?.
We followed Li and Roth (Li and Roth, 2002) to implement the features for the EAT classifier. $$$$$ Chunks are extracted using a previously learned classifier (Punyakanok and Roth, 2001; Li and Roth, 2001).
