The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known $$$$$ Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.
The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known $$$$$ To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t

This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. $$$$$ Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).
This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. $$$$$ Table 2 shows results for the various models and a random baseline (averaged by the various models on different sized corpora.

Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). $$$$$ Unsupervised learning of linguistic structure is a difficult problem.
Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). $$$$$ Unsupervised learning of linguistic structure is a difficult problem.

Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.
Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ In the experiments reported in the following section, we used two different versions of our model.

In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM (Smith and Eisner, 2005).
In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag.

Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). $$$$$ We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels.
Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). $$$$$ We therefore introduce another evaluation measure for these experiments, a distance metric on clusterings known as variation of information (Meilˇa, 2002).

Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.
Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ We relax the latter assumption in same tag.

Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. $$$$$ However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estiwhere nk is the number of times k occurred in x−i.
Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. $$$$$ Results from our model for a range of hyperparameters are presented in Table 1.

We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. $$$$$ Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.
We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. $$$$$ For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).

Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997).
Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ In many linguistic models, including HMMs, the distributions over variables are multinomial.

The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). $$$$$ As φ approaches 0, even a single between the variables in the model.
The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). $$$$$ We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.

In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ For a sequence of draws x = ... , xn) from a multinomial distribution B with observed counts ... , nK, a symmetric prior over B yields the MAP estimate Bk = When Q 1, standard MLE techniques such as EM can be used to find the MAP estimate simply by adding of size Q 1 to each of the expected counts nk at each iteration.
In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.

 $$$$$ Consequently, P(t

We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ A fully Bayesian approach to unsupervised part-of-speech tagging
We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.

First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.
First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.

We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. $$$$$ We also evaluate using tag accuracy when possible.
We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. $$$$$ The property of exchangeability is words for tag t.) The dictionary was constructed by crucial to the inference algorithm we describe next. listing, for each word, all tags found for that word in 747 the entire WSJ treebank.

In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. $$$$$ Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).
In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. $$$$$ In the following section, we discuss the motivation for a Bayesian approach and present our model and search procedure.

We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. $$$$$ To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t

In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. $$$$$ In the experiments reported in the following section, we used two different versions of our model.
In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. $$$$$ For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.

Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ By integrating over B, Equation 3 takes into account the consequences of possible variation in B.
Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ The sampling distribution for ti nents, and each of the output distributions ω(t) has is given in Figure 2.
