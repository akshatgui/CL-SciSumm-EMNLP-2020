We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ In this sense it is related to the algorithmof Liang et al (2006).
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ It is sim ple for us to map from lambda-calculus expressions to attribute-value entries of this form; for example, the expression to(x,Boston) would be mapped to destination = Boston.

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ 2.4 Zettlemoyer and Collins 2005.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ Fortest, we used the ATIS NOV93 test set which con tains 448 examples.

In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ 2.4 Zettlemoyer and Collins 2005.
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ (in step 2 of the algorithm), their initial weight is set to some value ?.

This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ However, a grammar based on a formalism such as CCG can be somewhat rigid, and this can cause problems when a system is faced with spontaneous, unedited natural language input, as is commonly seen in natural language interface applications.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ 2.3 Log-Linear CCGs.

 $$$$$ These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.
 $$$$$ Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ 2.4 Zettlemoyer and Collins 2005.

We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ Using these rules, we can parse the 2For example, many question sentences have semantics of type ?e, t?, as in ?x.flight(x) ? to(x, boston).
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ 2.4 Zettlemoyer and Collins 2005.

For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ 2.4 Zettlemoyer and Collins 2005.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ the from predicate signaling that the logical-form describes flights with more than one origin city.

Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ Online Learning of Relaxed CCG Grammars for Parsing to Logical Form
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ 2.4 Zettlemoyer and Collins 2005.

Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ 2.4 Zettlemoyer and Collins 2005.

We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ 2.4 Zettlemoyer and Collins 2005.
We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ The first column lists the triggers that identify some sub-structure within a logical form.

See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ 2.4 Zettlemoyer and Collins 2005.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ Zettlemoyer and Collins (2005) present more complete details.

 $$$$$ These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.
 $$$$$ Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.

The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ 2.4 Zettlemoyer and Collins 2005.
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ In section 4.2 we describe a new, online algorithm that uses GENLEX.

The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned.of techniques have been considered including ap proaches based on machine translation techniques (Papineni et al, 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques(Miller et al, 1996; Ge and Mooney, 2006), tech niques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al, 2005), andideas from string kernels and support vector ma chines (Kate and Mooney, 2006; Nguyen et al, 2006).

We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ In addition to the application and compositionrules, we will also make use of type raising and co ordination combinators.
We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ 3.1 Application and Composition Rules.

We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ Initialization: Set parameters w to initial values described in section 6.2.

 $$$$$ These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.
 $$$$$ Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ 2.4 Zettlemoyer and Collins 2005.

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ 2.4 Zettlemoyer and Collins 2005.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ Fortest, we used the ATIS NOV93 test set which con tains 448 examples.
