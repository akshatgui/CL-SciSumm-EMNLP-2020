Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. $$$$$ LexPageRank

 $$$$$ In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.
 $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.

 $$$$$ In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.
 $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.

The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ In this model, a sentence connectivity matrix is constructed based on cosine similarity.

Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ We have presented a novel approach to define sentence centrality based on graph-based prestige scoring of sentences.

 $$$$$ In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.
 $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.

Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. $$$$$ First is how to define similarity between two sentences.
Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. $$$$$ There are 8 different human judges for DUC 2004 Task 2, and 4 for DUC 2004 Task 4.

We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). $$$$$ In this model, a sentence connectivity matrix is constructed based on cosine similarity.
We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). $$$$$ The graph-based centrality approach we have introduced has several advantages over Centroid.

Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. $$$$$ This method can be directly applied to the cosine similarity graph to find the most prestigious sentences in a document.
Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. $$$$$ The degree of a node in the cosine similarity graph is an indication of how much common information the sentence has with other sentences.

In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). $$$$$ LexPageRank

Lex PageRank (Erkan and Radev, 2004) is one of such methods. $$$$$ We call this new measure of sentence similarity lexical PageRank, or LexPageRank.
Lex PageRank (Erkan and Radev, 2004) is one of such methods. $$$$$ We implemented the Degree and LexPageRank methods, and integrated into the MEAD system as new features.

TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ We use PageRank to weight each vote so that a vote that comes from a more prestigious sentence has a greater value in the centrality of a sentence.

Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ During the first step, the feature extractor, each sentence in the input document (or cluster of documents) is converted into a feature vector using the user-defined features.
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ This example indicates the three default MEAD features (Centroid, Position, LengthCutoff), and our new LexPageRank feature used in our experiments.

Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ We provide an evaluation of our method on DUC 2004 data.
Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ PageRank is a method proposed for assigning a prestige score to each page in the Web independent of a specific query.

LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ Then we introduce two new measures for centrality, Degree and LexPageRank, inspired from the “prestige” concept in social networks and based on our new approach.

The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ Centroid-based summarization has given promising results in the past (Radev et al., 2001).
The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ One of the most successful applications of prestige is PageRank (Page et al., 1998), the underlying technology behind the Google search engine.

Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ In centroid-based summarization (Radev et al., 2000), the sentences that contain more words from the centroid of the cluster are considered as central.
Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ Centroid-based summarization has given promising results in the past (Radev et al., 2001).

The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ In this model, a sentence connectivity matrix is constructed based on cosine similarity.

 $$$$$ In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.
 $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.

 $$$$$ In a cluster of related documents, many of the sentences are expected to be somewhat similar to each other since they are all about the same topic.
 $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.
