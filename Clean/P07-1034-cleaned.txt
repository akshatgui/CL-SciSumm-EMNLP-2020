For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ Following (Blitzer et al., 2006), we call the first the source domain, and the second the target domain.
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ Recently there have been some studies addressing domain adaptation from different perspectives (Roark and Bacchiani, 2003; Chelba and Acero, 2004; Florian et al., 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2006).

In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ However, based on our theoretical analysis, we can expect the labeled target instances to be more representative of the target domain than the source instances.
In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ Indeed, all the above methods do not make use of the unlabeled instances in the target domain.

 $$$$$ Our goal is to obtain a good estimate of θt that is optimized according to the target domain distribution pt(x, y).
 $$$$$ We thank the anonymous reviewers for their valuable comments.

We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ We call this second method the balanced bootstrapping method.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ As we can see, while bootstrapping can generally improve the performance over the baseline where no unlabeled data is used, the balanced bootstrapping method performed slightly better than the standard bootstrapping method.

Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ We now formally define our instance weighting framework.

Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ In Section 3, we then propose a general instance weighting framework for domain adaptation.

 $$$$$ Our goal is to obtain a good estimate of θt that is optimized according to the target domain distribution pt(x, y).
 $$$$$ We thank the anonymous reviewers for their valuable comments.

Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ We call this second method the balanced bootstrapping method.
Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ As we can see, while bootstrapping can generally improve the performance over the baseline where no unlabeled data is used, the balanced bootstrapping method performed slightly better than the standard bootstrapping method.

It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ We call this second method the balanced bootstrapping method.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ As we can see, while bootstrapping can generally improve the performance over the baseline where no unlabeled data is used, the balanced bootstrapping method performed slightly better than the standard bootstrapping method.

This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ Instance Weighting for Domain Adaptation in NLP
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ A possible reason for this is that the set of labeled target instances we use is a biased sample from the target domain, and therefore the model trained on these instances is not always a good predictor of “misleading” source instances.

(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ In the first setting, we assume there are a small number of labeled target instances available.
(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ In the first set of experiments, we gradually remove “misleading” labeled instances from the source domain, using the small number of labeled target instances we have.

Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ A detailed discussion on related work is given in Section 5.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ Therefore, in this case, we still need domain adaptation, which we refer to as instance adaptation.

This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ Based on this analysis, we propose a general instance weighting method for domain adaptation, which can be regarded as a generalization of an existing approach to semi-supervised learning.

We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ Instance Weighting for Domain Adaptation in NLP
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ In discriminative models, we are only concerned with p(y

For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ Following (Blitzer et al., 2006), we call the first the source domain, and the second the target domain.
For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ Blitzer et al. (2006) propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation, which can be regarded as weighting the features.

The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ For spam filtering, we used 200 labeled target instances and 1800 unlabeled target instances.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ Further removing source instances would push the emphasis more on the set of labeled target instances, which is only a biased sample of the whole target domain.

 $$$$$ Our goal is to obtain a good estimate of θt that is optimized according to the target domain distribution pt(x, y).
 $$$$$ We thank the anonymous reviewers for their valuable comments.

Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ In Section 3, we then propose a general instance weighting framework for domain adaptation.

 $$$$$ Our goal is to obtain a good estimate of θt that is optimized according to the target domain distribution pt(x, y).
 $$$$$ We thank the anonymous reviewers for their valuable comments.

 $$$$$ Our goal is to obtain a good estimate of θt that is optimized according to the target domain distribution pt(x, y).
 $$$$$ We thank the anonymous reviewers for their valuable comments.
