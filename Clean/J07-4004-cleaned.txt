Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. $$$$$ We demonstrate that both accurate and highly efficient parsing is possible with CCG.
Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. $$$$$ We demonstrate that both accurate and highly efficient parsing is possible with CCG.

From a parsing perspective, the C & C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009). $$$$$ The lexical category set used by the supertagger is described in Clark and Curran (2004a) and Curran, Clark, and Vadas (2006).
From a parsing perspective, the C & C parser (Clark and Curran, 2007) has been shown to be competitive with state-of-theart statistical parsers on a variety of test suites, including those consisting of grammatical relations (Clark and Curran, 2007), Penn Treebank phrase structure trees (Clark and Curran, 2009), and unbounded dependencies (Rimell et al, 2009). $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. $$$$$ The philosophy in earlier work which combined the supertagger and parser (Clark, Hockenmaier, and Steedman 2002; Clark and Curran 2003) was to use an unrestrictive setting of the supertagger, but still allow a reasonable compromise between speed and accuracy.
However, the parsing work by Clark and Curran (2007), and also Hockenmaier (2003) and Fowler and Penn (2010), has only considered chart-parsing. $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tag set) and a set of CCG lexical categories. $$$$$ Each of these features is again generalized by replacing the word associated with the lexical category with its POS tag.
Following Clark and Curran (2007), we assume that each input word has been assigned a POS-tag (from the Penn Treebank tag set) and a set of CCG lexical categories. $$$$$ For words seen less than k times, an alternative based on the word’s POS tag is used: The tagger can only assign categories which have been seen with the POS tag in the data.

Clark and Curran (2007) gives a more precise definition. $$$$$ Steedman (2000) gives a more precise definition of generalized forward composition.
Clark and Curran (2007) gives a more precise definition. $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

We ran the C & C parser using the normal-form model (we reproduced the numbers reported in Clark and Cur ran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release. $$$$$ The accuracy of this hybrid dependency model is given in Table 7.
We ran the C & C parser using the normal-form model (we reproduced the numbers reported in Clark and Cur ran (2007)), and copied the results of the hybrid model from Clark and Curran (2007), since the hybrid model is not part of the public release. $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

The numbers for C & C are for the hybrid model, copied from Clark and Curran (2007). $$$$$ The accuracy of this hybrid dependency model is given in Table 7.
The numbers for C & C are for the hybrid model, copied from Clark and Curran (2007). $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

The numbers for the normal-form model are evaluated by running the publicly available parser, while those for the hybrid dependency model are from Clark and Curran (2007). $$$$$ For the dependency model, the gold-standard dependency structures are produced by running our CCG parser over the normal-form derivations.
The numbers for the normal-form model are evaluated by running the publicly available parser, while those for the hybrid dependency model are from Clark and Curran (2007). $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). $$$$$ This rule can be implemented by assuming the following category schema for a coordination term: (X\X)/X, where X can be any category.
Following Hockenmaier (2003), we extract the grammar by reading rule instances directly from the derivations in CCGbank (Hockenmaier and Steedman, 2007), rather than defining the combinatory rule schema manually as in Clark and Curran (2007). $$$$$ Hockenmaier’s parser uses rule instantiations read off CCGbank (see Section 3.3) and some of these will be instances of type-raising and composition; hence the parser can produce non-normal-form derivations.

When it is reasonable to assume that the input sentence for the grammaticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via super tagging (Clark and Curran, 2007) on the input sequence. $$$$$ The supertagger uses statistical sequence tagging techniques to assign a small number of lexical categories to each word in the sentence.
When it is reasonable to assume that the input sentence for the grammaticality improvement system is sufficiently fluent, a list of candidate lexical categories can be assigned automatically to each word via super tagging (Clark and Curran, 2007) on the input sequence. $$$$$ Curran, Clark, and Vadas (2006) investigate the improvement obtained from using the forward–backward algorithm, and also address the drop in supertagger accuracy when using automatically assigned POS tags.

The average number of lexical categories per word drops to 1.3 when equals 0.075, which is the value used for parsing newspaper text in Clark and Curran (2007). $$$$$ The lexical category set used by the supertagger is described in Clark and Curran (2004a) and Curran, Clark, and Vadas (2006).
The average number of lexical categories per word drops to 1.3 when equals 0.075, which is the value used for parsing newspaper text in Clark and Curran (2007). $$$$$ The average number of categories assigned to each word is determined by the β parameter in the supertagger.

However, compared to the 93% lexical category accuracy of a CCG parser (Clark and Curran, 2007), which also uses a level of 0.075 for the majority of sentences, the accuracy of our grammaticality improvement system is much lower. $$$$$ The penultimate row corresponds to using only one supertagging level with β = 0.075; the parser ignores the sentence if it cannot get an analysis at this level.
However, compared to the 93% lexical category accuracy of a CCG parser (Clark and Curran, 2007), which also uses a level of 0.075 for the majority of sentences, the accuracy of our grammaticality improvement system is much lower. $$$$$ This suggests that, in order to increase the accuracy of the parser without losing efficiency, the accuracy of the supertagger at the β = 0.075 level needs to be improved, without increasing the number of categories assigned on average.

We parsed both corpora using the C & C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. $$$$$ The table gives results when using gold standard POS tags and, in the final two columns, when using POS tags automatically assigned by the POS tagger described in Curran and Clark (2003).
We parsed both corpora using the C & C parser (Clark and Curran, 2007) as we employ both GR and POS information in our learning method. $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art PennTreebank (PTB) parser. $$$$$ We chose this resource for the following reasons: It is publicly available, allowing other researchers to compare against our results; the GRs making up the annotation share some similarities with the predicate–argument dependencies output by the CCG parser; and we can directly compare our parser against a non-CCG parser, namely the RASP parser—and because we are converting the CCG output into the format used by RASP the CCG parser is not at an unfair advantage.
We compare the CCG parser of Clark and Curran (2007) with a state-of-the-art PennTreebank (PTB) parser. $$$$$ We have demonstrated, using state-of-the-art statistical models, that both accurate and highly efficient parsing is practical with CCG.

Examples of this approach include Riezler et al (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007). $$$$$ Briscoe and Carroll (2006) describe the differences between the two schemes.
Examples of this approach include Riezler et al (2002), Miyao and Tsujii (2005), Briscoe and Carroll (2006), and Clark and Curran (2007). $$$$$ The GRs are described in Briscoe (2006), Briscoe and Carroll (2006), and Briscoe, Carroll, and Watson (2006).

The formalism-based parser we use is the CCG parser of Clark and Curran (2007), which is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. $$$$$ The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.
The formalism-based parser we use is the CCG parser of Clark and Curran (2007), which is based on CCGbank (Hockenmaier and Steedman, 2007), a CCG version of the Penn Treebank. $$$$$ The treebank is CCGbank (Hockenmaier and Steedman 2002a; Hockenmaier 2003a), a CCG version of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993).

The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. $$$$$ CCGbank was created by converting the phrase-structure trees in the Penn Treebank into CCG normal-form derivations.
The CCG parser has been extensively evaluated elsewhere (Clark and Curran, 2007), and arguably GRs or predicate-argument structures provide a more suitable test set for the CCG parser than PTB phrase-structure trees. $$$$$ We define a CCG dependency structure as a set of CCG predicate–argument dependencies.

Since this short paper reports a small, focused research contribution, we refer readers to Clark and Curran (2007) and Petrov and Klein (2007) for details of the two parsers. $$$$$ The Clark and Curran paper shows this set to have very high coverage on unseen data.
Since this short paper reports a small, focused research contribution, we refer readers to Clark and Curran (2007) and Petrov and Klein (2007) for details of the two parsers. $$$$$ 11 The results reported in Clark and Curran (2007) differ from those here because Clark and Curran used the normal-form model and Viterbi decoder.

The schemas were developed by manual inspection using section 00 of CCGbank and the PTB as a development set, following the oracle methodology of Clark and Curran (2007), in which gold standard derivations from CCGbank are converted to the new representation and compared with the gold standard for that representation. $$$$$ The feature set is derived from the gold-standard normal-form derivations in CCGbank.
The schemas were developed by manual inspection using section 00 of CCGbank and the PTB as a development set, following the oracle methodology of Clark and Curran (2007), in which gold standard derivations from CCGbank are converted to the new representation and compared with the gold standard for that representation. $$$$$ All of these were set experimentally using Section 00 as development data.

While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale. $$$$$ There is some existing work comparing parser performance across formalisms.
While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale. $$$$$ The novel idea in Clark, Steedman, and Curran was to create new training data from questions, but to annotate at the lexical category level only, rather than annotate with full derivations.
