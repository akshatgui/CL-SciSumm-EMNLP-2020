Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). $$$$$ For these missing elements, we assign an ambiguity class by a simple ambiguity-class guesser, and set p(t

This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimation (SE), a fully-bayesian HMM model that jointly performs clustering and sequence learning (GG), and a Bayesian LDA-based model using only observed context features to predict tag words (TJ).
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ As we outperform this model in the complete dictionary case, it seems that the advantage of this model is due to its much stronger ambiguity class model, and not its Bayesian components.

 $$$$$ The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word.
 $$$$$ Experimenting with combining similar models (as well as TJ’s ambiguity class model) with our p(t

All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.
All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ For each of these, we first compare the computed p(tjw) against a gold standard distribution, taken from the test corpus (90K tokens), according to the measure used by (Levinger et al., 1995) (Dist).

See Goldberg et al (2008) for details. $$$$$ In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t

Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. $$$$$ We achieve accuracy of 92.85% for the 19tags set, and 91.3% for the complete 46-tags tagset.
Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. $$$$$ In particular, (Haghighi and Klein, 2006) presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words.

Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ As such, they are not very good at assigning ambiguity-classes to OOV tokens when starting with a very small dictionary.

(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ We reach 88% accuracy on full morphological and 92% accuracy for POS tagging and word segmentation, for the Morph+Linear initial conditions.
(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ In particular, (Haghighi and Klein, 2006) presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words.

 $$$$$ The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word.
 $$$$$ Experimenting with combining similar models (as well as TJ’s ambiguity class model) with our p(t

Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)
Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ EM-HMM, a second-order EM-HMM initialized with the estimated p(t

Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)
Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ Baselines As baseline, we use two EM-trained HMM taggers, initialized with a uniform p(t

As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ We also report state-of-the-art results for Hebrew full morphological disambiguation.
As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods.

Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. $$$$$ L+suff=W,S The word appears just after word W, with suffix 5 (L+suff=have,ed).
Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. $$$$$ Note that the context-free tagger based on our p(tjw) estimates is quite accurate.

The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). $$$$$ Morphology-based p(t

The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). $$$$$ The approximated probability of each analysis is based on the corpus frequency of its SW set.
The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). $$$$$ Note that the context-free tagger based on our p(tjw) estimates is quite accurate.

The importance is underscored succinctly by Goldberg et al (2008). $$$$$ In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t

EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). $$$$$ We have demonstrated that unsupervised POS tagging can reach good results using the robust EMHMM learner when provided with good initial conditions, even with incomplete dictionaries.

Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ EM Can Find Pretty Good HMM POS-Taggers (When Given a Good Start)
Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.

The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).
The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ Our method uses this algorithm as a first step, and refines the approximation by introducing additional linguistic constraints and an iterative refinement step.

Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008). $$$$$ All the p(tjw) estimates and HMM models are trained on the entire WSJ corpus.
Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008). $$$$$ While our models are trained on the unannotated text of the entire WSJ Treebank, CE and BHMM use much less training data (only the 24k words of the test-set).
