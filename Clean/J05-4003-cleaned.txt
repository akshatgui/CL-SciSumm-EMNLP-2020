Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ We work in the context of Arabic-English and Chinese-English statistical machine translation systems.

Sentence-level filter $$$$$ The filter verifies that the ratio of the lengths of the two sentences is no greater than two.
Sentence-level filter $$$$$ For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel.

The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ This is due to the low coverage of the dictionary learned from that corpus.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ Tables 3 and 4 present the coverage of our extracted corpora.

Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ The others are passed on to the parallel sentence selection stage.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ The most important feature of our parallel sentence selection approach is its robustness.

In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs.

Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ They start by attempting to identify similar article pairs from the two corpora.

Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments.

In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.

We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ One such model is the IBM Model 1 (Brown et al. 1993).

One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one.

In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ In this article, we have shown how they can be efficiently mined for parallel sentences.

Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.

Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004).
Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ The approach of Fung and Cheung (2004) is a simpler version of ours.

Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1).

The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.
The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ One such model is the IBM Model 1 (Brown et al. 1993).

In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.

Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ The approach of Fung and Cheung (2004) is a simpler version of ours.

This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.

We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel.

In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ The filter verifies that the ratio of the lengths of the two sentences is no greater than two.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model.
