The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). $$$$$ We propose a new supervised learning method for PPattachment based on a semantically tagged corpus.
The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). $$$$$ Unfortunately, at the time of writing this work, a sufficiently big corpus which was both syntactically analysed and semantically tagged did not exist.

clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary
clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ Because the induction of the decision tree for the PP attachment is based on a supervised learning from sense-tagged examples, it was necessary to sense-disambiguate the entire training set.

At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ This is because at the top of the decision tree all of the semantic tops of all of the content words of the given quadruple are compared with the semantic generalisations of the training examples represented through the nodes of the decision tree.
At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ The decision tree therefore represents a very useful mechanism for determining the semantic level at which the decision on the PP attachment is made.

Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ As we have already mentioned, Collins and Brooks [C&B951 based their method on matching the testing quadruples against the set of training examples.
Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ Collins and Brooks' have also demonstrated the importance of low count events in training data by an experiment where all counts less than 5 were put to zero.

Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ At first, all the training examples (separately for each preposition) are split into subsets which correspond to the topmost concepts of WordNet, which contains 11 topical roots for nouns and description nouns, and 337 for verbs (both nouns and verbs have hierarchical structure, although the hierarchy for verbs is shallower and wider).
Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ This is because training examples with an error or with a word not found in WordNet could not fully participate on the decision tree induction.

We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ As we have already mentioned, Collins and Brooks [C&B951 based their method on matching the testing quadruples against the set of training examples.
We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ Collins and Brooks' have also demonstrated the importance of low count events in training data by an experiment where all counts less than 5 were put to zero.

Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ In this case, we can almost certainly state that the PP is adverbial, i.e. attached to the verb.
Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ This was done by the iterative algorithm described in Chapter 2.

 $$$$$ Even without any sentential context, the human brain is capable of disambiguating word senses based on circumstances or experience3.
 $$$$$ At the moment, we are working on an implementation of the algorithm to work on with a wider sentential context and on its incorporation within a more complex NLP system.

The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ In this case, we can almost certainly state that the PP is adverbial, i.e. attached to the verb.
The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ The current statistical state-of-the art method is the backed-off model proposed by Collins and Brooks in [C&B95] which performs with 84.5% accuracy on stand-alone quadruples.

The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ An iterative, unsupervised method was then used to decide between adjectival and adverbial attachment in which the decision was based on comparing the co-occurence probabilities of the given preposition with the verb and with the noun in each quadruple.
The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ Every possible sense of all the related context words is evaluated and the best match.Chosen5.

The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ Corpus Based PP Attachment Ambiguity Resolution With A Semantic Dictionary
The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ Similar contextual situations (these include information on the PP-attachment) are found in the training corpora and are used for the sense disambiguation.

Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ Another promising approach is the transformation-based rule derivation presented by Brill and Resnik in [B&R941, which is a simple learning algorithm that derives a set of transformation rules.
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ Because the induction of the decision tree for the PP attachment is based on a supervised learning from sense-tagged examples, it was necessary to sense-disambiguate the entire training set.

The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ The current statistical state-of-the art method is the backed-off model proposed by Collins and Brooks in [C&B95] which performs with 84.5% accuracy on stand-alone quadruples.
The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ Two words are similar if their semantic distance is less than 1.0 and if either their character strings are different or if one of the words has been previously disambiguated.

 $$$$$ Even without any sentential context, the human brain is capable of disambiguating word senses based on circumstances or experience3.
 $$$$$ At the moment, we are working on an implementation of the algorithm to work on with a wider sentential context and on its incorporation within a more complex NLP system.
