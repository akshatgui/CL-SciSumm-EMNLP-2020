UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). $$$$$ Inverse Purity is defined as

We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). $$$$$ The SemEval-2007 WePS Evaluation

We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. $$$$$ Finally, ten additional names were ran domly selected from the Program Committee listing of a Computer Science conference (ECDL 2006).This set offers a scenario of potentially low am biguity (computer science scholars usually have a stronger Internet presence than other professionalfields) with the added value of the a priori knowl edge of a domain specific type of entity (scholar) present in the data.
We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. $$$$$ We also specified the source of each ambiguous name in the training data (Wikipedia, ECDL conference and US Census).

We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ The human annotation was used as the gold standard for the evaluation.Each system was evaluated using the standard pu rity and inverse purity clustering measures Purity isrelated to the precision measure, well known in In formation Retrieval.
We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ Inverse Purity is defined as

We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). $$$$$ The SemEval-2007 WePS Evaluation

We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). $$$$$ The SemEval-2007 WePS Evaluation

Two different evaluation measures are reported as described by the task $$$$$ Inverse Purity is defined as

Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. $$$$$ The SemEval-2007 WePS Evaluation

In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ Finally, Section 4 presents some con clusions.
In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ This section of thecorpus was used for the systems evaluation.

The data we have used for training our system were made available in the framework of the SemEval (task 13 $$$$$ The SemEval-2007 WePS Evaluation

For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. $$$$$ Our task is rather a case of Word Sense Discrimination, because the number of ?senses?
For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. $$$$$ Note that there is no a priori knowledge about the number of entities that will be discovered in a document set.

We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). $$$$$ The SemEval-2007 WePS Evaluation

Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). $$$$$ The task description and the initial trial data set were publicly released before the start of the official evaluation.The official evaluation period started with the simultaneous release of both training and test data, to gether with a scoring script with the main evaluation measures to be used.
Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). $$$$$ Out of them, 16 teams submitted results within the deadline; their results are reported below.

The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. $$$$$ And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants

In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). $$$$$ The SemEval-2007 WePS Evaluation

The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). $$$$$ The SemEval-2007 WePS Evaluation

Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007) $$$$$ The SemEval-2007 WePS Evaluation

Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. $$$$$ The SemEval-2007 WePS Evaluation

The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. $$$$$ The SemEval-2007 WePS Evaluation

Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007). $$$$$ And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants
