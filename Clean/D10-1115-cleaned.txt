Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training).

We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). $$$$$ Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.
We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). $$$$$ Guevara compares his model to the simplified additive and multiplicative models of Mitchell and Lapata.

The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.

Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ In Section 6, we show that our model outperforms other approaches at the task of approximating such vectors for unseen ANs.

Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ We conjecture that this is because the SVD dimensions can have negative values, leading to counter-intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values).
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ Moreover, coherently with this view, our evaluation below will be based on how closely the models approximate the observed vectors of unseen ANs.

Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ Multiplicative vectors (mult method) were obtained by componentwise multiplication of the adjective and noun vectors (normalization does not matter here since it amounts to multiplying the composite vector by a scalar, and the cosine similarity measure we use is scale-invariant).
Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ Thus, in Section 6 we report mult results from the full co-occurrence matrix; reduced space results for all other methods.

It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ We conjecture that this is because the SVD dimensions can have negative values, leading to counter-intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values).
It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ Thus, in Section 6 we report mult results from the full co-occurrence matrix; reduced space results for all other methods.

Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ We estimate the coefficients using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007).
Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ The number of latent variables to be used in the core regression are a free parameter of PLSR.

Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ In our approach, the weight matrix B is specific to a single adjective – as we will see in Section 7 below, it is our representation of the meaning of the adjective.
Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.

In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ In Section 6, we show that our model outperforms other approaches at the task of approximating such vectors for unseen ANs.
In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ Although, in relative terms and considering the difficulty of the task, alm performs well, it is still far from perfect – for 27% alm-predicted ANs, the observed vector is not even in the top 1K neighbor set!

Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments).
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ 5 Study 1: ANs in semantic space The actual distribution of ANs in the corpus, as recorded by their co-occurrence vectors, is fundamental to what we are doing.

In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ Original contribution We propose and evaluate a new method to derive distributional representations for ANs, where an adjective is a linear function from a vector (the noun representation) to another vector (the AN representation).
In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ In our approach, the weight matrix B is specific to a single adjective – as we will see in Section 7 below, it is our representation of the meaning of the adjective.

Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.

Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Unlike Guevara, (i) we train separate models for each adjective (we learn adjective-specific functions, whereas Guevara learns a generic “AN-slot” function) and, consequently, (ii) corpus-harvested adjective vectors play no role for us (their values would be constant across the training input vectors).
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Since the observed vectors look like plausible representations of composite meaning, we expect that the closer the modelgenerated vectors are to the observed ones, the better they should also perform in any task that requires access to the composite meaning, and thus that the results of the current evaluation should correlate with applied performance.

Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training).
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ Since our current focus is on alternative composition methods evaluated on a shared semantic space, exploring parameters pertaining to the construction of the semantic space is not one of our priorities, although we cannot of course exclude that the nature of the underlying semantic space affects different composition methods differently.

Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ In the framework of Mitchell and Lapata, our approach derives from the additive form in Equation (1) with the matrix multiplying the adjective vector (say, A) set to 0: p=Bv where p is the observed AN vector, B the weight matrix representing the adjective at hand, and v a noun vector.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.

Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.

The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ In Section 7, we discuss how adjectival meaning can be represented in our model and evaluate this representation in an adjective clustering task.
The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ Our proposed method, alm, emerges as the best approach.

Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Nouns are Vectors Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ However, to make the analysis more challenging and interesting, we populate the semantic space where we will look at the behaviour of the ANs with a large number of adjectives and nouns, as well as further ANs not in the test set.

In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ The linear map for a specific adjective is learnt, using linear regression, from pairs of noun and AN vectors extracted from a corpus.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.
