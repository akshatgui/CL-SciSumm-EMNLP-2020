For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi & Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes.

Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ Kok & Domingos (2007) applied Markov logic to relational clustering, but they used hard EM.

In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.

For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ However, unsupervised coreference resolution is much more difficult.

To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. $$$$$ This can be very inefficient for highly correlated clauses.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. $$$$$ Our optimization problem is not convex, so initialization is important.

Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson & Domingos, 2006).

As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ Efficient inference can be performed using MC-SAT (Poon & Domingos, 2006).
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ We developed a general method for producing a “lazy” version of relational inference algorithms (Poon & Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of “active” predicates/clauses, grounding more as needed.

A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson & Domingos, 2006).

Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum & Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon & Domingos, 2008)).

 $$$$$ In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k
 $$$$$  uk for all factors.
 $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.

We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). $$$$$ Efficient inference can be performed using MC-SAT (Poon & Domingos, 2006).

The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.

It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ We implemented our method as an extension to the Alchemy system (Kok et al., 2007).
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set.

Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ Joint Unsupervised Coreference Resolution with Markov Logic
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ Markov logic makes it possible to compactly specify probability distributions over complex relational domains.

ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). $$$$$ Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007).
ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). $$$$$ ACE-2 contains a training set and a test set.

Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ For example, suppose we also want to leverage predicate nominals (i.e., the subject and the predicating noun of a copular verb are likely coreferent).
Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein.

 $$$$$ In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k
 $$$$$  uk for all factors.
 $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.

On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum & Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon & Domingos, 2008)).

Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ Our approach greatly outperformed Haghighi & Klein (2007), the state-of-the-art unsupervised system.

 $$$$$ In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k
 $$$$$  uk for all factors.
 $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.
