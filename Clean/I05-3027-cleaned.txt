In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.

Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ However, it over generalized to words with frequent suffixes such as ??
Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ is two words.

For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ Both of these corpora have words that have more characters than HK and PK.
For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ Finally, some errors are due to inconsistencies in the gold segmentation of non-hanzi char acter.

(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Table 2 Comparisons of Peng et al (2004) and our F score on the closed track in Sighan bakeoff 2003 Sighan Bakeoff 2003 Our F-score F-score Peng et al (2004) CTB 0.863 0.849 AS 0.970 0.956 HK 0.947 0.928 PK 0.953 0.941 3.2.2 Results on Sighan bakeoff 2005 Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

 $$$$$ 3) C-1 prefix feature functions are defined over characters in the prefix table, and fire if the character in the proceeding state matches the feature function?s character.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ Our morphological features are based upon the intuition re garding unknown word features given in Gao et al.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ is a word, but sometimes it is segmented into two words.

The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ is a word, but sometimes it is segmented into two words.

We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ Table 2 Comparisons of Peng et al (2004) and our F score on the closed track in Sighan bakeoff 2003 Sighan Bakeoff 2003 Our F-score F-score Peng et al (2004) CTB 0.863 0.849 AS 0.970 0.956 HK 0.947 0.928 PK 0.953 0.941 3.2.2 Results on Sighan bakeoff 2005 Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

 $$$$$ 3) C-1 prefix feature functions are defined over characters in the prefix table, and fire if the character in the proceeding state matches the feature function?s character.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ Our morphological features are based upon the intuition re garding unknown word features given in Gao et al.

English-Chinese $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.
English-Chinese $$$$$ is a word, but sometimes it is segmented into two words.

 $$$$$ 3) C-1 prefix feature functions are defined over characters in the prefix table, and fire if the character in the proceeding state matches the feature function?s character.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.
For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ is a word, but sometimes it is segmented into two words.

 $$$$$ 3) C-1 prefix feature functions are defined over characters in the prefix table, and fire if the character in the proceeding state matches the feature function?s character.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ is a word, but sometimes it is segmented into two words.

This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ For each state, the character identity features (Ng & Low 2004, Xue & Shen 2003, Goh et al 2003) are represented using feature functions that key off of the identity of the character in the current, proceeding and subsequent positions.

The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ is a word, but sometimes it is segmented into two words.

For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ Both of these corpora have words that have more characters than HK and PK.
For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ Finally, some errors are due to inconsistencies in the gold segmentation of non-hanzi char acter.

The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ In the HK corpus, when we added in un known word features, our performance dropped.
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ is a word, but sometimes it is segmented into two words.

Stanford Chinese word segmenter (STANFORD) $$$$$ A Conditional Random Field Word Segmenter for Sighan Bakeoff 2005
Stanford Chinese word segmenter (STANFORD) $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.
