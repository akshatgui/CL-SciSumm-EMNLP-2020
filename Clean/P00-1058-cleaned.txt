One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ For example, for the sentence &quot;John should have left,&quot; Magerman's rules make should and have the heads of their respective VPs, so that there is no dependency between left and its subject John (see Figure 2a).
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ (We could make VP the head of VP instead, but this would generate auxiliaries independently of each other, so that, for example, P(John leave)
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$  0.)

The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG (Schabes and Waters, 1995).
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ We first compared the parser with (Hwa, 1998)

They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ Why use tree-adjoining grammar for statistical parsing?

In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ Why use tree-adjoining grammar for statistical parsing?

We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ Why use tree-adjoining grammar for statistical parsing?
We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ We introduce this operation simply so we can derive the flat structures found in the Penn Treebank.

In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ But beginning with (Magerman, 1995) statistical parsers have used bilexical dependencies with great success.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997).

Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ In this variant there are three kinds of elementary tree

Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ In this variant there are three kinds of elementary tree

Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ (Xia, 1999) describes a grammar extraction process similar to ours, and describes some techniques for automatically filtering out invalid elementary trees.

Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ Bilexical dependencies are not the only nonlocal dependencies that can be used to improve parsing accuracy.
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ But none of these cases really requires special treatment in a PTAG model, because each composition probability involves not only a bilexical dependency but a &quot;biarboreal&quot; (tree-tree) dependency.

While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ We find that the automatically-extracted grammar gives an improvement over the EM-based induction method of (Hwa, 1998), and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement.

While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Our work has a great deal in common with independent work by Chen and VijayShanker (2000).

In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ Auxiliary trees and adjunction are restricted as in TIG

Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Note that even in cases where the parser encounters a sentence for which the (fallible) extraction heuristics would have produced an unseen tree template, it is possible that the parser will use other trees to produce the correct bracketing.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Moreover, the greater flexibility of TAG suggests some potential improvements which would be cumbersome to implement using a lexicalized CFG.

The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ Statistical Parsing With An Automatically-Extracted Tree Adjoining Grammar
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ Why use tree-adjoining grammar for statistical parsing?

A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$ An arc corresponding to the sister-adjunction of a tree between the ith and i + 1th children of rl (allowing for two imaginary children beyond the leftmost and rightmost children) is labeled rl; i.
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$ (We could make VP the head of VP instead, but this would generate auxiliaries independently of each other, so that, for example, P(John leave)
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$  0.)

Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ For each node T1, these rules classify exactly one child of T1 as a head and the rest as either arguments or adjuncts.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ The backed-off models are combined by linear interpolation, with the weights chosen as in (Bikel et al., 1997).

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ The backed-off models are combined by linear interpolation, with the weights chosen as in (Bikel et al., 1997).
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ These results place our parser roughly in the middle of the lexicalized PCFG parsers.

 $$$$$ We want to obtain a maximum-likelihood estimate of these parameters, but cannot estimate them directly from the Treebank, because the sample space of PTAG is the space of TAG derivations, not the derived trees that are found in the Treebank.
 $$$$$ S. D. G.
