Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.
Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.

We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. $$$$$ A Comparison Of Algorithms For Maximum Entropy Parameter Estimation
We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. $$$$$ In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x

Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). $$$$$ And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.
Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). $$$$$ And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.

We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.
We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.

 $$$$$ Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.
 $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.

Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.
Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.

 $$$$$ Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.
 $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.

 $$$$$ Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.
 $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.

In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). $$$$$ A Comparison Of Algorithms For Maximum Entropy Parameter Estimation
In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). $$$$$ Thus, the use of such optimizations is independent of the choice of parameter estimation method.

because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates δ(k) at each search step differs substantially.
because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.

We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of θ(k) using the previous m values of y(k) and δ(k).
We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.

Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999).
Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ And, since the parameters of individual models can be estimated quite quickly, this will further open up the possibility for more sophisticated model and feature selection techniques which compare large numbers of alternative model specifications.

For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ Thus, the use of such optimizations is independent of the choice of parameter estimation method.
For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.

Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). $$$$$ Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: ratio of Ep[f] to Eq(k)[f], with the restriction that ∑j fj(x) = C for each event x in the training data (a condition which can be easily satisfied by the addition of a correction feature).
Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). $$$$$ In this case, the training data is very sparse.

For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ Thus, the use of such optimizations is independent of the choice of parameter estimation method.
For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the reference distribution), the number of iterations required, the number of log-likelihood and gradient evaluations required (algorithms which use a line search may require several function evaluations per iteration), and the total elapsed time (in seconds).2 There are a few things to observe about these results.

There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). $$$$$ In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.
There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). $$$$$ In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.

For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ A Comparison Of Algorithms For Maximum Entropy Parameter Estimation
For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ Thus, the use of such optimizations is independent of the choice of parameter estimation method.

Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. $$$$$ For the other optimization techniques, we used TAO (the “Toolkit for Advanced Optimization”), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).
Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. $$$$$ And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.

To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). $$$$$ For the other optimization techniques, we used TAO (the “Toolkit for Advanced Optimization”), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).
To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). $$$$$ And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.

For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed. $$$$$ Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-vector product: and the feature expectations are the transposed matrix-vector product: By expressing these computations as matrix-vector operations, we can take advantage of the high performance sparse matrix primitives of PETSc.
For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed. $$$$$ More dramatically, both iterative scaling methods perform very poorly on the ‘shallow’ dataset.
