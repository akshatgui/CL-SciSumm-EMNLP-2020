Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ To incorporate cue phrases, this generative model is modified to reflect the idea that some of the text will be topic-specific, but other terms will be topic-neutral cue phrases that express discourse structure.

Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ We consider two approaches to handling the language models: estimating them explicitly, and integrating them out, using the Dirichlet Compound Multinomial distribution (also known as the multivariate Polya distribution).
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ The Dirichlet compound multinomial integrates over language models, but we must still set the prior θ0.

Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ Both of these last two systems use dynamic programming to search the space of segmentations.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.

Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ For text, we introduce a dataset in which each document is a chapter selected from a medical textbook (Walker et al., 1990).7 The task is to divide each chapter into the sections indicated by the author.
Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ This dataset contains 227 chapters, with 1136 sections (an average of 5.00 per chapter).

This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ This system is referred to as BAYESSEG in Table 1.
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ BAYESSEG-CUE is the Bayesian system with cue phrases.

Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ For the task of segmenting documents, we are interested only in the segment indices, and would prefer not to have to search in the space of language models as well.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases.

Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006).
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.

If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ Topic segmentation is one of the fundamental problems in discourse analysis, where the task is to divide a text into a linear sequence of topicallycoherent segments.
If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.

The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).
The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric.

The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework.
The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ Cue phrases improve performance on the meeting corpus, but not on the textbook corpus.

This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ Cue phrases have previously been used in supervised topic segmentation (e.g., Galley et al. 2003); we show how they can be used in an unsupervised setting.

In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ In all cases, we use the publicly available executables provided by the authors.
In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006).

We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).

Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.
Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ The previous section modeled lexical cohesion by treating the bag of words in each sentence as a series of draws from a multinomial language model indexed by the topic segment.

We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).

 $$$$$ Let nj be a vector in which each element is the sum of the lexical counts over all the sentences in segment j: nj,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt — θj, then the posterior distribution for θj is Dirichlet with vector parameter nj +θ0 (Bernardo and Smith, 2000).
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.

Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ We use the evaluation source code provided by Malioutov and Barzilay (2006).
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006).
