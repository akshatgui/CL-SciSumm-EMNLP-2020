The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. $$$$$ Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears.
The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. $$$$$ System R. All Nouns Verbs FSc.

The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. $$$$$ SemEval-2007 Task 02

The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. $$$$$ SemEval-2007 Task 02

Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02

We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02

We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task ,i.e. supervised evaluation. $$$$$ SemEval-2007 Task 02

We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02

We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. $$$$$ The paper is organized as follows.
We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. $$$$$ Evaluate the induced senses as clusters of ex-.

For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02

Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ We call this evaluation supervised.
Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ is not a sense induction system.

The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. $$$$$ SemEval-2007 Task 02

While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ to ?word senses?.
While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ is not a sense induction system.

The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. $$$$$ UBC-AS?
The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. $$$$$ System Supervised evaluation I2R 82.2 UOY 81.3 UMND2 80.1 upv si 79.9 UBC-AS 79.0 MFS 78.4 Table 6

The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). $$$$$ SemEval-2007 Task 02

Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. $$$$$ Purity and entropy are also provided.
Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. $$$$$ Each of the induced clusters is mapped into a weighted vector of senses, and thus inducing a number of clusters similar to the number of senses is not a requirement for good results.

Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). $$$$$ to ?word senses?.
Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). $$$$$ is not a sense induction system.

The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ We call this evaluation supervised.
The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ 2.2 Supervised evaluation.

This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ We call this evaluation supervised.
This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ 2.2 Supervised evaluation.

Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 20071 (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02

The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007). $$$$$ SemEval-2007 Task 02
