Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
Tests were run on the ACL WSMT 2008 test set (Callison-Burch et al, 2008). $$$$$ The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008).

Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).

In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al, 2008), called newstest 2008 in the following. $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al, 2008), called newstest 2008 in the following. $$$$$ We refer to this as the News test set.

This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008). $$$$$ For instance the Edinburgh French-English system has a BLEU score of 26.8 on the part that was originally Spanish, but a score of on 9.7 on the part that was originally Hungarian.
This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al, 2008). $$$$$ This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).

PC Translator this year and also in Callison-Burch et al (2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
PC Translator this year and also in Callison-Burch et al (2008). $$$$$ For instance, is it easier, when the machine translation system translates in the same direction as the human translator?

The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008). $$$$$ 2 Overview of the shared translation task The shared translation task consisted of 10 language pairs

RWTH participated in this shared task with the two most promising metrics according to the previous experiments ,i.e. POSBLEU and POSF, and the detailed results can be found in (Callison-Burch et al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
RWTH participated in this shared task with the two most promising metrics according to the previous experiments ,i.e. POSBLEU and POSF, and the detailed results can be found in (Callison-Burch et al, 2008). $$$$$ Section 6 presents the results of the evaluation task.

The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
The Spearman's rank correlation coefficients on the document (system) level between the IBM1 metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. $$$$$ 2 Overview of the shared translation task The shared translation task consisted of 10 language pairs

The system level evaluation procedure follows WMT08 (Callison-Burch et al., 2008), which ranked each system submitted on WMT08 in three types of tasks $$$$$ We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.
The system level evaluation procedure follows WMT08 (Callison-Burch et al., 2008), which ranked each system submitted on WMT08 in three types of tasks $$$$$ The order of the types of evaluation were randomized.

test set in WMT08 (Callison-Burch et al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
test set in WMT08 (Callison-Burch et al, 2008). $$$$$ This out-of-domain test set contrasts with the in-domain Europarl test set.

For German, Spanish and Czech we use the news test sets proposed in (Callison-Burch et al 2010), for French and Italian the news test sets presented in (Callison-Burch et al 2008), for Arabic, Farsi and Turkish, sets of 2,000 news sentences extracted from the Arabic-English and English-Persian datasets and the SE-Times corpus. $$$$$ 2 Overview of the shared translation task The shared translation task consisted of 10 language pairs

Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
Thus, the human an notation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al, 2008). $$$$$ For instance, is it easier, when the machine translation system translates in the same direction as the human translator?

Following Callison-Burch et al (2008), we assigned a score to each of the 11 MT systems based on how of ten its translations were judged to be better than or equal to any other system. $$$$$ We assigned a ranking to the systems for each of the three types of manual evaluation based on

Detailed token and type statistics can be found in Callison-Burch et al (2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
Detailed token and type statistics can be found in Callison-Burch et al (2008). $$$$$ This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).

the model is tuned with mert (Bertoldi, et al) 5) the official test set from ACL WMT 2008 (Callison-Burch et al, 2008), consisting of 2000 sentences, is used as test set. $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
the model is tuned with mert (Bertoldi, et al) 5) the official test set from ACL WMT 2008 (Callison-Burch et al, 2008), consisting of 2000 sentences, is used as test set. $$$$$ Like the other system combination entrants, it was tuned on the Europarl test set and tested on the News test set, using systems that submitted entries to both tasks.

We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking. $$$$$ To measure the correlation of the automatic metrics with the human judgments of translation quality at the system-level we used Spearman’s rank correlation coefficient p. We converted the raw scores assigned each system into ranks.
We followed the benchmark assessment procedure in WMT and NIST MetricsMaTr (Callison-Burch et al, 2008, 2010), assessing the performance of the propose devaluation metric at the sentence level using ranking preference consistency, which also known as Kendall's rank correlation coefficient, to evaluate the correlation of the proposed metric with human judgments on translation adequacy ranking. $$$$$ The average correlation coefficient between the constituent-based judgments with the sentence ranking judgments is only p = 0.51.

Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008). $$$$$ Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties.
Traditionally, human ratings for MT quality have been collected in the form of absolute scores on a five or seven-point Likert scale, but low reliability numbers for this type of annotation have raised concerns (Callison-Burch et al, 2008). $$$$$ In addition, we evaluated seven commercial rule-based MT systems.

For details, see Callison-Burch et al (2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
For details, see Callison-Burch et al (2008). $$$$$ This would be useful because it could alleviate the problems associated with Bleu failing to recognize allowable variation in translation when multiple reference translations are not available (Callison-Burch et al., 2006).

only ,i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al, 2008). $$$$$ The same is done for sentence and each of the system translations.
only ,i.e., how often the translations of the system were rated as better than the translations from other systems (Callison-Burch et al, 2008). $$$$$ • The percent of time that its constituent translations were judged to be better than or equal to the translations of any other system.

Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al, 2008). $$$$$ This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
Both resources are taken from the shared translation task in WMT-2008 (Callison-Burch et al, 2008). $$$$$ The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008).
