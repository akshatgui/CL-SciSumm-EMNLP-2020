Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ We presented algorithms for learning what we call inferential selectional preferences, and presented evidence that learning selectional preferences can be useful in filtering out incorrect inferences.

This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ ISP: Learning Inferential Selectional Preferences
This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ For these cases, ISP algorithms learn many selectional preferences that accept the same types of entities as those that made DIRT learn the inference rule in the first place, hence ISP will not filter out many incorrect inferences.

Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002).

Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ In this paper, we focus on the inference rules contained in the DIRT resource (Lin and Pantel 2001).

For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ The semantic classes C(x) and C(y) can be obtained from a conceptual taxonomy as proposed in (Resnik 1996), such as WordNet, or from the classes extracted from a word clustering algorithm such as CBC (Pantel and Lin 2002).
For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002).

On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al.
On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).

A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ Model 2: Independent Inferential Model (IIM) Our independent model is the same as the joint model above except that it computes candidate inferential SPs using the Independent Relational Model (IRM) instead of the JRM.

While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). $$$$$ In this paper, we focus on the inference rules contained in the DIRT resource (Lin and Pantel 2001).

In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach.
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ In Section 3.1, we describe methods for automatically determining the semantic contexts of each single relation’s selectional preferences.

Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. $$$$$ CBC (Pantel and Lin 2002).
Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. $$$$$ Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002).

Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ 2006), and textual entailment (Szpektor et al. 2004).
Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).

To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ ISP: Learning Inferential Selectional Preferences
To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ Whereas in Section 3.1 we learned selectional preferences for the arguments of a relation p, in this section we learn selectional preferences for the arguments of an inference rule pi => pj.

We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ Our first set of semantic classes was directly extracted from the output of the CBC clustering algorithm (Pantel and Lin 2002).
We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ First, systems using the semantic classes from WordNet tend to perform less well than systems using CBC classes.

MI was also recently used for inference-rule SPs by Pantel et al (2007). $$$$$ TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001).
MI was also recently used for inference-rule SPs by Pantel et al (2007). $$$$$ Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences.

As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. $$$$$ What is missing is knowledge about the admissible argument values for which an inference rule holds, which we call Inferential Selectional Preferences.
As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. $$$$$ Section 3.2 uses these for developing our inferential selectional preference models.

 $$$$$ Intuitively, we have more confidence in a particular candidate if its semantic classes are closely associated given the relation p. Pointwise mutual information (Cover and Thomas 1991) is a commonly used metric for measuring this association strength between two events e1 and e2: 2 In this paper, the semantic classes C(x) and C(y) are extracted from WordNet and CBC (described in Section 4.2).
 $$$$$ This work constitutes a step towards better understanding of the interaction of selectional preferences and inferences, bridging these two aspects of semantics.

Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ In this paper, we propose ISP, a collection of methods for learning inferential selectional preferences and filtering out incorrect inferences.
Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ The presented algorithms apply to any collection of inference rules between binary semantic relations, such as example (1).

The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ ISP: Learning Inferential Selectional Preferences
The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ Section 3.2 uses these for developing our inferential selectional preference models.

In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. $$$$$ Whereas in Section 3.1 we learned selectional preferences for the arguments of a relation p, in this section we learn selectional preferences for the arguments of an inference rule pi => pj.
In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. $$$$$ Too general a class will provide no discriminatory power while too fine-grained a class will offer little generalization and apply in only extremely few cases.

This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ ISP: Learning Inferential Selectional Preferences
This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ The remainder of this section describes the ISP approach.
