Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C

We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ For the German task, the improvement yielded by classifier combination is smaller.

 $$$$$ In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.

 $$$$$ In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.

 $$$$$ In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.

Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C

 $$$$$ In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.

Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996).

 $$$$$ In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.

Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996).

It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).

Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). $$$$$ To facilitate comparison with other classifiers for this task, most reported results 3 The method of retaining only the boundaries and reclassifying the entities was shown to improve the performance of 11 of the 12 systems participating in the CoNLL-2002 shared tasks, in both languages (Florian, 2002b). are obtained by using features exclusively extracted from the training data.

Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).

Florian et al (2003) employed the same technique in a combination of learners. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C

Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).

Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ Table 2 presents the combination results, for different ways of estimating the interpolation parameters.

One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ Named Entity Recognition Through Classifier Combination
One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ Named Entity Recognition Through Classifier Combination
The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ German poses a completely different problem for named entity recognition: the data is considerably sparser.

For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.

Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ We note that the numbers are roughly twice as large for the development data in German as they are for English.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ Also, specifically for the German data, traditional approaches which utilize capitalization do not work as well as in English, because all nouns are capitalized in German.
