The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ Optimizing the corresponding parameters A1 and A2 of the model in Eq.
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).

Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ We even can use both features log Pr(eI1

An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ For example, using a function k(·) that counts how many verb groups exist in the source or the target sentence, we can define the following feature, which is 1 if each of the two sentences contains the same number of verb groups

MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ 5 (Och et al., 1999)

Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999).
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).

For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ Yet, the criterion as it is described in Eq.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi

The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ We even can use both features log Pr(eI1

These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions

For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ 5 (Och et al., 1999)

 $$$$$ The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered.
 $$$$$ In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).

Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ 5 (Och et al., 1999)

We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ • IER (information item error rate)

This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ Discriminative Training And Maximum Entropy Models For Statistical Machine Translation
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.

The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ Yet, the criterion as it is described in Eq.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ We see that adding new features also has an effect on the other model scaling factors.

The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ Discriminative Training And Maximum Entropy Models For Statistical Machine Translation
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.

The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ Typically, Eq.
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions

Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ The following three rows show the results if we add the word penalty, an additional class-based five-gram GIS algorithm for maximum entropy training of alignment templates. language model and the conventional dictionary features.
Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ We observe improved error rates for using the word penalty and the class-based language model as additional features.

 $$$$$ The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered.
 $$$$$ In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).

To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ 5 (Och et al., 1999)

In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ Discriminative Training And Maximum Entropy Models For Statistical Machine Translation
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.
