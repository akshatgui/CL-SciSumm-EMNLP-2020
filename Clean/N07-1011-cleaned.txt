Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ All entity types are candidates for coreference (pronouns, named entities, and nominal entities).
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses “early update” to update the parameters when an error is encountered.

In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ This approach allows the update to only penalize the difference between the two features of examples, thereby not penalizing features representing any overlapping coreferent clusters.
In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ We see that both the first-order features and the training enhancements improve performance consistently. wise Model in F1 measure for both standard training and error-driven training.

The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ In addition to the listed features, we also include conjunctions of size 2, for example “Genders match AND numbers match”.
The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work.

Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ We have presented learning and inference procedures for coreference models using first-order features.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ First-Order Probabilistic Models for Coreference Resolution
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ All entity types are candidates for coreference (pronouns, named entities, and nominal entities).

However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ All-X is true if all pairs share a feature X, Most-True-X is true if the majority of pairs share a feature X, and Most-False-X is true if most of the pairs do not share feature X.
However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ First-Order Probabilistic Models for Coreference Resolution
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ Conceptually, a first-order probabilistic model can be described quite compactly.

 $$$$$ Exactly how to update the parameter vector is addressed by the second enhancement.
 $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ This restriction can be detrimental if there exist features of sets of noun phrases that cannot be captured by a combination of pairwise features.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ We follow Soon et al. (2001) and Ng and Cardie (2002) to generate most of our features for the Pairwise Model.

Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ These include

For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ Our experiments show the advantages of this ranking-based loss function.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work.

Many of our features are similar to those described in Culotta et al (2007). $$$$$ We follow Soon et al. (2001) and Ng and Cardie (2002) to generate most of our features for the Pairwise Model.
Many of our features are similar to those described in Culotta et al (2007). $$$$$ Our current work has similar representational flexibility as Milch et al. (2005) but is discriminatively trained.

Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ Consider a training example cluster with a negative label, indicating that not all of the noun phrases it contains are coreferent.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ 336 documents are used for training, and the remainder for testing.

For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ Our experiments show the advantages of this ranking-based loss function.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work.

Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ First, it is not clear how best to convert the set of pairwise classifications into a disjoint clustering of noun phrases.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005).

Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ (However, we should note that there are also small differences in the feature sets used for error-driven and standard training results.)
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Also note that the Pairwise baseline obtains results similar to those in Ng and Cardie (2002).

Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates $$$$$ All-X is true if all pairs share a feature X, Most-True-X is true if the majority of pairs share a feature X, and Most-False-X is true if most of the pairs do not share feature X.
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates $$$$$ These pairwise decisions are made collectively using relational inference; however, as pointed out in Milch et al. (2004), this model has limited representational power since it does not capture features of entities, only of pairs of mention.

 $$$$$ Exactly how to update the parameter vector is addressed by the second enhancement.
 $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

 $$$$$ Exactly how to update the parameter vector is addressed by the second enhancement.
 $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ 336 documents are used for training, and the remainder for testing.
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ We use the B3 algorithm to evaluate the predicted coreferent clusters (Amit and Baldwin, 1998).
