Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. $$$$$ We investigated the accuracy of recovering semantically-relevant grammatical dependencies from the tree-structures produced by the Collins parser, comparing these dependencies to gold-standard dependencies which are available for a subset of 700 sentences randomly drawn from section 23 of the Wall Street Journal (see King et al. (2003)).
Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. $$$$$ We presented some experiments that compare the accuracy and performance of two stochastic parsing systems, the shallow Collins parser and the deep-grammar-based XLE system.

For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ The stochastic component is also “ambiguity-enabled” in the sense that the computations for statistical estimation and selection of the most probable analyses are done efficiently by dynamic programming, avoiding the need to unpack the parse forests and enumerate individual analyses.
For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ In the experiments reported in this paper, however, dynamic programming is crucial for efficient stochastic disambiguation, i.e. to efficiently find the most probable parse from a packed parse forest that is annotated with feature-values.

Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ The DEPBANK consists of dependency annotations for 700 sentences that were randomly extracted from section 23 of the UPenn Wall Street Journal (WSJ) treebank.

We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). $$$$$ This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems.
We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). $$$$$ The grammar used for this experiment was developed in the ParGram project (Butt et al., 2002).

We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. $$$$$ We investigated the accuracy of recovering semantically-relevant grammatical dependencies from the tree-structures produced by the Collins parser, comparing these dependencies to gold-standard dependencies which are available for a subset of 700 sentences randomly drawn from section 23 of the Wall Street Journal (see King et al. (2003)).
We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. $$$$$ For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al. (2002).

Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing
Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ This conversion was relatively straightforward for LFG structures (King et al., 2003).

Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ This conversion was relatively straightforward for LFG structures (King et al., 2003).
Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ While it is relatively straightforward to convert LFG f-structures to the dependency bank format because the f-structure is effectively a dependency format, it is more difficult to transform the output trees of the Model 3 Collins parser in a way that fairly allocates both credits and penalties.

In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ The DEPBANK consists of dependency annotations for 700 sentences that were randomly extracted from section 23 of the UPenn Wall Street Journal (WSJ) treebank.
In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ As input to the Collins parser, we used the part-ofspeech tagged version of section 23 that was provided with the parser.

Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ This is because the Collins parser shares the property of robustness with other statistical parsers, but more than other such parsers, the categories of its parse-trees make grammatical distinctions that presumably are useful for meaningsensitive applications.
Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al. (2002).

they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ The c-structures encode constituency and linear order.
they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ The Complete version of the grammar uses all of the (sub)rules in a multi-pass system that depends on the ranking of the OT marks in the rules.

The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ Performance parameters of both the Collins parser and the XLE system were adjusted on a heldout set consisting of a random selection of 1/5 of the PARC 700 dependency bank; experimental results were then based on the other 560 sentences.
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ We presented some experiments that compare the accuracy and performance of two stochastic parsing systems, the shallow Collins parser and the deep-grammar-based XLE system.

The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). $$$$$ We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing
The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). $$$$$ Contrary to conventional wisdom, we found that the shallow system was not substantially faster than the deep parser operating on a core grammar, while the deep system was significantly more accurate.

The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). $$$$$ This is because the Collins parser shares the property of robustness with other statistical parsers, but more than other such parsers, the categories of its parse-trees make grammatical distinctions that presumably are useful for meaningsensitive applications.
The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). $$$$$ As described by (King et al., 2003), the annotations were boot-strapped by parsing the sentences with a LFG grammar and transforming the resulting f-structures to a collection of dependency triples in the DEPBANK format.

Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ We investigated the accuracy of recovering semantically-relevant grammatical dependencies from the tree-structures produced by the Collins parser, comparing these dependencies to gold-standard dependencies which are available for a subset of 700 sentences randomly drawn from section 23 of the Wall Street Journal (see King et al. (2003)).
Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ This conversion was relatively straightforward for LFG structures (King et al., 2003).

 $$$$$ This effectively removes the (sub)rules that they mark from the grammar.
 $$$$$ Our scores and Gildea and Palmer’s are both substantially lower than the 90% typically cited for evaluations based on labeled or unlabeled bracketing, suggesting that extracting semantically relevant dependencies is a more difficult, but we think more valuable, task.

We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). $$$$$ We used the PARC 700 Dependency Bank (DEPBANK) as the gold standard in our experiments.
We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). $$$$$ From this we extracted the 700 sentences in the PARC 700.

Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. $$$$$ The experimental results show that stochastic parsing with the Core LFG grammar achieves a better F-score than the Collins parser at a roughly comparable parsing speed.
Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. $$$$$ From this we extracted the 700 sentences in the PARC 700.

Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ We used the PARC 700 Dependency Bank (DEPBANK) as the gold standard in our experiments.
Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ The complete version gives an overall improvement in F-score of 5% over the Collins parser at a cost of a factor of 5 in parsing time.

Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ We found the deep-parsing system to be more accurate than the Collins parser with only a reduction in parsing
Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ This conversion was relatively straightforward for LFG structures (King et al., 2003).

The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004). $$$$$ Performance parameters of both the Collins parser and the XLE system were adjusted on a heldout set consisting of a random selection of 1/5 of the PARC 700 dependency bank; experimental results were then based on the other 560 sentences.
The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004). $$$$$ As discussed below, these parameters were set on a held-out portion of the PARC700 which was also used to set the Collins parameters.
