The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.

Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. $$$$$ The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.

We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. $$$$$ Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).
We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. $$$$$ As uncertainty detection is extremely important for biomedical information extraction and most existing approaches have targeted such applications, participants were asked to develop systems for hedge detection in biological scientific articles.

We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features.
We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ Task2 systems differ in the number of class labels used as target and in the machine learning approaches applied.

 $$$$$ The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.
 $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http

We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations $$$$$ Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).
We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations $$$$$ Regarding cross submissions, Zhao et al. (2010) and Ji et al.

Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope.
Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.

Councill et al (2010) present a supervised scope detector using their own annotation. $$$$$ Regarding cross submissions, Zhao et al. (2010) and Ji et al.
Councill et al (2010) present a supervised scope detector using their own annotation. $$$$$ Zhao et al. (2010) extended the biological cue word dictionary of their system – using it as a feature for classification – by the frequent cues of the Wikipedia dataset, while Ji et al.

As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ However, the use of the above words or grammatical devices does not necessarily entail their being a weasel cue since their use may be justifiable in their contexts.
As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ Regarding cross submissions, Zhao et al. (2010) and Ji et al.

This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).
This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ Regarding cross submissions, Zhao et al. (2010) and Ji et al.

Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. $$$$$ Two uncertainty detection tasks (sentence classification and in-sentence hedge scope detection) in two domains (biological publications and Wikipedia articles) with three types of submissions (closed, cross and open) were given to the participants of the CoNLL-2010 Shared Task.
Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. $$$$$ The challenge consisted of a sentence identification task on uncertainty (Task1) and an in-sentence hedge scope detection task (Task2).

The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g.
The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ The identification of the scope for a certain cue was typically carried out by classifying each token in the sentence.

For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ However, the whole phrase is speculative therefore it is marked as a hedge cue.
For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ (2010) is the accurate detection of scope boundaries.

Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ Kilicoglu and Bergler (2008) proposed a linguistically motivated approach based on syntactic information to semi-automatically refine a list of hedge cues.
Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ The system of Kilicoglu and Bergler (2010) is the only open submission.

The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g.
The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ Five systems followed a pure token classification approach (TC) for cue detection while others used sequential labeling techniques (usually Conditional Random Fields) to identify cue phrases in sentences (SL).

 $$$$$ The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.
 $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http

 $$$$$ The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.
 $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http

The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. $$$$$ The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.

The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ The BioScope corpus (Vincze et al., 2008) is manually annotated with negation and speculation cues and their linguistic scope.

Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).
Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ Regarding cross submissions, Zhao et al. (2010) and Ji et al.
