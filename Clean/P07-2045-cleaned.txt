This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ 3 Factored Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1.
This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.

For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006).
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).

Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). $$$$$ Moses implements an efficient representation of the phrase translation table.
Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). $$$$$ Moses implements an efficient representation of the phrase translation table.

Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). $$$$$ Moses implements an efficient representation of the phrase translation table.
Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). $$$$$ Moses implements an efficient representation of the phrase translation table.

Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). $$$$$ It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).
The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006).
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ Moses was the subject of this year’s Johns Hopkins University Workshop on Machine Translation (Koehn et al. 2006).

For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004).

(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).
The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling.

We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. $$$$$ Every factor on the target language can have its own language model.
Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. $$$$$ It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al. 2002).

When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ However, Moses can have ambiguous input in the form of confusion networks.
When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ However, Moses can have ambiguous input in the form of confusion networks.

We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
