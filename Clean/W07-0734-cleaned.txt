Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). $$$$$ METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules.
Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). $$$$$ For Spanish, German and French we used the evaluation data provided by the shared task at last year’s WMT workshop.

We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).

We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

The translation quality is measured by three MT evaluation metrics $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
The translation quality is measured by three MT evaluation metrics $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).

We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ METEOR , initially proposed and released in 2004 (Lavie et al., 2004) was explicitly designed to improve correlation with human judgments of MT quality at the segment level.
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.

In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ Some, but not all, of these data sets have multiple human judgments per translation hypothesis.
In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ Recall, however, is still given more weight than precision.

The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. $$$$$ METEOR

The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). $$$$$ In the latest release, we tuned these parameters to optimize correlation with human judgments based on more extensive experimentation, as reported in section 4.
The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). $$$$$ We suspected that parameters that were optimized to maximize correlation with human judgments for English would not necessarily be optimal for other languages.

Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ When optimizing correlation with the sum of adequacy and fluency, optimal values fall in between the values found for adequacy and fluency.
Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ Resulting parameters are shown in Table 4.3.2.

Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.
Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.

In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). $$$$$ METEOR

In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.
In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. $$$$$ METEOR evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.

The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.

Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows $$$$$ The most commonly used MT evaluation metric in recent years has been IBM’s BLEU metric (Papineni et al., 2002).
Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows $$$$$ An alignment is mapping between words, such that every word in each string maps to at most one word in the other string.

Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ Human judgments come in the form of “adequacy” and “fluency&quot; quantitative scores.
Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ For English, we used the NIST 2003 Arabic-toEnglish MT evaluation data for training and the NIST 2004 Arabic-to-English evaluation data for testing.
