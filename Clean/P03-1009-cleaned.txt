Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ Brew and Schulte im Walde (2002)).
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ This involved applying the NN and IB methods to cluster polysemic SCF distributions extracted from corpus data using Briscoe and Carroll’s (1997) system.

The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ In contrast to earlier work, we give special emphasis to polysemy.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).

Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ We first evaluated the clusters against the predominant sense, i.e. using the monosemous gold standard.
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ Clearly, allowing for multiple gold standard classes makes it easier to obtain better results with evaluation.

Table 1 $$$$$ The test verbs and their classes are shown in table 1.
Table 1 $$$$$ 10 gold standard classes included 2 or more or these.

We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ This incorporates Levin’s classes, 26 additional classes by Dorr (1997)1, and 57 new classes for verb types not covered comprehensively by Levin or Dorr.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ The test verbs and their classes are shown in table 1.

We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ A number of different strategies have been proposed for evaluation of clustering.
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ In the future, we plan to investigate the use of soft clustering (without hardening the output) and develop methods for evaluating the soft output against polysemous gold standards.

 $$$$$ The lexicon was evaluated against manually analysed corpus data after an empirically defined threshold of 0.025 was set on relative frequencies of SCFs to remove noisy SCFs.
 $$$$$ We also plan to work on improving the accuracy of subcategorization acquisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different approaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes.

By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ As it primarily concentrates on verbs taking NP and PP complements and does not provide a comprehensive set of senses for verbs, it is not suitable for evaluation of polysemic classifications.
By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ These were obtained by parameterizing two high frequency SCFs for prepositions

We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ The monosemous one lists only a single sense for each test verb, that corresponding to its predominant (most frequent) sense in WordNet.

We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ We first evaluated the clusters against the predominant sense, i.e. using the monosemous gold standard.

For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ A number of different strategies have been proposed for evaluation of clustering.

Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ The classes are indicated by number codes from the classifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font.
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ This leads us to define modified purity

 $$$$$ The lexicon was evaluated against manually analysed corpus data after an empirically defined threshold of 0.025 was set on relative frequencies of SCFs to remove noisy SCFs.
 $$$$$ We also plan to work on improving the accuracy of subcategorization acquisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different approaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes.

There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ We expect the use of this system to be beneficial

In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ We employed as a gold standard a substantially extended version of Levin’s classification constructed by Korhonen (2003).
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ This paper has presented a novel approach to automatic semantic classification of verbs.

We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ We employed as a gold standard a substantially extended version of Levin’s classification constructed by Korhonen (2003).
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997).

The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ At best it classified half of the verbs correctly according to their predominant sense (mPUR = 50%).
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ 7 classes were correctly acquired using clustering (e.g.

Korhonen et al (2003) observed the opposite with general language data. $$$$$ Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002).
Korhonen et al (2003) observed the opposite with general language data. $$$$$ However, they also suggest that clustering can be used for novel, previously unexplored purposes

This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ Clustering Polysemic Subcategorization Frame Distributions Semantically
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002).
