When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ The first is Moses which is a state of the art SMT system widely used as a baseline in MT community.
When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms.

Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations.
Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution.

In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Sen tence simplification can therefore be classifiedinto two types: lexical simplification and syntac tic simplification (Carroll et al, 1999).

We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. $$$$$ All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB.
We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. $$$$$ Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from PWKP.

However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ in the ?Drop?
However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ But Moses mostly cannot split and drop.

AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.
AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ This is confirmed by MT evaluation measures: if we set CW as both source and reference, the BLEU score obtained by Moses is 0.78.

Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. $$$$$ Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.
Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. $$$$$ As faras lexical simplification is concerned, word substitution is usually done by selecting simpler syn onyms from Wordnet based on word frequency (Carroll et al, 1999).In this paper, we propose a sentence simplifica tion model by tree transformation which is based 1353 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008).

Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. $$$$$ Here are two example results obtained with our TSM system.Example 1.
Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. $$$$$ The results are pre sented in Tab.

Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ A Monolingual Tree-based Translation Model for Sentence Simplification
Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ As faras lexical simplification is concerned, word substitution is usually done by selecting simpler syn onyms from Wordnet based on word frequency (Carroll et al, 1999).In this paper, we propose a sentence simplifica tion model by tree transformation which is based 1353 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008).

This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ The authors are requested to ?use easy words and short sentences?
This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ with ?use?

Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. $$$$$ We collected a paired dataset from the English Wikipedia and Simple English Wikipedia.
Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. $$$$$ The sentences from Wikipedia and Simple Wikipedia are considered as ?complex?

Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ The evaluation shows that our model achieves better readability scores than a set of baseline systems.
Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ TSM achieves significantly better scores than Moses which has the best BLEUscore.

There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ We collected a paired dataset from the English Wikipedia and Simple English Wikipedia.
There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms.

We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ A Monolingual Tree-based Translation Model for Sentence Simplification
We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.

 $$$$$ should be put on the ?left?
 $$$$$ 1360

 $$$$$ should be put on the ?left?
 $$$$$ 1360

While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements. $$$$$ Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.
While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements. $$$$$ Nevertheless, the sentences generated by TSM seem better than Moses in terms of simplification.

 $$$$$ should be put on the ?left?
 $$$$$ 1360

Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations.
Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution.

Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ The evaluation shows that our model achieves better readability scores than a set of baseline systems.
Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ 9 shows the BLEU and NIST scores.
