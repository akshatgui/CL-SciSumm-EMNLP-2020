Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ Charniak et al. (2003) described a two-step stringto-CFG-tree translation model which employed a syntax-based language model to select the best translation from a target parse forest built in the first step.

In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ The values of the first four features are accumulated on the rules used in a translation.

The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ Galley et al. (2006) extended this string-to-tree model by using Context-Free parse trees to represent the target side.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ Next we introduce the four tree operations on dependency structures.

Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures.

The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ ',ï¿½ phrase alignments, where source phrase P ?
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees.

(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures.

(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ Hiero can be viewed as a hierarchical string-to-string model.

We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. $$$$$ For comparison purposes, we replicated the Hiero decoder (Chiang, 2005) as our baseline.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. $$$$$ On decoding output, the string-todependency system achieved 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to the baseline hierarchical stringto-string system.

In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures.

Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ In order to calculate the dependency language model score, or depLM score for short, on the fly for partial hypotheses in a bottom-up decoding, we need to save more information in categories and states.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline.

When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ We propose a string-to-dependency model for MT, which employs rules that represent the source side as strings and the target side as dependency structures.
When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ Both source and target are strings with NTs.

Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ A dependency structure is well-formed if and only if it is either fixed or floating.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ We can concatenate two fixed structures, one fixed structure with one floating structure, or two floating structures in the same direction.

Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ Based on the results in previous work (DeNeefe et al., 2007), we want to keep two kinds of dependency structures.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ All models are tuned on BLEU (Papineni et al., 2001), and evaluated on both BLEU and Translation Error Rate (TER) (Snover et al., 2006) so that we could detect over-tuning on one metric.

Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ Now we explain how we get the string-todependency rules from training data.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ Table 1 shows the number of transfer rules extracted from the training data for the tuning and test sets.

It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ It is easy to verify that the structures in Figures 2 and 3 are well-formed.
It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ Figure 5 shows the four operations to combine partial dependency structures, which are left adjoining (LA), right adjoining (RA), left concatenation (LC) and right concatenation (RC).

Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ Section 3 illustrates of the use of dependency language models.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ Next we introduce the four tree operations on dependency structures.

Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ If we can combine two categories with a certain category operation, we can use a corresponding tree operation to combine two dependency structures.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations.

Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ A New String-to-Dependency Machine Translation Algorithm with a Target Dependency Language Model
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ Based on the results in previous work (DeNeefe et al., 2007), we want to keep two kinds of dependency structures.

Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ For comparison purposes, we replicated the Hiero decoder (Chiang, 2005) as our baseline.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Furthermore, we combine partial dependency structures in a way such that we can obtain all possible well-formed but no ill-formed dependency structures during bottom-up decoding.

To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees.
