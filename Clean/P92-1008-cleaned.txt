Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al, 1992): 309 of 406 utterance containing nontrivial  repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utterances were incorrectly identified as containing repairs. $$$$$ The pattern-matching component reported on here looks for identical sequences of words, and simple syntactic anomalies, such as &quot;a the&quot; or &quot;to from.&quot; Of the 406 sentences containing nontrivial repairs, the program successfully found 309.
Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al, 1992): 309 of 406 utterance containing nontrivial  repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utterances were incorrectly identified as containing repairs. $$$$$ In addition, out of the 10,517 sentence corpus (10,718 â€” 201 trivial), it incorrectly hypothesized that an additional 191 contained repairs.

Bear et al (1992) also speculate that acoustic information might be used to filter out false positives for candidates matching two of their lexical patterns - repetitions of single words and cases of single inserted words - but do not report such experimentation. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).
Bear et al (1992) also speculate that acoustic information might be used to filter out false positives for candidates matching two of their lexical patterns - repetitions of single words and cases of single inserted words - but do not report such experimentation. $$$$$ In this section we describe first how prosodic information can help in distinguishing repairs from false positives for patterns involving matched words.

As noted in (Bear et al, 1992), knowledge about the location of word fragments would be an invaluable cue to both detection and correction of disfluencies. $$$$$ Integrating Multiple Knowledge Sources For Detection And Correction Of Repairs In Human-Computer Dialog
As noted in (Bear et al, 1992), knowledge about the location of word fragments would be an invaluable cue to both detection and correction of disfluencies. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).

For annotating speech repairs, we have extended the scheme proposed by Bear et al (1992) so that it better deals with overlapping and ambiguous repairs. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).
For annotating speech repairs, we have extended the scheme proposed by Bear et al (1992) so that it better deals with overlapping and ambiguous repairs. $$$$$ Since multiple repairs and false positives can occur in the same sentence, the pattern matching process is constrained to prefer fewer repairs to more repairs, and shorter repairs to longer repairs.

Bear et al (1992) explored pattern matching, parsing and acoustic cues and concluded that multiple sources of information would be needed to detect edit dis fluencies. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).
Bear et al (1992) explored pattern matching, parsing and acoustic cues and concluded that multiple sources of information would be needed to detect edit dis fluencies. $$$$$ We analyzed a subset of 607 sentences containing repairs and concluded that certain simple pattern-matching techniques could successfully detect a number of them.

The SRI group (Bear et al, 1992) employed simple pattern matching techniques for detecting and correcting modification repairs. $$$$$ In addition, we describe ways in which pattern matching, syntactic and semantic a.nalysis, and acoustic analysis can be helpful in detecting and correcting these repairs.
The SRI group (Bear et al, 1992) employed simple pattern matching techniques for detecting and correcting modification repairs. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).

(Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).
(Hindle, 1983) and (Bear et al., 1992) performed speech repair identification in their parsers, and removed the corrected material (reparandum) from consideration. $$$$$ Of these it successfully corrected 177.

This approach is similar to an experiment in (Bear et al, 1992) except that Bear et al were more interested in reducing false alarms. $$$$$ Our approach is most similar to that of Hindle.
This approach is similar to an experiment in (Bear et al, 1992) except that Bear et al were more interested in reducing false alarms. $$$$$ Table 1 shows examples of the notation used, which is described fully in Bear et al. (1992).

 $$$$$ Numbers in parentheses indicate the number of occurrences, and probabilities represent the likelihood that the phrase was actually a repair and not a false positive.
 $$$$$ We would also like to thank Robin Lickley for his feedback on the acoustics section, Elizabeth Wade for assistance with the statistics, and Mark Gawron for work on the Gemini grammar.
