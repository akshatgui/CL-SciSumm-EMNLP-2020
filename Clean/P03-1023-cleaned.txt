(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ In the MUC-6 data set, for example, the immediate antecedents of 95% pronominal anaphors can be found within the above distance.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ Without candidate filtering, the recall may rise as the correct antecedents would not be eliminated wrongly.

Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ To illustrate this problem, just suppose a data set where an instance could be described with four exclusive features

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).

learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ Coreference Resolution Using Competition Learning Approach
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ In this paper we have proposed a competition learning approach to coreference resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).

A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ By contrast, our approach adopts a twin-candidate learning model.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ One problem of the single-candidate model, however, is that it only takes into account the relationships between an anaphor and one individual candidate at a time, and overlooks the preference relationship between candidates.

Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ Compared with the gains in pronoun resolution, the improvement in non-pronoun resolution is slight.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ Although Connolly et al.’s approach also adopts the twin-candidate model, it achieves a poor performance for both pronoun resolution and nonpronoun resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

 $$$$$ Apparently, the classifier trained on the instance set { inst(ci , ana) }, T1, is equivalent to that trained on { inst(ci , c0, ana) }, T2.
 $$$$$ These features may be helpful to improve the performance of pronoun resolution.

On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ Compared with the baseline approach using the single-candidate model, the F-measure increases by 1.9 and 1.5 for MUC-6 and MUC-7 data set, respectively.

In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ Our competition learning approach adopts the twin-candidate model introduced in the Section 3.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ In this paper we have proposed a competition learning approach to coreference resolution.

Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ In particular, our approach obtains significant improvement (21.1 for MUC-6, and 13.1 for MUC-7) over Connolly et al.’s twin-candidate based approach.
Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ Compared with the baseline approach using the single-candidate model, the F-measure increases by 1.9 and 1.5 for MUC-6 and MUC-7 data set, respectively.
