The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ Generation Of Word Graphs In Statistical Machine Translation
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.

This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words â€” analogously to the word graph density in speech recognition.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ The effect of pruning on the graph error rate is shown in Table 3.

A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ As rest cost estimation, we use the probabilities determined in a backward pass as follows

The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ This method combines both advantages in the following way

The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.

Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ Generation Of Word Graphs In Statistical Machine Translation
Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.

The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ Generation Of Word Graphs In Statistical Machine Translation
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.

The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ ', which is then chosen by the single best search.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ The pruning is based on the beam search concept also used in the single best search

We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ The calculation of the graph error rate is performed by a dynamic programming based algorithm.

The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).
The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).

 $$$$$ If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.
 $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.

The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure.

(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.

The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).
The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.

We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ Then we apply word graph pruning with a threshold t < 1 and study the change of graph error rate (see Section 5).
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ Experiments have shown that the graph error rate significantly decreases for rising word graph densities.

A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability.

The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.

In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ One efficient way to store the different alternatives is a word graph.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.

For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ The pruning is based on the beam search concept also used in the single best search

For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).
