 $$$$$ The maximum likelihood estimate for each of the multinomial's distribution parameters, ai, is & = .
 $$$$$ The first author gratefully acknowledges the support of the Fulbright Foundation.

Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of Lewis and Gale (1994).
Active learning has been successfully applied to a number of natural language oriented tasks, including text categorization (Lewis and Gale, 1994) and part-of-speech tagging (Engelson and Dagan, 1996). $$$$$ Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994).

Engelson and Dagan (1996) experimented with QBC using HMMs for POS tagging and found that selective sampling of sentences can significantly reduce the number of samples required to achieve desirable tag accuracies. $$$$$ In bigram part-of-speech tagging the HMM model M contains three types of parameters

In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.
In all experiments, the agreement among the decision committee members is quantified by the Vote Entropy measure (Engelson and Dagan, 1996). $$$$$ Vote entropy is maximized when all committee members disagree, and is zero when they all agree.

Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe.
Active learning also has been applied to many NLP applications, including POS tagging (Engelson and Dagan, 1996) and parsing (Baldridge and Osborne, 2003). $$$$$ In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.

This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995).
This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). $$$$$ The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection.

A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.
A common metric to estimate the disagreement within an ensemble is the so-called vote entropy, the entropy of the distribution of labels li assigned to an example e by the ensemble of k classifiers (Engelson and Dagan, 1996). $$$$$ We define the selection probability as a linear function of vote entropy

AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)).
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ Our results suggest applying committee-based sample selection to other statistical NLP tasks which rely on estimating probabilistic parameters from an annotated corpus.

Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ We also show that sample selection yields a significant reduction in the size of the model used by the tagger.
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them. $$$$$ The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection.

QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.
QBC is based on the idea to select those examples for manual annotation on which a committee of classifiers disagree most in their predictions (Engelson and Dagan, 1996). $$$$$ Most simply, we can use a committee of size two and select an example when the two models disagree on its classification.

This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.
This is measured by the vote entropy (Engelson and Dagan, 1996), i.e., the entropy of the distribution of classifications assigned to an example by the classifiers. $$$$$ We define the selection probability as a linear function of vote entropy

Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ A more general algorithm results from allowing (i) a larger number of committee members, k, in order to sample P(MIS) more precisely, and (ii) more refined example selection criteria.
Engelson and Dagan (1996) confirm this observation that, in general, different (and even more refined) selection methods still yield similar results. $$$$$ We also find test set, comparable to other published results on bigram tagging. that, to a first approximation, all selection methods considered give similar results.

 $$$$$ The maximum likelihood estimate for each of the multinomial's distribution parameters, ai, is & = .
 $$$$$ The first author gratefully acknowledges the support of the Fulbright Foundation.
