Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ Learning Entailment Rules for Unary Templates
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).

Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Lin(l, r) ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules.

Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ 3.2 Unary Template Structure.
Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.

We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ 3.2 Unary Template Structure.
We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2).

DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR.

Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f

argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ Learning Entailment Rules for Unary Templates
argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ Such rules are leaned because many binary templates have a more complex structure than paths between arguments.

ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). $$$$$ (Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).
ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). $$$$$ All rules were learned in canonical form (Szpektor and Dagan, 2007).

Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ Learning Entailment Rules for Unary Templates
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ Most unsupervised rule learning algorithms focused on learning binary entailment rules.

 $$$$$ The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules.
 $$$$$ Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).

We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.
We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ is also generated.

Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.
Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ 2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate.

Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ (Romano et al, 2006).

apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ (Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother.
apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ The score of each generated rule is set to be the score of the original binary rule.The same unary rule can be derived from different binary rules.

We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). $$$$$ In addition, the Balanced-Inclusion measure outperformed all other tested methods.

In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ (Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ All rules were learned in canonical form (Szpektor and Dagan, 2007).

In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.
In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. $$$$$ We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.

Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ Learning Entailment Rules for Unary Templates
Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007).

Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f

Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ All rules were learned in canonical form (Szpektor and Dagan, 2007).
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges.
