Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ Our model generates word alignments that better respect the parse trees upon which they are conditioned, without sacrificing alignment quality.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.

We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ Using the simple but effective joint training technique of Liang et al. (2006), we initialized the model with lexical parameters from a jointly trained implementation of IBM Model 1.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.

When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ For Chinese, we trained on the FBIS corpus and the LDC bilingual dictionary, then tested on 491 hand-aligned sentences from the 2002 Hansards data from the NAACL 2003 Shared Task.3 We trained on 100k sentences for each language.
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ We observed a similar 50% reduction for the Chinese data.

We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ However, we found this variant to slightly underperform the full model described above.
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.

 $$$$$ Figure 1B exposes the consequences

This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ This model can be simplified by removing all conditioning on node types.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ We observed a similar 50% reduction for the Chinese data.

DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ However, its distortion model considers only string distance, disregarding the constituent structure of the English sentence.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ Lopez and Resnik (2005) considers a simpler tree distance distortion model.

This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ Consider the example sentence in figure 1A, which demonstrates how a particular type of alignment error prevents the extraction of many useful transducer rules.
This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ While alignment errors are undesirable in general, this error is particularly problematic for a syntax-based translation system.

The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ Using the joint training technique of Liang et al. (2006) to initialize the model parameters, we achieve an AER superior to the GIZA++ implementation of IBM model 4 (Och and Ney, 2003) and a reduction of 56.3% in aligned interior nodes, a measure of agreement between alignments and parses.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ Our models substantially outperform GIZA++, confirming results in Liang et al. (2006).

DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ While alignment errors are undesirable in general, this error is particularly problematic for a syntax-based translation system.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.

 $$$$$ Figure 1B exposes the consequences

We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ This alignment pattern was observed in our test set and corrected by our model.

 $$$$$ Figure 1B exposes the consequences
