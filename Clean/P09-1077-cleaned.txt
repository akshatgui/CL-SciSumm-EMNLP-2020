Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).

In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ Development testing showed that including features for all wordsâ€™ tags was not useful, so we include the Inquirer tags of only the verbs in the two arguments and their cross-product.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ Only word pairs were used as features for both.

Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ Verbs: These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class.
Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same.

Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ For all experiments, we used sections 2-20 of the PDTB for training and sections 21-22 for testing.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ Sections 0-1 were used as a development set for feature design.

Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ As each of the relations besides Expansion are infrequent, we train using equal numbers of positive and negative examples of the target relation.

Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ The performance using only our semantically informed features is shown in Table 7.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ Adding other features to word pairs leads to improved performance for Contingency, Expansion and Temporal relations, but not for Comparison.

Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ The performance using only our semantically informed features is shown in Table 7.
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ Our random baseline is the f-score one would achieve by randomly assigning classes in proportion to its true distribution in the test set.

 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
 $$$$$ We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.

Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ Our features provide 6% to 18% absolute improvements in f-score over the baseline for each of the four tasks.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ This combination led to a definite improvement, reaching an f-score of 47.13 (16% absolute improvement in f-score over Wordpairs-TextRels).

This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ In our experiments on implicits, the first and last words are not connectives.
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ The performance using only our semantically informed features is shown in Table 7.

Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).
Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal.

Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model.
Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ We trained a CRF classifier (Lafferty et al., 2001) over the sequence of implicit examples from all documents in sections 02 to 20.

Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ The absolute difference in f-score between the two models is close to 2% for Comparison, and 6% for Contingency.
Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ For detecting expansions, the best combination of our features (polarity+Inquirer tags+context) outperformed Wordpairs-PDTBImpl by a wide margin, close to 13% absolute improvement (fscores of 76.42 and 63.84 respectively).

(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ Each relation in the PDTB takes two arguments.
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ This makes sense, as Pitler et al. (2008) found that implicit contingencies are often found immediately following explicit comparisons.

Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ They delete the connective and use [Arg1, Arg2] as an example of an implicit relation.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ Each relation in the PDTB takes two arguments.

However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ Wordpairs-PDTBExpl In this case, the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ For all experiments, we used sections 2-20 of the PDTB for training and sections 21-22 for testing.

An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped.
An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ After removing word pairs that appear less than 5 times, the remaining features were ranked by information gain using the MALLET toolkit1.

Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ Given two text spans, previous work has used the cross-product of the words in the spans as features.
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ Only word pairs were used as features for both.

 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
 $$$$$ We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.

 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
 $$$$$ We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.
