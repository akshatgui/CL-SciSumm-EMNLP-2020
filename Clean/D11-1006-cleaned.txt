One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback.
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower.

Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ The key observation is that part-of-speech tags contain a significant amount of information for unlabeled dependency parsing.
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ First, part-of-speech tags contain a significant amount of information for parsing unlabeled dependencies.

More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ The study of unsupervised grammar induction has many merits.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008).

If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ The model is trained to update towards parses that are in high agreement with a source side English parse based on constraints drawn from alignments in the parallel data.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ For these experiments we still only use English-target parallel data because that is the format of the readily available data in the Europarl corpus.

It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ For the multi-source projected parser, the procedure is identical to that in Section 3.2 except that we use the multi-source direct transfer model to seed the algorithm instead of the English-only direct transfer model.

It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ The third (multi-dir.) is the multi-source direct transfer system.
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ For the multi-source projected system the results are mixed.

McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007).
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).

A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).

By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervised parsers by a significant margin.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ For example, when training and testing a parser on our English data, a parser with all features obtains an UAS of 89.3%3 whereas a delexicalized parser – a parser that only has nonlexical features – obtains an UAS of 82.5%.

We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ For example, when training and testing a parser on our English data, a parser with all features obtains an UAS of 89.3%3 whereas a delexicalized parser – a parser that only has nonlexical features – obtains an UAS of 82.5%.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1).

The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ Table 2 shows the matrix of source-target language UAS for all nine languages we consider (the original eight target languages plus English).
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks.

Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ An empirical comparison to Ganchev et al. (2009) is given in Section 5.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ Ganchev et al. also report results for Bulgarian.

 $$$$$ UAS for all sentence lengths without punctuation are given in Table 1.
 $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.

Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).
Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ We trained on sentences of length 10 or less and evaluated on all sentences from the test set.4 For DMV, we reversed the direction of all dependencies if this led to higher performance.

We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ For the multi-source projected parser, the procedure is identical to that in Section 3.2 except that we use the multi-source direct transfer model to seed the algorithm instead of the English-only direct transfer model.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ The third (multi-dir.) is the multi-source direct transfer system.

 $$$$$ UAS for all sentence lengths without punctuation are given in Table 1.
 $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.

The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ For this we used the Europarl corpus version 5 (Koehn, 2005).
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ Preliminary experiments using a different dependency parser – MSTParser (McDonald et al., 2005) – resulted in similar empirical observations.

Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ We use the augmented-loss learning procedure (Hall et al., 2011) which is closely related to constraint driven learning (Chang et al., 2007; Chang et al., 2010).
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ As previously mentioned, the latter has been explored in both Søgaard (2011) and Cohen et al. (2011).

Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ The first (best-source) is the direct transfer results for the oracle single-best source language per target language.

The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ In this section we examine whether this is also true for parser transfer.
