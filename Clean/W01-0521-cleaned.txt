Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ The lexical bigrams are contained in the most specific distribution for P,,,,.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data.

Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus.

Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.

A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ Corpus Variation And Parser Performance
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ We take as our baseline parser the statistical model of Model 1 of Collins (1997).

Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ Results are shown in Table 3.

When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.

See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.
See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.

We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ We conducted separate experiments using WSJ data, Brown data, and a combination of the two as training material.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.

Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ The large number of possible pairs of words in the vocabulary make the training data necessarily sparse.

Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.

Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ The standard WSJ task seems to be simplified by its homogenous style.

For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.
For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.

The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.

See Gildea (2001) for the exact setup. $$$$$ Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.
See Gildea (2001) for the exact setup. $$$$$ Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.

For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ The first distribution gives probability of the syntactic category H of the head child of a parent node with category P, head word Hhw with the head tag (the part of speech tag of the head word) Hht: The head word and head tag of the new node H are defined to be the same as those of its parent.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ (The head word of a parent is the same as the head word of its head child.)

For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.

In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ We take as our baseline parser the statistical model of Model 1 of Collins (1997).

Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.

They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ Combining the WSJ and Brown training data in one model improves performance further, but by less than 0.5% absolute.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ Perhaps the most striking result is just how little the elimination of lexical bigrams affects the baseline system: performance on the WSJ corpus decreases by less than 0.5% absolute.

Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing another.
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ Results are shown in Table 3.
