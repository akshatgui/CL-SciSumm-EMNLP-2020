Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ Combining outputs from different systems was shown to be quite successful in automatic speech recognition (ASR).

Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ Note that the word “have” in translation 2 is aligned to the word “like” in translation 1.
Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ Note that the extracted consensus translation can be different from the original M translations.

The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.

The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ The final alignments are determined using cost matrices defined by the state occupation probabilities of the trained HMM (Matusov et al., 2004).
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ Given the M−1 monotone one-to-one alignments, the transformation to a confusion network as described by (Bangalore et al., 2001) is straightforward.

Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ We use the IBM Model 1 (Brown et al., 1993) (uniform distribution) and the Hidden Markov Model (HMM, first-order dependency, (Vogel et al., 1996)) to estimate the alignment model.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ Given the M−1 monotone one-to-one alignments, the transformation to a confusion network as described by (Bangalore et al., 2001) is straightforward.

This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005).
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ Table 6 shows that the potential for improvement is significantly larger for the consensus-based combination of translation outputs than for simple selection of the best translation1.

 $$$$$ The training is performed in the directions En → Em and Em → En.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

 $$$$$ The training is performed in the directions En → Em and Em → En.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

 $$$$$ The training is performed in the directions En → Em and Em → En.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ The model parameters are trained iteratively in an unsupervised manner with the EM algorithm using the GIZA++ toolkit (Och and Ney, 2003).

Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ They are manually adjusted based on the performance of the involved MT systems on a held-out development set.
Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ In this experiment, the global system probabilities for scoring the confusion networks were tuned manually on a development set.

Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.
Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ Different applications of the proposed combination method have been evaluated.

Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ Using global system probabilities and other statistical models, the voting procedure selects the best consensus hypothesis from the confusion network.

Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ First, we focused on combining different MT systems which have the same source and target language.
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ From each system, we take the translation of the single best ASR output, and the translation of the ASR word lattice.

The basic concept of the approach has been described by Matusov et al (2006). $$$$$ Based on the alignment, we construct a confusion network from the (possibly reordered) translation hypotheses, similarly to the approach of (Bangalore et al., 2001).
The basic concept of the approach has been described by Matusov et al (2006). $$$$$ Given the M−1 monotone one-to-one alignments, the transformation to a confusion network as described by (Bangalore et al., 2001) is straightforward.

 $$$$$ The training is performed in the directions En → Em and Em → En.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.

And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ Based on the alignment, we construct a confusion network from the (possibly reordered) translation hypotheses, similarly to the approach of (Bangalore et al., 2001).
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ Given the M−1 monotone one-to-one alignments, the transformation to a confusion network as described by (Bangalore et al., 2001) is straightforward.

Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ Since each hypothesis may have an acceptable word order, we let every hypothesis play the role of the primary translation once, and thus align all pairs of hypotheses (En, Em); n =� m. In the following subsections, we will explain the word alignment procedure, the reordering approach, and the construction of confusion networks.

 $$$$$ The training is performed in the directions En → Em and Em → En.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
