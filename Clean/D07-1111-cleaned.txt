This technique, first proposed by Sagae and Lavie (2006), was used in the highest scoring system in both the multilingual track (Hall et al, 2007a) and the domain adaptation track (Sagae and Tsujii, 2007). $$$$$ 1044 task (Nivre et al, 2007), which differed from the 2006 edition by featuring two separate tracks, one in multilingual parsing, and a new track on domain adaptation for dependency parsers.
This technique, first proposed by Sagae and Lavie (2006), was used in the highest scoring system in both the multilingual track (Hall et al, 2007a) and the domain adaptation track (Sagae and Tsujii, 2007). $$$$$ Our system?s accuracy was the highest in the domain adaptation track (with labeled attachment score of 81.06%), and only 0.43% below the top scoring system in the multilingual parsing track (our average labeled attachment score over the ten languages was 79.89%).

The best performing (closed class) system in the domain adaptation track used a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis (Sagae and Tsujii, 2007). $$$$$ In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.
The best performing (closed class) system in the domain adaptation track used a combination of co-learning and active learning by training two different parsers on the labeled training data, parsing the unlabeled domain data with both parsers, and adding parsed sentences to the training data only if the two parsers agreed on their analysis (Sagae and Tsujii, 2007). $$$$$ SVM models using the out-of-domain labeled training data; 2.

Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. $$$$$ We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabil istic generalized LR dependency parsing.
Sagae and Tsujii (2007) apply a variant of co-training to dependency parsing and report positive results on out-of-domain text. $$$$$ In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-of domain training set, in a scheme similar to one iteration of co-training.

Lastly we used a native dependency parser, the GENIA Dependency parser (GDep) by Sagae and Tsujii (2007). $$$$$ Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles
Lastly we used a native dependency parser, the GENIA Dependency parser (GDep) by Sagae and Tsujii (2007). $$$$$ We then show how we extend the deterministic parser into a best first probabilistic parser.

As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test). $$$$$ For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows.
As far as pre-processing is concerned, we imported the sentence splitting, tokenization and GDep parsing results (Sagae and Tsujii, 2007) as prepared by the shared task organizers for all data sets (training, development and test). $$$$$ Acknowledgements We thank the shared task organizers and treebank providers.

This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al, 2007). $$$$$ 1044 task (Nivre et al, 2007), which differed from the 2006 edition by featuring two separate tracks, one in multilingual parsing, and a new track on domain adaptation for dependency parsers.
This approach is similar to the one used in (Sagae and Tsujii, 2007), which achieved the highest scores in the domain adaptation track of the CoNLL 2007 shared task (Nivre et al, 2007). $$$$$ See (Nivre et al, 2007).

The parse trees were produced by the GDep parser (Sagae and Tsujii, 2007) and supplied by the challenge organisers. $$$$$ We entered a system based on the approach de scribed in this paper in the CoNLL 2007 shared trees.
The parse trees were produced by the GDep parser (Sagae and Tsujii, 2007) and supplied by the challenge organisers. $$$$$ We then used each of the models to parse the.

While the TURKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the JULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007), the TOKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies. $$$$$ For each of the ten languages for which training data was provided in the multilingual track of the CoNLL 2007 shared task, we trained three LR models as follows.
While the TURKU system exploits the Stanford dependencies from the McClosky-Charniak parser (Charniak and Johnson, 2005), and the JULIELab system uses the CoNLL-like dependencies from the GDep parser (Sagae and Tsujii, 2007), the TOKYO system overlays the Shared Task data with two parsing representations, viz. Enju PAS structure (Miyao and Tsujii, 2002) and GDep parser dependencies. $$$$$ The likely reason for this difference is that over 80% of the dependencies in the Turkish data set have the head to the right of 1048 the dependent, while only less than 4% have the head to the left.

GDep (Sagae and Tsujii, 2007), a native dependency parser. $$$$$ Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles
GDep (Sagae and Tsujii, 2007), a native dependency parser. $$$$$ We then show how we extend the deterministic parser into a best first probabilistic parser.

 $$$$$ In addi tion to deciding the direction of a reduce action, the label of the newly formed dependency arc must also be decided.
 $$$$$ This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.

Sagae and Tsujii (2007) used the co-training technique to improve performance. $$$$$ ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be ob tained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algo rithms are used, although this is not pursued here); and, finally, 4.
Sagae and Tsujii (2007) used the co-training technique to improve performance. $$$$$ We then provide an analysis of the results obtained with our system, and discuss possible improve ments.

Sagae and Tsujii (2007) presented an co training approach for dependency parsing adaptation. $$$$$ Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles
Sagae and Tsujii (2007) presented an co training approach for dependency parsing adaptation. $$$$$ We first describe our approach to multilingual dependency parsing, fol lowed by our approach for domain adaptation.

Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shift reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. $$$$$ commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.
Dependency parsing has been performed with the GENIA dependency parser GDep (Sagae and Tsujii, 2007), which uses a best-first probabilistic shift reduce algorithm based on the LR algorithm (Knuth, 1965) and extended by the pseudo-projective parsing technique. $$$$$ pendency Parsing Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm based on the LR algorithm (Knuth, 1965).

Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. $$$$$ commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.
Sagae and Tsujii (2007) generalized the standard deterministic framework to probabilistic parsing by using a best-first search strategy. $$$$$ We then show how we extend the deterministic parser into a best first probabilistic parser.

For sentences in the dataset, their dependency structures are extracted using GENIA Dependency parser (Sagae and Tsujii, 2007), and phrase structure using Brown self-trained biomedical parser (McClosky, 2009). $$$$$ Dependency Parsing and Domain Adaptation with LR Models and Parser Ensembles
For sentences in the dataset, their dependency structures are extracted using GENIA Dependency parser (Sagae and Tsujii, 2007), and phrase structure using Brown self-trained biomedical parser (McClosky, 2009). $$$$$ parser domain adaptation using unlabeled data in the target domain.

 $$$$$ In addi tion to deciding the direction of a reduce action, the label of the newly formed dependency arc must also be decided.
 $$$$$ This work was supported in part by Grant-in-Aid for Specially Promoted Re search 18002007.

The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods. $$$$$ Interes tingly, the ensemble used in the multilingual track also produced good results on the development set for the domain adaptation data, without the use of the unlabeled data at all, with a score of 81.9 (al though the ensemble is more expensive to run).
The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble-based methods. $$$$$ A similar idea that may be more effective, but requires more effort, is to add parsers based on dif ferent approaches.

For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDepbeta2 (Sagae and Tsujii, 2007) parsers. $$$$$ word to the beginning of every sentence, which is used as the head of every word in the dependency structure that does not have a head in the sentence.
For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDepbeta2 (Sagae and Tsujii, 2007) parsers. $$$$$ See (Nivre et al, 2007).

Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. $$$$$ commonly been performed using parsing algo 1 Stepwise parsing considers each step in a parsing algo rithm separately, while all-pairs parsing considers entire rithms designed specifically for this task, such as those described by Nivre (2003) and Yamada and Matsumoto (2003), we show that this can also be done using the well known LR parsing algorithm (Knuth, 1965), providing a connec tion between current research on shift-reduce dependency parsing and previous parsing work using LR and GLR models; wise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Rat naparkhi (1997) and later by Sagae and Lavie (2006); 3.
Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm. $$$$$ pendency Parsing Our overall parsing approach uses a best-first probabilistic shift-reduce algorithm based on the LR algorithm (Knuth, 1965).

See Sagae and Tsujii (2007) for more information on the parser. $$$$$ Instead, we use a classifier with features derived from much of the same information contained in an LR table
