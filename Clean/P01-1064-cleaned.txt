CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). $$$$$ A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000).
CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). $$$$$ This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6 He evaluated (Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that achieved the best performance among these systems.

Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs. $$$$$ Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998).
Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs. $$$$$ For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998).

 $$$$$ We also assume that the words within the scope of a topic are statistically independent of each other given the topic.
 $$$$$ We thank Freddy Y. Y. Choi for his text segmentation package.

(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ Find the minimum-cost path from to .
(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ Algorithms for finding the minimum-cost path in a graph are well known.

The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. $$$$$ In this paper, we assume that the segment boundaries are at the ends of sentences.
The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. $$$$$ The segmentation boundaries were placed at the ends of sentences.

U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000).
U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ A probabilistic approach, however, has already been proposed by Yamron, et al. for domain-dependent text segmentation (broadcast news story segmentation) (Yamron et al., 1998).

In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. $$$$$ A Statistical Model For Domain-Independent Text Segmentation
In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. $$$$$ The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3.

Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. $$$$$ Hearst (1994), for example, used only the similarity of word distributions in a given text to segment the text.
Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. $$$$$ This means that our system is more accurate than or at least as accurate as previous domainindependent text segmentation systems, because has been shown to be more accurate than previous domain-independent text segmentation systems.10

This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. $$$$$ A Statistical Model For Domain-Independent Text Segmentation
This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. $$$$$ We assume that different topics have different word distributions.

In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. $$$$$ A Statistical Model For Domain-Independent Text Segmentation
In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. $$$$$ We have proposed a statistical model for domainindependent text segmentation.

Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. $$$$$ The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3.
Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. $$$$$ An example of is shown in Figure 1.

Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. $$$$$ Previous approaches usually used lexical cohesion to segment texts into topics.
Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. $$$$$ Therefore, a solution can be obtained by applying a dynamic programming (DP) algorithm.4 DP algorithms have also been used for text segmentation by other researchers (Ponte and Croft, 1997; Heinonen, 1998).

 $$$$$ We also assume that the words within the scope of a topic are statistically independent of each other given the topic.
 $$$$$ We thank Freddy Y. Y. Choi for his text segmentation package.

 $$$$$ We also assume that the words within the scope of a topic are statistically independent of each other given the topic.
 $$$$$ We thank Freddy Y. Y. Choi for his text segmentation package.

Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ Previous approaches usually used lexical cohesion to segment texts into topics.
Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ The segmentation boundaries were placed at the ends of sentences.

Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). $$$$$ In this application, systems relying on supervised learning (Yamron et al., 1998; Beeferman et al., 1999) achieve good performance because there are plenty of training data in the domain.
Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). $$$$$ For English, Klavans, et al. describe a segmentation corpus in which the texts were segmented by humans (Klavans et al., 1998).

A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). $$$$$ The sample texts were preprocessed – i.e., punctuation and stop words were removed and the remaining words were stemmed – by a program using the libraries available in Choi’s package.
A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). $$$$$ It is important, however, to assess the performance of systems by using real texts.

It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). $$$$$ We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of segments was somewhat larger than that of the minimum-cost path, whose number of segments was automatically determined by the algorithm.
It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). $$$$$ Another major difference from their algorithm is that our algorithm does not require training data to estimate probabilities, while their algorithm does.

In this paper, we selected for comparison three systems based merely on the lexical reiteration feature $$$$$ A lot of research has been done on text segmentation (Kozima, 1993; Hearst, 1994; Okumura and Honda, 1994; Salton et al., 1996; Yaari, 1997; Kan et al., 1998; Choi, 2000; Nakao, 2000).
In this paper, we selected for comparison three systems based merely on the lexical reiteration feature $$$$$ This data was used by Choi (2000) to compare various domain-independent text segmentation systems.6 He evaluated (Choi, 2000), TextTiling (Hearst, 1994), DotPlot (Reynar, 1998), and Segmenter (Kan et al., 1998) by using the data and reported that achieved the best performance among these systems.

The TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below. $$$$$ The statistical model for the algorithm is described in Section 2, and the algorithm for obtaining the maximum-probability segmentation is described in Section 3.
The TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below. $$$$$ In the next section, we then describe the algorithm for selecting the most likely segmentation.
