A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ Different techniques have been used, including cosine similarity of vectors representing text in various ways (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), generative machine learning models (Rudner and Liang, 2002), domain-specific feature extraction (Attali and Burstein, 2006), and/or modified syntactic parsers (Lonsdale and Strong-Krause, 2003).
A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al, 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al, 2011). $$$$$ Intelligent Essay Assessor (IEA) (Landauer et al., 2003) uses Latent Semantic Analysis (LSA) (Landauer and Foltz, 1998) to compute the semantic similarity between texts, at a specific grade point, and a test text.

Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
Finally, we explore the utility of our best model for assessing the incoherent 'outlier' texts used in Yannakoudakis et al. (2011). $$$$$ Finally, features whose overall frequency is lower than four are discarded from the model.

Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.
Also, in Yannakoudakis et al (2011), experiments are presented that test the validity of the system using a number of automatically-created 'outlier' texts. $$$$$ Preliminary experiments based on a set of ‘outlier’ texts have shown the types of texts for which the system’s scoring capability can be undermined.

We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.
We use the First Certificate in English (FCE) ESOL examinationscripts2 (upper-intermediate level assessment) described in detail in Yannakoudakis et al (2011), extracted from the Cambridge Learner Corpus3 (CLC). $$$$$ For the purpose of this work, we extracted scripts produced by learners taking the First Certificate in English (FCE) exam, which assesses English at an upper-intermediate level.

As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ We parsed the training and test data (see Section 2) using the Robust Accurate Statistical Parsing (RASP) system with the standard tokenisation and sentence boundary detection modules (Briscoe et al., 2006) in order to broaden the space of candidate features suitable for the task.
As in Yannakoudakis et al (2011), we analyze all texts using the RASP toolkit (Briscoe et al, 2006). $$$$$ P´erez-Marin et al. (2009), Williamson (2009), Dikli (2006) and Valenti et al.

Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ In particular, we use rank preference learning to explicitly model the grade relationships between scripts.
Among the features used in Yannakoudakis et al (2011), none explicitly captures coherence and none models inter sentential relationships. $$$$$ It is clear from the ‘outlier’ experiments reported here that our system would benefit from features assessing discourse coherence, and to a lesser extent from features assessing semantic (selectional) coherence over longer bounds than those captured by ngrams.

In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ Our data consists of 1141 scripts from the year 2000 for training written by 1141 distinct learners, and 97 scripts from the year 2001 for testing written by 97 distinct learners.
In the following experiments, we evaluate the best model identified on year 2000 on a set of 97 texts from the exam year 2001, previously used in Yannakoudakis et al (2011) to report results of the final best system. $$$$$ There is no overlap between the prompts used in 2000 and in 2001.

 $$$$$ RASP’s rule names are semiautomatically generated and encode detailed information about the grammatical constructions found (e.g.
 $$$$$ Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.

See Yannakoudakis et al (2011) for details. $$$$$ However, Powers et al. (2002) invited writing experts to trick the scoring capabilities of an earlier version of e-Rater (Burstein et al., 1998). e-Rater (see Section 6 for more details) assigns a score to a text based on linguistic feature types extracted using relatively domain-specific techniques.
See Yannakoudakis et al (2011) for details. $$$$$ P´erez-Marin et al. (2009), Williamson (2009), Dikli (2006) and Valenti et al.

Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Recently, Chen et al. (2010) has proposed an unsupervised approach to AA of texts addressing the same topic, based on a voting algorithm.
Their correction detection algorithm relies on a set of heuristics developed from one single data collection (the FCE corpus (Yannakoudakis et al, 2011)). $$$$$ Briscoe et al. (2010) describe an approach to automatic offprompt detection which does not require retraining for each new question prompt and which we plan to integrate with our system.

We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ In order to estimate the error-rate, we build a trigram language model (LM) using ukWaC (ukWaC LM) (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens.
We considered four corpora with different ESL populations and annotation standards, including FCE corpus (Yannakoudakis et al, 2011), NUCLE corpus (Dahlmeier et al, 2013), UIUCcor pus 2 (Rozovskaya and Roth, 2010) and HOO2011 corpus (Dale and Kilgarriff, 2011). $$$$$ P´erez-Marin et al. (2009), Williamson (2009), Dikli (2006) and Valenti et al.

The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ Next, we extend our language model with trigrams extracted from a subset of the texts contained in the CLC (CLC LM).
The CLC-FCE sub corpus was extracted, anonymized, and made available as a set of XML files by Yannakoudakis et al (2011). $$$$$ We extracted 6 high-scoring FCE scripts from the CLC that do not overlap with our training and test data.

Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ In developing our AA system, a number of different grammatical complexity measures were extracted from parses, and their impact on the accuracy of the system was explored.
Errors have been shown to have a significant impact on predicting learner level (Yannakoudakis et al, 2011). $$$$$ P´erez-Marin et al. (2009), Williamson (2009), Dikli (2006) and Valenti et al.

By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance.
By running ablation studies - i.e., removing one or more sets of features (cf. e.g. (Yannakoudakis et al., 2011)) - we can determine their relative importance and usefulness. $$$$$ An ablation test consists of removing one feature of the system at a time and re-evaluating the model on the test set.

We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ We make such a dataset of ESOL examination scripts available1 (see Section 2 for more details), describe our novel approach to the task, and provide results for our system on this dataset.
We extend our n-gram-based data-driven prediction approach from the Helping Our Own (HOO) 2011 Shared Task (Boyd and Meurers, 2011) to identify determiner and preposition errors in non-native English essays from the Cambridge Learner Corpus FCE Dataset (Yannakoudakis et al, 2011) as part of the HOO 2012 Shared Task. $$$$$ The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.

The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.
The documents are a subset of the 1,244 document sin the Cambridge Learner Corpus FCE (First Certificate in English) data set (Yannakoudakis et al, 2011). $$$$$ For the purpose of this work, we extracted scripts produced by learners taking the First Certificate in English (FCE) exam, which assesses English at an upper-intermediate level.

A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.
A suitable corpus for developing this program is the Cambridge Learner Corpus (CLC) (Yannakoudakis et al, 2011). $$$$$ We would like to thank Cambridge ESOL, a division of Cambridge Assessment, for permission to use and distribute the examination scripts.

The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ We make such a dataset of ESOL examination scripts available1 (see Section 2 for more details), describe our novel approach to the task, and provide results for our system on this dataset.
The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al, 2011). $$$$$ Next, we extend our language model with trigrams extracted from a subset of the texts contained in the CLC (CLC LM).

For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.
For the shared task, we made use of data drawn from the CLC FCE Dataset, a set of 1,244 exam scripts written by candidates sitting the Cambridge ESOL First Certificate in English (FCE) examination in 2000 and 2001, and made available by Cambridge Universiy Press; see (Yannakoudakis et al, 2011). $$$$$ We would like to thank Cambridge ESOL, a division of Cambridge Assessment, for permission to use and distribute the examination scripts.

We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus.
We also apply our visualiser to a set of 1,244 publically available FCE ESOL texts (Yannakoudakis et al, 2011) and make it available as a web service to other researchers. $$$$$ Although there are many published analyses of the performance of individual systems, as yet there is no publically available shared dataset for training and testing such systems and comparing their performance.
