One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.

Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ It does so by building statistical models for content selection and surface realization.
Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ We then tested mixtures of the lexical and POS models, lexical and positional models, and all three models combined together.

The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ Headline Generation Based On Statistical Translation
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.

Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Headline Generation Based On Statistical Translation
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Finally, to simplify parameter estimation for the content selection model, we can assume that the likelihood of a word in the summary is independent of other words in the summary.

Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ In the simplest, zerolevel model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search (Forney, 1973) to efficiently find a near-optimal summary.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ As can be seen in Table 2,7 Although adding the POS information alone does not seem to provide any benefit, positional information does.

Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Headline Generation Based On Statistical Translation
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ When used in combination, each of the additional information sources seems to improve the overall model of summary generation.

Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ Headline Generation Based On Statistical Translation
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ As can be seen, even with such an impoverished language model, the system does quite well: when the generated headlines are four words long almost one in every five has all of its words matched in the article s actual headline.

Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ Figure 2 shows the distribution of headline length.
Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ As can be seen, a Gaussian distribution could also model the likely lengths quite accurately.

This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).
This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.

For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ Headline Generation Based On Statistical Translation
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ It does so by building statistical models for content selection and surface realization.

Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ This paper has presented an alternative to extractive summarization: an approach that makes it possible to generate coherent summaries that are shorter than a single sentence and that attempt to conform to a particular style.

Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ In the simplest, zerolevel model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search (Forney, 1973) to efficiently find a near-optimal summary.
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ 2 Other statistical models might require the use of a different heuristic search algorithm.

In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ In a similar vein, a summarizer can be considered to be ‘translating’ between two languages: one verbose and the other succinct (Berger and Lafferty, 1999; Witbrock and Mittal, 1999).

From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Most previous work on summarization has focused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Given good training corpora, this approach can also be used to generate headlines from a variety of formats: in one case, we experimented with corpora that contained Japanese documents and English headlines.

For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ These extracts are then arranged in a linear order (usually the same order as in the original document) to form a summary document.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ The simplest model for document length is a fixed length based on document genre.
