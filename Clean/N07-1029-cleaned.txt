This work was extended in (Rosti et al, 2007) by introducing system weights for word confidences. $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).
This work was extended in (Rosti et al, 2007) by introducing system weights for word confidences. $$$$$ This work extends the approach proposed in (Sim et al., 2007).

The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al, 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. $$$$$ The third combination method is based on confusion network decoding.
The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al, 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. $$$$$ Six systems trained on all data available for GALE 2006 evaluation were used in the experiments to demonstrate the performance of all three system combination methods on Arabic and Chinese to English MT tasks.

Compared to the baseline from (Rosti et al, 2007), the new method improves the BLEU scores significantly. $$$$$ It was found that aligning more than one hypothesis ( ) from each system to the skeleton improves the combination outputs.
Compared to the baseline from (Rosti et al, 2007), the new method improves the BLEU scores significantly. $$$$$ It was found that minimizing TER on Arabic also resulted in higher BLEU scores compared to the best single system.

The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).
The subnetworks in the latter approach may be weighted by prior probabilities estimated from the alignment statistics (Rosti et al, 2007a). $$$$$ This work extends the approach proposed in (Sim et al., 2007).

Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al, 2007a; Huang and Papineni, 2007). $$$$$ The phrase-level combination is based on extracting a new phrase translation table from each system’s target-to-source phrase alignments and re-decoding the source sentence using this new translation table and a language model.
Confusion network based system combination for machine translation has shown promising advantage compared with other techniques based system combination, such as sentence level hypothesis selection by voting and source sentence re-decoding using the phrases or translation models that are learned from the source sentences and target hypotheses pairs (Rosti et al, 2007a; Huang and Papineni, 2007). $$$$$ The third combination method is based on confusion network decoding.

Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al 2007). $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).
Among the four steps, the hypothesis alignment presents the biggest challenge to the method due to the varying word orders between outputs from different MT systems (Rosti et al 2007). $$$$$ Testing the word-level combination has the same steps as the tuning apart from steps 6 and 7.

Sim et al (2007), Rosti et al (2007a), and Rosti et al (2007b) used minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network. $$$$$ Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis.
Sim et al (2007), Rosti et al (2007a), and Rosti et al (2007b) used minimum Translation Error Rate (TER) (Snover et al, 2006) alignment to build the confusion network. $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).

Similar to (Rosti et al, 2007a), each word in the hypothesis is assigned with a rank-based score of 1/(1+r), where r is the rank of the hypothesis. $$$$$ The confidence score assigned to each word was chosen to be where the was based on the rank of the aligned hypothesis in the system’s best.
Similar to (Rosti et al, 2007a), each word in the hypothesis is assigned with a rank-based score of 1/(1+r), where r is the rank of the hypothesis. $$$$$ Re-rank the -best list using the new weights.

The system used in this paper is a variant of the one proposed in Rosti et al (2007a), which we now describe in detail. $$$$$ Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis.
The system used in this paper is a variant of the one proposed in Rosti et al (2007a), which we now describe in detail. $$$$$ This work extends the approach proposed in (Sim et al., 2007).

Meanwhile, we also use a word-level combination framework (Rosti et al, 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. $$$$$ This method does not generate new hypotheses – unlike the phrase and word-level methods.
Meanwhile, we also use a word-level combination framework (Rosti et al, 2007) to combine the multiple translation hypotheses and employ a new rescoring model to generate the final result. $$$$$ Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination.

We also implemented the word-level system combination (Rosti et al, 2007) and the hypothesis selection method (Hildebrand and Vogel, 2008). $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).
We also implemented the word-level system combination (Rosti et al, 2007) and the hypothesis selection method (Hildebrand and Vogel, 2008). $$$$$ The word-level combination method described so far does not require any tuning.

 $$$$$ A confidence score from each system is assigned to each unique hypothesis in the merged list.
 $$$$$ The authors would like to thank ISI and University of Edinburgh for sharing their MT system outputs.

The current state-of-the-art is confusion-network-based MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b). $$$$$ The third combination method is based on confusion network decoding.
The current state-of-the-art is confusion-network-based MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b). $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).

Bangalore et al (2001) used a multiple string matching algorithm based on Levenshtein edit distance, and later Sim et al (2007) and Rosti et al (2007) extended it to a TER-based method for hypothesis alignment. $$$$$ Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis.
Bangalore et al (2001) used a multiple string matching algorithm based on Levenshtein edit distance, and later Sim et al (2007) and Rosti et al (2007) extended it to a TER-based method for hypothesis alignment. $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).

Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. $$$$$ The word-level combination is based on consensus network decoding.
Similar to (Rosti et al, 2007), each word in the confusion network is associated with a word posterior probability. $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).

Various techniques include hypothesis selection from different systems using sentence-level scores, re-decoding source sentences using phrases that are used by individual systems (Rosti et al., 2007a; Huang and Papineni, 2007) and word-based combination techniques using confusion networks (Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b). $$$$$ Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis.
Various techniques include hypothesis selection from different systems using sentence-level scores, re-decoding source sentences using phrases that are used by individual systems (Rosti et al., 2007a; Huang and Papineni, 2007) and word-based combination techniques using confusion networks (Matusov et al., 2006; Sim et al., 2007; Rosti et al., 2007b). $$$$$ So far, confusion networks have been applied in MT system combination using three different alignment procedures: WER (Bangalore et al., 2001), GIZA++ alignments (Matusov et al., 2006) and TER (Sim et al., 2007).

Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. $$$$$ The phrase-level combination is based on extracting a new phrase translation table from each system’s target-to-source phrase alignments and re-decoding the source sentence using this new translation table and a language model.
Rosti et al (2007a) collect source-to-target correspondences from the input systems, create a new translation option table using only these phrases, and re-decode the source sentence to generate better translations. $$$$$ The phrase translation table is generated for each source sentence using confidence scores derived from sentence posteriors with system-specific total score scaling factors and similarity scores based on the agreement among the phrases from all systems.

For selecting the best skeleton, two common methods are choosing the hypothesis with the Minimum Bayes Risk with translation error rate (TER) (Snover et al, 2006) (i.e., the hypothesis with the minimum TER score when it is used as the reference against the other hypotheses) (Sim et al, 2007) or choosing the best hypotheses from each system and using each of those as a skeleton in multiple confusion networks (Rosti et al, 2007b). $$$$$ Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and minimum Bayes risk decoding under TER (Sim et al., 2007) is used to select the alignment hypothesis.
For selecting the best skeleton, two common methods are choosing the hypothesis with the Minimum Bayes Risk with translation error rate (TER) (Snover et al, 2006) (i.e., the hypothesis with the minimum TER score when it is used as the reference against the other hypotheses) (Sim et al, 2007) or choosing the best hypotheses from each system and using each of those as a skeleton in multiple confusion networks (Rosti et al, 2007b). $$$$$ In speech recognition, this results in minimum expected word error rate (WER) hypothesis (Mangu et al., 2000) or equivalently minimum Bayes risk (MBR) under WER with uniform target sentence posterior distribution (Sim et al., 2007).

Rosti et al (2007) look at sentence-level combinations (as well as word and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. $$$$$ The sentence-level combination is based on selecting the best hypothesis out of the merged N-best lists.
Rosti et al (2007) look at sentence-level combinations (as well as word and phrase-level), using reranking of n-best lists and confidence scores derived from generalised linear models with probabilistic features from n-best lists. $$$$$ Both sentence and phrase-level combination methods can generate best lists which may also be used as new system outputs in the word-level combination.

A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al, 2007). $$$$$ Combined with the latest advances in phrase-based translation systems, it has become more attractive to take advantage of the various outputs in forming consensus translations (Frederking and Nirenburg, 1994; Bangalore et al., 2001; Jayaraman and Lavie, 2005; Matusov et al., 2006).
A new search space is constructed from these backbone-aligned outputs, and then a voting procedure or feature-based model predicts a final consensus translation (Rosti et al, 2007). $$$$$ The word-level combination is based on consensus network decoding.
