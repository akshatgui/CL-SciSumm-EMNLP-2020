We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.

In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).

We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.

One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ We are aware of two comparable previous approaches.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = −0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).

Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ We used the text from 1999 through 2008 as in-domain training data, and we used the first 2000 sentences from January 2009 as test data.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.

In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ Lin et al. (1997) and Gao et al.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.

For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.

Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ To implement our data selection method we required one language model trained on the Europarl training data and one trained on the Gigaword data.

This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ Lin et al. (1997) and Gao et al.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ (2002).

Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).

Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ Klakow (2000) estimates a unigram language model from the entire non-domain-specific corpus to be selected from, and scores each candidate text segment from that corpus by the change in the log likelihood of the in-domain data according to the unigram model, if that segment were removed from the corpus used to estimate the unigram model.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ To make these language models comparable, and to show the feasibility of optimizing the fit to the in-domain data without training a model on the entire Gigaword corpus, we trained the Gigaword language model for data selection on a random sample of the Gigaword corpus of a similar size to that of the Europarl training data: 1,874,051 sentences, 48,459,945 tokens.

Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ The test set perplexity for the language model trained on the full Gigaword corpus is 135.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.

We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. $$$$$ However, instead of scoring text segments by perplexity or cross-entropy according to the in-domain language model, we score them by the difference of the cross-entropy of a text segment according to the in-domain language model and the cross-entropy of the text segment according to a language model trained on a random sample of the data source from which the text segment is drawn.
We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. $$$$$ We computed the cross-entropy of each sentence in the Gigaword corpus according to both models, and scored each sentence by the difference in cross-entropy, HEp(s)−HG,,,(s).

For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.

However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ Intelligent Selection of Language Model Training Data
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.

For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ For the in-domain corpus, we chose the English side of the English-French parallel text from release v5 of the Europarl corpus (Koehn, 2005).
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.

We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ Lin et al. (1997) and Gao et al.
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.

These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ Equivalently, we can work in the log domain with the quantity log(P(s

We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.

It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.
