Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ Indeed, Liang et al (2006) employed an averaged perceptron algorithm in which ? value was always set to one.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ Tillmann and Zhang (2006) and Liang et al (2006) solved the problem by introducing a sentence-wise BLEU.

Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ Online Large-Margin Training for Statistical Machine Translation
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).

MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ L(e?, e?; et) ?(e?, e?)
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ )

The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ 3.2 Sparse Features.
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al,2006a).

Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ 4.1 Margin Infused Relaxed Algorithm.
Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).

Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ 3.2 Sparse Features.
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al,2006a).

(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ and Arabic.
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ Thedevelopment set comes from the MT2003 Arabic English NIST evaluation test set consisting of 663 sentences in the news domain with four reference translations.

In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ MIRA is successfully employed in dependency parsing (McDonald et al, 2005) or the joint-labeling/chunking task (Shimizu and Haas,2006).
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al, 2006) is an online version of thelarge-margin training algorithm for structured classification (Taskar et al, 2004) that has been suc cessfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).

The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ Online Large-Margin Training for Statistical Machine Translation
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus.
