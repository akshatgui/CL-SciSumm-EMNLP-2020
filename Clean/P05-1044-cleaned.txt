Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ 2 as contrastive estimation (CE).

Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ 2 as contrastive estimation (CE).
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ DEL1SUBSEQ and DEL1WORD are poor because they do not give helpful classes of negative evidence: deleting a word or a short subsequence often does very little damage.

 $$$$$ Of course, the validity of this hypothesis will depend on the form of the neighborhood function.
 $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.

We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). $$$$$ CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).
We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). $$$$$ We compare CE (using neighborhoods from §4) with EM on POS tagging using unlabeled data.

Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ Unsupervised model selection.
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ For each (criterion, dataset) pair, we selected the smoothing trial that gave the highest estimation criterion score on a 5K-word development set (also unlabeled).

The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ The log-linear models trained by CE used the same feature set, though the feature weights are no longer log-probabilities and there are no sum-to-one constraints.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ To allow more trials, we projected the original 45 tags onto a coarser set of 17 (e.g., RB* ADV).

Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ A diluted dictionary adds (tag, word) entries so that rare words are allowed with any tag, simulating zero prior knowledge about the word.
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ This is good for the LENGTH objective, but not for learning good POS tag sequences.

We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ The other reason to use CE is to improve accuracy.
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ The learner we describe here takes into account not only the observed positive example, but also a set of similar but deprecated negative examples.

 $$$$$ Of course, the validity of this hypothesis will depend on the form of the neighborhood function.
 $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.

Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ This is good for the LENGTH objective, but not for learning good POS tag sequences.

The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ 2 as contrastive estimation (CE).
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ Because the features can take any form and need not be orthogonal, log-linear models can capture arbitrary dependencies in the data and cleanly incorporate them into a model.

First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ To maximize the neighborhood likelihood (Eq.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.

 $$$$$ Of course, the validity of this hypothesis will depend on the form of the neighborhood function.
 $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.

System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing (Smith and Eisner, 2005).

Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ 2 as contrastive estimation (CE).

Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ 2 as contrastive estimation (CE).

The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).

Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ In Smith and Eisner (2005), we define a sentence’s neighborhood to be a set of slightly-altered sentences that use the same lexemes, as suggested at the start of this section.
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ Good neighborhoods, rather, perform well in their own right.

One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ CE offers an additional way to inject domain knowledge into unsupervised learning (Smith and Eisner, 2005).
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.

This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ 2 as contrastive estimation (CE).
