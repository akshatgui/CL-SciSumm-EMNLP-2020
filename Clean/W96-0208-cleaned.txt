This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). $$$$$ In particular, the UCI Machine Learning Data Repository (Merz, Murphy, & Aha, 1996) was assembled to facilitate empirical comparisons.
This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). $$$$$ Similar comparisons of a range of algorithms should also be performed on other natural language problems such as part-of-speech tagging (Church, 1988), prepositional phrase attachment (Hindle 8,/ Rooth, 1993), anaphora resolution (Anoe & Bennett, 1995), etc..

Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ Naive Bayes and perceptron are similar in that they both employ a weighted combination of all features.
Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ Therefore, our results indicate that lexical disambiguation is perhaps best performed using methods that combine weighted evidence from all of the features rather tures actually present in the examples.

While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. $$$$$ This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.
While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. $$$$$ Naive Bayes and perceptron are similar in that they both employ a weighted combination of all features.

Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). $$$$$ Comparative Experiments On Disambiguating Word Senses

Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ This provides information on the computational resources required by each method, which may also be useful in deciding between them for particular applications.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ The current results are for only one simple encoding of the lexical disambiguation problem into a feature vector representing an unordered set of word stems.

Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ Subsequent experiments on this problem have demonstrated that an inductive logic programming method produces even better results than decision trees (Mooney & Califf, 1995).
Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ Decision-tree methods have a bias for simple decision trees, rule induction methods have a bias for simple DNF expressions, neural-network methods have a bias for linear threshold functions, 1 and naive Bayes has a bias for functions which respect conditional independence of features.

Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. $$$$$ The resulting learning curves are shown in Figure 1 and results on training and testing time are shown in Figures 2 and 3.
Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. $$$$$ Naive Bayes and perceptron are not significantly different, except at 1,200 training examples where naive Bayes has a slight advantage.

Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. $$$$$ Naive Bayes and perceptron are not significantly different, except at 1,200 training examples where naive Bayes has a slight advantage.
Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. $$$$$ Therefore, each discrimination is clearly only testing a relatively small fraction of the 2,859 available features.

They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). $$$$$ Comparative Experiments On Disambiguating Word Senses

Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. $$$$$ Naive Bayes and perceptron are not significantly different, except at 1,200 training examples where naive Bayes has a slight advantage.
Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. $$$$$ Naive Bayes and perceptron are similar in that they both employ a weighted combination of all features.

Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). $$$$$ Several recent research projects have taken a corpus-based approach to lexical disambiguation (Brown, Della-Pietra, Della-Pietra, St Mercer, 1991; Gale, Church, St Yarowsky, 1992b; Leacock et al., 1993b; Lehman, 1994).
Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). $$$$$ Leacock et al. (1993b), Leacock, Towell, and Voorhees (1993a) and Voorhees, Leacock, and Towell (1995) present results on a Bayesian method (Gale, Church, & Yarowsky, 1992a), a content vector method from information retrieval (Salton, Wong, & Yang, 1975), and a neural network trained using backpropagation (Rumelhart, Hinton, & Williams, 1986).

Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996). $$$$$ Our tests are based on the corpus assembled by Leacock et al. (1993b).
Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996). $$$$$ Since the previous results of Leacock et al. (1993b) indicated that neural networks did not benefit from hidden units on the &quot;line&quot; disambiguation data, we employed a simple perceptron (Rosenblatt, 1962) as a representative connectionist method.
