Of the 1600 IBM sentences that have been parsed (those available from the Penn Treebank [Marcus et al, 19931), only 67 overlapped with the IBM-manual treebank that was bracketed by University of Lancaster. $$$$$ Building a Large Annotated Corpus of English

The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). $$$$$ 318 Mitchell P. Marcus et al.
The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) provides annotations for the arguments and relation senses of one hundred pre-selected discourse connectives over the news portion of the Penn Treebank corpus (Marcus et al, 1993). $$$$$ 326 Mitchell P. Marcus et al.

Marcus et al (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for part-of-speech annotation. $$$$$ Building a Large Annotated Corpus of English This experiment showed that manual tagging took about twice as long as correcting, with about twice the inter-annotator disagreement rate and an error rate that was about 50% higher.
Marcus et al (1993) found that direct annotation takes twice as long as automatic tagging plus correction, for part-of-speech annotation. $$$$$ Some users would like a less skeletal form of annotation of surface 328 Mitchell P. Marcus et al.

Agirre et al (2008) applied two state-of-the-art tree bank parsers to the sense tagged subset of the Brown corpus version of the Penn Treebank (Marcus et al, 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PP attachment. $$$$$ 318 Mitchell P. Marcus et al.
Agirre et al (2008) applied two state-of-the-art tree bank parsers to the sense tagged subset of the Brown corpus version of the Penn Treebank (Marcus et al, 1993), and added sense annotation to the training data to evaluate their impact on parse selection and specifically on PP attachment. $$$$$ 320 Mitchell P. Marcus et al.

These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al, 1993). $$$$$ 316 Mitchell P. Marcus et al.
These data sets were based on the Wall Street Journal corpus in the Penn Treebank (Marcus et al, 1993). $$$$$ 320 Mitchell P. Marcus et al.

We have used Sections 02-21 of CCG bank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al, 1993), as training data for the newspaper domain. $$$$$ 314 Mitchell P. Marcus et al.
We have used Sections 02-21 of CCG bank (Hockenmaier and Steedman, 2007), the CCG version of the Penn Treebank (Marcus et al, 1993), as training data for the newspaper domain. $$$$$ 316 Mitchell P. Marcus et al.

The English sentences were parsed using a state-of-the-art statistical parser (Charniak, 2000) trained on the University of Pennsylvania Treebank (Marcus et al, 1993). $$$$$ Building a Large Annotated Corpus of English

The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al, 1993 ) with a layer of discourse annotations. $$$$$ In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1consisting of over 4.5 million words of American English.
The overall goal of the Penn Discourse Treebank (PDTB) is to annotate the million word WSJ corpus in the Penn TreeBank (Marcus et al, 1993 ) with a layer of discourse annotations. $$$$$ 326 Mitchell P. Marcus et al.

These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al (1993). $$$$$ 316 Mitchell P. Marcus et al.
These remain fixed at all levels to the standard Penn-tree-bank set Marcus et al (1993). $$$$$ 318 Mitchell P. Marcus et al.

This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al, 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. $$$$$ 314 Mitchell P. Marcus et al.
This paper describes an algorithm for detecting empty nodes in the Penn Treebank (Marcus et al, 1993), finding their antecedents, and assigning them function tags, without access to lexical information such as valency. $$$$$ 2.1.3 Syntactic Function.

In the Penn Treebank (Marcus et al, 1993), null elements, or empty categories, are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements. $$$$$ X Null elements 2.
In the Penn Treebank (Marcus et al, 1993), null elements, or empty categories, are used to indicate non-local dependencies, discontinuous constituents, and certain missing elements. $$$$$ As can be seen from Table 3, the syntactic tagset used by the Penn Treebank in- cludes a variety of null elements, a subset of the null elements introduced by Fidditch.

This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al 1993). $$$$$ In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1consisting of over 4.5 million words of American English.
This paper reports on our experience hand tagging the senses of 25 of the most frequent verbs in 12,925 sentences of the Wall Street Journal Treebank corpus (Marcus et al 1993). $$$$$ 318 Mitchell P. Marcus et al.

The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al, 1993). $$$$$ 316 Mitchell P. Marcus et al.
The sentences included in the gold standard were chosen at random from the BNC, subject to the condition that they contain a verb which does not occur in the training sections of the WSJ section of the PTB (Marcus et al, 1993). $$$$$ 326 Mitchell P. Marcus et al.

We use the Penn WSJ treebank (Marcus et al, 1993) for our experiments. $$$$$ 314 Mitchell P. Marcus et al.
We use the Penn WSJ treebank (Marcus et al, 1993) for our experiments. $$$$$ 318 Mitchell P. Marcus et al.

The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al, 1993) consisting of 10948 sentences and 259104 words. $$$$$ 320 Mitchell P. Marcus et al.
The data used for our experiment consist of English sentences from the Penn Treebank project (Marcus et al, 1993) consisting of 10948 sentences and 259104 words. $$$$$ The ATIS sentences are transcribed versions of spontaneous sentences collected as training materials for the DARPA Air Travel Information System project.

For instance, about 38% of verbs in the training sections of the Penn Treebank (PTB) (Marcus et al, 1993) occur only once the lexical properties of these verbs. $$$$$ 318 Mitchell P. Marcus et al.
For instance, about 38% of verbs in the training sections of the Penn Treebank (PTB) (Marcus et al, 1993) occur only once the lexical properties of these verbs. $$$$$ 320 Mitchell P. Marcus et al.

The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al, 1993). $$$$$ 314 Mitchell P. Marcus et al.
The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al, 1993). $$$$$ 324 Mitchell R Marcus et al.

We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size in creases; the moderate-sized Resource Management corpus (Price et al, 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al, 1993). $$$$$ 324 Mitchell R Marcus et al.
We have observed in several experiments that the number of SuperARVs does not grow significantly as training set size in creases; the moderate-sized Resource Management corpus (Price et al, 1988) with 25,168 words produces 328 SuperARVs, compared to 538 SuperARVs for the 1 million word Wall Street Journal (WSJ) Penn Treebank set (Marcus et al, 1993). $$$$$ 326 Mitchell P. Marcus et al.

In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al, 1993), so as to abstract from a particular POS tagger and to provide an upper bound. $$$$$ 314 Mitchell P. Marcus et al.
In one experiment, it has to be performed on the basis of the gold-standard, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al, 1993), so as to abstract from a particular POS tagger and to provide an upper bound. $$$$$ 316 Mitchell P. Marcus et al.

Our chunks and functions are based on the annotations in the third release of the Penn Treebank (Marcus et al, 1993). $$$$$ 314 Mitchell P. Marcus et al.
Our chunks and functions are based on the annotations in the third release of the Penn Treebank (Marcus et al, 1993). $$$$$ 324 Mitchell R Marcus et al.
