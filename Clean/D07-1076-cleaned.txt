In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ It depends.
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??

As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node.
As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ becomes context-sensitive with its dependence on the root node path instead of the root node itself.

In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ D= m i NnNn ii C iiii nnTTK 1 ]2[]2[],1[]1[ 11 1111 ])2[],1[(])2[],1[( (3) Where ? ][1 jN i is the set of root node paths with length i in tree T[j] while the maximal length of a root node path is defined by m. ? ])[...(][ 211 jnnnjn ii = is a root node path with length i in tree T[j] , which takes into account the i-1 ancestral nodes in2 [j] of 1n [j] in T[j].
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ Context-Sensitive Convolution Tree Kernel In this paper, the m parameter of our context-sensitive convolution tree kernel as shown in Equation (3) indicates the maximal length of root node paths and is optimized to 3 using 5-fold cross validation on the ACE RDC 2003 training data.

We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ Zhang et al(2006) systematically explored seven different tree spans, including the Shortest Path-enclosed Tree (SPT) and a Context Sensitive Path-enclosed Tree1 (CSPT), and found that SPT per formed best.
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ 3.2 Context-Sensitive Convolution Tree Kernel.

 $$$$$ In the embedded cases, SPT is enough.
 $$$$$ We would also like to thank the critical and insightful comments from the four anonymous reviewers.

To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ However, there is one problem with this tree kernel: the sub-trees involved in the tree kernel computation are context-free (That is, they do not consider the information outside the sub-trees).
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ It works by taking ancestral information (i.e. the root node path) of sub-trees into consideration: ? ?

 $$$$$ In the embedded cases, SPT is enough.
 $$$$$ We would also like to thank the critical and insightful comments from the four anonymous reviewers.

Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ 3.2 Context-Sensitive Convolution Tree Kernel.
Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??

Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ Zhang et al(2006) also showed that the widely-used Shortest Path-enclosed Tree (SPT) performed best.
Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??

To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ Culotta and Sorensen (2004) extended this work to estimate simi larity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus.
To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ As a result, our tree kernel is not limited by the constraints in previous tree kernels (as discussed in Section 2), such as Collins and Duffy (2001), Zhang et al(2006), Culotta and Sorensen (2004) and Bunescu and Mooney (2005a).

 $$$$$ In the embedded cases, SPT is enough.
 $$$$$ We would also like to thank the critical and insightful comments from the four anonymous reviewers.

Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ Since then, many methods, such as feature-based (Kambhatla 2004; Zhou et al2005, 2006), tree ker nel-based (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a; Zhang et al2006) and composite kernel-based (Zhao and Gris hman 2005; Zhang et al2006), have been proposed in lit erature.
Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ System P(%) R(%) F Linear Kernel 78.2 (77.2) 63.4 (60.7) 70.1 (68.0) Context-Sensitive Con volution Tree Kernel 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Composite Kernel 82.2 (80.8) 70.2 (68.4) 75.8 (74.1) Table 3: Performance of the compos ite kernel via polynomial interpolation on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora Comparison with Other Systems ACE RDC 2003 P(%) R(%) F Ours: composite kernel 80.8 (65.2) 68.4 (54.9) 74.1 (59.6) Zhang et al(2006): composite kernel 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Ours: context-sensitive convolution tree kernel 80.1 (63.4) 63.8 (51.9) 71.0 (57.1) Zhang et al(2006): convolution tree kernel 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu et al(2005): shortest path dependency kernel 65.5 (-) 43.8 (-) 52.5 (-) Culotta et al(2004): dependency kernel 67.1 (-) 35.0 (-) 45.8 (-) Zhou et al (2005): feature-based 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based - (63.5) - (45.2) - (52.8) Table 4: Comparison of difference systems on the ACE RDC 2003 corpus over both 5 types (outside the parentheses) and 24 subtypes (inside the parentheses) ACE RDC 2004 P(%) R(%) F Ours: composite kernel 82.2 (70.3) 70.2 (62.2) 75.8 (66.0) Zhang et al(2006): composite kernel 76.1 (68.6) 68.4 (59.3) 72.1 (63.6) Zhao et al(2005):8 composite kernel 69.2 (-) 70.5 (-) 70.4 (-) Ours: context-sensitive convolution tree kernel 81.1 (68.8) 66.7 (60.3) 73.2 (64.3) Zhang et al(2006): convolution tree kernel 72.5 (-) 56.7 (-) 63.6 (-) Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) Finally, Tables 4 and 5 compare our system with other state-of-the-art systems9 on the ACE RDC 2003 and 2004 corpora, respectively.

Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ First, we systematically evaluate the context-sensitive convolution tree kernel and the dynamic context sensitive tree span proposed in this paper.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??

Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.
Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ Zhang et al(2006) systematically explored seven different tree spans, including the Shortest Path-enclosed Tree (SPT) and a Context Sensitive Path-enclosed Tree1 (CSPT), and found that SPT per formed best.

Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ 3.2 Context-Sensitive Convolution Tree Kernel.
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??

Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ This suggests that our tree kernel and the state-of-the-art linear kernel are quite complementary, and that our composite kernel can effectively integrate both flat and structured features.

Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.

(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ 3.2 Context-Sensitive Convolution Tree Kernel.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ information.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ That is, SPT even outperforms CSPT.

Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ As the state-of-the-art, Zhang et al(2006) applied the convo lution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al2005) on the 5 relation types in the ACE RDC 2003 corpus.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??
