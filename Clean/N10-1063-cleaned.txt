More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ Wikipedia’s markup contains other useful indicators for parallel sentence extraction.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001).

A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ We were pleasantly surprised at the amount of parallel sentences extracted from such a varied comparable corpus.

This methodology is similar to that of Smith et al (2010). $$$$$ This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996).
This methodology is similar to that of Smith et al (2010). $$$$$ A similar source of information has been used to create seed lexicons in (Koehn and Knight, 2002) and as part of the feature space in (Haghighi et al., 2008).

Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ Section 2 describes the multilingual resources available in Wikipedia.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009).

For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ The “Large” data condition includes all the medium data, and also includes using a broad range of available sources such as data scraped from the web (Resnik and Smith, 2003), data from the United Nations, phrase books, software documentation, and more.
For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements.

Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ The extracted Wikipedia data is likely to make the greatest impact on broad domain test sets – indeed, initial experimentation showed little BLEU gain on in-domain test sets such as Europarl, where out-of-domain training data is unlikely to provide appropriate phrasal translations.
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ Therefore, we experimented with two broad domain test sets.

For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ A noisy parallel corpus has documents which contain many parallel sentences in roughly the same order.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.

As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).
As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009).

While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.

Smith et al (2010) extended these previous lines of work in several directions. $$$$$ This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996).
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ This feature corresponds more closely to context similarity measures used in previous work on lexicon induction.

To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ Extracting Parallel Sentences from Comparable Corpora using Document Level Alignment
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.

Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ Comparable corpora contain topic aligned documents which are not translations of each other.
Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.
