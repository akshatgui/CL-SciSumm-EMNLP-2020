For instance, incorporating WSD predictions into an MT decoder based on inversion transduction grammars (Wu, 1997) such as the Bracketing ITG based models of Wu (1996), Zens et al (2004), or Cherry and Lin (2007) would present an intriguing comparison with the present work. $$$$$ We present a phrasal inversion transduction grammar as an alternative to joint phrasal translation models.
For instance, incorporating WSD predictions into an MT decoder based on inversion transduction grammars (Wu, 1997) such as the Bracketing ITG based models of Wu (1996), Zens et al (2004), or Cherry and Lin (2007) would present an intriguing comparison with the present work. $$$$$ Inversion transduction grammar (Wu, 1997), or ITG, is a wellstudied synchronous grammar formalism.

Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. $$$$$ Phrasal decoders require a phrase table (Koehn et al., 2003), which contains bilingual phrase pairs and scores indicating their utility.
Cherry and Lin (2007) incorporate phrase pairs in phrase-based SMT into ITG, and Haghighi et al (2009) introduce Block ITG (BITG), which adds 1-to-many or many-to-1 terminal unary rules. $$$$$ It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003).

Our first pruning technique is broadly similar to Cherry and Lin (2007a). $$$$$ Our phrasal ITG is quite similar to the JPTM.
Our first pruning technique is broadly similar to Cherry and Lin (2007a). $$$$$ We propose a new ITG pruning method that leverages high-confidence links by pruning all spans that are inconsistent with a provided alignment.

For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). $$$$$ Inversion Transduction Grammar for Joint Phrasal Translation Modeling
For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). $$$$$ Inversion transduction grammar (Wu, 1997), or ITG, is a wellstudied synchronous grammar formalism.

Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ Inversion transduction grammar (Wu, 1997), or ITG, is a wellstudied synchronous grammar formalism.
Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ We can use a linear-time algorithm (Zhang et al., 2006) to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus.

The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-to-one word alignments. The heuristic method is based on the Non Compositional Constraint of Cherry and Lin (2007). $$$$$ We are interested in phrasal alignment because it may be better suited to heuristic phraseextraction than word-based models.
The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-to-one word alignments. The heuristic method is based on the Non Compositional Constraint of Cherry and Lin (2007). $$$$$ The phrases produced with this constraint are very small, and include only non-compositional context.

Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases. $$$$$ Also, ITG algorithms explore their alignment space perfectly, but that space has been reduced by the ITG constraint described in Section 2.3.
Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases. $$$$$ The first thing to note is that GIZA++ Intersection is indeed very high precision.

Therefore, researches like Cherry and Lin (2007), Haghighi et al (2009) and Zhang et al (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks). $$$$$ It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003).
Therefore, researches like Cherry and Lin (2007), Haghighi et al (2009) and Zhang et al (2009) tackle this problem by enriching ITG, in addition to word pairs, with pairs of phrases (or blocks). $$$$$ We can use a linear-time algorithm (Zhang et al., 2006) to detect non-ITG movement in our high-confidence links, and remove the offending sentence pairs from our training corpus.

For some restricted combinatorial spaces of alignments those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al,2004) inference can be accomplished using polynomial time dynamic programs. $$$$$ This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other.
For some restricted combinatorial spaces of alignments those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al,2004) inference can be accomplished using polynomial time dynamic programs. $$$$$ This syntactic solution to phrase modeling admits polynomial-time training and alignment algorithms.

Similarly, Cherry and Lin (2007) use ITG for pruning. $$$$$ We propose a new ITG pruning method that leverages high-confidence links by pruning all spans that are inconsistent with a provided alignment.
Similarly, Cherry and Lin (2007) use ITG for pruning. $$$$$ The two methods also produce similarly-sized tables, despite the ITGâ€™s higher recall.

Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al, 382 2009), we further reduced AER on the test set to 12.25. $$$$$ Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006).
Without initializing by phrases extracted from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al, 382 2009), we further reduced AER on the test set to 12.25. $$$$$ Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002).
