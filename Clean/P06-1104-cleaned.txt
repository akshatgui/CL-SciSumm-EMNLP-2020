To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.

For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ Section 3 discusses our composite kernel.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ Moreover, the in-between word features and the entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively.

For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.
For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ To our knowledge, convolution kernels have not been explored for relation extraction.

For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.
For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)).

We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ The composite kernel consists of an entity kernel and a convolution parse tree kernel.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.

(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. $$$$$ Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.
(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. $$$$$ They suggest that the parse tree kernel can effectively explore the syntactic features which are critical for relation extraction. both the 2003 and 2004 data for the composite kernel by polynomial expansion (4) Error Analysis: Table 5 reports the error distribution of the polynomial composite kernel over the major types on the ACE data.

For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.
For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ It shows that: ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information) kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data.

In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005).
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.

For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ Zelenko et al. (2003) developed a kernel over parse trees for relation extraction.
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.

Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ The composite kernel consists of an entity kernel and a convolution parse tree kernel.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.

AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.

The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.

Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.

Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ We study five cases as shown in Fig.1. mon sub-tree including the two entities.
Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ Fig.

Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).
Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.

Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.
Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).

Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes.

Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).

Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ The composite kernel consists of an entity kernel and a convolution parse tree kernel.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.
