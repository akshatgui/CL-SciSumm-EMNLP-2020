However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26), and many subtrees are repeated across different parses (Huang, 2008).
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ The decoder performs two tasks on the translation forest

In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ Following Huang (2008), we modify the parser to output a packed forest for each sentence.
In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2.

Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ translation hyperedge translation rule parse forest, and try to pattern-match each translation rule r against the local sub-forest under node v. For example, in Figure 3(a), at node VP1,6, two rules r3 and r7 both matches the local subforest, and will thus generate two translation hyperedges e3 and e4 (see Figure 3(b-c)).
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules.

Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser.
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests.

In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ Depending on the type of input, these efforts can be divided into two broad categories

An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Depending on the type of input, these efforts can be divided into two broad categories

At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2).
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ The only extra term in forest-based decoding is P(t

Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ This situation becomes worse with resource-poor source languages without enough Treebank data to train a high-accuracy parser.
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ Following Huang (2008), we modify the parser to output a packed forest for each sentence.

For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ Depending on the type of input, these efforts can be divided into two broad categories

At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2).
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ On dev and test sets, we prune the Chinese parse forests by the forest pruning algorithm in Section 3.4 with a threshold of p = 12, and then convert them into translation forests using the algorithm in Section 3.2.

 $$$$$ Finally, from step (d) we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees, respectively, and get the Chinese translation in (e).
 $$$$$ We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual ∞-best list, and the anonymous reviewers for suggestions.

Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ The only extra term in forest-based decoding is P(t

For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002).
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006).

We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ Forest-Based Translation
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ Following Huang (2008), we modify the parser to output a packed forest for each sentence.

Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ In the decoding step, we first convert the parse forest into a translation forest using the translation rule set, by similar techniques of pattern-matching from tree-based decoding (Section 3.2).
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules.

Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989).
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ Given a parse forest and a translation rule set R, we can generate a translation forest which has a similar hypergraph structure.

However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 which compactly encodes many more alternatives than k-best parses.

The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ The pruned parse forest will then be used to direct the translation.
The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.

Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ Forest-Based Translation
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3).

For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ Both tasks can be done efficiently by forest-based algorithms based on k-best parsing (Huang and Chiang, 2005).
For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ We first word-align them by GIZA++ refined by “diagand” from Koehn et al. (2003), and apply the tree-to-string rule extraction algorithm (Galley et al., 2006; Liu et al., 2006), which resulted in 346K translation rules.
