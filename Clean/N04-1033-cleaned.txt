We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ We described a phrase-based translation approach.

We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ In Section 4, we will describe a monotone search algorithm.
We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ We described a highly efficient monotone search algorithm.

We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ Then, we will describe refinements of the baseline model.
We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ Thus the lexical choice of words is of the same quality.

The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ The monotone search can be efficiently computed with dynamic programming.
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ We obtain the following dynamic programming recursion.

There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ The alignment template system (Och et al., 1999) is similar to the system described in this work.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.

Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ Finally, we have to estimate the phrase translation probabilities p(Ëœf

Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ We are using relative frequencies to estimate the phrase translation probabilities.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ It should be pointed out that in practice the monotone search will perform better than what the preceding estimates indicate.

 $$$$$ Q(J + 1, $) is the probability of the optimum translation.
 $$$$$ This work has been partially funded by the EU project TransType 2, IST-2001-32091.

In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ As a decision rule, we obtain

In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ Improvements In Phrase-Based Statistical Machine Translation
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ In Section 4, we will describe a monotone search algorithm.

(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ In Section 4, we will describe a monotone search algorithm.
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ We described a highly efficient monotone search algorithm.

The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ Therefore, there is no constraint on the reordering within the phrases.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ For details, see (Tillmann and Ney, 2003).

In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ We use the Downhill Simplex algorithm from (Press et al., 2002).
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ In the experiments, the Downhill Simplex algorithm converged after about 200 iterations.

 $$$$$ Q(J + 1, $) is the probability of the optimum translation.
 $$$$$ This work has been partially funded by the EU project TransType 2, IST-2001-32091.

For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ We take the union ofboth alignments to obtain a symmetrized word alignment matrix.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ It means that two phrases are considered to be translations of each other, if the words are aligned only within the phrase pair and not to words outside.

combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ The unigram method hurts performance.
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ The translation results of the different systems are shown in Table 6.

This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ The translation results for the Xerox and Canadian Hansards task are very promising.
This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ We start with the Verbmobil results.

The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ Improvements In Phrase-Based Statistical Machine Translation
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ So, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations.

The following methods were investigated $$$$$ Obviously, the monotone phrase-based system outperforms the monotone single-word based system.
The following methods were investigated $$$$$ The described search is monotone at the phrase level.

We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ Improvements In Phrase-Based Statistical Machine Translation
We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.
