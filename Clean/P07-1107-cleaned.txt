More recently, Haghighi and Klein (2007) use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
More recently, Haghighi and Klein (2007) use the distinction between pronouns, nominals and proper nouns in their unsupervised, generative model for coreference resolution; for their model, this is absolutely critical for achieving better accuracy. $$$$$ We have presented a novel, unsupervised approach to coreference resolution

 $$$$$ In this model, each document is independent save for some global hyperparameters.
 $$$$$ Although our system does not perform quite as well as state-of-the-art supervised systems, its performance is in the same general range, despite the system being unsupervised.

Other recent unsupervised graphical model approaches using Gibbs sampling (Haghighi and Klein, 2007) may be able to incorporate partially annotated documents in semi-supervised training. $$$$$ Up until now, we’ve discussed Gibbs sampling, but we are not interested in sampling from the posterior P(Z

Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ Most recent coreference resolution work has focused on the task of deciding which mentions (noun phrases) in a document are coreferent.

In terms of applying non-parametric Bayesian approaches to NLP, Haghighi and Klein (2007) evaluated the clustering properties of DPMMs by performing anaphora resolution with good results. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
In terms of applying non-parametric Bayesian approaches to NLP, Haghighi and Klein (2007) evaluated the clustering properties of DPMMs by performing anaphora resolution with good results. $$$$$ This model fixes many anaphora errors and in particular fixes the second anaphora error in figure 2(c).

In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of (Haghighi and Klein, 2007). $$$$$ This anaphoric reference is canonically, though of course not always, accomplished with pronouns, and is governed by linguistic and cognitive constraints.
In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of (Haghighi and Klein, 2007). $$$$$ As can be seen in figure 2(b), this model does correct the systematic problem of pronouns being considered their own entities.

The probabilities are ordered according to, at least my, intuition with pronoun being the most likely (0.094), followed by proper nouns (0.057), followed by common nouns (0.032), a fact also noted by (Haghighi and Klein,2007). $$$$$ Once the properties are fetched, a mention type M is chosen (proper, nominal, pronoun), according to a global multinomial (again with a symmetric Dirichlet prior and parameter AM).
The probabilities are ordered according to, at least my, intuition with pronoun being the most likely (0.094), followed by proper nouns (0.057), followed by common nouns (0.032), a fact also noted by (Haghighi and Klein,2007). $$$$$ McCallum and Wellner (2004) also report a much lower 91.6 F1 on only proper nouns mentions.

Since Soon (Soon et al, 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). $$$$$ One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002).
Since Soon (Soon et al, 2001) started the trend of using the machine learning approach by using a binary classifier in a pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (Haghighi and Klein, 2007). $$$$$ One advantage of an unsupervised approach is that we can easily utilize more data when learning a model.

Since we can not afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)'s work on unsupervised co reference resolution, and develop a series of generative Bayesian models for our task. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Since we can not afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)'s work on unsupervised co reference resolution, and develop a series of generative Bayesian models for our task. $$$$$ In this section, we present a sequence of generative coreference resolution models for document corpora.

 $$$$$ In this model, each document is independent save for some global hyperparameters.
 $$$$$ Although our system does not perform quite as well as state-of-the-art supervised systems, its performance is in the same general range, despite the system being unsupervised.

In this work, we take a primarily unsupervised approach to co reference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
In this work, we take a primarily unsupervised approach to co reference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. $$$$$ Recent work has addressed this issue in more global ways.

As Blei and Frazier (2009) notes, when marginalizing out the Ai in this trivial case, the DD-CRP reduces to the traditional CRP (Pitman, 2002), so our discourse model roughly matches Haghighi and Klein (2007) for proper mentions. $$$$$ Each mention is a reference to some entity in the domain of discourse.
As Blei and Frazier (2009) notes, when marginalizing out the Ai in this trivial case, the DD-CRP reduces to the traditional CRP (Pitman, 2002), so our discourse model roughly matches Haghighi and Klein (2007) for proper mentions. $$$$$ Mentions can be divided into three categories, proper mentions (names), nominal mentions (descriptions), and pronominal mentions (pronouns).

Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. $$$$$ We present an unsupervised, nonparametric Bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document.

Poon and Domingos (2008) outperformed Haghighi and Klein (2007). $$$$$ Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries.
Poon and Domingos (2008) outperformed Haghighi and Klein (2007). $$$$$ Although our system does not perform quite as well as state-of-the-art supervised systems, its performance is in the same general range, despite the system being unsupervised.

The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. $$$$$ In particular, unlike much related work, we do not assume gold named entity recognition (NER) labels; indeed we do not assume observed NER labels or POS tags at all.
The model of Haghighi and Klein (2007) incorporated a latent variable for named entity class. $$$$$ We can use our model to for unsupervised NER tagging

Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. $$$$$ We would like our model to capture how mention types are generated for a given entity in a robust and somewhat language independent way.
Like Haghighi and Klein (2007), we give our model information about the basic types of pronouns in English. $$$$$ It is a pronoun if the head is on the list of English pronouns.

As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
As stated above, we aim to build an unsupervised generative model for named entity clustering, since such a model could be integrated with unsupervised coreference models like Haghighi and Klein (2007) for joint inference. $$$$$ We have presented a novel, unsupervised approach to coreference resolution

Our system improves over the latent named-entity tagging in Haghighiand Klein (2007), from 61% to 87%. $$$$$ The HDP layer of sharing improves the model’s predictions to 72.5 F1 on our development data.
Our system improves over the latent named-entity tagging in Haghighiand Klein (2007), from 61% to 87%. $$$$$ We can use our model to for unsupervised NER tagging

Turning to unsupervised CoRe, Haghighi and Klein (2007) proposed a generative Bayesian model with good performance. $$$$$ Unsupervised Coreference Resolution in a Nonparametric Bayesian Model
Turning to unsupervised CoRe, Haghighi and Klein (2007) proposed a generative Bayesian model with good performance. $$$$$ In this paper, we present a novel, fully generative, nonparametric Bayesian model of mentions in a document corpus.

P&D (Poon and Domingos, 2008) on ACE-2; and to Ng (Ng, 2008) and H&K5 (Haghighi and Klein, 2007) on ACE2003. $$$$$ One then applies discriminative learning methods to pairs of mentions, using features which encode properties such as distance, syntactic environment, and so on (Soon et al., 2001; Ng and Cardie, 2002).
P&D (Poon and Domingos, 2008) on ACE-2; and to Ng (Ng, 2008) and H&K5 (Haghighi and Klein, 2007) on ACE2003. $$$$$ We also performed experiments on ACE 2004 data.
