Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ Measures Of Distributional Similarity
Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991).

As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions

We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows

We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space $$$$$ (van Rijsbergen, 1979, pg.
We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions

While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs $$$$$ Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a).
While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs $$$$$ 4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared.

In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ Measures Of Distributional Similarity
In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ 40n a related note, an anonymous reviewer cited the 30 following example from the psychology literature

Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ Measures Of Distributional Similarity
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983).

The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions

Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. $$$$$ On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact

For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ Measures Of Distributional Similarity
For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ There are many plausible measures of distributional similarity.

 $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.
 $$$$$ IRI9712068.

We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ Measures Of Distributional Similarity
We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ Whether we mean f or C — f should be clear from context.

Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ This function yielded the best performance overall

The second approach is from Lee (1999). $$$$$ Both use P(v) to estimate P(v In) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions — although the words &quot;chance&quot; and &quot;probability&quot; are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.
The second approach is from Lee (1999). $$$$$ We view the empirical approach taken in this paper as complementary to Lin's.

This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ Measures Of Distributional Similarity
This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ There are many plausible measures of distributional similarity.

Another reason for this choice is that there are different ideas underlying these measures $$$$$ In previous work (Dagan et al., 1999), we compared the performance of three different functions

Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions

Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools.
Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).

 $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.
 $$$$$ IRI9712068.

We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions
