Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ Measures Of Distributional Similarity
Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991).

As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.
As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.

We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows: Note that it incorporates unigram probabilities as well as the two distributions q and r. Finally, Kendall's T which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993).
We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ Finally, we did not use the KL divergence because it requires a smoothed base language model.

We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ (van Rijsbergen, 1979, pg.
We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.

While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). $$$$$ Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a).
While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). $$$$$ 4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared.

In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ Measures Of Distributional Similarity
In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ 40n a related note, an anonymous reviewer cited the 30 following example from the psychology literature: we can say Smith's lecture is like a sleeping pill, but &quot;not the other way round&quot;. average error rate Error rates (averages and ranges) definition of commonality is left to the user (several different definitions are proposed for different tasks).

Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ Measures Of Distributional Similarity
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ The cosine metric and Jaccard's coefficient are commonly used in information retrieval as measures of association (Salton and McGill, 1983).

The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.
The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).

Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. $$$$$ On the other hand, while there have been many similarity measures proposed and analyzed in the information retrieval literature (Jones and Furnas, 1987), there has been some doubt expressed in that community that the choice of similarity metric has any practical impact: Several authors have pointed out that the difference in retrieval performance achieved by different measures of association is insignificant, providing that these are appropriately normalised.
Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. $$$$$ The Jensen-Shannon divergence and the L1 norm can be computed simply by knowing the values of q and r on Vqr.

For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ Measures Of Distributional Similarity
For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ There are many plausible measures of distributional similarity.

 $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.
 $$$$$ IRI9712068.

We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ Measures Of Distributional Similarity
We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ Whether we mean f or C — f should be clear from context.

Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999).
Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ But the substitutability of one word for another need not symmetric.

The second approach is from Lee (1999). $$$$$ Both use P(v) to estimate P(v In) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions — although the words &quot;chance&quot; and &quot;probability&quot; are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.
The second approach is from Lee (1999). $$$$$ We view the empirical approach taken in this paper as complementary to Lin's.

This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ Measures Of Distributional Similarity
This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ There are many plausible measures of distributional similarity.

Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). $$$$$ In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.
Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). $$$$$ Using this insight, we developed an information-theoretic metric, the skew divergence, which incorporates the support-intersection data in an asymmetric fashion.

Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.
Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ We also plan to evaluate skewed versions of the Jensen-Shannon divergence proposed by Rao (1982) and J. Lin (1991).

Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ That is, the data consisted of the verb-object cooccurrence pairs in the 1988 Associated Press newswire involving the 1000 most frequent nouns, extracted via Church's (1988) and Yarowsky's processing tools.
Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).

 $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.
 $$$$$ IRI9712068.

We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.
We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ For the cosine and the confusion probability, the distribution values on Vqr are key, but other information is also incorporated.
