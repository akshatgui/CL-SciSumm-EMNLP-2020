Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ An extended description of these experiments is in the JHU workshop report (Koehn et al, 2006).
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ For more on this experiment, see (Shen et al, 2006).

Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ Factored Translation Models
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ Many attempts have been made to add richer in formation to statistical machine translation models.Most of these focus on the pre-processing of the in put to the statistical system, or the post-processing of its output.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ linguistic factorsFactored translation models build on the phrase based approach (Koehn et al, 2003) that breaks up the translation of a sentence into the translation of small text chunks (so-called phrases).

Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ The main difference lies in thepreparation of the training data and the type of mod els learned from the data.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ For instance, if we want to add part-of-speech information on the input and output side, we need to obtain part-of-speech tagged training data.

Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ For instance: re ordering at the sentence level is mostly driven word word part-of-speech OutputInput morphology part-of-speech morphology word class lemma word class lemma ......Figure 1: Factored representations of input and out put words incorporate additional annotation into the statistical translation model.
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ 6.3 Use of Automatic Word Classes.

We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ lemma lemma part-of-speech OutputInput morphology part-of-speech word word morphologyFigure 2: Example factored model: morphologi cal analysis and generation, decomposed into three mapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen eration of surface forms).
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ lemma lemma part-of-speech OutputInput morphology part-of-speech word word morphologyFigure 2: Example factored model: morphologi cal analysis and generation, decomposed into three mapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen eration of surface forms).
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ In such a model, we would want to translate lemmaand morphological information separately, and com bine this information on the output side to ultimately generate the output surface words.

Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ For instance: re ordering at the sentence level is mostly driven word word part-of-speech OutputInput morphology part-of-speech morphology word class lemma word class lemma ......Figure 1: Factored representations of input and out put words incorporate additional annotation into the statistical translation model.
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ lemma lemma part-of-speech OutputInput morphology part-of-speech word word morphologyFigure 2: Example factored model: morphologi cal analysis and generation, decomposed into three mapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen eration of surface forms).

Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ These compo nents define one or more feature functions that are combined in a log-linear model: p(e

Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ Factored Translation Models
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ Generate surface forms given the lemma and.

Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ Factored Translation Models
Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ Factored translation models follow closely the sta tistical modeling approach of phrase-based models (in fact, phrase-based models are a special case of factored models).

Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ Factored Translation Models
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

 $$$$$ The three mapping steps in our morphologicalanalysis and generation model may provide the fol lowing applicable mappings: 1.
 $$$$$ Acknowledgments This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No NR0011-06-C-0022 and inpart under the EuroMatrix project funded by the Eu ropean Commission (6th Framework Programme).We also benefited greatly from a 2006 summer workshop hosted by the Johns Hopkins Uni versity and would like thank the other workshop participants for their support and insights, namelyNicola Bertoldi, Ondrej Bojar, Chris Callison Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst Christine Moran, Wade Shen, and Richard Zens.

Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ Translation: Mapping morphology.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ 5.1 Training.
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ A word in our framework is not only a token, but a vector of factors that represent different levels of annotation (see Figure 1).
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ Each word form is treated as a token in it self.

Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ See Figure 2 for an illustration of this model in our framework.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ Factored translation models follow closely the sta tistical modeling approach of phrase-based models (in fact, phrase-based models are a special case of factored models).

We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ Factored Translation Models
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ Factored Translation Models
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).

We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ 5.3 Efficient Decoding.
We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007).
