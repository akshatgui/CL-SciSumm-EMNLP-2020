Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ This supervised version compresses better than either version of the supervised noisy-channel model that lacks these rules.

We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ Let us consider a simplified version of a K&M example, but as reinterpreted for our model: how the noisy channel model assigns a probability of the compressed tree (A) in Figure 3 given the original tree B.

Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ We create joint rules using only the first section (0.mrg) of the Penn Treebank.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ We count all probabilistic context free grammar (PCFG) expansions, and then match up similar rules as unsupervised joint events.

Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ One of the biggest problems with this model of sentence compression is the lack of appropriate training data.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.

 $$$$$ There are several hundred rules of this type, and it is very simple to incorporate into our model.
 $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.

 $$$$$ There are several hundred rules of this type, and it is very simple to incorporate into our model.
 $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.

 $$$$$ There are several hundred rules of this type, and it is very simple to incorporate into our model.
 $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.

 $$$$$ There are several hundred rules of this type, and it is very simple to incorporate into our model.
 $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.

Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ The K&M model uses parse trees for the sentences.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.

Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ As will be shown, this rule is not constraining enough and allows some poor compressions, but it is remarkable that any sort of compression can be achieved without training data.
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ Example 2 shows how unsupervised and semisupervised techniques can be used to improve compression.

While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.

Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ The K&M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ To reiterate this section’s argument: A noisy channel model is not by itself an appropriate model for sentence compression.

See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ One of the biggest problems with this model of sentence compression is the lack of appropriate training data.

 $$$$$ There are several hundred rules of this type, and it is very simple to incorporate into our model.
 $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.

Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ Supervised And Unsupervised Learning For Sentence Compression
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ None of the special rules are applied.

Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.
Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ Knight and Marcu (henceforth K&M) introduce the task of statistical sentence compression in Statistics-Based Summarization - Step One: Sentence Compression (Knight and Marcu, 2000).

Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ None of the special rules are applied.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ In a pure PCFG this would only include the label of the node.

In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ The main difference between their model and ours is that instead of using the rather ad-hoc K&M language model, we substitute the syntax-based language model described in (Charniak, 2001).
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ One of the biggest problems with this model of sentence compression is the lack of appropriate training data.
