Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.
Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ For the test, four corpora were constructed from the LOB corpus

While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). $$$$$ Preparing tagged corpora either by hand is labour-intensive and potentially error-prone, and although a semi-automatic approach can be used (Marcus et al., 1993), it is a good thing to reduce the human involvement as much as possible.
While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.

Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. $$$$$ One widely used approach makes use of a statistical technique called a Hidden Markov Model (HMM).
Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. $$$$$ Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.

Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alternatives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and Kupiec (Kupiec, 1992).
Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ The results suggest that a completely unconstrained initial model does not produce good quality results, and that one 'The technique was originally developed by Kupiec (Kupiec, 1989). accurately trained from a hand-tagged corpus will generally do better than using an approach based on re-estimation, even when the training comes from a different source.

 $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.
 $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.

However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.
However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.

Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ We will discuss this work below.
Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.

On this point, I agree with Merialdo (1994) and Elworthy (1994). $$$$$ The conclusions are broadly in agreement with those of Merialdo (1994), but give greater detail about the contributions of different parts of the model.
On this point, I agree with Merialdo (1994) and Elworthy (1994). $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.

 $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.
 $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.

Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ Does Baum-Welch Re-Estimation Help Taggers?
Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ Similar results are presented by Merialdo (1994), who describes experiments to compare the effect of training from a hand-tagged corpora and using the Baum-Welch algorithm with various initial conditions.

 $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.
 $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.

Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). $$$$$ If significant human intervention is needed to provide the biasing, then the advantages of automatic training become rather weaker, especially if such intervention is needed on each new text domain.
Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). $$$$$ Secondly, training from a hand-tagged corpus (case DO+TO) always does best, even when the test data is from a different source to the training data, as it is for LOB-L.

Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.
Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ For the test, four corpora were constructed from the LOB corpus

We considered three taggers $$$$$ One of the most effective taggers based on a pure HMM is that developed at Xerox (Cutting et al., 1992).
We considered three taggers $$$$$ For example, CLAWS (Garside et al., 1987) normalises the lexical probabilities by the total frequency of the word rather than of the tag.

In [Elworthy, 1994], similar experiments were run. $$$$$ Work similar to that described here has been carried out by Merialdo (1994), with broadly similar conclusions.
In [Elworthy, 1994], similar experiments were run. $$$$$ The re-estimation was allowed to run for ten iterations.

The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ For a corpus in which a fraction a of the words are ambiguous, and p is the accuracy on ambiguous words, the overall accuracy can be recovered from 1 â€” a + pa. All of the accuracy figures quoted below are for ambiguous words only.
The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ As an example of how these figures relate to overall accuracies, LOB-B contains 32.35% ambiguous tokens with respect to the lexicon from LOB-B-J, and the overall accuracy in the DO+TO case is hence 98.69%.
