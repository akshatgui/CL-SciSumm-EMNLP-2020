Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ In other words, Gildea removed all bilexical statistics from the overall model.
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ Combined with the results from the previous experiment, this suggests rather convincingly that such statistics are far less significant than once thought to the overall discriminative power of Collins’ models, confirming Gildea’s result for Model 2.36 If not bilexical statistics, then surely, one might think, head-choice is critical to the performance of a head-driven lexicalized statistical parsing model.

Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ In other words, Gildea removed all bilexical statistics from the overall model.
Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ Note that probabilities making use of this full context, that is, making use of bilexical dependencies, are available only 1.49% of the time.

The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. $$$$$ In addition to the effects of the unpublished details, we also have new evidence to show that the discriminative power of Collins’ model does not lie where once thought: Bilexical dependencies play an extremely small role in Collins’ models (Gildea 2001), and head choice is not nearly as critical as once thought.
The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. $$$$$ The answer is that even when one removes bilexical dependencies from the model, there are still plenty of lexico-structural dependencies, that is, structures being generated conditioning on headwords and headwords being generated conditioning on structures.

Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ There are three primary motivations for this work.
Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ This work was supported in part by NSF grant no.

For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. $$$$$ These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context.
For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. $$$$$ In particular, it appeared that the head rules used in Collins’ parser had been tweaked specifically for the English Penn Treebank.

They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ There are two reasons to remove these types of subtrees when parsing the English Treebank: First, in the treebanking guidelines (Bies 1995), quotation marks were given the lowest possible priority and thus cannot be expected to appear within constituent boundaries in any kind of consistent way, and second, neither of these types of preterminals—nor any punctuation marks, for that matter—counts towards the parsing score.
They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context.

Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ Finally, the intervening punctuation is generated conditioning on the parent, the head, and the right conjunct, including the headwords of the two conjoined phrases, and the intervening CC is similarly generated.
Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ Accordingly, if a comma in an input Overall parsing results using only details found in Collins (1997, 1999).

 $$$$$ After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
 $$$$$ N66001-00-1-8915.

 $$$$$ After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
 $$$$$ N66001-00-1-8915.

 $$$$$ After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
 $$$$$ N66001-00-1-8915.

 $$$$$ After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
 $$$$$ N66001-00-1-8915.

We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). $$$$$ Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.
We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). $$$$$ First, Collins’ parsing model represents a widely used and cited parsing model.

Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ For example, Gildea (2001) reimplemented Collins’ Model 1 but obtained results with roughly 16.7% more relative error than Collins’ reported results using that model.
Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ This is a crucial part of the way smoothing is done: If a particular history context φi(B) has never been observed in training, the smoothed estimate using less context, φi+1(B), is simply substituted as the “best guess” for the estimate using more context; that is, ˜ei = ˜ei+1.28 As mentioned in Section 6.4, fully lexicalized modifying nonterminals are generated in two steps.

This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. $$$$$ This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. $$$$$ Without bigrams, performance drops only to 89.49% on recall, 89.95% on precision— an exceedingly small drop in performance (see Table 8, Model Mtw,t).

We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ Every headword in a lexicalized parse tree is the modifier of some other headword—except the word that is the head of the entire sentence (i.e., the headword of the root nonterminal).
We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ The first two lines show the results of Collins’ parser and those of our parser in its “complete” emulation mode (i.e., including unpublished details).

This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.
This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ These confidence values can be derived in a number of sensible ways; the technique used by Collins was adapted from that used in Bikel et al. (1997), which makes use of a quantity called the diversity of the history context (Witten and Bell 1991), which is equal to the number of unique futures observed in training for that history context.

Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ All parameters that generate trees in Collins’ model are estimates of conditional probabilities.
Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ Many of the parameter classes in Collins’ model—and indeed, in most statistical parsing models—define conditional probabilities with very large conditioning contexts.

The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). $$$$$ Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.
The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). $$$$$ This step enables the parsing model to be sensitive to the different contexts in which such subjectless sentences occur as compared to normal S nodes, since the subjectless sentences are functionally acting as noun phrases.

For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ Recently, in order to continue our work combining word sense with parsing (Bikel 2000) and the study of language-dependent and -independent parsing features (Bikel and Chiang 2000), we built a multilingual parsing engine that is capable of instantiating a wide variety of generative statistical parsing models (Bikel 2002).1 As an appropriate baseline model, we chose to instantiate the parameters of Collins’ Model 2.
For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ In particular, it appeared that the head rules used in Collins’ parser had been tweaked specifically for the English Penn Treebank.

 $$$$$ After these events have been collected 13 This phrase was taken from a comment in one of Collins’ preprocessing Perl scripts.
 $$$$$ N66001-00-1-8915.
