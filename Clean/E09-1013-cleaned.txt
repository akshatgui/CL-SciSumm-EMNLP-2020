More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.

(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ Bayesian Word Sense Induction
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.

Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.

Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.

We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ 1 word window).
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).

This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.

Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.
Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.

LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ Graph-based methods have also been applied to the sense induction task.
LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.

Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.

It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.
It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.

In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.

Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ Bayesian Word Sense Induction
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.

We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).

Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ Graph-based methods have also been applied to the sense induction task.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.

Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. $$$$$ For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.

In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ The remainder of this paper is structured as follows.
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ We discuss our results in Section 6, and conclude in Section 7.

In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.

 $$$$$ In other words, we could simply assume that the contexts we wish to model are the union of all our features.
 $$$$$ We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.

Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ The most probable words for each sense are also shown.

Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).
