Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. $$$$$ To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.
Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. $$$$$ While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments.

In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.
The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ For exact details on these constraints, see Snover et al. (2006).
Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

 $$$$$ Matching using stems and synonyms (Banerjee and Lavie, 2005) and using paraphrases (Zhou et al., 2006; Kauchak and Barzilay, 2006) have previously been shown to be beneficial for automatic MT evaluation.
 $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at

Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.
The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). $$$$$ Two words are determined to be synonyms if they share the same synonym set according to WordNet (Fellbaum, 1998).
Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ For exact details on these constraints, see Snover et al. (2006).
Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. $$$$$ Automatic MT evaluation metrics compare the hypothesis against this set of reference translations and assign a score to the similarity; higher scores are given to hypotheses that are more similar to the references.
Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. $$$$$ Document level Adequacy scores are determined by taking the length weighted average of the segment level scores.

The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. $$$$$ To study this we introduce a new evaluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.
TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. $$$$$ While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments.

However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. $$$$$ Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp.
However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ TER-Plus (TERp) is an extension of TER that aligns words in the hypothesis and reference not only when they are exact matches but also when the words share a stem or are synonyms.
These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ TERp identifies words in the hypothesis and reference that share the same stem using the Porter stemming algorithm (Porter, 1980).

After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).
After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ The paraphrases used in TERp were extracted using the pivot-based method as described in (Bannard and Callison-Burch, 2005) with several additional filtering mechanisms to increase the precision.

TERp (Snover et al, 2009) $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
TERp (Snover et al, 2009) $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. $$$$$ The IBM version of BLEU was used in case insensitive mode with an ngram-size of 4 to calculate the BLEU scores.
The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. $$$$$ TER was also used in case insensitive mode.

This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. $$$$$ For exact details on these constraints, see Snover et al. (2006).
This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. $$$$$ When using METEOR, the exact matching, porter stemming matching, and WordNet synonym matching modules were used.

Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).

In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997). $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997). $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).
