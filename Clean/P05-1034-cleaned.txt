 $$$$$ If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string

This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.

Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ Yamada and Knight (01) employ a parser in the target language to train probabilities on a set of operations that convert a target language tree to a source language string.
Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ A detailed description of these heuristics can be found in Quirk et al. (04).

We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ A detailed description of these heuristics can be found in Quirk et al. (04).

 $$$$$ If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string

Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ A detailed description of these heuristics can be found in Quirk et al. (04).
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ We will initially approach the decoding problem as a bottom up, exhaustive search.

This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ Dependency Treelet Translation

For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ The word alignments are used to project the source dependency parses onto the target sentences.
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.

We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words.
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper.

(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ From this aligned parallel dependency corpus we extract a treelet translation model incorporating source and target treelet pairs, where a treelet is defined to be an arbitrary connected subgraph of the dependency tree.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ This approach offers the following advantages over string-based SMT systems

 $$$$$ If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string

Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ The word alignments are used to project the source dependency parses onto the target sentences.
Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.

The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ One simple means of incorporating syntax into SMT is by re-ranking the n-best list of a baseline SMT system using various syntactic models, but Och et al. (04) found very little positive impact with this approach.
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ A detailed description of these heuristics can be found in Quirk et al. (04).

Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.
Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Under the assumption that constituents generally move as a whole, we predict the probability of each given ordering of modifiers independently.

Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ Along similar lines, Alshawi et al. (2000) treat translation as a process of simultaneous induction of source and target dependency trees using headtransduction; again, no separate parser is used.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.

 $$$$$ If we read off the leaves in a left-to-right in-order traversal, we do not get the original input string

Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ The target language model was trained using only the French side of the corpus; additional data may improve its performance.

We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ The word alignments are used to project the source dependency parses onto the target sentences.

For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ The word alignments are used to project the source dependency parses onto the target sentences.
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ Given a word aligned sentence pair and a source dependency tree, we use the alignment to project the source structure onto the target sentence.

We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ We also keep treelet counts for maximum likelihood estimation.
We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ Thus we try to predict as follows
