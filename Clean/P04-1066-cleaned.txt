 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ With it, the combined model has substantially lower error than the heuristic model without re-estimation; without it, for any number of EM iterations, the combined model has higher error than the heuristic model.

Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ Since Model 1, like many other word-alignment models, requires each target word to be generated by exactly one source word (including the null word), an alignment a can be represented by a vector a1,... , am, where each aj is the sentence position of the source word generating tj according to the alignment.
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ Improving IBM Word Alignment Model 1
Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence.

this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ The first of these nonstructural problems with Model 1, as standardly trained, is that rare words in the source language tend to act as “garbage collectors” (Brown et al., 1993b; Och and Ney, 2004), aligning to too many words in the target language.
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ This is exactly the property needed to prevent rare source words from becoming garbage collectors.

Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ The simplest approach would be to divide each LLR score by the sum of the scores for the source word of the pair, which would produce a normalized conditional probability distribution for each source word.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.

Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ Suppose the frequent source word has the translation present in the target sentence only 10% of the time in our training data, and thus has an estimated translation probability of around 0.1 for this target word.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ The columns of the table present (in order) a description of the model being tested, the AER on the trial data, the AER on the test data, test data recall, and test data precision, followed by the optimal values on the trial data for the LLR exponent, the initial (heuristic model) null-word weight, the nullword weight used in EM re-estimation, the add-n parameter value used in EM re-estimation, and the number of iterations of EM.

This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ To maintain this property, for each source word we compute the sum of the LLR scores over all target words, but we then divide every LLR score by the single largest of these sums.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ Improving IBM Word Alignment Model 1
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence, given an instance of Model 1; where, by an alignment, we mean a specification of which source words generated which target words according to the generative model.

For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ But this is only the case if we want to obtain the parameter values at convergence, and we have strong reasons to believe that these values do not produce the most accurate sentence alignments.
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ Hence we initialize the distribution for the null word to be the unigram distribution of target words, so that frequent function words will receive a higher probability of aligning to the null word than rare words, which tend to be content words that do have a translation.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ It should certainly be better than smoothing with a unigram distribution, since we especially want to benefit from smoothing the translation probabilities for the rarest words, and smoothing with a unigram distribution would assume that rare words are more likely to translate to frequent words than to other rare words, which seems counterintuitive.
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003).
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1.
