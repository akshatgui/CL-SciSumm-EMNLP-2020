Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ Discriminative Reranking For Natural Language Parsing
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ A baseline statistical parser is used to generate N-best output both for its training set and for test data sentences.

Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the model.
Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ These methods all emphasize models which define a joint probability over the space of all parse trees (or structures in question)

In our work, we included all features described in (Collins and Koo, 2005). $$$$$ The following types of features were included in the model.
In our work, we included all features described in (Collins and Koo, 2005). $$$$$ Both approaches still rely on decomposing a parse tree into a sequence of decisions, and we would argue that the techniques described in this article have more flexibility in terms of the features that can be included in the model.

We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ 5.4.3 Efficiency Gains.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ In this section we explore the empirical gains in efficiency seen on the parsing data sets in this article.

In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ For example, the iterative scaling procedure described above must be applied for a number of features.
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ The number of such parses varies sentence by sentence.

Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ Two-level rules.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ Same as Rules, but also including the entire rule above the rule.

A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse

There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ Discriminative Reranking For Natural Language Parsing
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.

These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ Discriminative Reranking For Natural Language Parsing
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.

Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ This means that given an input example x and parameter values ¯a, the output from the classifier is Collins and Koo Discriminative Reranking for NLP where hyperplane which passes through the origin4 of the space and has a¯ as its normal.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ We now describe the form of these updates.

As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ They are important for a few reasons.
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ For example, a score of 101.5 indicates a 1.5% increase in this score.

The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ In general, a separate set of instances is used in cross-validation to choose the stopping point, that is, to decide on the number of features in the model.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ The remaining 4,000 sentences were used as development data and to cross-validate the number of rounds (features) in the model.

We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ Johnson et al. (1999) and Riezler et al.

For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ The method gives substantial improvements over the original parser and results which are very close to the results of the boosting method we have described in this article (see section 5 for experimental results comparing the two methods).

As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.

We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ We use Si,j to refer to this weight.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Assuming we greedily pick a single feature with some weight to update the model, and given that the current parameter settings are ¯a, the optimal feature/weight pair (k*, d*) is Note that this is essentially the idea behind the “boosting”approach to feature selection introduced in section 3.3.

At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ We argue that the efficient boosting algorithm introduced in this article is an attractive alternative to maximum-entropy models, in particular, feature selection methods that have been proposed in the literature on maximum-entropy models.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ The additional features are incorporated using a method inspired by maximum-entropy models (e.g., the model of Ratnaparkhi [1997]).

Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ The reranking models in this article were originally introduced in Collins (2000).
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ Same as Rules, but also including the entire rule above the rule.

Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ Discriminative Reranking For Natural Language Parsing
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).

In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ We define GEN(x)ÎY to be the set of candidates for a given input x.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ The model of Collins (1999) was the base model; the ExpLoss model gave a 1.5% absolute improvement over this method.
