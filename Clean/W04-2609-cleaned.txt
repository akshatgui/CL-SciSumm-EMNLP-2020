The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation from those listed in Table 1.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ Here is an example.

It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ Roles.
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.

The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ Details about this approach are provided in (Girju et al. 2004)).

We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ So far we have experimented with three models: (1) semantic scattering, (2) decision trees, and (3) naive Bayes.
We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ Semantic Scattering.

We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ Semantic Scattering.
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ This is a new model developed by us particularly useful for the classification of compound nominals without nominalization.

Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text.

Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993).
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ Also, the judges tagged the NP nouns in the training corpus with their corresponding WordNet senses.

Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Noun - noun (adjective - noun, respectively) sequences of words were extracted using the Lauer heuristic (Lauer 1995) which looks for consecutive pairs of nouns that are neither preceded nor succeeded by a noun after each sentence was syntactically parsed with Charniak parser (Charniak 2001) (for XWN we used the gold parse trees).
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Using a stateof-the-art open-text WSD system, each word is mapped into its corresponding WordNet 2.0 sense.

In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ In order to focus our research, we will concentrate for now only on noun - noun or adjective - noun compositional constructions at NP level, ie those whose meaning can be derived from the meaning of the constituent nouns (“door knob”, “cup of wine”).
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ Using a stateof-the-art open-text WSD system, each word is mapped into its corresponding WordNet 2.0 sense.

We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text.
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ Details about this approach are provided in (Girju et al. 2004)).

Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ Models For The Semantic Classification Of Noun Phrases
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ Details about this approach are provided in (Girju et al. 2004)).

Moldovan et al (2004) also use WordNet. $$$$$ This is called semantic blend (Quirk et al.1985).
Moldovan et al (2004) also use WordNet. $$$$$ Details about this approach are provided in (Girju et al. 2004)).

The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993).
The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ Details about this approach are provided in (Girju et al. 2004)).

Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ This is called semantic blend (Quirk et al.1985).
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ Semantic Scattering.

A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ So far, we have identified and experimented with the following NP features: specifies the WordNet synset of the modifier noun.
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ WordNet synsets) of the NP modifiers and, respectively NP heads (ie features 2 and 1).

Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ Several approaches have been proposed for empirical noun-compound interpretation, such as syntactic analysis based on statistical techniques (Lauer and Dras 1994), (Pustejovsky et al. 1993).
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ Details about this approach are provided in (Girju et al. 2004)).

Moldovan et al (2004) also use WordNet. $$$$$ This is called semantic blend (Quirk et al.1985).
Moldovan et al (2004) also use WordNet. $$$$$ Details about this approach are provided in (Girju et al. 2004)).

Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ Semantic Scattering.
Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ Details about this approach are provided in (Girju et al. 2004)).

From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ Our list contains the most frequently used semantic relations we have observed on a large corpus.
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ We computed the K coefficient only for those instances tagged with one of the 35 semantic relations.

For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ Relation no.
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ Details about this approach are provided in (Girju et al. 2004)).
