They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ We first describe the PageRank and Personalized PageRank algorithms.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.

A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context.
A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context.

Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin.
Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.

PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). $$$$$ In this section we will briefly describe some graph-based methods for knowledge-based WSD.
PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). $$$$$ In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambuation.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ In this paper we present a novel graph-based WSD algorithm which uses the full graph of WordNet efficiently, performing significantly better that previously published approaches in English all-words datasets.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ We first describe the PageRank and Personalized PageRank algorithms.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.

Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ We first describe the PageRank and Personalized PageRank algorithms.
Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ (1); and whenever a modified v is used, we will call it Personalized PageRank.

 $$$$$ (1); and whenever a modified v is used, we will call it Personalized PageRank.
 $$$$$ This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01).

In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.
In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ In this section we will briefly describe some graph-based methods for knowledge-based WSD.

In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ We first describe the PageRank and Personalized PageRank algorithms.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.

In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.
In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.

The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ In our experiments we build a context of at least 20 content words for each sentence to be disambiguated, taking the sentences immediately before and after it in the case that the original sentence was too short.
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ The results for the supervised system are given for reference, and we can see that the gap is relatively small, specially for Senseval3.

Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ We first describe the PageRank and Personalized PageRank algorithms.
Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ They also compare different graph-based centrality algorithms to rank the vertices of the complete graph.

In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ We first describe the PageRank and Personalized PageRank algorithms.
In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ (1); and whenever a modified v is used, we will call it Personalized PageRank.

In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ Personalizing PageRank for Word Sense Disambiguation
In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.

The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ As mentioned before, personalized PageRank allows us to use the full LKB.
The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ As stated before, all methods presented here use some LKB for performing WSD.

A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.
A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.

This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ Personalizing PageRank for Word Sense Disambiguation
This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ We first describe the PageRank and Personalized PageRank algorithms.

The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.
The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ The results on our datasets, when available, are shown in Table 2.

In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Regarding LKBs, the best results are obtained using WordNet 1.7 and eXtended WordNet.
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ In (Agirre and Soroa, 2008) we experiment with different LKBs formed by combining relations of different MCR versions along with relations extracted from SemCor, which we call supervised and unsupervised relations, respectively.
