The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ Classifier performance is poor in such cases because the classifier was trained on well-edited text, i.e., without misspelled words.
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.

(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ What these two evaluation methods have in common is that they side-step the issue of annotator reliability.
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ 3.2 Reliability.

This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ and ?OK? sub corpora.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ The original corpus totaled over 22,000 prepositions which would normally take several weeks for two raters to double annotate and thenadjudicate.

Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system.
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%).

(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one.
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ human rater.

For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ Despite the rigorous training regimen, kappa ranged from 0.411 to 0.786, with an overall combined value of 0.630.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments.

There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ c ? 2008.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ In analyzing the contexts, we used only tagging and heuris tic phrase-chunking, rather than parsing, so as to avoid problems that a parser might encounter with ill-formed non-native text 1 . In test mode, the clas-.

In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ 3.1 Annotation.
In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008).

Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ Finally, the raters suggested prepositions that would best fit the context, even if there were no error (some contexts can license multiple prepositions).
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ prepositions.

The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ Usage errors involving prepositions are among the most common types seen in thewriting of non-native English speakers.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ What is responsiblefor making preposition usage so difficult for non native speakers?

Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.

The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.

Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ c ? 2008.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ which represent richer contextual struc ture in the form of syntactic patterns.Table 1 (first column) illustrates the four com bination features used for the example context ?take our place in the line?.

Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.

 $$$$$ We compared 867 Class Components Combo

For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ c ? 2008.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ We have used a Maximum Entropy (ME) classi fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo sitions.

For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall.
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008).

We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al, 2004), (Han et al, 2006), (Nagata et al, 2006), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.

We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ c ? 2008.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ 2.3 Combination Features.

The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ 3.1 Annotation.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ Of the prepositions that Rater 1 judged to be errors, Rater 2 judged 30.2% to be acceptable.
