a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ Here each state item contains a partial parse tree as well as a stack configuration, and state items are built incrementally by transition actions.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ In particular, the graph-based model is unable to score the actions “Reduce” and “Shift”, since they do not modify the parse tree.

As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuraof respectively.
As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ Like Duan et al. (2007), we use gold-standard POS-tags for the input.

We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ We develop three parsers.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ We develop our transition-based parser using the transition model of the MaltParser (Nivre et al., 2006), which is characterized by the use of a stack and four transition actions: Shift, ArcRight, ArcLeft and Reduce.

Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches.

 $$$$$ The MaltParser works deterministically.
 $$$$$ We thank the anonymous reviewers for their detailed comments.

We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ Bracketed sentences from the Penn Treebank (PTB) 3 are split into training, development and test sets as shown in Table 4, and then translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003).
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ We excluded these from Table 5 because our work is not concerned with the use of such additional knowledge. split the corpus into training, development and test data as shown in Table 6, and use the head-finding rules in Table 8 in the Appendix to turn the bracketed sentences into dependency structures.

Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ The graph-based and transition-based approaches adopt very different views of dependency parsing.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved.

CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ In line with previous work on dependency parsing using the Penn Treebank, we focus on projective dependency parsing.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ It gave the best accuracy we are aware of for dependency parsing using CTB.

To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ In experiments with the English and Chinese Penn Treebank data, the combined parser gave 92.1% and 86.2% accuracy, respectively, which are comparable to the best parsing results for these data sets, while the Chinese accuracy outperforms the previous best reported by 1.8%.
To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ In line with previous work on dependency parsing using the Penn Treebank, we focus on projective dependency parsing.

 $$$$$ The MaltParser works deterministically.
 $$$$$ We thank the anonymous reviewers for their detailed comments.

To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006).

The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ Both rank state items by their current scores, and use the averaged perceptron with early update for training.
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy.

Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ When B = 1, the transition-based parser becomes a deterministic parser.
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ Using only graph-based features in Table 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (“Graph [M]”).

For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ We evaluate the parsers using the English and Chinese Penn Treebank corpora.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ Bracketed sentences from the Penn Treebank (PTB) 3 are split into training, development and test sets as shown in Table 4, and then translated into dependency structures using the head-finding rules from Yamada and Matsumoto (2003).

 $$$$$ The MaltParser works deterministically.
 $$$$$ We thank the anonymous reviewers for their detailed comments.

 $$$$$ The MaltParser works deterministically.
 $$$$$ We thank the anonymous reviewers for their detailed comments.

Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ During training, the “early update” strategy of Collins and Roark (2004) is used: when the correct state item falls out of the beam at any stage, parsing is stopped immediately, and the model is updated using the current best partial item.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ As with the graph-based parser, we use the discriminative perceptron (Collins, 2002) to train the transition-based model (see Figure 5).

 $$$$$ The MaltParser works deterministically.
 $$$$$ We thank the anonymous reviewers for their detailed comments.

The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ Following Duan et al. (2007), we 1A recent paper, Koo et al.
The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ Both Hall et al. (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models.

We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ The terms “graph-based” and “transition-based” were used by McDonald and Nivre (2007) to describe the difference between MSTParser (McDonald and Pereira, 2006), which is a graph-based parser with an exhaustive search decoder, and MaltParser (Nivre et al., 2006), which is a transition-based parser with a greedy search decoder.
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ When B = 1, the transition-based parser becomes a deterministic parser.
