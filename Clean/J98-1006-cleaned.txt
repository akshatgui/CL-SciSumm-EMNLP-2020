Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ The Cognitive Science Laboratory at Princeton University, with support from NSF-ARPA, is producing textual corpora that can be used in developing and evaluating automatic methods for disambiguation.
Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials

The following similarity measures were considered $$$$$ The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon.
The following similarity measures were considered $$$$$ Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance.

Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). $$$$$ Thus, Leacock, Towell, and Voorhees (1993) found that some senses of the noun line are not susceptible to disambiguation with topical context.
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). $$$$$ An estimate is possible.

Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj s,), including providing probabilities for cues that did not occur in the training.
Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).

Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.
Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).

It was first suggested by Leacock et al (1998). $$$$$ Leacock, Towell, and Voorhees (1996) showed that performance of the content vector topical classifier could be improved with the addition of local templates— specific word patterns that were recognized as being indicative of a particular sense— in an extension of an idea initially suggested by Weiss (1973).
It was first suggested by Leacock et al (1998). $$$$$ The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).

Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.
Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ Finally, we hope to avoid inclusion of spurious senses by using monosemous relatives.

Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus.
Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ Since the frequencies of the monosemous relatives do not correlate with the frequencies of the senses, prior probabilities must be estimated for classifiers that use them.

Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994).
Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ Because the supply of manually tagged training data will always be limited, we propose a method to obtain training data automatically using commonly available materials

Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ We propose to minimize spurious training by using monosemous words and collocations—on the assumption that, if a word has only one sense in WordNet, it is monosemous.
Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).

Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ Positions j = —2, —1, 1,2 are used.
Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ When it is configured for local information, cue types (2), (3), and (4) are used.

In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ This was done on the assumption that these words are not ambiguous.
In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).

Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. $$$$$ This step normalizes across morphological variants without resorting to the more drastic measure of stemming.
Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. $$$$$ The verbal hierarchy is based on troponymy, the is a manner of relation.

Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ The similarity measures on WordNet found that sauerbraten was most similar to dinner in the training, and dumpling to bacon.
Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ Augmentation of the local .context classifier with WordNet similarity measures showed a small but consistent improvement in the classifier's performance.

Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. $$$$$ Leacock, Towel!, and Voorhees (1993) compared this Bayesian classifier with a content vector classifier as used in information retrieval and a neural network with backpropagation.
Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. $$$$$ All classifiers performed best with at least 200 training examples per sense, but the learning curve tended to level off beyond a minimum 100 training examples.

The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. $$$$$ Resnik (1992) uses an information-based measure, the most informative class, on the WordNet taxonomy.
The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. $$$$$ A class consists of the synonyms found at a node and the synonyms at all the nodes that it dominates (all of its hyponyms).

To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ They range from dictionary-based approaches that rely on definitions (Vdronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Schtitze 1995; Dagan and Itai 1994).
To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).

The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). $$$$$ The system first looks for the strongest or top-level relatives

In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ Table 6 shows TLC's performance on the other eight words after training with monosemous relatives and testing on manually tagged examples.
In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ We found this method to be effective, although not as effective as using manually tagged training.

This method is inspired in (Leacock et al, 1998). $$$$$ The results of disambiguation strategies reported for pseudowords and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towel!, and Voorhees 1993).
This method is inspired in (Leacock et al, 1998). $$$$$ Schulze has used the method to disambiguate pseudowords, homographs, and polysemous words.
