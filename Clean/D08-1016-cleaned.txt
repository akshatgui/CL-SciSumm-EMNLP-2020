Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Dependency Parsing by Belief Propagation
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.

The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ TREE.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function.

We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ Dependency Parsing by Belief Propagation
We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ (The method is approximate because a first-order parser must equally penalize all parses containing e', even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser.

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). $$$$$ Dependency Parsing by Belief Propagation
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). $$$$$ Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ If we had simply replaced the global TRIGRAM factor with its subfactors in the full factor graph, we would have had to resort to Generalized BP (Yedidia et al., 2004) to obtain the same exact results.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.

Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ Dependency Parsing by Belief Propagation
Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ A family of hard binary constraints.

Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ It is convenient to visualize an undirected factor graph (Fig.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ Fig.

Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ A hard global constraint on all the Lij variables at once.
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ Given the BP architecture, do we even need the hard TREE constraint?

Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ It is convenient to visualize an undirected factor graph (Fig.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ Fig.

Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ But how about global factors?
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Finally, we can take advantage of improvements to BP proposed in the context of other applications.

 $$$$$ Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.
 $$$$$ For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.

 $$$$$ Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.
 $$$$$ For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.

However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ We will give specialized algorithms for handling these summations more efficiently.
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996).

Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ LINK.
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ Let F be a factor and V be one of its neighboring variables.

To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ Dependency Parsing by Belief Propagation
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ Our experiments use the same features as McDonald et al. (2005).

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Our experiments use the same features as McDonald et al. (2005).
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ For TREE (nonprojective), Koo et al. (2007) and Smith and Smith (2007) show how to employ the matrix-tree theorem.

For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ Dependency Parsing by Belief Propagation
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006).

In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ We employ stochastic gradient descent (Bottou, 2003), since this does not require us to compute the objective function itself but only to (approximately) estimate its gradient as explained above.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ (Parenthetical numbers show that the harm is compounded if the weaker constraints are used in training as well; even though this matches training to test conditions, it may suffer more from BP’s approximate gradients.)

This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ More complex models would widen BP’s advantage.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ All results use 5 iterations of BP.

One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ 17But taking it = 1 gives the same results, up to a constant.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ All results use 5 iterations of BP.
