Wu (1997) and Alshawi et al (2000) showed statistical models based on syntactic structure. $$$$$ In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1, 0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (Alshawi and Douglas 2000) with a larger range of positions.
Wu (1997) and Alshawi et al (2000) showed statistical models based on syntactic structure. $$$$$ Brown et al. 1993).

Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al (2000). $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.
Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al (2000). $$$$$ These model parameters can be used to generate pairs of synchronized dependency trees starting with the topmost nodes of the two trees and proceeding recursively to the leaves.

For a different approach that is based on dependency tree transformations, see Alshawi et al (2000). $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.
For a different approach that is based on dependency tree transformations, see Alshawi et al (2000). $$$$$ At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.

Yamada and Knight (2000, 2001) and Alshawi et al (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. $$$$$ Learning Dependency Translation Models As Collections Of Finite-State Head Transducers
Yamada and Knight (2000, 2001) and Alshawi et al (2000) have effectively extended such syntactic transduction models to fully functional SMT systems, based on channel model tree transducers and finite state head transducers respectively. $$$$$ In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1, 0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (Alshawi and Douglas 2000) with a larger range of positions.

Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. $$$$$ Brown et al. 1993).
Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. $$$$$ At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.

Alshawi et al (2000) and Hwa et al (2005) explore transfer of deeper syntactic structure $$$$$ Brown et al. 1993).
Alshawi et al (2000) and Hwa et al (2005) explore transfer of deeper syntactic structure $$$$$ At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.

Other statistical machine translation systems such as (Wu, 1997) and (Alshawi et al, 2000) also produce a tree given a sentence. $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.
Other statistical machine translation systems such as (Wu, 1997) and (Alshawi et al, 2000) also produce a tree given a sentence. $$$$$ 1993) for training translation systems automatically.

The latter are small and simple (Alshawi et al, 2000) $$$$$ A dependency tree for a sentence, in the sense of dependency grammar (for example Hays [1964] and Hudson [1984]), is a tree in which the words of the sentence appear as nodes (we do not have terminal symbols of the kind used in phrase structure grammar).
The latter are small and simple (Alshawi et al, 2000) $$$$$ Brown et al. 1993).

However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al (2000) are strictly less powerful than STSG. $$$$$ In the transducers produced by the training method described in this paper, the source and target positions are in the set {-1, 0,1}, though we have also used handcoded transducers (Alshawi and Xia 1997) and automatically trained transducers (Alshawi and Douglas 2000) with a larger range of positions.
However, the binary-branching SCFGs used by Wu (1997) and Alshawi et al (2000) are strictly less powerful than STSG. $$$$$ Brown et al. 1993).

Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. $$$$$ Brown et al. 1993).
Methods such as (Wu, 1997), (Alshawi et al, 2000) and (Lopez et al, 2002) employ a synchronous parsing procedure to constrain a statistical alignment. $$$$$ At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.

(Alshawi et al, 2000) represents each production in parallel dependency trees as a finite-state transducer. $$$$$ Learning Dependency Translation Models As Collections Of Finite-State Head Transducers
(Alshawi et al, 2000) represents each production in parallel dependency trees as a finite-state transducer. $$$$$ The dependency transduction model produces synchronized dependency trees in which each local tree is produced by a head transducer.

Along similar lines, Alshawi et al (2000) treat translation as a process of simultaneous induction of source and target dependency trees using head transduction; again, no separate parser is used. $$$$$ In this section we describe dependency transduction models, which can be used for machine translation and other transduction tasks.
Along similar lines, Alshawi et al (2000) treat translation as a process of simultaneous induction of source and target dependency trees using head transduction; again, no separate parser is used. $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.

Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. $$$$$ The source and target dependency trees derived by a dependency transduction model are ordered, i.e., there is an ordering on the nodes of each local tree.
Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. $$$$$ Brown et al. 1993).

Alshawi et al (2000) also presented a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. $$$$$ These models consist of a collection of head transducers that are applied hierarchically.
Alshawi et al (2000) also presented a two-level arranged word ordering and chunk ordering by a hierarchically organized collection of finite state transducers. $$$$$ The source and target dependency trees derived by a dependency transduction model are ordered, i.e., there is an ordering on the nodes of each local tree.

Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer. $$$$$ In such a tree, the parent of a node is its head and the child of a node is the node's dependent.
Wu (1997) showed that restricting word-level alignments between sentence pairs to observe syntactic bracketing constraints significantly reduces the complexity of the alignment problem and allows a polynomial-time solution. Alshawi et al (2000) also induce parallel tree structures from unbracketed parallel text, modeling the generation of each node's children with a finite-state transducer. $$$$$ This procedure has 0(n6) complexity in the number of words in the source (or target) sentence.

In a somewhat related manner, Alshawi et al (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.
In a somewhat related manner, Alshawi et al (2000) use cascaded head automata to derive dependency trees, but leave the nature of the cascading under-formalized. $$$$$ Head transducers and dependency transduction models are thus related as follows

It is described in Alshawi et al (2000b). $$$$$ Brown et al. 1993).
It is described in Alshawi et al (2000b). $$$$$ At the same time, we believe our method has advantages over the approach developed initially at IBM (Brown et al. 1990; Brown et al.

Wu (1997) and Alshawi et al (2000) used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation, though not necessarily similar to what a human annotator would select. $$$$$ Learning Dependency Translation Models As Collections Of Finite-State Head Transducers
Wu (1997) and Alshawi et al (2000) used unsupervised learning on parallel text to induce syntactic analysis that was useful for their respective applications in phrasal translation extraction and speech translation, though not necessarily similar to what a human annotator would select. $$$$$ These metrics, simple accuracy and translation accuracy, are used to compare the target string produced by the system against a reference human translation from held-out data.

(Alshawi et al, 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. $$$$$ The dependency transduction model produces synchronized dependency trees in which each local tree is produced by a head transducer.
(Alshawi et al, 2000) extended the tree-based approach by representing each production in parallel dependency trees as a finite-state transducer. $$$$$ In the case of machine translation, the transducers derive pairs of dependency trees, a source language dependency tree and a target dependency tree.

The input to our algorithm is a corpus consisting of pairs of sentences related by an hierarchical alignment (Alshawi et al, 2000). $$$$$ Of course, translations of phrases are not always transparently related by a hierarchical alignment.
The input to our algorithm is a corpus consisting of pairs of sentences related by an hierarchical alignment (Alshawi et al, 2000). $$$$$ In Alshawi and Douglas (2000) we describe a version of the alignment algorithm in which heads may have an arbitrary number of dependents, and in which the hierarchical alignments for the training corpus are refined by iterative reestimation.
