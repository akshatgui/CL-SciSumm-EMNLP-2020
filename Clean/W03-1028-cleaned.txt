In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ However, relatively few documents have keywords assigned, and therefore finding methods to automate the assignment is desirable.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ The results are presented next.

In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ Improved Automatic Keyword Extraction Given More Linguistic Knowledge
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ The results are presented next.

For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ The machine learning approach used for the experiments is that of rule induction, i.e., the model that is constructed from the given examples, consists of a set of rules2.

It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ In the next set of experiments a partial parserl was used to select all NP-chunks from the documents.
It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ The total number of manually assigned terms present in the abstracts is 3 816, and the mean is 7.63 terms per document. tion approaches, extracting NP-chunks gives a better precision, while extracting all words or sequences of words matching any of a set of POS tag patterns gives a higher recall compared to extracting ngrams.

In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ For all experiments the same training, validation, and test sets were used.
In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ When calculating the recall, the value for the total number of manually assigned keywords present in the documents is used, independent of the number actually present in the different representations.

In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ Several runs were made for each representation, with the goal to maximise the performance as evaluated on the validation set

In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ The trained model is subsequently applied to documents for which no keywords are assigned

Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ There are two drawbacks in common with the approaches proposed by Turney (2000) and Frank et al. (1999).
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ As opposed to Turney (2000) and Frank et al. (1999), who experiment with keyword extraction from full-length texts, this work concerns keyword extraction from abstracts.

In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ Four different features are used

Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Four different features are used

More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ Improved Automatic Keyword Extraction Given More Linguistic Knowledge
More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.

In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ Four different features are used

Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Four different features are used

More linguistic knowledge has been explored by Hulth (2003). $$$$$ Improved Automatic Keyword Extraction Given More Linguistic Knowledge
More linguistic knowledge has been explored by Hulth (2003). $$$$$ As the results were poor, two alternatives to extracting n-grams as the potential terms were explored.

Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ The collection used for the experiments described in this paper consists of 2 000 abstracts in English, with their corresponding title and keywords from the Inspec database.
Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ The abstracts are from the years 1998 to 2002, from journal papers, and from the disciplines Computers and Control, and Information Technology.

As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ The first approach was to extract all noun phrases in the documents as judged by an NP-chunker.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ When inspecting manually assigned keywords, the vast majority turn out to be nouns or noun phrases with adjectives, and as discussed in Section 2, the research on term extraction focuses on noun patterns.

Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).
Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ As opposed to Turney (2000) and Frank et al. (1999), who experiment with keyword extraction from full-length texts, this work concerns keyword extraction from abstracts.

Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).

This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ Part of the same training and test material is later used by Frank et al. (1999) for evaluating their algorithm in relation to Turneyâ€™s algorithm.
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ or mass) Initially, the same features that Frank et al. (1999) used for their domain-independent experiments were used.

While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ Using phrases means that the length of the potential terms is not restricted to something arbitrary, rather the terms are treat as the units they are.
While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ In this paper I have not touched upon the more intricate aspects of evaluation, but simply treated the manually assigned keywords as the gold standard.
