We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ Two popular techniques that incorporate the error criterion are Minimum Error Rate Training (MERT) (Och, 2003) and Minimum BayesRisk (MBR) decoding (Kumar and Byrne, 2004).

Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ We therefore approximate the quantity f(e, w, E) with f*(e, w,G) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice G. f*(e, w,G) can be computed locally, and the n-gram posterior probability based on f* can be determined as follows

For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices.
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs.

Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ Next, the posterior probability of each n-gram is computed.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ Table 4 compares Hypergraph MBR (HGMBR) with MAP and MBR decoding on 1000 best lists.

We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004).
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ We now describe our experiments to evaluate MERT and MBR on lattices and hypergraphs, and show how MERT can be used to tune MBR parameters.

The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ Table 2 shows runtime experiments for the hypergraph MERT implementation in comparison with the phrase-lattice implementation on both the aren and the zhen system.
The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ In other cases, Hypergraph MBR performs at least as well as N-best MBR.

These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ The suggested algorithm has similar properties as the algorithm presented in (Macherey et al., 2008).

MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ MERT is then performed to optimize the BLEU score on a development set; For MERT, we use 40 random initial parameters as well as parameters computed using corpus based statistics (Tromble et al., 2008).
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ In both tables, the following results are reported

While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ The key idea behind this new algorithm is to rewrite the n-gram posterior probability (Equation 4) as follows

Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ For comparison, we prune hypergraphs to the same density (# of edges per edge on the best path) and achieve identical running times for computing the error surface.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ In other cases, Hypergraph MBR performs at least as well as N-best MBR.

Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ This operation is a “max” operation and it is identical to the algorithm described in (Macherey et al., 2008) for phrase lattices.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ The algorithm to perform Lattice MBR is given in Algorithm 3.

All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ Next, the posterior probability of each n-gram is computed.

We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ We report results on two tasks.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ This shows the usefulness of Hypergraph MBR decoding as an efficient alternative to Nbest MBR.

(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ The lattice MBR decoder (Equation 3) can be written as a linear model

However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ We first review Minimum Bayes-Risk (MBR) decoding for statistical MT.

The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ A weight assignment of 1.0 for this feature function and zeros for the other feature functions would imply that the MAP translation is chosen.
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ We hypothesize that the default MBR parameters (Tromble et al., 2008) are well tuned.

These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ MBR decoding for translation can be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004).
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ We have presented efficient algorithms which extend previous work on lattice-based MERT (Macherey et al., 2008) and MBR decoding (Tromble et al., 2008) to work with hypergraphs.

The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008).
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ We will refer to this procedure as FSAMBR.

The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ In particular, MERT avoids the need for manually tuning these parameters by language pair.

More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT.
