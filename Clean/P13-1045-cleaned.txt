 $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.

 $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.

Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Socher et al. (2012) proposed to give every single word a matrix and a vector.

To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ This paper uses several ideas of (Socher et al., 2011b).
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ Socher et al. (2012) proposed to give every single word a matrix and a vector.

To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ 3.1 and the POS tags come from a PCFG.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).

 $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.

This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ Analysis of Composition Matrices.

Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ In this small model analysis, we use two pairs of sentences that the original Stanford parser and the CVG did not parse correctly after training on the WSJ.

We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ Our syntactically untied RNNs outperform them by a significant margin.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).

Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012).

Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ For more details on how standard RNNs can be used for parsing, see Socher et al. (2011b).

First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ The dev set accuracy of the best model is 90.93% labeled F1 on all sentences.

Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ Costa et al. (2003) apply recursive neural networks to re-rank possible phrase attachments in an incremental parser.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.

 $$$$$ Therefore we combine syntactic and semantic information by giving the parser access to rich syntacticosemantic information in the form of distributional word vectors and compute compositional semantic vector representations for longer phrases (Costa et al., 2003; Menchetti et al., 2005; Socher et al., 2011b).
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.
