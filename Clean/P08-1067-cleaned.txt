 $$$$$ We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.
 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.

All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec.

From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ Following Charniak and Johnson (2005), we extracted the features from the 50-best parses on the training set (sec.

The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ We first establish a unified framework for parse reranking with both n-best lists and packed forests.
The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.

A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ In n-best reranking, all features are treated equivalently by the decoder, which simply computes the value of each one on each candidate parse.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ This result confirms that our feature set design is appropriate, and the averaged perceptron learner is a reasonable candidate for reranking.

See Huang (2008) for more details. $$$$$ ” which is an instance of the WordEdges feature (see Figure 2(c) and Section 3.2 for details).
See Huang (2008) for more details. $$$$$ Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008).

However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ 2(b) is non-local.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ For both systems, we first use only the local features, and then all the features.

 $$$$$ We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.
 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.

With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ 2(b) is non-local.
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ For both systems, we first use only the local features, and then all the features.

That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ This result is also better than any previously reported systems trained on the Treebank.
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep. phrase PP3,6 “with a mirror”

Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ However, we miss the benefits of non-local features that are not representable here.
Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ The development set and the test set are parsed with a model trained on all 39832 training sentences.

 $$$$$ We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.
 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.

Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ 2(b) is non-local.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ For both systems, we first use only the local features, and then all the features.

Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ McClosky et al. (2006) achieved an even higher accuarcy (92.1) by leveraging on much larger unlabelled data.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008).

 $$$$$ We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.
 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.

In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ Forest Reranking

We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ For example, the Rule feature in Fig.

Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ We first establish a unified framework for parse reranking with both n-best lists and packed forests.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ We implemented both n-best and forest reranking systems in Python and ran our experiments on a 64bit Dual-Core Intel Xeon with 3.0GHz CPUs.

To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ We also use the notation (e, j) to denote the derivation along hyperedge e, using the jith subderivation for tail ui, so (e, 1) is the best derivation along e. The exact decoding algorithm, shown in Pseudocode 2, is an instance of the bottom-up Viterbi algorithm, which traverses the hypergraph in a topological order, and at each node v, calculates its 1-best derivation using each incoming hyperedge e E IN(v).
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ Basically, we use an Inside-Outside algorithm to compute the Viterbi inside cost Q(v) and the Viterbi outside cost a(v) for each node v, and then compute the merit aQ(e) for each hyperedge

For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ For example, one such feature f2000 might be a question “how many times is a VP of length 5 surrounded by the word ‘has’ and the period?
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ The cost of e, c(e), is the score of its Pseudocode 2 Exact Decoding with Local Features Pseudocode 3 Cube Pruning for Non-local Features (pre-computed) local features w · fL(e).
