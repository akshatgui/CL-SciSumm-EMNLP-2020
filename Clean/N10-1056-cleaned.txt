Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.

(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ For the sake of simplicity

Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ For the sake of simplicity

The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ For the sake of simplicity

More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.

In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ We consider two main approaches

Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.
Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ Related work and related problems Previous work usually involves general syntactic-level transformation rules [1, 9, 10].2 In contrast, we explore data-driven methods to learn lexical simplifications (e.g., “collaborate” → “work together”), which are highly specific to the lexical items involved and thus cannot be captured by a few general rules.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ He provides a list of 17,900 simple words — words that do not need further simplification — and a list of 2000 transformation pairs.

In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.

Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Simplification is strongly related to but distinct from paraphrasing and machine translation (MT).
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing).

(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ (We defer detailed description of how we extract lexical edit instances from data to §3.1.)

The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing).
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ We obtained the revision histories of both SimpleEW (November 2009 snapshot) and ComplexEW (January 2008 snapshot).

Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ Nothing is more simple than greatness; indeed, to be simple is to be great.

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ One major effort to produce such text is the Simple English Wikipedia (henceforth SimpleEW)1, a sort of spin-off of the well-known English Wikipedia (henceforth ComplexEW) where human editors enforce simplicity of language through rewriting.

Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ Importantly, not all the changes on SimpleEW are simplifications; we thus also make use of ComplexEW edits to filter out non-simplifications.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ We therefore sought to use comments to identify “trusted” revisions wherein the extracted lexical edit instances (see §3.1) would be likely to be simplifications.
