The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ We downloaded these embeddings from Turian et al (2010).
The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Erk and Pado?

We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ Reisinger and Mooney (2010b) introduced a multi-prototype VSM whereword sense discrimination is first applied by clus tering contexts, and then prototypes are built using the contexts of the sense-labeled words.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since werely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ We downloaded these embeddings from Turian et al (2010).

We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. $$$$$ C&W* is the word embeddings trained and provided by C&W. OurModel* is trained without stop words, while Our Model g uses only global context.
We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. $$$$$ Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).

In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ We downloaded these embeddings from Turian et al (2010).
In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ Two other recent papers (Dhillon et al, 2011; Reddy et al, 2011) present models for constructing word representations that deal with context.

Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012). $$$$$ We downloaded these embeddings from Turian et al (2010).
Source embeddings: We employ three external embeddings (obtained from (Turian et al, 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models, 2) Collobert and Weston's (C&W) deep-learning architecture, and 3) Huang et al's polysemous neural language model (HUANG) (Huang et al, 2012). $$$$$ Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).

 $$$$$ Other rare words not in the dictionary are mapped to an UNKNOWN token.For all experiments, our models use 50 dimensional embeddings.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.

Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ Neural language models (Bengio et al, 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Schwenk and Gauvain, 2002; Emami et al, 2003) have been shown to be very powerful at languagemodeling, a task where models are asked to ac curately predict the next word given previously seen words.
Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ Our model uses a similar neural network architecture as these models and usesthe ranking-loss training objective proposed by Col lobert and Weston (2008), but introduces a new way to combine local and global context to train word embeddings.Besides language modeling, word embeddings induced by neural language models have been use ful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al,2011a).

The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ These representations can be used to induce similarity measures by computingdistances between the vectors, leading to many useful applications, such as information retrieval (Manning et al, 2008), document classification (Sebas tiani, 2002) and question answering (Tellex et al, 2003).
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Several studies in psychology have also shown that global context can help language comprehension (Hess et al., 1995) and acquisition (Li et al, 2000).We introduce a new neural-network-based lan guage model that distinguishes and uses both local and global context via a joint training objective.
