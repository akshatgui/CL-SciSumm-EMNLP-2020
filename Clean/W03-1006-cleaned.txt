Similarly, Chen and Rambow (2003) argue that the kind of deep linguistic features we harvest from FrameNet is beneficial for the successful assignment of PropBank roles to constituents, in this case using TAGs generated from PropBank to generate the relevant features. $$$$$ We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.
Similarly, Chen and Rambow (2003) argue that the kind of deep linguistic features we harvest from FrameNet is beneficial for the successful assignment of PropBank roles to constituents, in this case using TAGs generated from PropBank to generate the relevant features. $$$$$ We also experiment with semantic features derived from the PropBank.

However, in most cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow, 2003), which is too low to provide high SRL accuracy. $$$$$ This can be the tree frame corresponding to either the predicate or the argument.
However, in most cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow, 2003), which is too low to provide high SRL accuracy. $$$$$ Furthermore, note the low recall and high precision of the “base + arg” evaluation.

(Chenand Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL. $$$$$ They may be composed by several well-defined operations to form parse trees.
(Chenand Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL. $$$$$ Supertagging returns an almost-parse in the sense that it is performing much parsing disambiguation.

Path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. $$$$$ Path.
Path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. $$$$$ This is a path in a tree frame from its preterminal to a particular argument node in a tree frame.

Prop was extracted using the PropBank annotations for ar gument/modifier distinction by a method similar to Chen and Rambow (2003). $$$$$ Finally, because a TAG is extracted from the PropBank, we have a set of semantic features derived indirectly from the PropBank through TAG.
Prop was extracted using the PropBank annotations for ar gument/modifier distinction by a method similar to Chen and Rambow (2003). $$$$$ In part this is due to the nature of the PropBank corpus that we are using.

Although the results can not directly be compared with another work using LTAG (Chen and Rambow, 2003) because their target annotations were limited to those localized in an elementary tree, considering that their target annotations were 87% of core arguments, our results are competitive with their results (82.57/71.41). $$$$$ We then describe the results of using different feature sets.
Although the results can not directly be compared with another work using LTAG (Chen and Rambow, 2003) because their target annotations were limited to those localized in an elementary tree, considering that their target annotations were 87% of core arguments, our results are competitive with their results (82.57/71.41). $$$$$ These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora.

Another possibility is to directly extract PropBank-style semantic representations by reforming the grammar extraction algorithm (Chen and Rambow, 2003), and to estimate a disambiguation model using the PropBank. $$$$$ We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features from the resulting TAG.
Another possibility is to directly extract PropBank-style semantic representations by reforming the grammar extraction algorithm (Chen and Rambow, 2003), and to estimate a disambiguation model using the PropBank. $$$$$ For our current work, we extract two kinds of TAG from the PropBank.

As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. $$$$$ An important principle is that dependencies, including long-distance dependencies, are typically localized the same elementary tree by appropriate grouping of syntactically or semantically related elements.
As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. $$$$$ For example, we determine the deep syntactic role of a whmoved element by “undoing” the wh-movement by using the trace information in the PTB.

(Chen and Rambow, 2003) discuss amodel for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). $$$$$ They may be composed by several well-defined operations to form parse trees.
(Chen and Rambow, 2003) discuss amodel for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). $$$$$ Supertagging returns an almost-parse in the sense that it is performing much parsing disambiguation.

Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. $$$$$ This can be the tree frame corresponding to either the predicate or the argument.
Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. $$$$$ The typical technique to perform supertagging is the trigram model, akin to models of the same name for partof-speech tagging.

As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). $$$$$ So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available.
As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). $$$$$ These experiments are performed using the C4.5 decision tree machine learning algorithm.

The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG: (Liu and Sarkar, 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework; (Chenand Rambow, 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees, which leads to a loss of over 17% instances of semantic roles even from gold-standard trees. $$$$$ Phrase Type.
The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG: (Liu and Sarkar, 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework; (Chenand Rambow, 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees, which leads to a loss of over 17% instances of semantic roles even from gold-standard trees. $$$$$ In particular, we only include as semantic roles those instances in the propbank such that in the extracted TAG they are localized in the same elementary tree.
