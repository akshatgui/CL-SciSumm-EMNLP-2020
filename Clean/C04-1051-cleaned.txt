We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ To isolate just those sentence pairs that represent likely paraphrases without requiring significant string similarity, we exploited a common journalistic convention: the first sentence or two of 3A maximum Levenshtein distance of 12 was selected for the purposes of this paper on the basis of experiments with corpora extracted at various edit distances.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ a newspaper article typically summarize its content.

The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al 2002; Barzilay and Lee 2003) have been restricted to at most two news sources.

The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ We will refer to this dataset as L12.
The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ We will refer to this collection as the F2 corpus.

For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ Each corpus was used as input to the word alignment algorithms available in Giza++ (Och & Ney 2000).

We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ Our two paraphrase datasets are distilled from a corpus of news articles gathered from thousands of news sources over an extended period.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ While the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, Shinyama et al 2002; Barzilay and Lee 2003) have been restricted to at most two news sources.

Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ A simple edit distance metric (Levenshtein 1966) was used to identify pairs of sentences within a cluster that are similar at the string level.

Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

 $$$$$ The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves.
 $$$$$ We remain, however, responsible for all content.

 $$$$$ The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves.
 $$$$$ We remain, however, responsible for all content.

 $$$$$ The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves.
 $$$$$ We remain, however, responsible for all content.

Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ To prevent too high an incidence of unrelated sentences, one string-based heuristic filter was found useful: a pair is discarded if the sentences do not share at least 3 words of 4+ characters.
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ First, the F2 data is less parallel, as evidenced by the higher percentage of Elaborations found in those sentence pairs.

the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

 $$$$$ The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves.
 $$$$$ We remain, however, responsible for all content.

In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by a number of recent efforts (Barzilay & McKeown 2001; Shinyama et al 2002; Lee & Barzilay 2003; Lin & Pantel 2001).
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).

More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ (Shinyama, et al 2002; Lin & Pantel 2001).
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ Giza++ is a freely available implementation of IBM Models 1-5 (Brown et al 1993) and the HMM alignment (Vogel et al 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).

 $$$$$ The assumption that two first sentences are semantically related is thus based in part on linguistic information that is external to the sentences themselves.
 $$$$$ We remain, however, responsible for all content.

We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ Our two paraphrase datasets are distilled from a corpus of news articles gathered from thousands of news sources over an extended period.
We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ We will refer to this dataset as L12.
