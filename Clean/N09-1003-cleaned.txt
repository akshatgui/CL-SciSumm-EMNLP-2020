Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ Table 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods.

Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ Therefore, terms that are topically related can appear in the same textual passages and will get high values using this model.
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ Therefore, true synonyms and hyponyms/hyperonyms will receive high similarities, whereas terms related topically or based on any other semantic relation (e.g. movie and star) will have lower scores.

Spearman's rank correlation (œÅ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ In order to calculate similarities in a cross-lingual setting, where some of the words are in a language l other than English, the following algorithm is used

Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.

Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ This work pioneers cross-lingual extension and evaluation of both distributional and WordNet-based measures.

In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ The final corpus remaining at the end of this process contains roughly 1.6 Terawords.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ All calculations are done in parallel sharding by dimension, and it is possible to calculate all pairwise similarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure.

 $$$$$ The two Spanish WordNet versions are referred to as MCR16 and WN30g.
 $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5

We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).
We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ This paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data.

Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). $$$$$ The techniques used to solve this problem can be roughly classified into two main categories

In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ To calculate the similarity of two words w1 and w2, Ruiz-Casado et al. (2005) collect snippets containing w1 from a Web search engine, extract a context around it, replace it with w2 and check for the existence of that modified context in the Web.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ We have used a corpus of four billion documents, crawled from the Web in August 2008.

 $$$$$ The two Spanish WordNet versions are referred to as MCR16 and WN30g.
 $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5

 $$$$$ The two Spanish WordNet versions are referred to as MCR16 and WN30g.
 $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5

While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ Our distributional methods also outperform all other corpus-based methods.
While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia.

We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.

This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ As the results in Section 4 show, different techniques are more appropriate to calculate either similarity or relatedness.

Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ Section 5 presents some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.

For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ The most similar approach to our distributional technique is Finkelstein et al. (2002), who combined distributional similarities from Web documents with a similarity from WordNet.

Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0.
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ This annotation was used to group the pairs in three categories

 $$$$$ The two Spanish WordNet versions are referred to as MCR16 and WN30g.
 $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5

 $$$$$ The two Spanish WordNet versions are referred to as MCR16 and WN30g.
 $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5
