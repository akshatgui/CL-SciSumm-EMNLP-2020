It is only recently employed in NER (Shen et al, 2004). $$$$$ 1999; Tang et al. 2002; Steedman et al.
It is only recently employed in NER (Shen et al, 2004). $$$$$ Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.

This issue was previously addressed in Shen et al (2004) in the context of named-entity recognition, where they used a two-step procedure to first select the most informative and representative samples, followed by a diversity filter. $$$$$ Multi-Criteria-Based Active Learning For Named Entity Recognition
This issue was previously addressed in Shen et al (2004) in the context of named-entity recognition, where they used a two-step procedure to first select the most informative and representative samples, followed by a diversity filter. $$$$$ When selecting a machine-annotated named entity, we compare it with all previously selected named entities in the current batch.

In a more recent study, Shen et al (2004) consider AL for entity recognition based on Support Vector Machines. $$$$$ We build our NER model using Support Vector Machines (SVM).
In a more recent study, Shen et al (2004) consider AL for entity recognition based on Support Vector Machines. $$$$$ Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.

 $$$$$ In SVM, only the support vectors are useful for the classification, which is different from statistical models.
 $$$$$ Another interesting work is to study when to stop active learning.

Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al, 2004) or semi-automatically (active learning approaches like Shen et al (2004) or systems using seed rules such as Mikheev et al. $$$$$ 1999; Tang et al. 2002; Steedman et al.
Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al, 2004) or semi-automatically (active learning approaches like Shen et al (2004) or systems using seed rules such as Mikheev et al. $$$$$ Many existing work in the area focus on two approaches

Active learning, which has been applied to the problem of NER in (Shen et al, 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. $$$$$ Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.
Active learning, which has been applied to the problem of NER in (Shen et al, 2004), is used in situations where a large amount of unlabeled data exists and data labeling is expensive. $$$$$ The support vectors can later be used to classify the test data.

Diversity measures as proposed by (Shen et al, 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. $$$$$ In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.
Diversity measures as proposed by (Shen et al, 2004) might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and NER. $$$$$ On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification.

It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al, 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. $$$$$ The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80% without degrading the quality of the named entity recognizer.
It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al, 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. $$$$$ In this paper, we study the active learning in a more complex NLP task, named entity recognition.

In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al, 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al, 2003). $$$$$ 1999; Tang et al. 2002; Steedman et al.
In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al, 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al, 2003). $$$$$ Many existing work in the area focus on two approaches

Shen et al (2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition. $$$$$ Multi-Criteria-Based Active Learning For Named Entity Recognition
Shen et al (2004) combine multiple criteria to measure the informativeness, representativeness, and diversity of examples in active learning for named entity recognition. $$$$$ In this section, we will study how to combine and strike a proper balance between these criteria, viz. informativeness, representativeness and diversity, to reach the maximum effectiveness on NER active learning.

Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al, 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum. $$$$$ We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences.
Therefore, in order to avoid recursion and over-complexity, we employ a diversity-motivated intra-stratum sampling scheme (Shen et al, 2004), called KDN (K-diverse neighbors), which aims to maximize the training utility of all seeds from a stratum. $$$$$ Diversity criterion is to maximize the training utility of a batch.

(Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities in news domain, (Shen et al, 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. $$$$$ Multi-Criteria-Based Active Learning For Named Entity Recognition
(Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al, 2005a) used self-training and co-training to detect and classify named entities in news domain, (Shen et al, 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. $$$$$ 1999; Tang et al. 2002; Steedman et al.

 $$$$$ In SVM, only the support vectors are useful for the classification, which is different from statistical models.
 $$$$$ Another interesting work is to study when to stop active learning.

Shen et al (2004) proposed an approach to selecting examples based on informativeness, representativeness and diversity criteria. $$$$$ In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.
Shen et al (2004) proposed an approach to selecting examples based on informativeness, representativeness and diversity criteria. $$$$$ We propose a multi-criteria-based approach to select examples based on their informativeness, representativeness and diversity, which are incorporated all together by two strategies (local and global).
