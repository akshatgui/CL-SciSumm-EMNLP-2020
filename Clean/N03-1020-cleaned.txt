In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based.
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ It consistently correlated highly with human assessments and had high recall and precision in significance test with manual evaluation results.

Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.

it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.
it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.

Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ (2) The performance of weighted average n-gram scores is in the range between bi-gram and tri-gram co-occurrence scores.

Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ 2002).
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.

Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ Only one baseline (baseline1) was created for the single document summarization task.

Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.

Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996).
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.

Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.

The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to &quot;real&quot; significance, i.e. the statistical significance in a human assessment of run A and run B?

ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.

Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ We then discussed the IBM BLEU MT evaluation metric, its application to summary evaluation, and the difference between precisionbased BLEU translation evaluation and recall-based DUC summary evaluation.

One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.

We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ Automatic Evaluation Of Summaries Using N-Gram Co-Occurrence Statistics
We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.

Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ These numbers indicate that they positively correlate at α = 0.018.

 $$$$$ These numbers indicate that they positively correlate at α = 0.018.
 $$$$$ In this way the evaluation technologies can advance at the same pace as the summarization technologies improve.

Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.
Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).

We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ We therefore prefer the metric given by equation 6 and use it in all our experiments.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics.

Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction.

We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.
