Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ We now review experiments using segmentation lattices produced by the segmentation model we just introduced in German-English, Hungarian-English, and Turkish-English translation tasks and then show results elucidating the effect of the lattice density parameter.
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ These segmentation lattices improve translation quality (over an already strong baseline) in three typologically distinct languages (German, Hungarian, Turkish) when translating into English.

To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ Taken as a stand-alone task, the goal of a compound splitter is to produce a segmentation for some input that matches the linguistic intuitions of a native speaker of the language.
To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ The features we used in our compound segmentation model for the experiments reported below are shown in Table 2.

Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). $$$$$ The weights shown in Table 2 are those learned by maximum likelihood training on models both with and without the special German features, which are indicated with â€ .
Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). $$$$$ Second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs.

The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). $$$$$ In this section, we report the results of an experiment to see if the compound lattices constructed using our maximum entropy model yield better translations than either an unsegmented baseline or a baseline consisting of a single-best segmentation.
The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). $$$$$ In the LATTICE condition, we constructed segmentation lattices using the technique described in Section 3.1.

These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries.
These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.

We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-crafted rules (DeNeefe et al., 2008).

Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ Using a maximum entropy model to build segmentation lattices for MT
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.

Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ In all experiments reported in this paper, we use m = 3.
Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ Our model defines a conditional probability distribution over virtually all segmentations of a word w. To train our model, we wish to maximize the likelihood of the segmentations contained in the reference lattices by moving probability mass away from the segmentations that are not in the reference lattice.

These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ Recent work has shown that translating segmentation lattices (lattices that encode alternative ways of breaking the input to an MT system into words), rather than text in any particular segmentation, improves translation quality of languages whose orthography does not mark morpheme boundaries.
These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ In our case, we want to propagate uncertainty about the proper segmentation of a compound forward to the decoder, which can use its full translation model to select proper segmentation for translation.

All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ In the BASELINE condition, a lowercased and tokenized (but not segmented) version of the test data is translated using the grammar derived from a nonsegmented training data.
All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ The smaller effect in German is probably due to there being more in-domain training data in the German system than in the (otherwise comparably sized) Hungarian system.

Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011). $$$$$ We show that these lattices significantly improve translation quality when translating into English from three languages exhibiting productive compounding

An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. $$$$$ In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word.
An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.

Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. $$$$$ Perhaps most surprisingly, the improvements observed when using lattices with the Hungarian and Turkish systems were larger than the corresponding improvement in the German system, but German was the only language for which we had segmentation training data.
Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. $$$$$ The smaller effect in German is probably due to there being more in-domain training data in the German system than in the (otherwise comparably sized) Hungarian system.

Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding.
Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.

Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input. $$$$$ In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding.
Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input. $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.
