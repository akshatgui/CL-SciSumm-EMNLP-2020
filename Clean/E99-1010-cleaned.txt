The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).
The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ The alignment templates are automatically trained using a parallel training corpus.

We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.
We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. $$$$$ The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.
Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. $$$$$ The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.
This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ For the first we use the class-based bigram probability from Eq.
The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ The alignment templates are automatically trained using a parallel training corpus.

We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.
We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ The target language of our experiments is English.
The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.

This feature implements a 5-gram language model of target statistical classes (Och, 1999). $$$$$ The target language of our experiments is English.
This feature implements a 5-gram language model of target statistical classes (Och, 1999). $$$$$ The classes BIL-2 are determined by first optimizing mono-lingually classes for the target language (English) and afterwards optimizing classes for the source language (Eq.

This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. $$$$$ We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. $$$$$ The statistical machine-translation method described in (Och and Weber, 1998) makes use of bilingual word classes.

As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).
As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ We chose the number of classes in such a way that the final performance of the translation system was optimal.

A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. $$$$$ The advantage of the alignment template approach against word-based statistical translation models is that word context and local re-orderings are explicitly taken into account.
A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. $$$$$ The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.

This approach was shown to give the best results in (Och, 1999). $$$$$ Examples of alignment templates are shown in Figure 2.
This approach was shown to give the best results in (Och, 1999). $$$$$ The bilingual classes show better results than the monolingual classes MONO.

Practically, we can use word alignment as used in (Och, 1999). $$$$$ The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).
Practically, we can use word alignment as used in (Och, 1999). $$$$$ The bilingual word classes are used to generalize the applicability of the alignment templates in search.

First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ Rewriting the corpus probability using classes we arrive at the following probability model p(wiv IC): In this model we have two types of probabilities: the transition probability p(C1C1) for class C given its predecessor class C' and the membership probability p(wIC) for word w given class C. To determine the optimal classes C for a given number of classes M we perform a maximumlikelihood approach: = arg mrc p(wiv IC) (2) We estimate the probabilities of Eq.
First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. $$$$$ An Efficient Method For Determining Bilingual Word Classes
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. $$$$$ The bilingual word classes are used to generalize the applicability of the alignment templates in search.

Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). $$$$$ We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).
Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). $$$$$ The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vogel et al., 1996).

For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). $$$$$ For EuTRANs-I we used 60 classes and for EuTRANs-II we used 500 classes.
For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). $$$$$ By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.

Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ An Efficient Method For Determining Bilingual Word Classes
Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ The described method to determine bilingual word classes is an extension and improvement of the method mentioned in (Och and Weber, 1998).

However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ The alignment templates are automatically trained using a parallel training corpus.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ More details are given in (Och and Weber, 1998; Och and Ney, 1999).

This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). $$$$$ (6) and secondly we determine classes F optimizing the bilingual part (without changing 6): By using these two optimization processes we enforce that the classes E are mono-lingually 'good' classes and that the classes .7- correspond to 6.
This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). $$$$$ The alignment templates are automatically trained using a parallel training corpus.
