NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010). $$$$$ In the last decade, the field of Natural Language Processing (NLP), has seen a surge in the use of corpus motivated techniques.
NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010). $$$$$ However, most language analysis tools are too infeasible to run on the scale of the web.

 $$$$$ This algorithm was further improved by Charikar (2002).
 $$$$$ We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.

Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ Using fast search algorithm to find nearest neighbors.
Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ B of its closest neighbors in the sorted list.

III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). $$$$$ We use the context words as features of the noun vector.
III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). $$$$$ We do not use grammatical features in the web corpus since parsing is generally not easily web scalable.

Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ The algorithm given in Charikar (2002) is described to find the nearest neighbor for a given vector.
Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).

 $$$$$ This algorithm was further improved by Charikar (2002).
 $$$$$ We wish to thank USC Center for High Performance Computing and Communications (HPCC) for helping us use their cluster computers.

Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). $$$$$ We use the same 6GB corpus that was used for training by Pantel and Lin (2002) so that the results are comparable.
Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). $$$$$ We then compare this output to the one provided by the system of Pantel and Lin (2002).

It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ Interestingly, cosine similarity is widely used in NLP for various applications such as clustering.
It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ But with the rapidly growing amount of raw text available on the web, one could improve clustering performance by carefully harnessing its power.

The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989).
The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ We first construct a frequency count vector C(e) = (ce1, ce2, ..., cek), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miek) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: where n is the number of words and N = En Em j=1 cij is the total frequency count of all i=1 features of all words.

When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. $$$$$ This algorithm was further improved by Charikar (2002).
When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. $$$$$ These similarity lists are cut off at a threshold of 0.15.

We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ In doing so, we are going to explore the literature and techniques of randomized algorithms.
We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ LSH functions are generally based on randomized algorithms and are probabilistic.

This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus.
This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ The web corpus is used to show that our framework is webscalable, while the newspaper corpus is used to compare the output of our system with the similarity lists output by an existing system, which are calculated using the traditional formula as given in equation 1.

This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ In the previous section, we introduced the theory for calculation of fast cosine similarity.
This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ Why does the fast hamming distance algorithm work?

We followed the notation of the original paper (Ravichandran et al, 2005) here. $$$$$ In this paper, we investigate the first two avenues.
We followed the notation of the original paper (Ravichandran et al, 2005) here. $$$$$ This is a huge saving from the original O(n2k) algorithm.

Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.
Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ For each noun we take the grammatical context of the noun as identified by Minipar5.

However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. $$$$$ With large amounts of data, say n in the order of millions or even billions, having an n2k algorithm would be very infeasible.
However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. $$$$$ The experiment was infeasible.

In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. $$$$$ Why does the fast hamming distance algorithm work?
In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. $$$$$ However, to unleash the real power of clustering one has to work with large amounts of text.

Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ All clustering algorithms make use of some distance similarity (e.g., cosine similarity) to measure pair wise distance between sets of vectors.
Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ For evaluating the quality of our final similarity lists, we use the system developed by Pantel and Lin (2002) as gold standard on a much smaller data set.

We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.
We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ Number of bit index random permutations functions q; 2.

Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ Thus, the above theorem, converts the problem of finding cosine distance between two vectors to the problem of finding hamming distance between their bit streams (as given by equation 4).
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ We eliminate duplicate and near duplicate documents by using the algorithm described by Kolcz et al. (2004).
