ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others.
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).

 $$$$$ Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.

Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) < k < max(i, j), there is a directed path in y from i to k).

 $$$$$ Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.

 $$$$$ Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.

Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ A dependency tree is called projective if it only contains projective arcs.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ 12, with O(

Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word.
Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.

The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x).
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3) variables and constraints.

We present a unified view of two state-of-the art non-projective dependency parsers, both approximate $$$$$ Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local)

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local)

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details.

Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ Fig.
Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local)

Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x).
Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ 9 by za ∈ B, a ∈ A.

Figure 3 $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).
Figure 3 $$$$$ 12The loss-augmented inference problem can also be expressed as an LP for Hamming loss functions that factor over arcs; we refer to Martins et al. (2009) for further details.

We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ Concise Integer Linear Programming Formulations for Dependency Parsing
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).

(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers).

In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).

 $$$$$ Te fct tht ter e xponentially Nn-local informaton, such as arity (or valency) obtaining high parsing accuracie (Kein and Manmay candidates in Y(x) maks dependency parsand neighbouring dependencis, cn be crucial to ning, 2002; McDonald and Pereira, 2006) Howinga strucured clasification problem. obaing high parsng accuracie (Klei evr, in the data-driven parsing setting er, in the data-driven parsing setting rentations over the input (McDonald et There has been much recent work on dependency pay advd by h go pog feu p ur o rr ndndi f h so h ip (cald a, 00) pial nre f n parsing using graph-based, transition-based, and pjeti parsig lgithm f bth lig ad Th goal of hi wok i furthe r urrent hybrid methods; see Nivre and McDonald (2008) inference within the datadrven setting We sart by dtdi of th pttil t f for an overview.
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local)

Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ The impact of LP-relaxed inference in the learning problem was studied elsewhere (Martins et al., 2009).
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).
