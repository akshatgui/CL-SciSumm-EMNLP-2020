For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ Girju et al. (2005) draw a training set from raw WSJ text and use it to train a decision tree classifier achieving 73.1% accuracy.
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ When they shuffled their data with Lauer’s to create a new test and training split, their accuracy increased to 83.1% which may be a result of the 10% duplication in Lauer’s test set.

We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ There have also been attempts to solve this task using supervised methods, even though the lack of gold-standard data makes this difficult.

Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ With a large annotated corpus, we can now run supervised NP bracketing experiments.

 $$$$$ The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective.
 $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.

The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ For NML and JJP, we also show the words bracketed, as they would appear under an NP node.
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ The performance on only the new NML and JJP brackets is not very high.

PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ Girju et al. (2005) draw a training set from raw WSJ text and use it to train a decision tree classifier achieving 73.1% accuracy.
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ With our new data set, we began running experiments similar to those carried out in the literature (Nakov and Hearst, 2005).

This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations.
This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ Since CCG derivations are binary branching, they cannot directly represent the flat structure of the Penn Treebank base-NPs.

This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ We found however, that while there are certainly difficult cases, the vast majority are quite simple and can be annotated reliably.

Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ We ran the tool over the original corpus, following NE-based suggestions where possible.
Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ The best model in this case comes from using all the features.

Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ Table 8 shows that the new brackets make parsing marginally more difficult overall (by about 0.5% in F-score).
Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ The performance on only the new NML and JJP brackets is not very high.

Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs.
Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ We find there are 32772 NP children, and 579 ADJP, which are quite similar numbers to the amount of nodes we have added.

 $$$$$ The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective.
 $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.

Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ We present two configurations in Table 7: training on our corpus and testing on Lauer’s set; and performing 10-fold cross validation using our corpus alone.

Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ Adding Noun Phrase Structure to the Penn Treebank
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ With a large annotated corpus, we can now run supervised NP bracketing experiments.

 $$$$$ The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective.
 $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.

 $$$$$ The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective.
 $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.

 $$$$$ The label of the newly created constituent is NML or JJP, depending on whether its head is a noun or an adjective.
 $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.

We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ The following NP is an example of the flat structure of base-NPs within the Penn Treebank:
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.

Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP).
Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.

Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ Adding Noun Phrase Structure to the Penn Treebank
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ We use Bikel’s implementation (Bikel, 2004) of Collins’ parser (Collins, 1999) in order to carry out these experiments, using the non-deficient Collins settings.
