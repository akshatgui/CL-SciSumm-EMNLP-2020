Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case. $$$$$ Entity-Based Cross-Document Core f erencing Using the Vector Space Model
Biographic Data Past work on this task (e.g. Bagga and Baldwin, 1998) has primarily approached personal name disambiguation using document context profiles or vectors, which recognize and distinguish identical name instances based on partially indicative words in context such as computer or car in the Clark case. $$$$$ While the (Vilain 95) provides intuitive results for coreference scoring, it however does not work as well in the context of evaluating cross document coreference.

Some refer to the task as cross-document co reference resolution (Bagga and Baldwin, 1998), name discrimination (Pedersen et al, 2005) or Web People Search (WebPS) (Artiles et al, 2007). $$$$$ In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.
Some refer to the task as cross-document co reference resolution (Bagga and Baldwin, 1998), name discrimination (Pedersen et al, 2005) or Web People Search (WebPS) (Artiles et al, 2007). $$$$$ In this paper we describe a highly successful crossdocument coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.

In our experiments, we use the training texts to acquire co reference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used co reference scoring programs $$$$$ The first was the standard algorithm for within-document coreference chains which was used for the evaluation of the systems participating in the MUC-6 and the MUC-7 coreference tasks.
In our experiments, we use the training texts to acquire co reference classifiers and evaluate the resulting systems on the test texts with respect to two commonly-used co reference scoring programs $$$$$ This distinction is not reflected in the (Vilain 95) scorer which scores both responses as having a precision score of 90% (Figure 9).

Four-fold cross validation is employed and B-CUBED metric (Bagga and Baldwin, 1998) is adopted to evaluate the clustering results. $$$$$ While the (Vilain 95) provides intuitive results for coreference scoring, it however does not work as well in the context of evaluating cross document coreference.
Four-fold cross validation is employed and B-CUBED metric (Bagga and Baldwin, 1998) is adopted to evaluate the clustering results. $$$$$ Further details about the B-CUBED algorithm including a model theoretic version of the algorithm can be found in (Bagga 98a).

We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. $$$$$ Further details about the B-CUBED algorithm including a model theoretic version of the algorithm can be found in (Bagga 98a).
We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. $$$$$ In comparison, Figure 12 shows the results (using the B-CUBED scoring algorithm) when the vector space model constructed the space of terms from the articles input to the system (it still used the summaries when computing the similarity).

Bagga and Baldwin (1998) proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. $$$$$ The VSM-Disambiguate module, for each summary Si, computes the similarity of that summary with each of the other summaries.
Bagga and Baldwin (1998) proposed entity based cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. $$$$$ Assuming that each of the documents in the data set was about a single John Smith, the cross-document coreference chains produced by the system could now be evaluated by scoring the corresponding within-document coreference chains in the meta document.

BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. $$$$$ The highest F-Measure for the former case is 84.6% while the highest F-Measure for the latter case is 78.0%.
BCubed (Bagga and Baldwin, 1998) is an attractive measure that addresses both completeness and homogeneity. $$$$$ Figures 13 and 14 show the precision, recall, and F-Measure calculated using the MUC scoring algorithm.

Similar approach was developed by (Bagga and Baldwin, 1998), who created first order context vectors that represent the instance in which the ambiguous name occurs. $$$$$ Figure 1 shows the architecture of the crossdocument system developed.
Similar approach was developed by (Bagga and Baldwin, 1998), who created first order context vectors that represent the instance in which the ambiguous name occurs. $$$$$ The answer keys regarding the cross-document chains were manually created, but the scoring was completely automated.

In this paper, we present a new text semantic similarity approach for fine-grained person name categorization and discrimination which is similar to those of (Pedersen et al, 2005) and (Bagga and Baldwin, 1998), but instead of simple word co-occurrences, we consider the whole text segment and relate the deduced semantic information of Latent Semantic Analysis (LSA) to trace the text cohesion between thousands of sentences containing named entities which belong to different fine-grained categories or individuals. $$$$$ This follows from the convention in coreference annotation of not identifying those entities that are markable as possibly coreferent with other entities in the text.
In this paper, we present a new text semantic similarity approach for fine-grained person name categorization and discrimination which is similar to those of (Pedersen et al, 2005) and (Bagga and Baldwin, 1998), but instead of simple word co-occurrences, we consider the whole text segment and relate the deduced semantic information of Latent Semantic Analysis (LSA) to trace the text cohesion between thousands of sentences containing named entities which belong to different fine-grained categories or individuals. $$$$$ Rather, entities are only marked as being coreferent if they actually are coreferent with other entities in the text.

The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. $$$$$ The coreference chains output by CAMP for the two extracts are shown in Figures 3 and 5. tractor module produces a &quot;summary&quot; of the article with respect to the entity of interest.
The original work in (Bagga and Baldwin, 1998) proposed a CDC system by first performing WDC and then disambiguating based on the summary sentences of the chains. $$$$$ The VSM-Disambiguate module, for each summary Si, computes the similarity of that summary with each of the other summaries.

Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. $$$$$ The Vector Space Model in this case constructed the space of terms only from the summaries extracted by SentenceExtractor.
Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. $$$$$ Our system takes summaries about an entity of interest and uses various information retrieval metrics to rank the similarity of the summaries.

We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). $$$$$ While the (Vilain 95) provides intuitive results for coreference scoring, it however does not work as well in the context of evaluating cross document coreference.
We base our work partly on previous work done by Bagga and Baldwin (Bagga and Baldwin, 1998), which has also been used in later work (Chen and Martin, 2007). $$$$$ Details about these experiments can be found in (Bagga 98b).

To score the output of a coreference model, we employ three scoring programs $$$$$ In order to score the cross-document coreference chains output by the system, we had to map the cross-document coreference scoring problem to a within-document coreference scoring problem.
To score the output of a coreference model, we employ three scoring programs $$$$$ We used two different scoring algorithms for scoring the output.

The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. $$$$$ First, let S be an equivalence set generated by the key, and let R1 ... R,â€ž. be equivalence classes generated by the response.
The aforementioned complication does not arise from the construction of the mapping, but from the fact that Bagga and Baldwin (1998) and Luo (2005) do not specify how to apply B3 and CEAF to score partitions generated from system mentions. $$$$$ The high initial precision is mainly due to the fact that the MUC algorithm assumes that all errors are equal.

We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. $$$$$ Figure 11 shows the precision, recall, and F-Measure (with equal weights for both precision and recall) using the B-CUBED scoring algorithm.
We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. $$$$$ Their performance using our Threshold Figure 11

Early work in the field of name disambiguation is that of (Bagga and Baldwin, 1998) who proposed cross-document coreference resolution algorithm which uses vector space model to resolve the ambiguities between people sharing the same name. $$$$$ In this paper we describe a cross-document coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.
Early work in the field of name disambiguation is that of (Bagga and Baldwin, 1998) who proposed cross-document coreference resolution algorithm which uses vector space model to resolve the ambiguities between people sharing the same name. $$$$$ In this paper we describe a highly successful crossdocument coreference resolution algorithm which uses the Vector Space Model to resolve ambiguities between people having the same name.

On this dataset, our proposed model yields a B3 (Bagga and Baldwin, 1998) F1 score of 73.7%, improving over the baseline by 16% absolute (corresponding to 38% error reduction). $$$$$ It is our position that the second error is more damaging because, compared to the first error, the second error makes more entities coreferent that should not be.
On this dataset, our proposed model yields a B3 (Bagga and Baldwin, 1998) F1 score of 73.7%, improving over the baseline by 16% absolute (corresponding to 38% error reduction). $$$$$ Also, the baseline case when all the John Smiths are considered to be the same person achieves 83% precision and 100% recall.

One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. $$$$$ In addition, we also describe a scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.
One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. $$$$$ In order to score the cross-document coreference chains output by the system, we had to map the cross-document coreference scoring problem to a within-document coreference scoring problem.

The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks $$$$$ While the (Vilain 95) provides intuitive results for coreference scoring, it however does not work as well in the context of evaluating cross document coreference.
The disambiguation of person names in Web results is usually compared to two other Natural Language Processing tasks $$$$$ We have also tested our system on other classes of cross-document coreference like names of companies, and events.

Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities across documents according to their real referents. $$$$$ Cross-document coreference is a distinct technology from Named Entity recognizers like IsoQuest's NetOwl and IBM's Textract because it attempts to determine whether name matches are actually the same individual (not all John Smiths are the same).
Cross document coreference (CDC) (Bagga and Baldwin, 1998) is a distinct technology that consolidates named entities across documents according to their real referents. $$$$$ Neither NetOwl or Textract have mechanisms which try to keep same-named individuals distinct if they are different people.
