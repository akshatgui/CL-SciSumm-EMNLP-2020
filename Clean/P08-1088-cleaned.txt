Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ Precision and recall are then measured over these bilingual lexicons.
Haghighi et al (2008) mention one disadvantage of using edit distance, that is, precision quickly degrades with higher recall. $$$$$ Running EDITDIST (see section 4.3) on ENES-W yielded 61.1 p0.33, but precision quickly degrades for higher recall levels (see EDITDIST in table 1).

Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Haghighi et al (2008), amongst a few others, propose using canonical correlation analysis to reduce the dimension. $$$$$ We propose the following generative model over matchings m and word types (s, t), which we call matching canonical correlation analysis (MCCA).

Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs.
Haghighi et al (2008) only use a small-sized bilingual lexicon containing 100 word pairs as seed lexicon. $$$$$ All of our experiments so far have exploited a small seed lexicon which has been derived from the evaluation lexicon (see section 4.3).

Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).
Examples can be found in Fung and Cheung (2004), followed by Haghighi et al (2008). $$$$$ We ran the intersected HMM wordalignment model (Liang et al., 2008) and added (s, t) to the lexicon if s was aligned to t at least three times and more than any other word.

Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ We manually examined the top 100 errors in the English-Spanish lexicon produced by our system on EN-ES-W. Of the top 100 errors: 21 were correct translations not contained in the Wiktionary lexicon (e.g. pintura to painting), 4 were purely morphological errors (e.g. airport to aeropuertos), 30 were semantically related (e.g. basketball to bÂ´eisbol), 15 were words with strong orthographic similarities (e.g. coast to costas), and 30 were difficult to categorize and fell into none of these categories.
Haghighi et al (2008) have reported that the most common errors detected in their analysis on top 100 errors were from semantically related words, which had strong context feature correlations. $$$$$ Of the true errors, the most common arose from semantically related words which had strong context feature correlations (see table 4(b)).

Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.
Earlier work, Haghighi et al (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. $$$$$ In this section, we explore feature representations of word types in our model.

CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ Learning Bilingual Lexicons from Monolingual Corpora
CCA has been used in bilingual lexicon extraction from comparable corpora (Gaussier et al, 2004) and monolingual corpora (Haghighi et al., 2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).

Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ We can represent orthographic features of a word type w by assigning a feature to each substring of length G 3.
Haghighi et al (2008) use a probabilistic model over word feature vectors containing co occurrence and orthographic features. $$$$$ For the remainder of this work, we will use MCCA to refer to our model using both orthographic and context features.

 $$$$$ We found that it was helpful to explicitly control the number of edges.
 $$$$$ It remains to be seen how such lexicons can be best utilized, but they invite new approaches to the statistical translation of resource-poor languages.

The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.
The availability of parsers is a more stringent constraint, but our results suggest that more basic NLP methods may be sufficient for bilingual lexicon extraction. In this work, we have used a set of seed translations (unlike e.g., Haghighi et al (2008)). $$$$$ We used two methods to derive a seed lexicon.

For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ One non-orthographic clue that word types s and t form a translation pair is that there is a strong correlation between the source words used with s and the target words used with t. To capture this information, we define context features for each word type w, consisting of counts of nouns which occur within a window of size 4 around w. Consider the translation pair (time, tiempo) illustrated in figure 2.
For example, the paper by Haghighi et al (2008) (which demonstrates how orthography and contextual information can be successfully used) reports 61.7% accuracy on the 186 most confident predictions of nouns. $$$$$ In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002).

Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al, 2008), we are able to find translations for otherwise OOV terms. $$$$$ We take as input two monolingual corpora and perhaps some seed translations, and we produce as output a bilingual lexicon, defined as a list of word pairs deemed to be word-level translations.

Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Our dictionary mining approach is based on Canonical Correlation Analysis, as used previously by (Haghighi et al, 2008). $$$$$ Our model is based on canonical correlation analysis (CCA)1 and explains matched word pairs via vectors in a common latent space.

Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).
Here we describe the use of CCA to find the translations for the OOV German words (Haghighi et al, 2008). $$$$$ We will use MCCA (for matching CCA) to denote our model using the optimal feature set (see section 5.3).

We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.
We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al, 2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).

We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ Current statistical machine translation systems use parallel corpora to induce translation correspondences, whether those correspondences be at the level of phrases (Koehn, 2004), treelets (Galley et al., 2006), or simply single words (Brown et al., 1994).
We evaluate sets of high-probability words in each topic and multilingual "synsets" by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al (2008). $$$$$ The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces.

Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ In all experiments, unless noted otherwise, we used a seed of size 100 obtained from Le and considered lexicons between the top n = 2,000 most frequent source and target noun word types which were not in the seed lexicon; each system proposed an already-ranked one-to-one translation lexicon amongst these n words.
Previous research on bilingual lexicon induction learned translations only for a small number of high frequency words (e.g. 100 nouns in Rapp (1995), 1,000 most frequent words in Koehn and Knight (2002), or 2,000 most frequent nouns in Haghighi et al (2008)). $$$$$ There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here.

Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ Learning Bilingual Lexicons from Monolingual Corpora
Haghighi et al (2008) also used this method to show how well translations could be learned from monolingual corpora under ideal conditions, where the contextual and temporal distribution of words in the two monolingual corpora are nearly identical. $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.

Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ Learning Bilingual Lexicons from Monolingual Corpora
Haghighi et al, (2008) made use of contextual and orthographic clues for learning a generative model from monolingual corpora and a seed lexicon. $$$$$ We present a method for learning bilingual translation lexicons from monolingual corpora.

Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ Translations are induced using a generative model based on canonical correlation analysis, which explains the monolingual lexicons in terms of latent matchings.
Haghighi et al (2008) presented a generative model based on canonical correlation analysis, in which monolingual features such as the context and orthographic substrings of words were taken into account. $$$$$ We propose the following generative model over matchings m and word types (s, t), which we call matching canonical correlation analysis (MCCA).
