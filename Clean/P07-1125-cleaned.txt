Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. $$$$$ Weakly Supervised Learning for Hedge Classification in Scientific Literature
Medlock and Briscoe (2007) proposed a weakly supervised setting for hedge classification in scientific texts where the aim is to minimise human supervision needed to obtain an adequate amount of training data. $$$$$ In many cases hedge classification is challenging even for a human annotator.

The authors are only aware of the following related corpora: the Hedge classification corpus (Medlock and Briscoe, 2007), which has been annotated for hedge cues (at the sentence level) and consists of five full biological research papers (1537 sentences). $$$$$ We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.
The authors are only aware of the following related corpora: the Hedge classification corpus (Medlock and Briscoe, 2007), which has been annotated for hedge cues (at the sentence level) and consists of five full biological research papers (1537 sentences). $$$$$ We annotated six of the papers to form a test set with a total of 380 spec sentences and 1157 nspec sentences, and randomly selected 300,000 sentences from the remaining papers as training data for the weakly supervised learner.

5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for negations, uncertainty and their scopes. $$$$$ Relative F1 (Frel 1 ) and Cohen’s Kappa (n) were then used to quantify the level of agreement.
5 articles from FlyBase (the same data were used by Medlock and Briscoe (2007) for evaluating sentence-level hedge classifiers) and 4 articles from the open access BMC Bioinformatics website were downloaded and annotated for negations, uncertainty and their scopes. $$$$$ After each learning iteration, we compute the precision/recall BEP for the spec class using both classifiers trained on the current labelled data.

Solving the sentence level task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or non speculative. $$$$$ Riloff et al. Given a collection of sentences, S, the task is to (2003) explore bootstrapping techniques to identify label each sentence as either speculative or nonsubjective nouns and subsequently classify subjec- speculative (spec or nspec henceforth).
Solving the sentence level task, Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles as speculative or non speculative. $$$$$ As discussed earlier, the speculative/non-speculative distinction hinges on the presence or absence of a few hedge cues within the sentence.

A misinterpretation of the BioScope paper (Szarvas et al, 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). $$$$$ This idea was for- Light et al. (item 1) and introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training.
A misinterpretation of the BioScope paper (Szarvas et al, 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). $$$$$ The two annotators labelled the data independently using the guidelines outlined in section 3.

Medlock and Briscoe (2007) extended the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification. $$$$$ prove annotation consistency, we have developed a 2.2 Weakly Supervised Learning new set of guidelines, building on the work of Light Recent years have witnessed a significant growth et al. (2004).
Medlock and Briscoe (2007) extended the work of Light et al (2004) by refining their annotation guidelines and creating a publicly available data set (FlyBase data set) for speculative sentence classification. $$$$$ This idea was for- Light et al. (item 1) and introduce a set of further malised by Blum and Mitchell (1998) in their guidelines to help elucidate various ‘grey areas’ and presentation of co-training.

Szarvas (2008) extended the weakly supervised machine learning methodology of Medlock and Briscoe (2007) by applying feature selection to reduce the number of candidate keywords, by using limited manual supervision to filter the features, and by extending the feature representation with bigrams and trigrams. $$$$$ We investigate automatic classification of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning.
Szarvas (2008) extended the weakly supervised machine learning methodology of Medlock and Briscoe (2007) by applying feature selection to reduce the number of candidate keywords, by using limited manual supervision to filter the features, and by extending the feature representation with bigrams and trigrams. $$$$$ The class priors can be estimated based on the relative distribution sizes derived from the current training sets: where

In addition, by following the annotation guidelines of Medlock and Briscoe (2007), Szarvas (2008) made available the BMC Bioinformatics data set, by annotating four full text papers from the open access BMC Bioinformatics website. $$$$$ We annotated six of the papers to form a test set with a total of 380 spec sentences and 1157 nspec sentences, and randomly selected 300,000 sentences from the remaining papers as training data for the weakly supervised learner.
In addition, by following the annotation guidelines of Medlock and Briscoe (2007), Szarvas (2008) made available the BMC Bioinformatics data set, by annotating four full text papers from the open access BMC Bioinformatics website. $$$$$ The two annotators labelled the data independently using the guidelines outlined in section 3.

In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. $$$$$ Weakly Supervised Learning for Hedge Classification in Scientific Literature
In related work, Szarvas (2008) extended the methodology of Medlock and Briscoe (2007), and presented a hedge detection method in biomedical texts with a weakly supervised selection of keywords. $$$$$ We investigate automatic classification of speculative language (‘hedging’), in biomedical text using weakly supervised machine learning.

For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. $$$$$ Weakly Supervised Learning for Hedge Classification in Scientific Literature
For speculative sentences detection, Medlock and Briscoe (2007) report their approach based on weakly supervised learning. $$$$$ In this section, we derive a simple probabilistic model for acquiring training data for a given learning task, and use it to motivate our approach to weakly supervised hedge classification.

Medlock and Briscoe (2007) also used single words as input features in order to classify sentences from scientific articles in biomedical domain as speculative or non-speculative. $$$$$ Riloff et al. Given a collection of sentences, S, the task is to (2003) explore bootstrapping techniques to identify label each sentence as either speculative or nonsubjective nouns and subsequently classify subjec- speculative (spec or nspec henceforth).
Medlock and Briscoe (2007) also used single words as input features in order to classify sentences from scientific articles in biomedical domain as speculative or non-speculative. $$$$$ As discussed earlier, the speculative/non-speculative distinction hinges on the presence or absence of a few hedge cues within the sentence.

Medlock and Briscoe (2007) use a similar baseline as the one adopted by Light et al (2004), i.e. a naive algorithm based on substring matching, but with a different list of terms to match against. $$$$$ Banko and Brill (2001) use ‘bagging’ and agreeThe most clearly relevant study is Light et al. ment to measure confidence on unlabelled samples, (2004) where the focus is on introducing the prob- and more recently McClosky et al.
Medlock and Briscoe (2007) use a similar baseline as the one adopted by Light et al (2004), i.e. a naive algorithm based on substring matching, but with a different list of terms to match against. $$$$$ As a baseline classifier we use the substring matching technique of (Light et al., 2004), which labels a sentence as spec if it contains one or more of the following: suggest, potential, likely, may, at least, in part, possibl, further investigation, unlikely, putative, insights, point toward, promise and propose.

However Medlock and Briscoe (2007) note that their model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically. $$$$$ Statement of knowledge paucity. ture selection procedure.
However Medlock and Briscoe (2007) note that their model is unsuccessful in identifying assertive statements of knowledge paucity which are generally marked rather syntactically than lexically. $$$$$ For example, the learning models were unsuccessful in identifying assertive statements of knowledge paucity, eg: There is no clear evidence for cytochrome c release during apoptosis in C elegans or Drosophila.

Kilicoglu and Bergler (2008) did experiments on the same dataset as Medlock and Briscoe (2007) and their experimental results proved that the classification accuracy can be improved by approximately 9% (from an F-score of 76% to an F-score of 85%) if syntactic and semantic information are incorporated. $$$$$ The baseline classifier achieves a BEP of 0.60 while both classifiers using our learning model reach approximately 0.76 BEP with little to tell between them.
Kilicoglu and Bergler (2008) did experiments on the same dataset as Medlock and Briscoe (2007) and their experimental results proved that the classification accuracy can be improved by approximately 9% (from an F-score of 76% to an F-score of 85%) if syntactic and semantic information are incorporated. $$$$$ These results suggest that performance may be enhanced when the learning and classification tasks are carried out by different models.

The experiments run by Medlock (2008) on the same dataset as Medlock and Briscoe (2007) show that adding features based on part-of speech tags to a bag-of-words input representation can slightly improve the accuracy, but the improvements are marginal and not statistically significant. $$$$$ In this study we use single terms as features, based on the intuition that many hedge cues are single terms (suggest, likely etc.) and due to the success of ‘bag of words’ representations in many classification tasks to date.
The experiments run by Medlock (2008) on the same dataset as Medlock and Briscoe (2007) show that adding features based on part-of speech tags to a bag-of-words input representation can slightly improve the accuracy, but the improvements are marginal and not statistically significant. $$$$$ This has the dual benefit of removing irrelevant features and also reducing dependence between features, as the selected features will often be nonlocal and thus not too tightly correlated.

Other early work focused on semi supervised learning due to a lack of annotated datasets (Medlock and Briscoe, 2007). $$$$$ Weakly Supervised Learning for Hedge Classification in Scientific Literature
Other early work focused on semi supervised learning due to a lack of annotated datasets (Medlock and Briscoe, 2007). $$$$$ Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective.

We can then use the large amounts of unannotated sentences that are available to extract n-gram features that have high uncertainty class conditional probability and add them to our training set with those features labeled as hedges as described in Medlock and Briscoe (2007). $$$$$ Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features.
We can then use the large amounts of unannotated sentences that are available to extract n-gram features that have high uncertainty class conditional probability and add them to our training set with those features labeled as hedges as described in Medlock and Briscoe (2007). $$$$$ This has the dual benefit of removing irrelevant features and also reducing dependence between features, as the selected features will often be nonlocal and thus not too tightly correlated.

Medlock and Briscoe (2007) used single words as input feature sin order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. $$$$$ Riloff et al. Given a collection of sentences, S, the task is to (2003) explore bootstrapping techniques to identify label each sentence as either speculative or nonsubjective nouns and subsequently classify subjec- speculative (spec or nspec henceforth).
Medlock and Briscoe (2007) used single words as input feature sin order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples. $$$$$ As discussed earlier, the speculative/non-speculative distinction hinges on the presence or absence of a few hedge cues within the sentence.

Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. $$$$$ This has the dual benefit of removing irrelevant features and also reducing dependence between features, as the selected features will often be nonlocal and thus not too tightly correlated.
Szarvas (2008) extended the methodology of Medlock and Briscoe (2007) to use n-gram features and a semi-supervised selection of the keyword features. $$$$$ We use m=5 based on the intuition that five is a rough upper bound on the number of hedge cue features likely to occur in any one sentence.

Early work on speculative language detection tried to classify a sentence either as speculative or non-speculative (see, for example, Medlock and Briscoe (2007)). $$$$$ Early work direct relevance to the task of classifying speculative by Yarowsky (1995) falls within this framework. language from an NLP/ML perspective.
Early work on speculative language detection tried to classify a sentence either as speculative or non-speculative (see, for example, Medlock and Briscoe (2007)). $$$$$ As discussed earlier, the speculative/non-speculative distinction hinges on the presence or absence of a few hedge cues within the sentence.
