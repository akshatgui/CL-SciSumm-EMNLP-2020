This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002). $$$$$ Riloff and Shepherd (1997) also give some credit for ?related words?
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently byWiddows and Dorow (2002). $$$$$ (The first 4 cat egories are also used by (Riloff and Shepherd, 1997) and (Roark and Charniak, 1998) and so were included for comparison.)

The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (Veronis, 2004) or Pagerank (Agirre et al, 2006). $$$$$ Another way to obtain word-senses directly from corpora is to use clustering algorithms on feature-vectors (Lin, 1998; Schu?tze, 1998).Clustering techniques can also be used to discriminate between different senses of an ambiguous word.
The senses of a word may then be discovered using graph clustering techniques (Widdows and Dorow, 2002), or algorithms such as HyperLex (Veronis, 2004) or Pagerank (Agirre et al, 2006). $$$$$ Breadth of cover age does not in itself solve this problem: general lexical resources such as WordNet can provide too many senses many of which are rarely used in particular domains or corpora (Gale et al, 1992).The graph model presented in this paper suggests a new method for recognising relevant polysemy.

In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002). $$$$$ Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.
In concept acquisition, pattern-based methods were shown to outperform LSA by a large margin (Widdows and Dorow, 2002). $$$$$ So far we have presented a graph model built upon noun co-occurrence which performs much better than previously reported methods at the task of automatic lexical acquisition.

 $$$$$ This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled.
 $$$$$ 2 1http://infomap.stanford.edu/graphs 2http://muchmore.dfki.deFigure 1: Automatically generated graph show ing the word apple and semantically related nouns

The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. $$$$$ A Graph Model For Unsupervised Lexical Acquisition
The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. $$$$$ Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.

This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally. $$$$$ Definition 2 (Bolloba?s, 1998, Ch 1 ?1) Let G = (V,E) be a graph, where V is the set of vertices (nodes) of G and E ? V ? V is the set of edges of G. ? Two nodes v1, vn are said to be connected if there exists a path {v1, v2, . . .
This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally. $$$$$ Definition 3 Let G be a graph of words closely related to a seed-word w, and let G \ w be the subgraph which results from the removal of the seed-node w. The connected components of the subgraph G \ w are the senses of the word w with respect to the graph G. As an illustrative example, consider the localgraph generated for the word apple (6).

The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002). $$$$$ Section 2 reviews previous work on semanticsimilarity and lexical acquisition.
The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002). $$$$$ 5.2 Results.

 $$$$$ This algorithm has been built into an on-line demonstration where the user inputs a givenseed word and can then see the cluster of re lated words being gradually assembled.
 $$$$$ 2 1http://infomap.stanford.edu/graphs 2http://muchmore.dfki.deFigure 1: Automatically generated graph show ing the word apple and semantically related nouns

The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words. $$$$$ The graph model is built by linking pairs of words which participate in particular syntacticrelationships.
The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words. $$$$$ We focus on the symmetric relationship between pairs of nouns which occur to gether in lists.

This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications. The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small. $$$$$ For a given category, choose a small.
This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications. The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small. $$$$$ or ?WordNet category?

Fortunately, the subject list is such that WordNet words (Widdows and Dorow, 2002) also reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. $$$$$ used in these cases was based on co occurrence in lists.
Fortunately, the subject list is such that WordNet words (Widdows and Dorow, 2002) also reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. $$$$$ 5.2 Results.

This metric was reported to be 82% in (Widdows and Dorow, 2002). $$$$$ (Our results are also slightly better than those reported by Riloff and Jones (1999)).
This metric was reported to be 82% in (Widdows and Dorow, 2002). $$$$$ The LSI similarity thesaurus obtained an accuracy of 31%, much less than the graph model?s 82%.

In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node x to y if the corresponding content words appear in the pattern such that x precedes y. $$$$$ Our full graph contains many directed links between words of different parts of speech.
In order to identify such useful patterns, for each pattern we build a graph following (Widdows and Dorow, 2002). The graph is constructed from a node for each content word, and a directed arc from the node x to y if the corresponding content words appear in the pattern such that x precedes y. $$$$$ The best new node is taken to be the node b ? N(A)\A with the highest proportion of links to N(A).

In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. $$$$$ A Graph Model For Unsupervised Lexical Acquisition
In the context of graph-based clustering of words, Widdows and Dorow (2002) used a graph model for unsupervised lexical acquisition. $$$$$ Most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.

Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. $$$$$ A Graph Model For Unsupervised Lexical Acquisition
Widdows and Dorow (2002) use a graph model for unsupervised lexical acquisition. $$$$$ So far we have presented a graph model built upon noun co-occurrence which performs much better than previously reported methods at the task of automatic lexical acquisition.

In order to identify symmetric patterns, for each pattern we define a pattern graph G (P), as proposed by (Widdows and Dorow, 2002). $$$$$ Define the ?WordNet class?
In order to identify symmetric patterns, for each pattern we define a pattern graph G (P), as proposed by (Widdows and Dorow, 2002). $$$$$ The equivalence classes of the graph G un der this relation are called the components of G. We are now in a position to define the senses of a word as represented by a particular graph.

The concept discovery algorithmis essentially the same as used by (Davidov and Rappoport, 2006) and has some similarity with the one used by (Widdows and Dorow, 2002). $$$$$ used in these cases was based on co occurrence in lists.
The concept discovery algorithmis essentially the same as used by (Davidov and Rappoport, 2006) and has some similarity with the one used by (Widdows and Dorow, 2002). $$$$$ Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used.

We have also performed an indirect comparison to (Widdows and Dorow, 2002). While there is a significant number of other related studies on concept acquisition (see Section 2), Most are supervised and/or use language-specific tools. $$$$$ Section 2 reviews previous work on semanticsimilarity and lexical acquisition.
We have also performed an indirect comparison to (Widdows and Dorow, 2002). While there is a significant number of other related studies on concept acquisition (see Section 2), Most are supervised and/or use language-specific tools. $$$$$ The same number of nouns was re trieved for each class using the graph model and LSI.

All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998).
All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ Since the algorithms used are in many waysvery similar, this improvement demands expla nation.Some of the difference in accuracy can be at tributed to the corpora used.

This also allows indirect comparison to several other studies, thus (Widdows and Dorow, 2002) reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. $$$$$ used in these cases was based on co occurrence in lists.
This also allows indirect comparison to several other studies, thus (Widdows and Dorow, 2002) reports results for an LSA-based clustering algorithm that are vastly inferior to the pattern-based ones. $$$$$ 5.2 Results.
