Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ Again, proper nouns and low-frequency nouns were discarded from this list.
Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ Here, the conditional probability model reached a performance of 83.9% correct on the low-frequency data set.

Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ For the seen bigrams, we showed that the Web frequencies correlate better with judged plausibility than corpus frequencies.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ We found that Web frequencies and re-created frequencies are reliably correlated, and that Web frequencies are better at predicting plausibility judgments than smoothed frequencies.

We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ Previous work has demonstrated that corpus counts correlate with human plausibility judgments for adjective-noun bigrams.
We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ For seen and unseen adjective-noun bigrams, we used the two sets of plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001), respectively.

The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling.
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ Preliminary results are reported by Lapata and Keller (2003), who use Web counts successfully for a range of NLP tasks, including candidate selection for machine translation, context-sensitive spelling correction, bracketing and interpretation of compounds, adjective ordering, and PP attachment.

Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ Keller and Lapata Web Frequencies for Unseen Bigrams Another source of noise is the fact that Google (but not AltaVista) will sometimes return pages that do not include the search term at all.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ The difference in correlation coefficients was not significant for noun-noun and verb-object bigrams, for either search engine.

More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ Using The Web To Obtain Frequencies For Unseen Bigrams
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ For both seen and unseen bigrams, there was only a very small difference between the correlation coefficients obtained with the two search engines.

See Keller and Lapata (2003) for more issues. $$$$$ This result holds both for seen bigrams (Lapata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been re-created using smoothing techniques (Lapata, Keller, and McDonald 2001).
See Keller and Lapata (2003) for more issues. $$$$$ For seen and unseen adjective-noun bigrams, we used the two sets of plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001), respectively.

The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ We found that the counts obtained from the Web are highly correlated with the counts obtained from the BNC.
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ Again, we found that Web counts are highly correlated with corpus counts.

The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ A similar picture was observed for the NANTC counts.
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ The performance on the whole data set is 77.7%, which is below the performance of 80.0% reported by Rooth et al. (1999).

It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ Other samples of spoken language are also included, ranging from business or government meetings to radio shows and phoneins.
It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ The results are shown in Table 9.

We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ This result holds both for seen bigrams (Lapata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been re-created using smoothing techniques (Lapata, Keller, and McDonald 2001).

Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ This result holds both for seen bigrams (Lapata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been re-created using smoothing techniques (Lapata, Keller, and McDonald 2001).
Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ For each pair (v, n) a fairly frequent verb v' was randomly chosen such that the pair (v', n) did not occur in the data set.

 $$$$$ From these data, we computed the average factor by which the Web counts are larger than the BNC counts.
 $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.

Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ Recall that the NANTC is 3.5 times larger than the BNC, which does not seem to be enough to substantially mitigate data sparseness.
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ A similar picture was observed for the NANTC counts.

 $$$$$ From these data, we computed the average factor by which the Web counts are larger than the BNC counts.
 $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.

The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ For seen and unseen adjective-noun bigrams, we used the two sets of plausibility judgments collected by Lapata, McDonald, and Keller (1999) and Lapata, Keller, and McDonald (2001), respectively.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ Preliminary results are reported by Lapata and Keller (2003), who use Web counts successfully for a range of NLP tasks, including candidate selection for machine translation, context-sensitive spelling correction, bracketing and interpretation of compounds, adjective ordering, and PP attachment.

It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ We found that Web counts were highly correlated with frequencies from two different corpora.
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ We found that the counts obtained from the Web are highly correlated with the counts obtained from the BNC.

However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ We conclude that simple heuristics (see Section 2.3) are sufficient to obtain useful frequencies from the Web; it seems that the large amount of data available for Web counts outweighs the associated problems (noisy, unbalanced, etc.).
However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ It seems that the large amount of data available outweighs the problems associated with using the Web as a corpus (such as the fact that it is noisy and unbalanced).

 $$$$$ From these data, we computed the average factor by which the Web counts are larger than the BNC counts.
 $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.

NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ This result holds both for seen bigrams (Lapata, McDonald, and Keller 1999) and for unseen bigrams whose counts have been re-created using smoothing techniques (Lapata, Keller, and McDonald 2001).
NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ We used correlation analysis to compare corpus counts and Web counts with plausibility judgments.
