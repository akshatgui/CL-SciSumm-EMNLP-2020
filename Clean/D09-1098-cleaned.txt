Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ 2008) and textual advertising (Chang et al. 2009).
Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.

 $$$$$ Let PMI(w) denote a pointwise mutual information vector, constructed for each term as follows: PMI(w) = (pmiw1, pmiw2, ..., pmiwm), where pmiwf is the pointwise mutual information between term w and feature f: where cwf is the frequency of feature f occurring for term w, n is the number of unique terms and N is the total number of features for all terms.
 $$$$$ Finally, we release to the community a testbed for experimentally analyzing automatic set expansion, which includes a large collection of nearly random entity sets extracted from Wikipedia and over 22,000 randomly sampled seed expansion trials.

KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ Etzioni et al. (2005) and Pantel et al.
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state-of-the-art expansion algorithm using Wikipedia.

Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ 2008) and textual advertising (Chang et al. 2009).
Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ Etzioni et al. (2005) and Pantel et al.

To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ Etzioni et al. (2005) and Pantel et al.

Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Etzioni et al. (2005) and Pantel et al.
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ We then computed the similarity between all noun phrase chunks using the model of Section 3.1.

Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ We study the impact of seed selection effect by inspecting the system performance for several randomly selected seed sets of fixed size and we find that seed set composition greatly affects performance.
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ We found that a) very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set; b) 520 seeds yield on average best performance; and c) surprisingly, increasing the seed set size beyond 20 or 30 on average does not find any new correct instances.

Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ 2008) and textual advertising (Chang et al. 2009).
Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.

As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ Etzioni et al. (2005) and Pantel et al.
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ On average, the best performing seed sets had 42% higher precision and 39% higher recall than the worst performing seed set.

According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ We found that a) very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set; b) 520 seeds yield on average best performance; and c) surprisingly, increasing the seed set size beyond 20 or 30 on average does not find any new correct instances.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ Error analysis on the Web100 corpus shows that once our model has seen 10-20 seeds, the distributional similarity model seems to have enough statistics to discover as many new correct instances as it could ever find.

We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ Etzioni et al. (2005) and Pantel et al.
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ We then computed the similarity between all noun phrase chunks using the model of Section 3.1.

Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Web-Scale Distributional Similarity and Entity Set Expansion
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Etzioni et al. (2005) and Pantel et al.

Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ Etzioni et al. (2005) and Pantel et al.
Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ We omitted statistics from Wikipedia “List of” pages in order to not bias our evaluation to the test set described in Section 5.1.

To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ Etzioni et al. (2005) and Pantel et al.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ On average they only find 73% 5 To avoid biasing our Wikipedia corpus with the test sets, Wikipedia “List of” pages were omitted from our statistics as were any page linked to gold standard list members from “List of” pages. of the top-1000 similar terms of a random term whereas we find all of them.

The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ Web-Scale Distributional Similarity and Entity Set Expansion
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.

Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence- and corpus-level features.
Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.

CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ Apart from the choice of a data source, state-of-the-art entity extraction methods differ in their use of numerous, few or no labeled examples, the open or targeted nature of the extraction as well as the types of features employed.
CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.

As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ Etzioni et al. (2005) and Pantel et al.
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ This study shows that only few seeds (10-20) yield best performance and that adding more seeds beyond this does not on average affect performance in a positive or negative way.

Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ Intuitively, some seeds are better than others.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ We see that most gold standard instances are discovered with the first 5-10 seeds.

This combination was also used in (Pantel et al, 2009). $$$$$ 2008) and textual advertising (Chang et al. 2009).
This combination was also used in (Pantel et al, 2009). $$$$$ Etzioni et al. (2005) and Pantel et al.
