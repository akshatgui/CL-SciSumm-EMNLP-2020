We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. $$$$$ Our objective is to restore ill-formed words to their canonical lexical forms in standard English.
We use unsupervised methods to build a pipeline that identifies ill-formed English SMS word tokens and builds a dictionary of their most likely normalized forms. $$$$$ We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms, focusing exclusively on English for the purposes of this paper.

Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity. $$$$$ Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.
Hanand Baldwin (2011) use a classifier to detect ill formed words, and then generate correction candidates based on morphophonemic similarity. $$$$$ We found that most illformed words are based on morphophonemic variation and proposed a cascaded method to detect and normalise ill-formed words.

w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). $$$$$ Both word similarity and context are then exploited to select the most probable correction candidate for the word.
w2wN: The output of the word-to-word normalization of Han and Baldwin (2011). $$$$$ We found several limitations in our proposed approach by analysing the output of our method.

5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). $$$$$ This data is randomly sampled from the 1.5GB of clean Twitter data, and errors are generated according to distribution of SMS corpus.
5.2.1 Twitter To evaluate the performance on Twitter data, we use the dataset of randomly sampled tweets produced by (Han and Baldwin, 2011). $$$$$ We evaluate detection performance by token-level precision, recall and F-score (Q = 1).

We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). $$$$$ If there were some way of preprocessing the message to produce a more canonical lexical rendering, we would expect the quality of the parser to improve appreciably.
We expect that our language model could improve other Social Media tasks, for example lexical normalisation (Han and Baldwin, 2011) or even event detection (Lin et al., 2011). $$$$$ First, we plan to improve our ill-formed word detection classifier by introducing an OOV word whitelist.

Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. $$$$$ Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.
Han and Baldwin (2011) use a classifier to detect ill-formed words, and generate correction candidates based on morphophonemic similarity. $$$$$ We found that most illformed words are based on morphophonemic variation and proposed a cascaded method to detect and normalise ill-formed words.

Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. $$$$$ We identified 254 token instances of lexical normalisation, and broke them down into categories, as listed in Table 1.
Recently, Han and Baldwin (2011) and Gouwsetal2011) propose two-step unsupervised approaches to normalisation, in which lexical variants are first identified, and then normalised. $$$$$ This step is crucial to further normalisation, because if correct OOV words are identified as ill-formed, the candidate selection step can never be correct.

They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011). $$$$$ For example, Kobus et al. (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup.
They approach lexical variant detection by using a context fitness classifier (Han and Baldwin, 2011) or through dictionary lookup (Gouws et al 2011). $$$$$ To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al., 2003; Sun et al., 2007).

In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available. $$$$$ Our aim in this paper is this task of lexical normalisation of noisy English text, with a particular focus on Twitter and SMS messages.
In contrast to the normalisation dictionaries of Han and Baldwin (2011) and Gouws et al 2011) which focus on very frequent lexical variants, we focus on moderate frequency lexical variants of a minimum character length, which tend to have unambiguous standard forms; our intention is to produce normalisation lexicons that are complementary to those currently available. $$$$$ We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms, focusing exclusively on English for the purposes of this paper.

To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011). $$$$$ In confusion set generation, we generate a set of IV normalisation candidates for each OOV word type based on morphophonemic variation.
To further narrow the search space, we only consider IV words which are morphophonemic ally similar to the OOV type, following settings in Han and Baldwin (2011). $$$$$ The most direct source of evidence is IV words around an OOV word.

Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). $$$$$ The message normalisation task is challenging.
Given the re-ranked pairs from Section 5, here we apply them to a token-level normalisation task using the normalisation dataset of Han and Baldwin (2011). $$$$$ For candidate selection, we once again evaluate using token-level precision, recall and F-score.

In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. $$$$$ In addition to comparing our method with competitor methods, we also study the contribution of different feature groups.
In addition, the contribution of these dictionaries in hybrid normalisation approaches is also presented, in which we first normalise OOVs using a given dictionary (combined or otherwise), and then apply the normalisation method of Gouws et al2011) based on consonant edit distance (GHM-norm), or the approach of Han and Baldwin (2011) based on the summation of many unsupervised approaches (HB-norm), to the remaining OOVs. $$$$$ The best F-score is achieved when combining dictionary lookup, word similarity and context support (“DL+WS+CS”), in which ill-formed words are first looked up in the slang dictionary, and only if no match is found do we apply our normalisation method.

the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. $$$$$ “Slang” refers to instances of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1).
the Internet slang dictionary (HB-dict) from Han and Baldwin (2011), and combinations of these dictionaries. $$$$$ We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three.

In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). $$$$$ In addition, the detection of ill-formed words is difficult due to noisy context.
In addition, we combine the dictionaries with the normalisation method of Gouws et al2011) (GHM-norm) and the combined unsupervised approach of Han and Baldwin (2011) (HB-norm). $$$$$ We found several limitations in our proposed approach by analysing the output of our method.

6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors. $$$$$ First, higher detection threshold values (td) give better precision but lower recall.
6.2.3 Hybrid Approaches The methods of Gouws et al2011) (i.e. GHM-dict+GHM-norm) and Han and Baldwin (2011) (i.e. HB-dict+HB-norm) have lower precision and higher false alarm rates than the dictionary based approaches; this is largely caused by lexical variant detection errors. $$$$$ The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors.

The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). $$$$$ In our annotation, the annotators only normalised ill-formed word if they had high confidence of how to normalise, as with talkin “talking”.
The present lexical normalisation used by our system is the dictionary lookup method of Hanand Baldwin (2011) which normalises noisy tokens only when the normalised form is known with high confidence (e.g. you for u). $$$$$ Furthermore, we intend to alleviate noisy contexts with a bootstrapping approach, in which ill-formed words with high confidence and no ambiguity will be replaced by their standard forms, and fed into the normalisation model as new training data.

Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). $$$$$ Statistical machine translation (SMT) has been proposed as a means of context-sensitive text normalisation, by treating the ill-formed text as the source language, and the standard form as the target language.
Ultimately, however, we are interested in performing context sensitive lexical normalisation, based on a reimplementation of the method of Han and Baldwin (2011). $$$$$ For such cases, the method falls back to context-independent normalisation.

(Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. $$$$$ If we truncate the ranking to the top 10% of candidates, the recall drops back to 84% with a 90% reduction in candidates.
(Hanand Baldwin, 2011) reported an average of 127 candidates per nonstandard token with the correct-word coverage of 84%. $$$$$ We separately introduce a threshold td E 11, 2, ...,10} on the number of positive predictions returned by the detection classifier over the set of normalisation candidates for a given OOV token: the token is considered to be ill-formed iff td or more candidates are positively classified, i.e. predicted to be correct candidates.

(Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity. $$$$$ Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity.
(Han and Baldwin, 2011) developed classifiers for detecting the ill-formed word sand generated corrections based on the morphophonemic similarity. $$$$$ Then, all candidates are ranked according to a list of features generated from noisy context and similarity between ill-formed words and candidates.

 $$$$$ “Slang” refers to instances of Internet slang (e.g. lol “laugh out loud”), as found in a slang dictionary (see Section 3.1).
 $$$$$ NICTA is funded by the Australian government as represented by Department of Broadband, Communication and Digital Economy, and the Australian Research Council through the ICT centre of Excellence programme.
