Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ Therefore, again, unweighted measures, and in particular K, tend to be used for measuring inter-coder agreement.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ We believe that part of the reluctance to report chance-corrected measures is the difficulty in interpreting them.

Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ One important reason why most agreement results on segmentation are on the lower end of the reliability scale is the fact, known to researchers in discourse analysis from as early as Levin and Moore (1978), that although analysts generally agree on the “bulk” of segments, they tend to disagree on their exact boundaries.
Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ The fact that coders mostly agree on the “bulk” of discourse segments, but tend to disagree on their boundaries, also makes it likely that an all-or-nothing coefficient like K calculated on individual boundaries would underestimate the degree of agreement, suggesting low agreement even among coders whose segmentations are mostly similar.

Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. $$$$$ Survey Article

For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ We refer the reader to Krippendorff (1995) for details.
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Our own experience is consistent with that of Krippendorff

For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ We refer the reader to Krippendorff (1995) for details.
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Our own experience is consistent with that of Krippendorff

It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ Recent content analysis practice seems to have settled for even more stringent requirements

Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ On this task, a (pairwise) percentage agreement of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding to K values of 0.36, 0.37, and 0.67, respectively.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ Our own experience is consistent with that of Krippendorff

Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ A number of coding schemes for dialogue acts have achieved values of K over 0.8 and have therefore been assumed to be reliable

As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ However, it is important to keep in mind that achieving good agreement cannot ensure validity

Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ 2.6.2 Cohen’s κw.
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ A weighted variant of Cohen’s κ is presented in Cohen (1968).

Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ The problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004).
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4).

The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ The problem is not unlike that of interpreting the values of correlation coefficients, and in the area of medical diagnosis, the best known conventions concerning the value of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in Figure 1, are indeed similar to those used for correlation coefficients, where values above 0.4 are also generally considered adequate (Marion 2004).
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ In fact, weighted coefficients, while arguably more appropriate for many annotation tasks, make the issue of deciding when the value of a coefficient indicates sufficient agreement even Kappa values and strength of agreement according to Landis and Koch (1977). more complicated because of the problem of determining appropriate weights (see Section 4.4).

A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ Survey Article

In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ This reproducibility is measured by π.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ A second annotation study we carried out (Artstein and Poesio 2006) shows even more clearly the possible side effects of using weighted coefficients.

However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ Our own experience is consistent with that of Krippendorff

Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ This is the approach taken by Fleiss (1971).
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ Observed agreement for all of the unweighted coefficients (S, κ, and π) is calculated by counting the items on which the coders agree (the Artstein and Poesio Inter-Coder Agreement for CL figures on the diagonal of the confusion matrix in Table 4) and dividing by the total number of items.

Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ Fleiss (1971) therefore uses a different type of table which lists each item with the number of judgments it received for each category; Siegel and Castellan (1988) use a similar table, which Di Eugenio and Glass (2004) call an agreement table.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ The expected agreement is the sum of this joint probability over all the categories k E K. Multi-π is the coefficient that Siegel and Castellan (1988) call K. 2.5.2 Multi-κ.

 $$$$$ Looking at these two problems in detail is useful for understanding the differences between the coefficients.
 $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.

Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ All of these proposals are based on a misconception

We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ If observed agreement is measured on the basis of pairwise agreement (the proportion of agreeing judgment pairs), it makes sense to measure expected agreement in terms of pairwise comparisons as well, that is, as the probability that any pair of judgments for an item would be in agreement—or, said otherwise, the probability that two arbitrary coders would make the same judgment for a particular item by chance.
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ The difference between the coefficients is only in the interpretation of “chance agreement”
