Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus (Marcus et al, 1993), and additional parsed text was obtained by parsing the Wall Street Journal text using the parser described in Charniak et al (1998). $$$$$ For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing.
Some of the data comes from the parsed files 2-21 of the Wall Street Journal Penn Treebank corpus (Marcus et al, 1993), and additional parsed text was obtained by parsing the Wall Street Journal text using the parser described in Charniak et al (1998). $$$$$ Our figures were obtained using ri = 1.2.


Charniak et al (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. $$$$$ Edge-Based Best-First Chart Parsing
Charniak et al (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. $$$$$ Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&C) (Forthcoming).

Therein, the idea of coarse-to-fine parsing (Charniak et al, 1998) is extended to handle the repeated parsing of the same sentences. $$$$$ One well-known 0(n3) parsing method (Kay, 1980) is chart parsing.
Therein, the idea of coarse-to-fine parsing (Charniak et al, 1998) is extended to handle the repeated parsing of the same sentences. $$$$$ Once this has been done there is no need for incomplete edges at all in bottomup parsing, and parsing can be performed using the CKY algorithm, suitably extended to handle unary productions.

One paper that focuses on efficiency of statistical parsing is Charniak et al (1998). $$$$$ As noted, our paper takes off from that of C&C and uses the same FOM.
One paper that focuses on efficiency of statistical parsing is Charniak et al (1998). $$$$$ We make the same assumption in this paper.

Charniak et al (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. $$$$$ Note that regardless of 7/ the accuracy of the parse increases given extra time, but that all of the increase is achieved with only 1.5 to 2 times as many edges as needed for the first parse.
Charniak et al (1998) introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found. $$$$$ As such we strongly recommend this technique to others interested in PCFG parsing.

This binarization process is similar to the one described in (Charniak et al, 1998). $$$$$ Goodman uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate.
This binarization process is similar to the one described in (Charniak et al, 1998). $$$$$ We applied the binarization technique described above to the grammar.

Here, we observe an effect seen in previous work (Charniak et al (1998), Petrov and Klein (2007), Petrov et al (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). $$$$$ This has the effect of multiplying the inside probability /3(N.4) by rik-J.
Here, we observe an effect seen in previous work (Charniak et al (1998), Petrov and Klein (2007), Petrov et al (2008)), that a certain amount of pruning helps accuracy, perhaps by promoting agreement between the coarse and full grammars (model intersection). $$$$$ As can be seen, our parser requires about one twentieth the number of edges required by C&C.

Two grammars are equivalent if they define the same probability distribution over strings (Charniak et al, 1998). $$$$$ We have empirically measured the normalization factor and found that the bi-tag distribution produces probabilities that are approximately 1.3 times those produced by the PCFG distribution, on a per-word basis.
Two grammars are equivalent if they define the same probability distribution over strings (Charniak et al, 1998). $$$$$ One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs (Charniak, 1997), or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997).

As shown in Charniak et al (1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing. $$$$$ Edge-Based Best-First Chart Parsing
As shown in Charniak et al (1998), we can binarize explicitly and use intermediate symbols to replace dotted rules in chart parsing. $$$$$ The average number of popped edges to first parse as a function of q is shown in Figure 1, and the average precision and recall are shown in Figure 2.

Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. $$$$$ Edge-Based Best-First Chart Parsing
Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework. $$$$$ Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here.

I-TRIE is a non-deterministic left branching trie with weights on rule entry as in Charniak et al (1998). $$$$$ The key observation is that the 'new' non-terminals `01,i' in a CKY parse using a left-factored grammar correspond to the set of non-empty incomplete edges A -4 01,z.
I-TRIE is a non-deterministic left branching trie with weights on rule entry as in Charniak et al (1998). $$$$$ Specifically, the fundamental rule of chart parsing (Kay, 1980), which combines an incomplete edge A --* a • BO with a complete edge B 7- to yield the edge A -+ a B • 0, corresponds to the left-factored productions `aB' --+ a B if /3 is non-empty or A 'a' B if i3 is empty.

Following (Charniak et al, 1998), we parsed unseen sentences of length 18-26 from the Penn Treebank, using the grammar induced from the remainder of the treebank. $$$$$ In our work we have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence.
Following (Charniak et al, 1998), we parsed unseen sentences of length 18-26 from the Penn Treebank, using the grammar induced from the remainder of the treebank. $$$$$ In Figure 4 we reproduce C&C's results on the percentage of sentences (length 18-26) parsed as a function of number of edges used.

On the other hand, the more complex, tuned FOM in (Charniak et al, 1998) is able to parse all of these sentences using around 2K edges, while BF requires 7K edges. $$$$$ In our work we have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence.
On the other hand, the more complex, tuned FOM in (Charniak et al, 1998) is able to parse all of these sentences using around 2K edges, while BF requires 7K edges. $$$$$ As can be seen, our parser requires about one twentieth the number of edges required by C&C.

The complex FOMs in (Charniak et al, 1998) require somewhat more online computation to assemble. $$$$$ Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox.
The complex FOMs in (Charniak et al, 1998) require somewhat more online computation to assemble. $$$$$ Goodman uses an FOM that is similar to that of C&C but one that should, in general, be somewhat more accurate.

 $$$$$ Here NJ, is a constituent of type i (e.g., NP, VP, etc.) that spans the constituents from j up to but not including k, and tom are the n parts-of-speech (tags) of the sentence.
 $$$$$ As such we strongly recommend this technique to others interested in PCFG parsing.

In the training phase, each target-style parse tree in the training data is transformed into a binary tree (Charniak et al, 1998) and then decomposed into a (golden) action-state sequence. $$$$$ Of the five terms in Equation 4, two can be directly estimated from training data

This measure is used by Charniak et al (1998) and Klein and Manning (2003b). $$$$$ For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing.
This measure is used by Charniak et al (1998) and Klein and Manning (2003b). $$$$$ We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse.

With respect to chart parsing, (Charniak et al, 1998) report that their parser can achieve good results while producing about three times tile mininmm number of edges required to produce the final parse. $$$$$ As can be seen, our parser requires about one twentieth the number of edges required by C&C.
With respect to chart parsing, (Charniak et al, 1998) report that their parser can achieve good results while producing about three times tile mininmm number of edges required to produce the final parse. $$$$$ Since the average number of edges required to construct just the (left-factored) test corpus trees is 47.5, our parsing system considers as few as 3 times as many edges as are required to actually produce the output tree.

Related approaches are used in Hall (2004) and Charniak and Johnson (2005). $$$$$ In Figure 4 we reproduce C&C's results on the percentage of sentences (length 18-26) parsed as a function of number of edges used.
Related approaches are used in Hall (2004) and Charniak and Johnson (2005). $$$$$ We have two (possibly related) theories of these phenomona.
