In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of Culotta et al. (2007) and Bengston and Roth (2008).
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ There are several general trends to the errors made by our system.

In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ The required lexical pattern X who served as Y is a general appositive-like pattern that almost surely indicates coreference.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ Each row is a mention type and the column the predicted mention type antecedent.

For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ Syntactic constraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.

 $$$$$ A common error arose from the use of mention head distance as a poor proxy for discourse salience.
 $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ This procedure is trivial where most systems are rich, and so does not need any supervised coreference data.

We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.
We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ Finally, we offer our approach as a very strong, yet easy to implement, baseline.

Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4.
Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ The closest (in tree distance, see Section 3.1.1) compatible mention is The Israelis, which is correct particular, this fixes examples such as those in Figure 1 where the true antecedent has many embedded mentions between itself and the pronoun.

We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.

The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ The resolution of entity reference is influenced by a variety of constraints.
The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ First, a selfcontained syntactic module carefully represents syntactic structures using an augmented parser and extracts syntactic paths from mentions to potential antecedents.

Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ Mapping Mentions to Parse Nodes

For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ Role Appositives

 $$$$$ A common error arose from the use of mention head distance as a poor proxy for discourse salience.
 $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Importantly, the bulk of the work in the syntactic module is in making sure the parses are correctly constructed and used, and this moduleâ€™s most important training data is a treebank.
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ The one exploited most in coreference work (Soon et al., 1999; Ng and Cardie, 2002; Luo et al., 2004; Culotta et al., 2007; Poon and Domingos, 2008; Bengston and Roth, 2008) is the appositive construction.

 $$$$$ A common error arose from the use of mention head distance as a poor proxy for discourse salience.
 $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ The one exploited most in coreference work (Soon et al., 1999; Ng and Cardie, 2002; Luo et al., 2004; Culotta et al., 2007; Poon and Domingos, 2008; Bengston and Roth, 2008) is the appositive construction.

 $$$$$ A common error arose from the use of mention head distance as a poor proxy for discourse salience.
 $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

We start by preprocessing all the news in the news collections with a standard NLP pipeline $$$$$ The resolution of entity reference is influenced by a variety of constraints.
We start by preprocessing all the news in the news collections with a standard NLP pipeline $$$$$ In this work, we break from the standard view.

ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Simple Coreference Resolution with Rich Syntactic and Semantic Features
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Across metrics, the syntactic constraints and semantic compatibility components contribute most to the overall final result.

We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ In this work, we break from the standard view.
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ However, it is rich in important ways which we argue are marginalized in recent coreference work.

F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ Our best b3 result of 79.0 is broadly in the range of these results.
F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ On the MUC6-TEST dataset, our system outpersion made by the system.
