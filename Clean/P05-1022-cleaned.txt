Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ We use the 50-best parses produced by this algorithm as input to a MaxEnt discriminative reranker.

Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. $$$$$ The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.

Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ Coarse-To-Fine N-Best Parsing And MaxEnt Discriminative Reranking
Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.

 $$$$$ Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.
 $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).

 $$$$$ Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.
 $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).

We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ Because there may not be a unique best parse for each sentence (i.e.,

This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ By “coarse-to-fine” we mean that it first produces a crude version of the parse using coarse-grained dynamic programming states, and then builds fine-grained analyses by splitting the most promising of coarse-grained states.
This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ The unpruned edges are then exhaustively evaluated according to the fine-grained probabilistic model; in effect, each coarse-grained dynamic programming state is split into one or more fine-grained dynamic programming states.

 $$$$$ Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.
 $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).

Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n−1th-best parses.
Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ The n-best parser’s most probable parses are already of state-of-the-art quality, but the reranker further improves the f-score.

We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.
We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.

 $$$$$ Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.
 $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).

Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.
Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ From the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements.

 $$$$$ Thus the basic idea behind this approach to n-best parsing is to first find the best parse, then find the second-best parse, then the third-best, and so on.
 $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).

We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ So a feature extractor maps each y to a vector of feature values f(y) = (f1(y), ..., fm(y)).
We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ Feature schema are often parameterized in various ways.

To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. $$$$$ This section describes how each parse y is mapped to a feature vector f(y) = (f1(y), ... , fm(y)).
To improve performance and robustness, features are pruned as in Charniak and Johnson (2005): selected features must distinguish a parse with the highest F1 score in a n-best list, from a parse with a suboptimal F1 score at least five times. $$$$$ The first feature f1(y) = log p(y) is the logarithm of the parse probability p according to the n-best parser model.

To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art.
To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser.

Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). $$$$$ Because there may not be a unique best parse for each sentence (i.e.,

We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). $$$$$ There is, however, one particular feature of the Charniak parser that mitigates the space problem: it is a “coarse-to-fine” parser.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). $$$$$ (The results are for all sentences of section 23 of the WSJ tree-bank of length ≤ 100.)

Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ Indeed, Collins’s n-best list of parses for section 24 of the Penn tree-bank has some sentences with only a single parse, because the n-best parser could not find any parses.
Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ Instead, the output of the first stage is a polynomial-sized packed parse forest which records the left and right string positions for each local tree in the parses generated by this grammar.

The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. $$$$$ Table 1 shows how the number of fine-grained dynamic programming states increases as a function of sentence length for the sentences in section 24 of the Treebank.
The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. $$$$$ The results are presented in Table 3.
