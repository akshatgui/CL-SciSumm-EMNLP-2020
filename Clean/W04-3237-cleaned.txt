Chelba and Acero (2004) first traina classifier on the source data. $$$$$ We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ The “There’s no data like more data” rule-of-thumb could be amended by “..., especially if it’s the right data!”.

(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.
(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems.

The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ The regularized log-likelihood of the adaptation training data becomes: The adaptation is performed in stages: Fadapt \ Fbackground4 introduced in the model receive 0 weight.
The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ The resulting model is thus equivalent with the background model.

Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ A simple approach to sequence labeling is the maximum entropy Markov model.

Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.

One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).

In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.

This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ In the adaptation scenario we already have a MaxEnt model trained on the background data and we wish to make best use of the adaptation data by balancing the two.
This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ We did not experiment with various tying schemes although this is a promising research direction.

Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).
Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ As shown in Appendix A, the update equations are very similar to the 0-mean case: The effect of the prior is to keep the model parameters λi close to the background ones.

Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.

Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ A simple approach to sequence labeling is the maximum entropy Markov model.

 $$$$$ The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000).
 $$$$$ Special thanks to Adwait Ratnaparkhi for making available the code for his MEMM tagger and MaxEnt trainer.

Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ There are a number of parameters to be tuned on development data.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ The adaptation procedure was found cut-off threshold used for feature selection on CNNtrn adaptation data; the entry corresponding to the cut-off threshold of 106 represents the number of features in the background model to be insensitive to the number of reestimation iterations, and, more surprisingly, to the number of features added to the background model from the adaptation data, as shown in 5.

The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text.
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).
