More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem.

Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links.
Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ Given a weighted sentence pair, high scoring phrases are linked together greedily to reach an initial alignment.

Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). $$$$$ The Complexity of Phrase Alignment Problems
Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). $$$$$ Marcu and Wong (2002) describes an approximation to O.

Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.

(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP).
(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP).

Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. $$$$$ On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.
Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. $$$$$ Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP).

While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ We first define the notion of a partition

The first challenge is with inference $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
The first challenge is with inference $$$$$ In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.

Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard.

The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). $$$$$ A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem.

However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). $$$$$ Let a weighted sentence pair additionally include a real-valued function 0

It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. $$$$$ We first define the notion of a partition

A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.
A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ Sentences per hour on a four-core server Frequency of optimal solutions found Frequency of e-optimal solutions found Using an off-the-shelf ILP solver,4 we were able to quickly and reliably find the globally optimal phrase alignment under 0(eij, fkl) derived from the Moses pipeline (Koehn et al., 2007).5 Table 1 shows that finding the optimal phrase alignment is accurate and efficient.6 Hence, this simple search technique effectively addresses the intractability challenges inherent in evaluating new phrase alignment ideas.

However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). $$$$$ For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs.
However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). $$$$$ However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited.
