These systems were selected from WMT09 (Callison-Burch et al, 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
These systems were selected from WMT09 (Callison-Burch et al, 2009). $$$$$ The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008).

To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
To identify the most suitable system for our requirements, we run a set of experiments training the three models with Europarl V4 German-English (Koehn, 2005) and optimizing and testing on the News corpus (Callison-Burch et al 2009). $$$$$ The edited output of the best performing systems under this evaluation model were deemed acceptable around 50% of the time for French-English, English-French, EnglishSpanish, German-English, and English-German.

To train our models we use the freely available corpora (when possible) $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
To train our models we use the freely available corpora (when possible) $$$$$ In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus.

We trained two SMT systems, SMT content and SMTtitle, using the Europarl V4 German-English data as training corpus, and two different development sets $$$$$ On the other hand, the training data used by Google is unconstrained, which means that it may have an advantage compared to the research systems evaluated in this workshop, since they were trained using only the provided materials.
We trained two SMT systems, SMT content and SMTtitle, using the Europarl V4 German-English data as training corpus, and two different development sets $$$$$ The edited output of the best performing systems under this evaluation model were deemed acceptable around 50% of the time for French-English, English-French, EnglishSpanish, German-English, and English-German.

The recent Fr-En 109 (Callison-Burch et al, 2009) corpus aggregates huge numbers of parallel French English sentences from the web. $$$$$ 109 word parallel corpus To create the large French-English parallel corpus, we conducted a targeted web crawl of bilingual web sites.
The recent Fr-En 109 (Callison-Burch et al, 2009) corpus aggregates huge numbers of parallel French English sentences from the web. $$$$$ In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus.

Evaluation campaigns like WMT (Callison-Burch et al, 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. $$$$$ Findings of the 2009 Workshop on Statistical Machine Translation
Evaluation campaigns like WMT (Callison-Burch et al, 2009) and IWSLT (Paul, 2009) also contains a wealth of information for feature engineering in various MT tasks. $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).

We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al, 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
We test our metrics in the setting of the WMT 2009 evaluation task (Callison-Burch et al, 2009). $$$$$ There were three shared tasks this year

Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Work shop on Statistical Machine Translation shared task (Callison-Burch et al, 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al,2009). $$$$$ Findings of the 2009 Workshop on Statistical Machine Translation
Finally, we plan to repeat this experiment over other test beds with document structure, such as those from the 2009 Work shop on Statistical Machine Translation shared task (Callison-Burch et al, 2009) and the 2009 NIST MT Evaluation Campaign (Przybocki et al,2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).

We use the data collected during three Workshops on Statistical Machine Translation $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
We use the data collected during three Workshops on Statistical Machine Translation $$$$$ The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008).

The results shown in the remainder of this paper are reported in terms of case insensitive BLEU which showed last year a better correlation with human judgments than case sensitive BLEU for the two languages we con sider (Callison-Burch et al, 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
The results shown in the remainder of this paper are reported in terms of case insensitive BLEU which showed last year a better correlation with human judgments than case sensitive BLEU for the two languages we con sider (Callison-Burch et al, 2009). $$$$$ The results of this are reported in Section 4.

To train our models based on Moses we used the freely available corpora $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
To train our models based on Moses we used the freely available corpora $$$$$ As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune parameters.

We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation. $$$$$ We additionally provided training data and a baseline system.
We train a baseline phrase-based French-English system using WMT-09 corpora (Callison-Burchetal., 2009) for training and evaluation. $$$$$ As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune parameters.

The main part of the corpus in this task consists of the Europarl corpus as used in the WMT evaluation (Callison-Burch et al, 2009), with some additional data collected in the scope of the project. $$$$$ Previous evaluations additionally used test sets drawn from the Europarl corpus.
The main part of the corpus in this task consists of the Europarl corpus as used in the WMT evaluation (Callison-Burch et al, 2009), with some additional data collected in the scope of the project. $$$$$ In addition to cleaning the sentence-aligned parallel corpus we also de-duplicated the corpus, removing all sentence pairs that occured more than once in the parallel corpus.

Recently, most evaluations of machine translation systems (Callison-Burch et al, 2009) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method. $$$$$ We also evaluated 7 commercial rule-based MT systems, and Google’s online statistical machine translation system.
Recently, most evaluations of machine translation systems (Callison-Burch et al, 2009) indicate that the performance of corpus-based statistical machine translation (SMT) has come up to the traditional rule-based method. $$$$$ This year’s evaluation also included 7 commercial rule-based MT systems and Google’s online statistical machine translation system.

There have been various evaluation metrics developed and validated for reliability in fields such as MT and summarization (Callison-Burch et al,2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
There have been various evaluation metrics developed and validated for reliability in fields such as MT and summarization (Callison-Burch et al,2009). $$$$$ We experimented with a new type of evaluation this year where we asked judges to edit the output of MT systems.

Rule-based systems could fulfill this role; they are also an attractive choice given their high quality (as judged by human evaluators) in earlier evaluations (e.g. WMT2009 (Callison-Burch et al, 2009)). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
Rule-based systems could fulfill this role; they are also an attractive choice given their high quality (as judged by human evaluators) in earlier evaluations (e.g. WMT2009 (Callison-Burch et al, 2009)). $$$$$ We also evaluated 7 commercial rule-based MT systems, and Google’s online statistical machine translation system.

While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al, 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al, 2009). $$$$$ The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008).

I mainly take advantage of this type of evaluation as part of participating with my research group in MT 13 shared tasks with large evaluation campaigns such as WMT (e.g. Callison-Burch et al (2009)). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
I mainly take advantage of this type of evaluation as part of participating with my research group in MT 13 shared tasks with large evaluation campaigns such as WMT (e.g. Callison-Burch et al (2009)). $$$$$ We experimented with a new type of evaluation this year where we asked judges to edit the output of MT systems.

Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
Spearman's rank correlation coefficients on the document (system) level between all the metric sand the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the framework of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009) and fifth (Callison-Burch et al, 2010) shared translation tasks. $$$$$ The ULC metric had the strongest correlation with human judgments in WMT08 (CallisonBurch et al., 2008).

Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al, 2009). $$$$$ Findings of the 2009 Workshop on Statistical Machine Translation
Human judgement of rank has been chosen as the official determinant of translation quality for the 2009 Workshop on Machine Translation (Callison-Burch et al, 2009). $$$$$ This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
