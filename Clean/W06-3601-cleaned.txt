In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. $$$$$ The model is then extended to the general log-linear framework in order to rescore with other fealike language models.
In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. $$$$$ This can be done by a simple top-down traversal (or depth-first search) from the root of T*

Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.
Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one

Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. $$$$$ In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).
Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. $$$$$ Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one

The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.
The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ With the extended LHS of our transducer, there may be many different rules applicable at one tree node.

The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.
The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). $$$$$ This problem can be solved by memoization (Cormen et al., 2001)

To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. $$$$$ 2 would be While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target)

Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.
Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).

Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.
Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).

Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005).
Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.

The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ With the extended LHS of our transducer, there may be many different rules applicable at one tree node.

It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). $$$$$ We also define

Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.
Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based).

We push the idea behind this method further and make the following contributions in this paper $$$$$ Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order.
We push the idea behind this method further and make the following contributions in this paper $$$$$ We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data.

We test our methods on two large-scale English-to-Chinese translation systems $$$$$ The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps.
We test our methods on two large-scale English-to-Chinese translation systems $$$$$ Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one

Our data preparation follows Huang et al (2006) $$$$$ Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).
Our data preparation follows Huang et al (2006) $$$$$ We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus.

We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. $$$$$ Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side).
We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. $$$$$ Our evaluation data consists of 140 short sentences (< 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set.

For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring.
For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ We finally take the best of all translations.

Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).
Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.

Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). $$$$$ Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004).
Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). $$$$$ In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993).

The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules.
The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004).
