The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ Conditional random fields: Probabilistic models for segmenting labeling sequence data.
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ N. A. Smith and J. Eisner.

A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ N. A. Smith and J. Eisner.
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).

Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ N. A. Smith and J. Eisner.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.

This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ N. A. Smith and J. Eisner.
This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)).

In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ 2003.
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ N. A. Smith and J. Eisner.

 $$$$$ 2003.
 $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.

Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ Instead of considering only the best hypothesis for any θ, we can minimize risk, i.e., the expected loss under pθ across all analyses yi: This “smoothed” objective is now continuous and differentiable.
Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ At each temperature setting of deterministic annealing, we need to minimize the expected loss on the training corpus.

Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ 2006.
Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ N. A. Smith and J. Eisner.

The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ N. A. Smith and J. Eisner.

The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ N. A. Smith and J. Eisner.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ We again use a Taylor series to approximate the expected log brevity penalty.

Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ N. A. Smith and J. Eisner.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ Final results are reported for a larger, disjoint test set.

 $$$$$ 2003.
 $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.

In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ N. A. Smith and J. Eisner.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).

Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ 2006.
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ N. A. Smith and J. Eisner.

This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ 2005.
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ N. A. Smith and J. Eisner.

 $$$$$ 2003.
 $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.

An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ For Bulgarian, both minimum error and annealed minimum risk training achieve significant gains over maximum likelihood, but are indistinguishable from each other.
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ We have seen that annealed minimum risk training provides a useful alternative to maximum likelihood and minimum error training.

We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ 2006.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ N. A. Smith and J. Eisner.

In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ 2006.
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ N. A. Smith and J. Eisner.

Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ 2006.
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ N. A. Smith and J. Eisner.
