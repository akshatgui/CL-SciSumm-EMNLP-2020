Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. $$$$$ The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input.
Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. $$$$$ The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.

In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. $$$$$ Using Mostly Native Data to Correct Errors in Learners&rsquo; Writing
In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. $$$$$ The metaclassifier, in contrast, is trained on a smaller set of error-annotated learner data.

Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. $$$$$ Precision and recall for the overall system are controlled by thresholding the meta-classifier class probability.
Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. $$$$$ Precision and recall for the error-specific classifier is controlled by thresholding class probability.

Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ In addition, we eliminated all annotations for nonpertinent errors (i.e., non-preposition/article errors, or errors that do not involve any of the targeted prepositions), but we maintained the original (erroneous) text for these.
Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ Finally, we eliminated sentences containing nested errors and immediately adjacent errors when they involve pertinent (preposition/article) errors.

Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008).
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ Finally, Yi et al. (2008) and Hermet et al.

Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. $$$$$ The meta-classifier takes the output of the primary models (language model scores and class probabilities) as input.
Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. $$$$$ We have shown that a meta-classifier approach outperforms using a language model or a classifier alone.

Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision.
Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates.

In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. $$$$$ Figure 1 and Figure 2 show the evaluation results of the meta-classifier for prepositions and articles, compared to the performance of the error-specific classifier and language model alone.
In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. $$$$$ Figure 3 and Figure 4 show results obtained by using decreasing amounts of training data.

 $$$$$ Note that the 2.5 million sentences used in the classifier training already produce 16.5 million training vectors.
 $$$$$ We thank Claudia Leacock from the Butler Hill Group for detailed error analysis and the anonymous reviewers for helpful and constructive feedback.

The heuristics are based on those used in Gamon (2010) (personal communication). $$$$$ Knight and Chander (1994) and Gamon et al. (2008) used decision tree classifiers but, in general, maximum entropy classifiers have become the classification algorithm of choice.
The heuristics are based on those used in Gamon (2010) (personal communication). $$$$$ Gamon et al. (2008, 2009) use a combination of error-specific classifiers and a large generic language model with handtuned heuristics for combining their scores to maximize precision.

Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ Using a meta-classifier for ensemble learning has been proven effective for many machine learning problems (see e.g.
Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ This allows us to address the problem of domain mismatch

This is a baseline run that represents the language model approach proposed by Gamon (2010). $$$$$ The dotted line shows the language model baseline.
This is a baseline run that represents the language model approach proposed by Gamon (2010). $$$$$ We can reduce the amount of training data for prepositions to 10% of the original data and still outperform the language model baseline.

Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ This result can most likely be explained by the different complexity of the preposition and article tasks.
Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.

A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data $$$$$ Finally, we eliminated sentences containing nested errors and immediately adjacent errors when they involve pertinent (preposition/article) errors.
A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data $$$$$ (This last step eliminated 31% of the sentences annotated with preposition errors and 29% or the sentences annotated with article errors.)

Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010). $$$$$ Features range from words and morphological information (Knight and Chander, 1994) to the inclusion of part-of-speech tags (Minnen et al., 2000; Han et al., 2004, 2006; Chodorow et al., 2007; Gamon et al., 2008, 2009; Izumi et al., 2003, 2004; Tetrault and Chodorow, 2008) to features based on linguistic analysis and on WordNet (Lee, 2004; DeFelice and Pulman, 2007, 2008).
Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010). $$$$$ Features include contextual features from a window of six tokens to the right and left, such as lexical features (word), part-ofspeech tags, and a handful of “custom features”, for example lexical head of governing VP or governed NP (as determined by part-of-speech-tag based heuristics).
