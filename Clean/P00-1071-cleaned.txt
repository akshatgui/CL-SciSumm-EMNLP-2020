 $$$$$ The Zprise IR engine was built using a cosine vector space model.
 $$$$$ As we look for the future, in order to address questions of higher classes we need to handle real-time knowledge acquisition and classification from different domains, coreference, metonymy, special-purpose reasoning, semantic indexing and other advanced techniques.

Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Secondly, the search for the answer is based on a novel form of indexing, called paragraph indexing (Moldovan and Mihalcea 2000).
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Finally, in order to extract answers and to evaluate their correctness, we use a battery of abductive techniques (Hobbs et al.1993), some based on empirical methods, some on lexicosemantic information.

Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ 2 Overview of the LASSO Q/A System The architecture of LASSO (Moldovan, Harabagiu et. al 1999) comprises three modules

 $$$$$ The Zprise IR engine was built using a cosine vector space model.
 $$$$$ As we look for the future, in order to address questions of higher classes we need to handle real-time knowledge acquisition and classification from different domains, coreference, metonymy, special-purpose reasoning, semantic indexing and other advanced techniques.

Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ Finally, in order to extract answers and to evaluate their correctness, we use a battery of abductive techniques (Hobbs et al.1993), some based on empirical methods, some on lexicosemantic information.
Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ Thus we classify the QA systems, not the questions.

(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ Finally, in order to extract answers and to evaluate their correctness, we use a battery of abductive techniques (Hobbs et al.1993), some based on empirical methods, some on lexicosemantic information.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ Thus, a mixture of natural language processing and information retrieval methods may be the solution for now.

To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)) $$$$$ Each heuristic returns a set of keywords that are added in the same order to the question keywords.
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)) $$$$$ If further keywords are needed in the retrieval loop, keywords provided by the other two heuristics are added.

Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ To facilitate the identification of the document sources, the engine was required to put the document id in front of each line in the document.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ The parameter n selects the number of paragraphs, thus controlling the size of the text retrieved from a document considered relevant.

The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ First, we perform the processing of the question by combining syntactic information, resulting from a shallow parse, with semantic information that characterizes the question (e.g. question type, question focus).
The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ First, we have acquired new tagging rules and secondly, we have unified the dictionaries of the tagger with semantic dictionaries derived from the Gazetteers and from WordNet (Miller 1995).

Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ The Parser The parser combines information from broad coverage lexical dictionaries with semantic information that contributes to the identification of the named entities.
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Similar heuristics recognize named entities successfully in IE systems.

In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Thus we automatically find (a) the question type from the taxonomy of questions built into the system, (b) the expected answer type from the semantic analysis of the question, and most importantly, (c) the question focus defined as the main information required by that question.
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Crucial to the identification of the answer is the recognition of the answer type.
