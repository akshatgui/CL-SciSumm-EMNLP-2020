For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Dual Decomposition for Parsing with Non-Projective Head Automata
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Table 3 shows results for projective and non-projective parsing using the dual decomposition approach.

In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Meshi et al. (2010).

Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.

In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training.
In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ In all of our experiments we set the value K, the maximum number of iterations of dual decomposition in Figures 1 and 2, to be 5,000.

First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ Meshi et al. (2010).

Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Methods that incorporate combinatorial solvers within loopy belief propagation (LBP) (Duchi et al., 2007; Smith and Eisner, 2008) are also closely related to our approach.
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008).

While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ Meshi et al. (2010).

This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ We again recover exact solutions more often than the Martins et al. relaxations.
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ Meshi et al. (2010).

Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ Second, we may have z∗↑(i, j) =� z∗(i, j) for some values of (i, j).
Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ Meshi et al. (2010).

 $$$$$ The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.
 $$$$$ We have not tested other values for Q.

Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ (We use the acronym UAS (unlabeled attachment score) for dependency accuracy.)
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ In contrast, there is little difference in accuracy between projective and nonprojective decoding on English.

Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ Thus, it is these constraints that complicate the optimization.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ 4 was the z = y constraints.

DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ Our training approach is closely related to local training methods (Punyakanok et al., 2005).
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ Meshi et al. (2010).

We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ Meshi et al. (2010).
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ Then define αk = S/(1 + 77k), where 77k is the number of times that L(u(k�))
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$  L(u(k'−1)) for k' < k. Hence the learning rate drops at a rate of 1/(1 + t), where t is the number of times that the dual increases from one iteration to the next.

 $$$$$ The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.
 $$$$$ We have not tested other values for Q.

For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ There are a number of possible areas for future work.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ Meshi et al. (2010).

 $$$$$ The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.
 $$$$$ We have not tested other values for Q.

Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.

There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). $$$$$ Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008).
There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). $$$$$ Meshi et al. (2010).

 $$$$$ The u(k) variables are updated if y(k)(i, j) =� z(k)(i, j) 1This is equivalent to Eq.
 $$$$$ We have not tested other values for Q.
