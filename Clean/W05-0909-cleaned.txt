Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005). $$$$$ We evaluated the METEOR metric and compared its performance with BLEU and NIST on the DARPA/TIDES 2003 Arabic-to-English and Chinese-to-English MT evaluation data released through the LDC as a part of the workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, at the Annual Meeting of the Association of Computational Linguistics (2005)* The Chinese data set consists of 920 sentences, while the Arabic data set consists of 664 sentences* Each sentence has four reference translations* Furthermore, for 7 systems on the Chinese data and 6 on the Arabic data, every sentence translation has been assessed by two separate human judges and assigned an Adequacy and a Fluency Score* Each such score ranges from one to five (with one being the poorest grade and five the highest)* For this paper, we computed a Combined Score for each translation by averaging the adequacy and fluency scores of the two judges for that translation* We also computed an average System Score for each translation system by averaging the Combined Score for all the translations produced by that system* (Note that although we refer to these data sets as the &quot;Chinese&quot; and the &quot;Arabic&quot; In this paper, we are interested in evaluating METEOR as a metric that can evaluate translations on a sentence-by-sentence basis, rather than on a coarse grained system-by-system basis* The standard metrics — BLEU and NIST — were however designed for system level scoring, hence computing sentence level scores using BLEU or the NIST evaluation mechanism is unfair to those algorithms* To provide a point of comparison however, table 1 shows the system level correlation between human judgments and various MT evaluation algorithms and sub components of METEOR over the Chinese portion of the Tides 2003 dataset* Specifically, these correlation figures were obtained as follows

Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. $$$$$ METEOR

In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique. $$$$$ evaluation, such metrics are still of great value and utility* In order to be both effective and useful, an automatic metric for MT evaluation has to satisfy several basic criteria* The primary and most intuitive requirement is that the metric have very high correlation with quantified human notions of MT quality* Furthermore, a good metric should be as sensitive as possible to differences in MT quality between different systems, and between different versions of the same system* The metric should be consistent (same MT system on similar texts should produce similar scores), reliable (MT systems that score similarly can be trusted to perform similarly) and general (applicable to different MT tasks in a wide range of domains and scenarios)* Needless to say, satisfying all of the above criteria is extremely difficult, and all of the metrics that have been proposed so far fall short of adequately addressing most if not all of these requirements* Nevertheless, when appropriately quantified and converted into concrete test measures, such requirements can set an overall standard by which different MT evaluation metrics can be compared and evaluated* In this paper, we describe METEOR', an automatic metric for MT evaluation which we have been developing* METEOR was designed to explicitly address several observed weaknesses in IBM's BLEU metric* It is based on an explicit word-to-word matching between the MT output being evaluated and one or more reference translations* Our current matching supports not only matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i*e* they have an identical stem), and words that are synonyms of each other* We envision ways in which this strict matching can be further expanded in the future, and describe these at the end of the paper* Each possible matching is scored based on a combination of several features* These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference* The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches over all reference translations* The maximal-scoring match
In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.

Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006). $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006). $$$$$ The METEOR metric we described and evaluated in this paper, while already demonstrating great promise, is still relatively simple and naive* We are in the process of enhancing the metric and our experimentation in several directions

Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics $$$$$ METEOR

In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. $$$$$ METEOR

Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall — the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric

We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005). $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005). $$$$$ We acknowledge Kenji Sagae and Shyamsundar Jayaraman for their work on the METEOR system* We also wish to thank John Henderson and William Morgan from MITRE for providing us with the normalized human judgment scores used for this work*

For evaluation we have selected a set of 8 metric variants corresponding to seven different families $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
For evaluation we have selected a set of 8 metric variants corresponding to seven different families $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

BLEU (Papineni et al, 2002), TER (Snover et al, 2006) and METEOR (Banerjee and Lavie, 2005) scores will be reported. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

In this paper ,translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (Banerjee and Lavie, 2005). $$$$$ evaluation, such metrics are still of great value and utility* In order to be both effective and useful, an automatic metric for MT evaluation has to satisfy several basic criteria* The primary and most intuitive requirement is that the metric have very high correlation with quantified human notions of MT quality* Furthermore, a good metric should be as sensitive as possible to differences in MT quality between different systems, and between different versions of the same system* The metric should be consistent (same MT system on similar texts should produce similar scores), reliable (MT systems that score similarly can be trusted to perform similarly) and general (applicable to different MT tasks in a wide range of domains and scenarios)* Needless to say, satisfying all of the above criteria is extremely difficult, and all of the metrics that have been proposed so far fall short of adequately addressing most if not all of these requirements* Nevertheless, when appropriately quantified and converted into concrete test measures, such requirements can set an overall standard by which different MT evaluation metrics can be compared and evaluated* In this paper, we describe METEOR', an automatic metric for MT evaluation which we have been developing* METEOR was designed to explicitly address several observed weaknesses in IBM's BLEU metric* It is based on an explicit word-to-word matching between the MT output being evaluated and one or more reference translations* Our current matching supports not only matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i*e* they have an identical stem), and words that are synonyms of each other* We envision ways in which this strict matching can be further expanded in the future, and describe these at the end of the paper* Each possible matching is scored based on a combination of several features* These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference* The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches over all reference translations* The maximal-scoring match
In this paper ,translation quality is evaluated according to (1) the BLEU metrics which calculates the geometric mean of n gram precision by the system output with respect to reference translations (Papineni et al, 2002), and (2) the METEOR metrics that calculates uni gram overlaps between translations (Banerjee and Lavie, 2005). $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows

The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

e.g. Meteor (Banerjee and Lavie, 2005). $$$$$ METEOR

Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall — the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric

For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows

In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision

Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005). $$$$$ evaluation, such metrics are still of great value and utility* In order to be both effective and useful, an automatic metric for MT evaluation has to satisfy several basic criteria* The primary and most intuitive requirement is that the metric have very high correlation with quantified human notions of MT quality* Furthermore, a good metric should be as sensitive as possible to differences in MT quality between different systems, and between different versions of the same system* The metric should be consistent (same MT system on similar texts should produce similar scores), reliable (MT systems that score similarly can be trusted to perform similarly) and general (applicable to different MT tasks in a wide range of domains and scenarios)* Needless to say, satisfying all of the above criteria is extremely difficult, and all of the metrics that have been proposed so far fall short of adequately addressing most if not all of these requirements* Nevertheless, when appropriately quantified and converted into concrete test measures, such requirements can set an overall standard by which different MT evaluation metrics can be compared and evaluated* In this paper, we describe METEOR', an automatic metric for MT evaluation which we have been developing* METEOR was designed to explicitly address several observed weaknesses in IBM's BLEU metric* It is based on an explicit word-to-word matching between the MT output being evaluated and one or more reference translations* Our current matching supports not only matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i*e* they have an identical stem), and words that are synonyms of each other* We envision ways in which this strict matching can be further expanded in the future, and describe these at the end of the paper* Each possible matching is scored based on a combination of several features* These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference* The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches over all reference translations* The maximal-scoring match
Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005). $$$$$ The METEOR metric we described and evaluated in this paper, while already demonstrating great promise, is still relatively simple and naive* We are in the process of enhancing the metric and our experimentation in several directions
