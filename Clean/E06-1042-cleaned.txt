Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ Dictionarybased systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.

The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ Literal precision is defined as (correct literals in literal cluster / size of literal cluster).
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ If there are no literals, literal recall is 100%; literal precision is 100% if there are no nonliterals in the literal cluster and 0% otherwise.

Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, TroFi makes use of an existing similarity-based word-sense disambiguation algorithm developed by (Karov & Edelman, 1998), henceforth KE.

A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ If SuperTags are used, we also move or remove feature sets whose SuperTag trigram indicates phrasal verbs (verb-particle expressions).

In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ We calculated a second baseline using a simple attraction algorithm.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ In this experiment alone, we get an average f-score of 46.3% for the sum of similarities results – a 9.4% improvement over the high similarity results (36.9%) and a 16.9% improvement over the baseline (29.4%).

In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ The target verbs may not, and typically do not, appear in the feedback sets.
In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.

Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ Literal recall is defined as (correct literals in literal cluster /total correct literals).
Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ Literal precision is defined as (correct literals in literal cluster / size of literal cluster).

Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ It is important to note that in building the example base, we used TroFi with an Active Learning component (see (Birke, 2005)) which improved our average f-score from 53.8% to 64.9% on the original 25 target words.
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ Annotations are from testing or from active learning during example-base construction.

NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ We calculated two baselines for each word.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ We calculated a second baseline using a simple attraction algorithm.

Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ The target verbs may not, and typically do not, appear in the feedback sets.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ The second is that phrasal and expression verbs, for example “throw away”, are often indicative of nonliteral uses of verbs – i.e. they are not the sum of their parts – so they can be used for scrubbing.

 $$$$$ Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, TroFi makes use of an existing similarity-based word-sense disambiguation algorithm developed by (Karov & Edelman, 1998), henceforth KE.
 $$$$$ Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.

Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Since, in addition, the feedback sets are collected automatically, they are very noisy.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ This is helpful in cases where the same set of features can be used as part of both literal and nonliteral expressions.

Birke and Sarkar (2006) also used WSD. $$$$$ The reason is that the DoKMIE are unlikely to list all possible instances of nonliteral language and because knowing that an expression can be used nonliterally does not mean that you can tell when it is being used nonliterally.
Birke and Sarkar (2006) also used WSD. $$$$$ It is important to note that in building the example base, we used TroFi with an Active Learning component (see (Birke, 2005)) which improved our average f-score from 53.8% to 64.9% on the original 25 target words.

Birke and Sarkar (2006) requires WordNet. $$$$$ When we use WordNet as a source of example sentences, or of seed words for pulling sentences out of the WSJ, for building the literal feedback set, we cannot tell if the WordNet synsets, or the collected feature sets, are actually literal.
Birke and Sarkar (2006) requires WordNet. $$$$$ Consequently we take them as primary and use them to scrub the WordNet synsets.

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.

Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ A Clustering Approach For Nearly Unsupervised Recognition Of Nonliteral Language
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.

Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. $$$$$ Nonliteral is then anything that is “not literal”, including most tropes, such as metaphors, idioms, as well phrasal verbs and other anomalous expressions that cannot really be seen as literal.
Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. $$$$$ Note that the KE algorithm concentrates on similarities in the way sentences use the target literal or nonliteral word, not on similarities in the meanings of the sentences themselves.

Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004).
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ The target sets contain from 1 to 115 manually annotated sentences for each verb.

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.

Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ The TroFi algorithm requires a target set (called original set in (Karov & Edelman, 1998)) – the set of sentences containing the verbs to be classified into literal or nonliteral – and the seed sets: the literal feedback set and the nonliteral feedback set.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ If the similarity is above this threshold, we label a target-word sentence as literal or nonliteral.
