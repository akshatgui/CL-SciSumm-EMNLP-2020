These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013). $$$$$ For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.
These vector representations capture interesting linear relationships (up to some accuracy), such as king-man+woman=queen (Mikolov et al, 2013). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).

However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships. $$$$$ A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.
However, not only are these techniques intractable to train with high-order context vectors, they also lack the neural network's ability to semantically generalize (Mikolov et al, 2013) and learn nonlinear relationships. $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).

Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013). $$$$$ In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.
Continuous space models have also been used for generating translations for new words (Mikolov et al 2013a) and ITG reordering (Li et al 2013). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).

While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models. $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
While all the previous data sets are relatively standard in the DSM field to test traditional count models, our last benchmark was introduced in Mikolov et al (2013a) specifically to test predict models. $$$$$ To compare to previous systems, we report the average over all 69 relations in the test set.

Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
Mikolov et al (2013a) reach top accuracy on the syntactic subset (an syn) with a CBOW predict model akin to ours (but trained on a corpus twice as large). $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).

Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model. $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).
Top accuracy on the entire data set (an) and on the semantic subset (ansem) was reached by Mikolov et al (2013c) using a skip-gram predict model. $$$$$ We conducted similar experiments with the semantic test set.

Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words. $$$$$ In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.
Note however that, because of the way the task is framed, performance also depends on the size of the vocabulary to be searched: Mikolov et al (2013a) pick the nearest neighbour among vectors for 1M words, Mikolov et al (2013c) among 700K words, and we among 300K words. $$$$$ The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.

 $$$$$ We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.
 $$$$$ Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.

Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b). $$$$$ Linguistic Regularities in Continuous Space Word Representations
Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al, 2013b). $$$$$ Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.

More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. $$$$$ As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.
More recently, Mikolov et al (2013a) propose two log-linear models, namely the Skip-gram and CBOW model, to efficiently induce word embeddings. $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).

Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. $$$$$ Linguistic Regularities in Continuous Space Word Representations
Mikolov et al (2013b) observe that word embeddings preserve interesting linguistic regularities, capturing a considerable amount of syntactic/semantic relations. $$$$$ We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.

The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
The word vectors we use are from Mikolov et al (2013) 13 (Mikolov13), and additional results are also shown using Turian et al (2010) 14 (Turian10). $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).

Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data. $$$$$ For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.
Mikolov et al (2013b) show that pre-trained embeddings can capture interesting semantic and syntactic information such as king-man+woman=queen on English data. $$$$$ LSA was trained on the same data as the RNN.

These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. $$$$$ In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.
These are precisely the kinds of distinctions between determiners that state-splitting in the Berkeley parser has shown to be useful (Petrov and Klein, 2007), and existing work (Mikolov et al., 2013b) has observed that such regular embedding structure extends to many other parts of speech. $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).

Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software. $$$$$ 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.
Among the state-of-the-art word embedding methods is the skip-gram with negative sampling model (SKIPGRAM), introduced by Mikolov et al (2013b) and implemented in the word2vec software. $$$$$ We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.

Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
Our departure point is the skip-gram neural embedding model introduced in (Mikolov et al, 2013a) trained using the negative-sampling procedure presented in (Mikolov et al, 2013b). $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).

We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
We also tried using the sub sampling option (Mikolov et al, 2013b) with BOW contexts (not shown). $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).

In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
In particular, we found that DEPS perform dramatically worse than BOW contexts on analogy tasks as in (Mikolov et al, 2013c; Levy and Goldberg, 2014). $$$$$ As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.

Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c). $$$$$ For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.
Furthermore, their text-based vectors encode very rich information, such as king-man+woman=queen (Mikolov et al, 2013c). $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).

This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically. $$$$$ These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
This method is similar to the one introduced by Mikolov et al (2013a) for estimating a translation matrix, only solved analytically. $$$$$ This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).
