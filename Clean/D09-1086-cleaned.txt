Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ Finally, Huang et al. (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, targetlanguage parser.

 $$$$$ All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.
 $$$$$ Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t, a, t'

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007).
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ One option would be to leverage unannotated text (McClosky et al., 2006; Smith and Eisner, 2007).
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).

 $$$$$ All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.
 $$$$$ Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t, a, t'

Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ Since the target-language-only baseline converged much more slowly, we used a version of the corpora with sentences 15 target words or fewer.
Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ Spanish treebank conventions for punctuation were also a common source of errors.

Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).

Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.
Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ Our experiments compare learning on target language text to learning on parallel text.

 $$$$$ All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.
 $$$$$ Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t, a, t'

Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ We propose quasigrammar features for these structured learning tasks.

Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ Parser Adaptation and Projection with Quasi-Synchronous Grammar Features
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a).

 $$$$$ All models, for instance, have a feature template that considers the parts of speech of a potential parent-child relation.
 $$$$$ Building on our recent belief propagation work (Smith and Eisner, 2008), we can jointly infer two dependency trees and their alignment, under a joint distribution p(t, a, t'
