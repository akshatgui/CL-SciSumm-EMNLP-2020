A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. $$$$$ We use a DP-based beam search procedure similar to the one presented in (Tillmann and Xia, 2003).
A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. $$$$$ We maximize over all block segmentations with orientation for which the source phrases yield a segmentation of the input sentence.

For details see (Tillmann, 2004). $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).
For details see (Tillmann, 2004). $$$$$ This is the model presented in (Tillmann and Xia, 2003).

 $$$$$ 1, the orientation sequence is , i.e. block and block are generated using left orientation.
 $$$$$ The paper has greatly profited from discussion with Kishore Papineni and Fei Xia.

We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. $$$$$ The neutral orientation is not modeled explicitly in this paper, rather it is handled as a default case as explained below.
We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. $$$$$ This is the model presented in (Tillmann and Xia, 2003).

The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. $$$$$ The decoding orientation restrictions are illustrated in Fig 3: a monotone block sequence with right ( 'We keep all blocks for which and the phrase length is less or equal .

The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). $$$$$ 1: Two unigram count-based models: and .
One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). $$$$$ The and models use the block bigram model in Eq.

Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) . $$$$$ Three systems are evaluated in our experiments: is the baseline block unigram model without re-ordering.
Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) . $$$$$ This is the model presented in (Tillmann and Xia, 2003).

(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. $$$$$ A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks.
(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).

In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a word bonus model and a tuple-bonus model which compensate for the system preference for short translations. $$$$$ The orientation model is shown to improve translation performance over two models: 1) no block re-ordering is used, and 2) the block swapping is controlled only by a language model.
In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a word bonus model and a tuple-bonus model which compensate for the system preference for short translations. $$$$$ Interestingly, the swapping model without orientation performs worse than the baseline model: the word-based trigram language model alone is too weak to control the block swapping: the model is too unrestrictive to handle the block swapping reliably.

In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).
In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). $$$$$ Three systems are evaluated in our experiments: is the baseline block unigram model without re-ordering.

From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). $$$$$ In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks.
From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

The (Tillmann, 2004) paper introduced lexical features for distortion modeling. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The (Tillmann, 2004) paper introduced lexical features for distortion modeling. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). $$$$$ The unigram orientation model is trained from word-aligned training data.
An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). $$$$$ The blocks are counted from word-aligned training data.

Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. $$$$$ In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks.
Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

 $$$$$ 1, the orientation sequence is , i.e. block and block are generated using left orientation.
 $$$$$ The paper has greatly profited from discussion with Kishore Papineni and Fei Xia.
