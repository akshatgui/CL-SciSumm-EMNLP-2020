A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. $$$$$ We use a DP-based beam search procedure similar to the one presented in (Tillmann and Xia, 2003).
A DP-based beam search procedure identical to the one used in (Tillmann,2004) is used to maximize over all oriented block segmentations. $$$$$ We maximize over all block segmentations with orientation for which the source phrases yield a segmentation of the input sentence.

For details see (Tillmann, 2004). $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).
For details see (Tillmann, 2004). $$$$$ This is the model presented in (Tillmann and Xia, 2003).

 $$$$$ 1, the orientation sequence is , i.e. block and block are generated using left orientation.
 $$$$$ The paper has greatly profited from discussion with Kishore Papineni and Fei Xia.

We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. $$$$$ The neutral orientation is not modeled explicitly in this paper, rather it is handled as a default case as explained below.
We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. $$$$$ This is the model presented in (Tillmann and Xia, 2003).

The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bilingual phrase located in position (s-1,v+1) and then treats the orientation of bp as S. $$$$$ The decoding orientation restrictions are illustrated in Fig 3

The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

One novelty this year are the introduction of lexicalized reordering models (Tillmann, 2004). $$$$$ 1

Both Moses and our system are evaluated with and without lexicalized reordering (Tillmann, 2004) . $$$$$ Three systems are evaluated in our experiments

(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. $$$$$ A monotone block sequence is generated except for the possibility to swap a pair of neighbor blocks.
(Tillmann, 2004) learns for each phrase a tendency to either remain monotone or to swap with other phrases. $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).

In addition to the translation model, eleven feature functions are combined $$$$$ The orientation model is shown to improve translation performance over two models

In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). $$$$$ Our baseline model is the unigram monotone model described in (Tillmann and Xia, 2003).
In baseline experiments we used a phrase dependent lexicalized reordering model, as proposed in Tillmann (2004). $$$$$ Three systems are evaluated in our experiments

From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). $$$$$ In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks.
From the merged alignments we also extracted a bidirectional lexical reordering model conditioned on the source and the target phrases (Tillmann, 2004) (Koehn et al, 2007). $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

The (Tillmann, 2004) paper introduced lexical features for distortion modeling. $$$$$ In this paper, we present a phrase-based unigram system similar to the one in (Tillmann and Xia, 2003), which is extended by an unigram orientation model.
The (Tillmann, 2004) paper introduced lexical features for distortion modeling. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). $$$$$ The unigram orientation model is trained from word-aligned training data.
An other obvious system improvement would be to incorporate more advanced word-based features in the DTs, such as questions about word classes (Tillmann and Zhang 2005, Tillmann 2004). $$$$$ The blocks are counted from word-aligned training data.

Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. $$$$$ In recent years, phrase-based systems for statistical machine translation (Och et al., 1999; Koehn et al., 2003; Venugopal et al., 2003) have delivered state-of-the-art performance on standard translation tasks.
Tillmann (2004) used a lexical reordering model, and Galley et al (2004) followed a syntactic-based model. $$$$$ The orientation model is related to the distortion model in (Brown et al., 1993), but we do not compute a block alignment during training.

 $$$$$ 1, the orientation sequence is , i.e. block and block are generated using left orientation.
 $$$$$ The paper has greatly profited from discussion with Kishore Papineni and Fei Xia.
