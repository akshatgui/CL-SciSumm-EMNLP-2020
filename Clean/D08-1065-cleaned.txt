We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ We have presented a procedure for performing Minimum Bayes-Risk Decoding on translation lattices.

We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ We have presented a procedure for performing Minimum Bayes-Risk Decoding on translation lattices.

We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ We now present MBR decoding on translation lattices.
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ We first compare lattice MBR to N-best MBR decoding and MAP decoding (Table 2).

Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions.
Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures.

 $$$$$ There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system.
 $$$$$ We hope that our approach will provide some insight into the design of lattice-based search procedures along with the use of non-linear, global loss functions such as BLEU.

Pass 1 $$$$$ The translation lattices are pruned using Forward-Backward pruning (Sixtus and Ortmanns, 1999) so that the average numbers of arcs per word (lattice density) is 30.
Pass 1 $$$$$ The lattice density is the average number of arcs per word and can be varied using Forward-Backward pruning (Sixtus and Ortmanns, 1999).

Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.

For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ Finally, we show how the performance of lattice MBR changes as a function of the lattice density.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ On aren and enzh, there are some gains beyond a lattice density of 30.

In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ We present lattice MBR experiments in Section 7.
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ We have shown the effect of the MBR scaling factor on the performance of lattice MBR.

Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.
Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ Our Lattice MBR implementation is made possible due to the linear approximation of the BLEU score.

up to 1081 as per Tromble et al (2008). $$$$$ At a lattice density of 30, the lattices in aren contain on an average about 1081 hypotheses!
up to 1081 as per Tromble et al (2008). $$$$$ However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al., 2008).

Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ For each language pair, we use two development sets

The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ In contrast to a phrase-based SMT system, a syntax based SMT system (e.g.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ Lattice and Forest based search and training procedures are not yet common in statistical machine translation.

Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ A different MBR inspired decoding approach is pursued in Zhang and Gildea (2008) for machine translation using Synchronous Context Free Grammars.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ While we have applied lattice MBR decoding to the approximate BLEU score, we note that our procedure (Section 3) is applicable to other gain functions which can be decomposed as a sum of local gain functions.

Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ Such an approach is also followed in Dreyer et al. (2007).

Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ The results show that Lattice MBR performance generally improves when the size of the lattice is increased.

Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ We now present experiments to evaluate MBR decoding on lattices under the linear corpus BLEU gain.

The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hypothesis length.
The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ This score is therefore a linear function in counts of words Δc0 and n-gram matches Δcn.

Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ This is despite the fact that the sentence-level BLEU loss function is an approximation to the exact corpus-level BLEU.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ We compare this to N-best MBR with

Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ We now present MBR decoding on translation lattices.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ Several feature functions are then computed over the phrasepairs.
