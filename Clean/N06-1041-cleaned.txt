Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ Our general approach is to use distributional similarity to link any given word to similar prototypes.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.

We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ We used the dot product between left singular vectors as a measure of distributional similarity.

For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).

Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ This manner of specifying prior knowledge about the task has several advantages.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.

Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ Prototype-Driven Learning For Sequence Models
Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.

We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ 4Details of distributional similarity features

The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in Grenager et al. (2005).

Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988).

(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ It appears as though the prototype information is not spreading to non-prototype words.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ However, rare words with a single prototype feature are almost always given that prototype’s label.

Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ Our general approach is to use distributional similarity to link any given word to similar prototypes.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ We limited each word to have similarity features for its top 5 most similar prototypes.

For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.
For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.

In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ We believe the performance for Chinese POS tagging is not as high as English for two reasons

The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ In particular, we argue for a certain kind of semi-supervised learning, which we call prototype-driven learning.
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.

Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Both of these works require specification of the legal tags for each word.
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Since we have a log-linear formulation, we instead use a gradientbased search.

Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ Given enough refinement set of the CLASSIFIEDS data.

For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.
For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ However, rare words with a single prototype feature are almost always given that prototype’s label.

In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005)

Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).

Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005)

We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ The model is wholly unmodified from the English version except that the suffix features are removed since, in Chinese, suffixes are not a reliable indicator of part-of-speech as in English (Tseng et al., 2005).
