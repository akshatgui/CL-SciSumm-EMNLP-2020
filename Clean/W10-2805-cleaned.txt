Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ A Regression Model of Adjective-Noun Compositionality in Distributional Semantics
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.

Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. $$$$$ This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.
Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. $$$$$ Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.

Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.

The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ Thus, the resulting space consists in a matrix with 40, 000 x 500 dimensions.
The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.

Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.
Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ In particular, we produced our regression analysis with the pls package (Mevik & Wehrens, 2007), which implements PLSR and a number of very useful functions for cross-validation, prediction, error analysis, etc.

Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ A Regression Model of Adjective-Noun Compositionality in Distributional Semantics
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ Modelling compositionality as a machine learning task implies that a great number of different “types” of composition (functions combining vectors) may be learned from natural language samples.

We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ We then extrapolate three different models of compositionality

The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. $$$$$ A Regression Model of Adjective-Noun Compositionality in Distributional Semantics
The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. $$$$$ Since the main use of DSMs is to extract similar vectors from a multidimensional space (representing related documents, distributional synonyms, etc.

In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. $$$$$ A Regression Model of Adjective-Noun Compositionality in Distributional Semantics
In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. $$$$$ We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.

(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. $$$$$ We extrapolate three different models of compositionality

Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. $$$$$ Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).
Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. $$$$$ The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.

This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.
This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ This is work in progress, but the results look very promising.

Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. $$$$$ We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.
Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. $$$$$ A last word on the view of semantic compositionality suggested by our approach.

The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. $$$$$ The model’s parameters were estimated by performing 10-fold cross-validation during the training phase.
The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. $$$$$ The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.

Let us start by setting the syntactic relation that we want to focus on for the purposes of this study $$$$$ On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.
Let us start by setting the syntactic relation that we want to focus on for the purposes of this study $$$$$ In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.

Guevara (2010) and Mitchell and Lapata (2010). $$$$$ Widdows (2008), Mitchell & Lapata (2008), Giesbrecht (2009), Baroni & Lenci (2009), who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.
Guevara (2010) and Mitchell and Lapata (2010). $$$$$ Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).

A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). $$$$$ The additive model has the most varied set of neighbours, but the majority of them are additive-neighbours.
A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). $$$$$ The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.

Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices $$$$$ In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by

Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.

Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors $$$$$ Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.
Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors $$$$$ This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.
