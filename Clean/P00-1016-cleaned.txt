(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ Engelson & Dagan implemented a committee of learners, and used vote entropy to pick examples which had the highest disagreement among the learners.
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ The rule-writing experiments were conducted by a group of 17 advanced computer science students, using the identical test set as in the annotation experiments and the same initial 100 gold standard sentences for both initial bracketing standards guidance and rule-quality feedback throughout their work.

Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation.

Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. $$$$$ Rule Writing Or Annotation

One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. $$$$$ This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker

The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ On the experimental side, active learning has been applied to several different problems.
The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ Table 1 presents a comparison of our rule format against that of Brill & Ngai's.

One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. $$$$$ Rule Writing Or Annotation

In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ To acquaint the subjects with the Treebank conventions, they were first asked to spend some time in a feedback phase, where they would annotate up to 50 sentences (they were allowed to stop at any time) drawn from the initial 100 sentences in T. The sentences were annotated one at a time, and the Treebank annotation was shown to them after every sentence.
In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ The x-axes show the time spent by each human subject (either annotating or writing rules) in minutes; the y-axes show the f-measure performance achieved by the systems built using the given level of supervision.

Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. $$$$$ To divide the corpus into the different subsets in Step 3, we tried using two approaches

In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). $$$$$ The idea is that the more uncertain the example, the less well modeled this situation is, and therefore, the more useful it would be to have this example annotated.
In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). $$$$$ The f-complement disagreement measure was used to select 50 sentences from the rest of Ramshaw & Marcus' training set and the annotator was instructed to annotate them.

AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ The construction of the Penn Treebank significantly improved performance for English systems dealing in the &quot;traditional&quot; NLP domains (eg parsing, part-of-speech tagging, etc).
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ Lewis & Gale (1994), Lewis & Catlett (1994) and Liere & Tadepalli (1997) all applied it to text categorization; Engelson & Dagan (1996) applied it to part-of-speech tagging.

These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ Seung, Opper and Sompolinsky (1992) and Freund et al. (1997) proposed a theoretical queryby-committee approach.

In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. $$$$$ Rule Writing Or Annotation
