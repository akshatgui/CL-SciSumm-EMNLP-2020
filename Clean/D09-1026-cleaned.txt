However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009). $$$$$ This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.
However, one interesting result came from extending the feature space with topics derived from Latent Dirichlet Allocation (LDA) using similar methods to Ramage et al (2009). $$$$$ Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic.

 $$$$$ The derivation of the algorithm so far has focused on its relationship to LDA.
 $$$$$ This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.

 $$$$$ The derivation of the algorithm so far has focused on its relationship to LDA.
 $$$$$ This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.

Labeled LDA (LLDA) (Ramage et al 2009a) can be used to solve this problem. $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.
Labeled LDA (LLDA) (Ramage et al 2009a) can be used to solve this problem. $$$$$ By themselves, most words used here have a higher probability in programming than in design.

Modeling Tweets in a Latent Space $$$$$ This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.
Modeling Tweets in a Latent Space $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.

Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts. $$$$$ Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.
Latent Dirichlet Allocation and its supervised extensions such as Labeled LDA (LLDA) (Ramage et al, 2009) and supervised LDA (sLDA) (Blei and McAuliffe, 2008) are powerful generative models that capture the underlying semantics of texts. $$$$$ Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic.

 $$$$$ The derivation of the algorithm so far has focused on its relationship to LDA.
 $$$$$ This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.

To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al, 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions. $$$$$ One simple approach to these challenges can be found in models that explicitly address the credit attribution problem by associating individual words in a document with their most appropriate labels.
To address these issues we propose a distantly supervised approach which applies LabeledLDA (Ramage et al, 2009) to leverage large amounts of unlabeled data in addition to large dictionaries of entities gathered from Freebase, and combines information about an entity's context across its mentions. $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.

Distant Supervision with Topic Models $$$$$ A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.
Distant Supervision with Topic Models $$$$$ Unlike LDA, L-LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a document’s (observed) label set.

In other models, this input is sometimes used to "fix," i.e. deterministically hold constant topic assignments (Ramage et al, 2009). $$$$$ A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.
In other models, this input is sometimes used to "fix," i.e. deterministically hold constant topic assignments (Ramage et al, 2009). $$$$$ Since the word-topic assignments zi (see step 9 in Table 1) are drawn from this distribution, this restriction ensures that all the topic assignments are limited to the document’s labels.

To enable this, we first take class labeled data (doesn't need to be multi-class labeled data unlike (Ramage et al 2009)) and identify the discriminating features for each class. $$$$$ We will refer to this collection of data as the del.icio.us tag dataset.
To enable this, we first take class labeled data (doesn't need to be multi-class labeled data unlike (Ramage et al 2009)) and identify the discriminating features for each class. $$$$$ It is worth noting that the Yahoo datasets are skewed by construction to contain many documents with highly overlapping content

Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009). $$$$$ A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.
Of these models, the most related one to SeededLDA is the Labeled LDA model (Ramage et al 2009). $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.

Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al, 2009a) or FixedhLDA (Reisinger and Pas? ca, 2009) where the set of topics associated with a document is known a priori. $$$$$ A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.
Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al, 2009a) or FixedhLDA (Reisinger and Pas? ca, 2009) where the set of topics associated with a document is known a priori. $$$$$ Labeled LDA’s topics are named by their associated tag.

 $$$$$ The derivation of the algorithm so far has focused on its relationship to LDA.
 $$$$$ This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.

There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al, 2009), and sequential (Du et al, 2010) topic models. $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.
There have been various extensions to multi-grain (Titov and McDonald, 2008), labeled (Ramage et al, 2009), and sequential (Du et al, 2010) topic models. $$$$$ (2008)).

There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. $$$$$ A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.
There have been various extensions to multi-grain (Titov and McDonald, 2008a), labeled (Ramage et al, 2009), partially-labeled (Ramage et al, 2011), constrained (Andrzejewski et al, 2009) models, etc. $$$$$ Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.

We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics $$$$$ Labeled LDA’s topics are named by their associated tag.
We present two variants of LDA that differ in the way attributes are associated with the induced LDA topics $$$$$ Figure 2 shows the top words associated with 20 topics learned by Labeled LDA and 20 topics learned by unsupervised LDA on the del.icio.us document collection.

Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien et al, 2008). $$$$$ Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.
Recent work investigates ways of accommodating supervision with LDA, e.g. supervised topic models (Blei and McAuliffe, 2007), Labeled LDA (L-LDA) (Ramage et al, 2009) or DiscLDA (Lacoste-Julien et al, 2008). $$$$$ Labeled LDA’s topics are directly named with the tag that corresponds to each topic, an improvement over standard practice of inferring the topic name by inspection (Mei et al., 2007).

 $$$$$ The derivation of the algorithm so far has focused on its relationship to LDA.
 $$$$$ This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.

L-LDA (Ramage et al, 2009) extends standard LDA to include supervision for specific target categories, yet in a different way $$$$$ The generative process for the algorithm is found in Table 1.
L-LDA (Ramage et al, 2009) extends standard LDA to include supervision for specific target categories, yet in a different way $$$$$ Although the equation above looks exactly the same as that of LDA, we have an important distinction in that, the target topic j is restricted to belong to the set of labels, i.e., j ∈ X(d).
