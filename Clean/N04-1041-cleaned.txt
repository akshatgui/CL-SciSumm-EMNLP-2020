We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ The current state of the art discovers many semantic classes but fails to label their concepts.
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system.

Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Ours is a top down approach.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Finally, we use simple syntactic patterns to discover class names from each class' signature.

Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ Automatically Labeling Semantic Classes
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts.

These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ The top-4 highest scoring relationships are: To name a class, we simply search for these syntactic relationships in the signature of a concept.
These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.

The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).
The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ Ours is a top down approach.

Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).
Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.

Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ Finally, we use simple syntactic patterns to discover class names from each class' signature.
Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.

Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work. $$$$$ Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.
Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work. $$$$$ In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).

 $$$$$ That is, they use patterns to independently discover semantic relationships of words.
 $$$$$ This research was partly supported by NSF grant #EIA-0205111.

Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.
Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.

We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ Because of the low coverage of proper nouns in WordNet, only 33 of the 125 concepts we evaluated had WordNet generated labels.
We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ For the 33 concepts that WordNet named, it achieved a score of 75.3% and a lenient score of 82.7%, which is high considering the simple algorithm we used to extract labels using WordNet.

Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. $$$$$ There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.
Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. $$$$$ We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.

First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. $$$$$ The assumption is that the best representative for a concept is a large set of very similar instances.
First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. $$$$$ For each concept that contains at least five instances in the WordNet hierarchy, we named the concept with the most frequent common ancestor of each pair of instances.

Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. $$$$$ Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class.
Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. $$$$$ Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.

In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. $$$$$ Finally, we use simple syntactic patterns to discover class names from each class' signature.
In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. $$$$$ Given a question such as &quot;What color ...&quot;, the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately.

Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Ours is a top down approach.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Finally, we use simple syntactic patterns to discover class names from each class' signature.

We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).
We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ We therefore multiply mief with the following discounting factor: n m where n is the number of words and N = cef Ã— By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.

(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ Automatically Labeling Semantic Classes
(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.

Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. $$$$$ For example, WordNet includes a rare sense of computer that means `the person who computes'.
Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. $$$$$ Also, the words dog, computer and company all have a sense that is a hyponym of person.

We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: Following (Pantel and Lin 2002), we construct a committee for each semantic class.
We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ The two columns of numbers indicate the frequency and mutual information score for each feature respectively.
