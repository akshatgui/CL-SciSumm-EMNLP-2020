For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student's lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). $$$$$ This section highlights examples and features of some commonly used measures of reading level and discusses current research on the topic of reading level assessment using NLP techniques.
For first language (L1) learners (i.e., children learning their native tongue), reading level has been predicted using a variety of techniques, based on models of a student's lexicon, grammatical surface features such as sentence length (Flesch, 1948), or combinations of such features (Schwarm and Ostendorf, 2005). $$$$$ The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.

Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases. $$$$$ In this work, we develop a method of reading level assessment that uses support vector machines (SVMs) to combine features from statistical language models (LMs), parse trees, and other traditional features used in reading level assessment.
Prior work on first language readability by Schwarm and Ostendorf (2005) incorporated grammatical surface features such as parse tree depth and average number of verb phrases. $$$$$ The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.

 $$$$$ Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.
 $$$$$ We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.

 $$$$$ Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.
 $$$$$ We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.

Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. $$$$$ In addition to using the likelihood ratio for classification, we can use scores from language models as features in another classifier (e.g. an SVM).
Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. $$$$$ By combining language model scores with other features in an SVM framework, we achieve our best results.

Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). $$$$$ Reading Level Assessment Using Support Vector Machines And Statistical Language Models
Support vector machines have already been shown to be useful for readability purposes (Schwarm and Ostendorf, 2005). $$$$$ Combining information from statistical LMs with other features using support vector machines provided the best results.

A corpus of Weekly Reader articles was previously used in work by Schwarm and Ostendorf (2005). $$$$$ Our work is currently focused on a corpus obtained from Weekly Reader, an educational newspaper with versions targeted at different grade levels (Weekly Reader, 2004).
A corpus of Weekly Reader articles was previously used in work by Schwarm and Ostendorf (2005). $$$$$ We found that our SVM classifier, trained on the Weekly Reader corpus, classified four of these articles as grade 4 and seven articles as grade 5 (with one overlap with grade 4).

Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences). $$$$$ The widelyused Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al., 1975) (as cited in (Collins-Thompson and Callan, 2004)).
Schwarm and Ostendorf (2005) studied four parse tree features (average parse tree height, average number of SBARs, noun phrases, and verb phrases per sentences). $$$$$ The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.

For comparison, we replicated 6 out-of-vocabulary features described in Schwarm and Ostendorf (2005). $$$$$ Perplexity scores are used as features in the SVM model described in Section 4.3.
For comparison, we replicated 6 out-of-vocabulary features described in Schwarm and Ostendorf (2005). $$$$$ For comparison to other methods, e.g.

We also replicated the 12 perplexity features implemented by Schwarm and Ostendorf (2005) (see Section 3.2). $$$$$ Perplexity scores are used as features in the SVM model described in Section 4.3.
We also replicated the 12 perplexity features implemented by Schwarm and Ostendorf (2005) (see Section 3.2). $$$$$ This resulted in 12 LM perplexity features per article based on trigram, bigram and unigram LMs trained on Britannica (adult), Britannica Elementary, CNN (adult) and CNN abridged text.

Table 8 compares a classifier trained on the four parse features of Schwarm and Ostendorf (2005) to a classifier trained on our expanded set of parse features. $$$$$ The smoothed unigram classifier is also more generalizable, since it can be trained on any collection of data.
Table 8 compares a classifier trained on the four parse features of Schwarm and Ostendorf (2005) to a classifier trained on our expanded set of parse features. $$$$$ The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus.

The most closely related previous study is the work of Schwarm and Ostendorf (2005). $$$$$ Section 2 describes related work on reading level assessment.
The most closely related previous study is the work of Schwarm and Ostendorf (2005). $$$$$ Lee and Myaeng’s (2002) genre and subject detection work and Boulis and Ostendorf’s (2005) work on feature selection for topic classification.

Also, relatedly, (Schwarm and Ostendorf, 2005) use a statistical language model to train SVM classifiers to classify text for grade levels 2-5. $$$$$ Our SVM classifiers for reading level use the following features: The OOV scores are relative to the most common 100, 200 and 500 words in the lowest grade level (grade 2) 2.
Also, relatedly, (Schwarm and Ostendorf, 2005) use a statistical language model to train SVM classifiers to classify text for grade levels 2-5. $$$$$ Thus we made use of the Britannica and CNN articles to train models of three n-gram orders on “child” text and “adult” text.

A measure by Schwarmand Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features. $$$$$ These methods are quick and easy to calculate but have drawbacks: sentence length is not an accurate measure of syntactic complexity, and syllable count does not necessarily indicate the difficulty of a word.
A measure by Schwarmand Ostendorf (2005) incorporates syntactic analyses, among a variety of other types of features. $$$$$ For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article.

 $$$$$ Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.
 $$$$$ We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.

 $$$$$ Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.
 $$$$$ We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.

Schwarm and Ostendorf (2005) implemented four parse tree features (average parse tree height, aver age number of SBARs, NPs per sentence and VPs per sentence) in their work. $$$$$ The widelyused Flesch-Kincaid Grade Level index is based on the average number of syllables per word and the average sentence length in a passage of text (Kincaid et al., 1975) (as cited in (Collins-Thompson and Callan, 2004)).
Schwarm and Ostendorf (2005) implemented four parse tree features (average parse tree height, aver age number of SBARs, NPs per sentence and VPs per sentence) in their work. $$$$$ The Flesch-Kincaid Grade Level index is a commonly used measure of reading level based on the average number of syllables per word and average sentence length.

In order to verify the impact of our choice of features, we also did a replication of the parsed syntactic feature measures reported by (Schwarm and Ostendorf, 2005) on the WeeklyReader corpus and obtained essentially the same accuracy as the one published (50.7% vs. 50.91%), supporting the comparability of the WeeklyReader data used. $$$$$ Information gain measures the difference in entropy when w is and is not included as a feature.
In order to verify the impact of our choice of features, we also did a replication of the parsed syntactic feature measures reported by (Schwarm and Ostendorf, 2005) on the WeeklyReader corpus and obtained essentially the same accuracy as the one published (50.7% vs. 50.91%), supporting the comparability of the WeeklyReader data used. $$$$$ Although our training corpus is small the feature selection described in Section 4.2 allows us to use these higher-order trigram models.

 $$$$$ Further experiments are planned on the generalizability of our classifier to text from other sources (e.g. newspaper articles, web pages); to accomplish this we will add higher level text as negative training data.
 $$$$$ We also plan to test these techniques on languages other than English, and incorporate them with an information retrieval system to create a tool that may be used by teachers to help select reading material for their students.

Syntactic complexity is an obvious factor: indeed (Heilman et al, 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. $$$$$ Many traditional methods of reading level assessment focus on simple approximations of syntactic complexity such as sentence length.
Syntactic complexity is an obvious factor: indeed (Heilman et al, 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. $$$$$ Martin et al. (1997).
