Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ In the subject-verb-object table, the root form of the head of phrases is recorded, and the deep subject and object are used when available.
Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ This demonstration has depended on

Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. $$$$$ We propose the following metric of similarity, based on the mutual information of verbs and arguments.

The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ The second column in each table shows the number of instances that the noun appears in a predicate-argument pair (including verb environments not in the list in the fifth column).
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ But it is unlikely that the class is well-defined.

NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983).
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ The first column lists the noun which is similar to boat.

To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994). $$$$$ Some are not synonyms but are nevertheless closely related

 $$$$$ The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.
 $$$$$ Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.

 $$$$$ The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.
 $$$$$ Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.

Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ Of course, not all nouns fall into such neat clusters

Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.

 $$$$$ The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.
 $$$$$ Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.

The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). $$$$$ This paper reports an investigation of automatic distributional classification of words in English, using a parser developed for extracting grammatical structures from unrestricted text (Hindle 1983).

Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ Nouns may then be grouped according to the extent to which they appear in similar environments.
Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ The first column lists the noun which is similar to boat.

The only difference is that we also work on partial parsing as a task in its own right $$$$$ The work that has been done based on Harris' distributional hypothesis (most notably, the work of the associates of the Linguistic String Project (see for example, Hirschman, Grishrnan, and Sager 1975)) unfortunately does not provide a direct answer, since the corpora used have been small (tens of thousands of words rather than millions) and the analysis has typically involved considerable intervention by the researchers.
The only difference is that we also work on partial parsing as a task in its own right $$$$$ (It may also skew the sample to the extent that the parsing errors are consistent.)

The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). $$$$$ The distributional hypothesis is that nouns are similar to the extent that they share contexts.
The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). $$$$$ Thus, of the ten nouns most similar to boat (Table 4), nine are words for vehicles; the most similar noun is the near-synonym ship.

Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. $$$$$ We propose the following metric of similarity, based on the mutual information of verbs and arguments.
Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. $$$$$ The similarity metric proposed here, based on subject-verb-object relations, represents a considerable reduction in the information available in the subjec-verbobject table.

 $$$$$ The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.
 $$$$$ Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.

In (Hindle, 1990), a small set of sample results are presented. $$$$$ It would be possible to predetermine a set of empty words in advance of analysis, and thus avoid some of the problem presented by empty words.
In (Hindle, 1990), a small set of sample results are presented. $$$$$ The current sample is too small; many words occur too infrequently to be adequately sampled, and it is easy to think of usages that are not represented in the sample.

Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.

For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ We propose the following metric of similarity, based on the mutual information of verbs and arguments.
For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ Each noun has a set of verbs that it occurs with (either as subject or object), and for each such relationship, there is a mutual information value.

To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). $$$$$ Now define the overall similarity of two nouns as the sum across all verbs of the object similarity and the subject similarity, as in (3).
To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). $$$$$ The first column lists the noun which is similar to boat.
