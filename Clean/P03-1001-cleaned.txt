(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations.
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ While this paper examines only this type of relation, the techniques we propose are easily extensible to other question types.

We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ The concept-instance pairs extracted using the above patterns are very noisy.
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Answers to these questions were then automatically generated both by look-up in the 2,000,000 extracted concept-instance pairs and by TextMap, a state of the art web-based Question Answering system which ranked among the top 10 systems in the TREC 11 Question Answering track (Hermjakob et al., 2002).

Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ Finally, Mann (2002) describes a method for extracting instances from text that takes advantage of part of speech patterns involving proper nouns.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ Table 1 shows the regular expression used to extract such patterns along with examples of extracted patterns.

Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ Such patterns (e.g.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ 7 In this way, we envision a system similar to (Lin et al., 2002).

Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ 100 questions of the form “Who is x” were randomly selected from this set.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ Answers to these questions were then automatically generated both by look-up in the 2,000,000 extracted concept-instance pairs and by TextMap, a state of the art web-based Question Answering system which ranked among the top 10 systems in the TREC 11 Question Answering track (Hermjakob et al., 2002).

 $$$$$ Table 2 shows the regular expression used to extract appositions and examples of extracted patterns.
 $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.

We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ The algorithms are compared to a baseline filter that accepts concept-instance pairs if and only if the concept head is a descendent of either the concept “Human” or the concept “Occupation” in Omega.
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ 7 In this way, we envision a system similar to (Lin et al., 2002).

This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ Our approach follows closely from Mann (2002).
This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ 7 In this way, we envision a system similar to (Lin et al., 2002).

The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ We then filter out the noise from these extracted relations using a machine-learned classifier.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ Finally, in order to address the Precision problem, we use machine learning techniques to filter the output of the part of speech patterns, thus purifying the extracted instances.

The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ In order to address the Recall problem, we extend the list of patterns used for extraction to take advantage of appositions.

Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ Such patterns (e.g.
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ The concept-instance pairs extracted using the above patterns are very noisy.

To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns.
To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Such a system would have a large library of extraction patterns for many different types of relations.

This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. $$$$$ Results of this comparison are presented in Figure 3.
This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. $$$$$ Further, many question types require post processing.

In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ Results of this comparison are presented in Figure 3.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ In order to achieve real-time, accurate Question Answering, repositories of data much larger than that described here must be generated.

After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ For the extracted instances, the answer was that concept-instance pair that appeared most frequently in the list of extracted examples.
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ 7 In this way, we envision a system similar to (Lin et al., 2002).

In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Brill et al. (2001) describe using the vast amount of data available on the World Wide Web to achieve impressive performance with relatively simple techniques.
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Table 4 shows the list of features used to describe each concept-instance pair for training the CN/PN filter.

Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ Approximately 15GB of newspaper text was collected from: the TREC 9 corpus (~3.5GB), the TREC 2002 corpus (~3.5GB), Yahoo!
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ The concept-instance pairs extracted using the above patterns are very noisy.

The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Such a system would have a large library of extraction patterns for many different types of relations.
