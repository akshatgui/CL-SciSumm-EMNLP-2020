 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

Our approach for labelling temporal relations (or TLINKs) is based on NLTK's maximum entropy classifier, using the feature sets initially proposed in Mani et al (2006). $$$$$ To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.
Our approach for labelling temporal relations (or TLINKs) is based on NLTK's maximum entropy classifier, using the feature sets initially proposed in Mani et al (2006). $$$$$ We showed that temporal reasoning can be used as an oversampling method to dramatically expand the amount of training data for TLINK labeling, resulting in labeling predictive accuracy as high as 93% using an off-the-shelf Maximum Entropy classifier.

Thus, the features in Mani et al (2006) are augmented with those used to describe signals detailed in Derczynski and Gaizauskas (2010), with some slight changes. $$$$$ For event-time links, we used the above event and signal features along with TIMEX3 time features.
Thus, the features in Mani et al (2006) are augmented with those used to describe signals detailed in Derczynski and Gaizauskas (2010), with some slight changes. $$$$$ (Berglund et al. 2006) use a document-level evaluation approach pioneered by (Setzer and Gaizauskas 2000), which uses a distinct evaluation metric.

The performance of classifier based approaches to temporal link labelling seems to be levelling off - the 60% - 70% relation labelling accuracy of work such as Mani et al (2006) has not been greatly exceeded. $$$$$ To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.
The performance of classifier based approaches to temporal link labelling seems to be levelling off - the 60% - 70% relation labelling accuracy of work such as Mani et al (2006) has not been greatly exceeded. $$$$$ Other work in machine-learning and hand-coded approaches, while interesting, is harder to compare in terms of accuracy since they do not use common task definitions, annotation standards, and evaluation measures.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

While machine learning approaches attempt to improve classification accuracy through feature engineering, Mani et al (2006) introduced a temporal reasoning component to greatly expand the training data. $$$$$ To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data.
While machine learning approaches attempt to improve classification accuracy through feature engineering, Mani et al (2006) introduced a temporal reasoning component to greatly expand the training data. $$$$$ It’s possible that feature engineering could improve performance, but since this is “perfect” data, the result is not encouraging.

Recently, extensions of Mani et al (2006)'s research is briefly described in (Mani et al, 2007). $$$$$ (Mani et al. 2003) obtained 80.2 F-measure training a decision tree on 2069 clauses in anchoring events to reference times that were inferred for each clause.
Recently, extensions of Mani et al (2006)'s research is briefly described in (Mani et al, 2007). $$$$$ Recently, researchers have developed other tools for automatically tagging aspects of TimeML, including EVENT (Sauri et al. 2005) at 0.80 F-measure and TIMEX36 tags at 0.82-0.85 F-measure.

This technical report addresses two problems found in (Mani et al, 2006): (1) feature vector duplication caused by the data normalization process (once fixed, the accuracy drops to 76.56% and 83.23%) and (2) a somewhat unrealistic evaluation scheme (we describe Mani et al (2007)'s results in Section 4.1). $$$$$ (Berglund et al. 2006) use a document-level evaluation approach pioneered by (Setzer and Gaizauskas 2000), which uses a distinct evaluation metric.
This technical report addresses two problems found in (Mani et al, 2006): (1) feature vector duplication caused by the data normalization process (once fixed, the accuracy drops to 76.56% and 83.23%) and (2) a somewhat unrealistic evaluation scheme (we describe Mani et al (2007)'s results in Section 4.1). $$$$$ (Schilder and Habel 2001) report 84% accuracy inferring temporal relations in German data, and (Li et al. 2001) report 93% accuracy on extracting temporal relations in Chinese.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

Although Mani et al (2006) use the links introduced by closure to boost the amount of training data for a tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations. $$$$$ We can see that even after closure, the baseline of learning from unclosed human annotations is much poorer than ME-C, and is in fact substantially worse than the majority class on event ordering.
Although Mani et al (2006) use the links introduced by closure to boost the amount of training data for a tlink classifier, this technique is not suitable for our learning task since the closure might easily propagate errors in the automatic annotations. $$$$$ This means that for preprocessing new data sets to produce noisily annotated data for this classification task, it is far better to use machinelearning from closed human annotations rather than machine-learning from closed annotations produced by an intuitive baseline.

Following (Mani et al, 2006), prior approaches exploit temporal inferences to enrich the set of training in stances used for learning. $$$$$ Machine Learning Of Temporal Relations
Following (Mani et al, 2006), prior approaches exploit temporal inferences to enrich the set of training in stances used for learning. $$$$$ To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

These differences are likely to come from the fact that: (i) (Mani et al, 2006) perform a 6-way classification, and not a 13-way classification, and (ii) (Chambers and Jurafsky, 2008) use a relation set that is even more restrictive than TempEval's. $$$$$ The improvement provided by temporal closure can be explained by three factors: (1) closure effectively creates a new classification problem with many more instances, providing more data to train on; (2) the class distribution is further skewed which results in a higher majority class baseline (3) closure produces additional data in such a way as to increase the frequencies and statistical power of existing features in the unclosed data, as opposed to adding new features.
These differences are likely to come from the fact that: (i) (Mani et al, 2006) perform a 6-way classification, and not a 13-way classification, and (ii) (Chambers and Jurafsky, 2008) use a relation set that is even more restrictive than TempEval's. $$$$$ Future research will investigate methods for tighter integration of temporal reasoning and statistical classification.

As such, it emphasizes robustness at Web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (Pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (Mani et al, 2006). $$$$$ TimeML (Pustejovsky et al. 2005) (www.timeml.org) is an annotation scheme for markup of events, times, and their temporal relations in news articles.
As such, it emphasizes robustness at Web scale, without taking advantage of existing specification languages for representing events and temporal expressions occurring in text (Pustejovsky et al, 2003), and forgoing the potential benefits of more complex methods that extract temporal relations from relatively clean text collections (Mani et al, 2006). $$$$$ Two corpora have been released based on TimeML: the TimeBank (Pustejovsky et al. 2003) (we use version 1.2.a) with 186 documents and 64,077 words of text, and the Opinion Corpus (www.timeml.org), with 73 documents and 38,709 words.

Taking a cue from Mani et al (2006), we also increased Timebank's size by applying transitivity rules to the hand labeled data. $$$$$ SputLink’s transitivity table is represented by 745 axioms.
Taking a cue from Mani et al (2006), we also increased Timebank's size by applying transitivity rules to the hand labeled data. $$$$$ The bottom-line here is that even when heuristic preferences are intuited, those preferences need to be guided by empirical data, whereas hand-coded rules are relatively ignorant of the distributions that are found in data.

 $$$$$ In addition, a program, as a baseline, can trivially link all tagged events and times, getting 100% recall on Task A.
 $$$$$ To further facilitate further research, our tools as well as labeled vectors (unclosed as well as closed) are available for others to experiment with.

Mani et al (2006) introduced a temporal reasoning component that greatly expands the available training data. $$$$$ To address data sparseness, we used temporal reasoning as an oversampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data.
Mani et al (2006) introduced a temporal reasoning component that greatly expands the available training data. $$$$$ To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data.

In order to connect the event graph, we draw on work from (Mani et al, 2006) and apply transitive closure to our documents. $$$$$ The anchor relation is an Event-Time TLINK, and the order relation is an Event-Event TLINK.
In order to connect the event graph, we draw on work from (Mani et al, 2006) and apply transitive closure to our documents. $$$$$ There are only an average of 0.84 TLINKs per event before closure, but after closure it shoots up to 9.49 TLINKs per event.
