They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Although selectional preferences are a possible knowledge source in an automatic word sense disambiguation (WDS) system, they are not a panacea.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ We do not use any first-sense information.

Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Selectional preferences do not only play an important role in human sentence processing (McRae et al, 1998), but are also helpful for NLP tasks like word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Attention has mostly been limited to selectional preferences of verbs, which have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003) and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
In computational linguistics, a multitude of tasks is sensitive to selectional preferences, such as the resolution of ambiguous attachments (Hindle and Rooth, 1993), word sense disambiguation (McCarthy and Carroll, 2003), semantic role labelling (Gildea and Jurafsky, 2002), or testing the applicability of inference rules (Pantel et al, 2007). $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ The first application is text simplification, as outlined by Carroll, Minnen, Pearce et al. (1999).
Note that these are the two extremes of semantic granularity in WordNet, and we plan to experiment with intermediate representation levels in future research (c.f. Li and Abe (1998), McCarthy and Carroll (2003), Xiong et al (2005), Fujita et al (2007)). $$$$$ We evaluated our system using the SENSEVAL-2 test corpus on the English allwords task (Cotton et al., 2001).

Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ Previously, we evaluated noun and verb disambiguation on the English all-words task in the SENSEVAL-2 exercise (Cotton et al. 2001).
Agirre and Martinez (2001) and Zheng et al (2007) adopted the Class-only Model in research, while in McCarthy and Carroll (2003) and Merlo and Stevenson (2001) the Word-Class Model is employed. $$$$$ We evaluated our system using the SENSEVAL-2 test corpus on the English allwords task (Cotton et al., 2001).

McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ The parser uses a wide-coverage unification-based shallow grammar of English POS tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from extra-grammaticality by returning partial parses.
McCarthy and Carroll (2003) reports that the Word-Class Model performs well in unsupervised WSD. $$$$$ We would expect that WSD results using selectional preferences would be better for the latter class of verbs.

One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ Although the preferences perform well in comparison with other unsupervised WSD systems on the same corpus, the results show that for many applications, further knowledge sources would be required to achieve an adequate level of accuracy and coverage.
One known unsupervised learning approach for WSD in SP is McCarthy and Carroll (2003) which addresses the issue via conditional probability. $$$$$ We intend to investigate this issue further with the SENSEVAL-2 lexical sample data, which contains more instances of a smaller number of words.

This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
This experiment is used as baseline as the approach is also used in McCarthy and Carroll (2003) for verb and adjective disambiguation. $$$$$ Li and Abe used TCMs for the task of structural disambiguation.

McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ The parser uses a wide-coverage unification-based shallow grammar of English POS tags and punctuation (Briscoe and Carroll 1995) and performs disambiguation using a context-sensitive probabilistic model (Briscoe and Carroll 1993), recovering from extra-grammaticality by returning partial parses.
McCarthy and Carroll (2003) also uses this type of redundancy for disambiguation in SP. $$$$$ Moreover, for some words the predominant sense varies depending on the domain and text type.

The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.
The approach is also used in (McCarthy and Carroll 2003) to disambiguate verbs and adjectives in collocations. $$$$$ The selectional preferences are specific to verb or adjective classes, rather than individual word forms, so they can be used to disambiguate the co-occurring adjectives and verbs, rather than just the nominal argument heads.

To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ We set the threshold by examining a plot of BNC frequency and the percentage of verbs at particular frequencies that are not listed in WordNet (Figure 2).
To be particular, the method used by McCarthy and Carroll (2003) is formula (6). $$$$$ No particular patterns were evident in this respect, perhaps because of the small size of the test data.

For example $$$$$ Li and Abe used TCMs for the task of structural disambiguation.
For example $$$$$ We do not use any first-sense information.

Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
Examples include information retrieval (Manning et al, 2008), word sense discrimination (Schtze, 1998) and disambiguation (McCarthy and Carroll, 2003), to name but a few. $$$$$ We do not use any first-sense information.

McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.
McCarthy and Carroll (2003) have shown the effectiveness of the selectional preference information for WSD. $$$$$ Selectional preferences have been used by word sense disambiguation (WSD) systems as one source of disambiguating information.

McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ There is a significant limitation to the word tokens that can be disambiguated using selectional preferences, in that they are restricted to those that occur in the specified grammatical relations and in argument head position.
McCarthy and Carroll (2003) also use an unsupervised approach and grammatical relations to learn selectional preferences for word classes. $$$$$ Selectional preferences work well for some word combinations and grammatical relationships, but not well for others.

The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ Thus AC = {ac E WordNet adjective synsets linked by similar-to}.
The most similar approach to the one we describe, that has been tested on Senseval-2, is the one described in (McCarthy and Carroll, 2003). $$$$$ The method disambiguates nouns and verbs to the WordNet synset level and adjectives to a coarse-grained level of WordNet synsets linked by the similar-to relation, as described previously.
