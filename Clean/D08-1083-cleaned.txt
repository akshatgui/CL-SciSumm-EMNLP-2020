 $$$$$ For brevity, we refer to NEG(1) and NEG(N) collectively as NEG, and NEGEX(1) and NEGEX(N) collectively as NEGEX.
 $$$$$ We also thank Eric Breck, Lillian Lee, Mats Rooth, the members of the Cornell NLP reading seminar, and the EMNLP reviewers for insightful comments on the submitted version of the paper.

Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008). $$$$$ For SC-NEGEX, we count the number of content-word negators as well as function-word negators to determine whether the final polarity should be flipped.
Content-word negators are words that are not function words, but act semantically as negators (Choi and Cardie, 2008). $$$$$ On the other hand, the NEGEX methods (87.7%) that do consider content-word negators as well as function-word negators perform better than VOTE.

 $$$$$ For brevity, we refer to NEG(1) and NEG(N) collectively as NEG, and NEGEX(1) and NEGEX(N) collectively as NEGEX.
 $$$$$ We also thank Eric Breck, Lillian Lee, Mats Rooth, the members of the Cornell NLP reading seminar, and the EMNLP reviewers for insightful comments on the submitted version of the paper.

For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). $$$$$ We also add a number of boolean features that provide following properties of xi using the polarity lexicon and the negator lexicon: – whether xi is a function-word negator – whether xi is a content-word negator – whether xi is a negator of any kind – the polarity of xi according to Wilson et al. (2005)’s polarity lexicon – the polarity of xi according to the lexicon derived from the General Inquirer dictionary – conjunction of the above two features As in the heuristic-based compositional semantics approach (§ 2.2), we experiment with two variations of this learning-based approach: CCI-COMPOPR and CCI-COMPOMC, whose compositional inference rules are COMPOPR and COMPOMC respectively.
For the general-purpose polarity lexicon, we expand the polarity lexicon of Wilson et al (2005) with General Inquirer dictionary as suggested by Choi and Cardie (2008). $$$$$ However, Wilson et al. (2005) formulated the task differently by limiting their evaluation to individual words that appear in their polarity lexicon.

According to Choi and Cardie (2008), voting algorithms that recognize content-word negators achieve a competitive performance, so we will use a variant of it for simplicity. $$$$$ For the purpose of voting, if a word is defined as a negator per the voting scheme, then that word does not participate in the majority vote.
According to Choi and Cardie (2008), voting algorithms that recognize content-word negators achieve a competitive performance, so we will use a variant of it for simplicity. $$$$$ On the other hand, the NEGEX methods (87.7%) that do consider content-word negators as well as function-word negators perform better than VOTE.

Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2. $$$$$ In particular, we exploit the fact that in our task, we can automatically construct a reasonably accurate gold standard for z, denoted as z*: as shown in Figure 2, we simply rely on the negator and polarity lexicons.
Because none of the algorithms proposed by Choi and Cardie (2008) is designed to handle the neutral polarity, we invent our own version as shown in Figure 2. $$$$$ The exact procedure is given in Figure 1, and will be discussed again shortly.

Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. $$$$$ That is, we count the number of positive polarity words and negative polarity words in a given expression, and assign the majority polarity to the expression.
Choi and Cardie (2008) also focus on the expression-level polarity classification, but their evaluation setting is not as practical as ours in that they assume the inputs are guaranteed to be either strongly positive or negative. $$$$$ We evaluate on all strong (i.e., intensity of expression is ‘medium’ or higher), sentimentbearing (i.e., polarity is ‘positive’ or ‘negative’) expressions.8 As a result, we can assume the boundaries of the expressions are given.

Choi and Cardie (2008) proposed a learning-based framework. $$$$$ In this paper, we begin to close the gap between learning-based approaches to expression-level polarity classification and those founded on compositional semantics: we present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.
Choi and Cardie (2008) proposed a learning-based framework. $$$$$ The learningbased approach proposed in this paper takes a first step in this direction.

Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. $$$$$ In this paper, we begin to close the gap between learning-based approaches to expression-level polarity classification and those founded on compositional semantics: we present a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.
Choi and Cardie (2008) present a more lightweight approach using compositional semantics towards classifying the polarity of expressions. $$$$$ To this end, we present a novel learning-based approach that incorporates inference rules inspired by compositional semantics into the learning procedure (§3.2).

The rules presented by Choi and Cardie (2008) are, however, much more specific, as they define syntactic contexts of the polar expressions. $$$$$ We evaluate on all strong (i.e., intensity of expression is ‘medium’ or higher), sentimentbearing (i.e., polarity is ‘positive’ or ‘negative’) expressions.8 As a result, we can assume the boundaries of the expressions are given.
The rules presented by Choi and Cardie (2008) are, however, much more specific, as they define syntactic contexts of the polar expressions. $$$$$ We presented a novel learning-based approach that incorporates structural inference motivated by compositional semantics into the learning procedure.

Unlike Choi and Cardie (2008), these rules require a proper parse and reflect grammatical relationships between different constituents. $$$$$ Unlike function-word negators, such as “not” or “never”, content-word negators have been recognized and utilized less actively in previous work.
Unlike Choi and Cardie (2008), these rules require a proper parse and reflect grammatical relationships between different constituents. $$$$$ However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis.

In this work we focus on explicit negation mentions, also called functional negation by Choi and Cardie (2008). $$$$$ Next we present experimental results in §4, followed by related work in §5.
In this work we focus on explicit negation mentions, also called functional negation by Choi and Cardie (2008). $$$$$ Note that if the second argument is a negator, we do not flip the polarity of the first argument, because the first argument in general is not in the semantic scope of the negation.4 Instead, we treat the second argument as a constituent with negative polarity.

Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. $$$$$ Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis
Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. $$$$$ However, their notion of polarity is quite different from that assumed here and in the literature on sentiment analysis.

Here, the verbs prevent and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. $$$$$ For SC-NEGEX, we count the number of content-word negators as well as function-word negators to determine whether the final polarity should be flipped.
Here, the verbs prevent and ease act as content-word negators (Choi and Cardie, 2008) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive. $$$$$ On the other hand, the NEGEX methods (87.7%) that do consider content-word negators as well as function-word negators perform better than VOTE.

Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. $$$$$ Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis
Choi and Cardie (2008), for example, propose an algorithm for phrase-based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori (i.e., out of context) polarity of the words in the phrase and the (correct) phrase-level polarity. $$$$$ The experiments below evaluate our heuristic- and learning-based methods for subsentential sentiment analysis (§ 4.1).

Choi and Cardie (2008) hand-code compositional rules in order to model compositional effects of combining different words in the phrase. $$$$$ Whereas the heuristics above use voting-based inference, those below employ a set of hand-written rules motivated by compositional semantics.
Choi and Cardie (2008) hand-code compositional rules in order to model compositional effects of combining different words in the phrase. $$$$$ Once we determine the intermediate decision variables, we apply the heuristic rules motivated by compositional semantics (from Table 2) in order to obtain the final polarity y of x.

We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). $$$$$ Determining the polarity of sentiment-bearing expressions at or below the sentence level requires more than a simple bag-of-words approach.
We extract all sentences containing strong (i.e. intensity is medium or higher), sentiment-bearing (i.e. polarity is positive or negative) expressions following Choi and Cardie (2008). $$$$$ We evaluate on all strong (i.e., intensity of expression is ‘medium’ or higher), sentimentbearing (i.e., polarity is ‘positive’ or ‘negative’) expressions.8 As a result, we can assume the boundaries of the expressions are given.

An English polarity reversing word dictionary was constructed from the General Inquirer dictionary in the same way as Choi and Cardie (2008), by collecting words which belong to either NOTLW or DECREAS categories (The dictionary contains 121 polarity reversing words). $$$$$ For the (function- and content-word) negator lexicon, we collect a handful of seed words as well as General Inquirer words that appear in either NOTLW or DECREAS category.
An English polarity reversing word dictionary was constructed from the General Inquirer dictionary in the same way as Choi and Cardie (2008), by collecting words which belong to either NOTLW or DECREAS categories (The dictionary contains 121 polarity reversing words). $$$$$ For each xi in x, we encode the following features: with unseen words in the test data, we add features that describe word categories based on the General Inquirer dictionary.

Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate. $$$$$ For SC-NEGEX, we count the number of content-word negators as well as function-word negators to determine whether the final polarity should be flipped.
Choi and Cardie (2008) categorized polarity reversing words into two categories: function-word negators such as not and content-word negators such as eliminate. $$$$$ On the other hand, the NEGEX methods (87.7%) that do consider content-word negators as well as function-word negators perform better than VOTE.

Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. $$$$$ Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis
Choi and Cardie (2008) proposed a method to classify the sentiment polarity of a sentence basing on compositional semantics. $$$$$ Our experiments show that (1) simple heuristics based on compositional semantics can perform better than learning-based methods that do not incorporate compositional semantics (accuracy of 89.7% vs. 89.1%), but (2) a method that integrates compositional semantics into learning performs better than all other alternatives (90.7%).
