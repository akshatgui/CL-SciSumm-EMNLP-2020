These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model.
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ We use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.

(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ We use a greedy search algorithm to search the alignment with highest probability in the space of all possible alignments.
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ The greedy search algorithm for general loglinear models is formally described as follows: Input: e, f, eT, fT, and D Output: a The above search algorithm, however, is not efficient for our log-linear models.

We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ Log-Linear Models For Word Alignment
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003).

(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ The relationship between the translation model and the alignment model is given by: Although IBM models are considered more coherent than heuristic models, they have two drawbacks.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ We have presented a framework for word alignment based on log-linear models between parallel texts.

The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ We present in this section results of experiments on a parallel corpus of Chinese-English texts.
The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003).

Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ Log-Linear Models For Word Alignment
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ For log-linear models, POS information and an additional dictionary are used, which is not the case for GIZA++/IBM models.

 $$$$$ Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.
 $$$$$ 2004AA114010).

Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ Log-Linear Models For Word Alignment
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ The use of POS information for improving statistical alignment quality of the HMM-based model is described 1If there is a target word which is assigned to more than one source words, h(a, e, f) = 0. in (Toutanova et al., 2002).

Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ Log-Linear Models For Word Alignment
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.

To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ Statistical approaches, which depend on a set of unknown parameters that are learned from training data, try to describe the relationship between a bilingual sentence pair (Brown et al., 1993; Vogel and Ney, 1996).
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ However, the search algorithm we proposed is supervised, relying on a hand-aligned bilingual corpus, while the baseline approach of IBM alignments is unsupervised.

 $$$$$ Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.
 $$$$$ 2004AA114010).

Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Och and Ney (2003) proposed Model 6, a log-linear combination of IBM translation models and HMM model.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Table 2 compares the results of our log-linear models with IBM Model 3.

A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ Log-Linear Models For Word Alignment
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ An alignment a is defined as a subset of the Cartesian product of the word positions: We define the alignment problem as finding the alignment a that maximizes Pr(a

For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ 4 requires a sum over a large number of possible alignments.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ The set of considered alignments are also called n-best list of alignments.

This is a key difference between our model and (Liu et al, 2005). $$$$$ An especially well-founded framework is maximum entropy (Berger et al., 1996).
This is a key difference between our model and (Liu et al, 2005). $$$$$ Brown et al. (1993) proposed a series of statistical models of the translation process.
