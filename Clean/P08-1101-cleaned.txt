 $$$$$ All features are treated equally and processed together according to the linear model, regardless of whether they are from the baseline segmentor or tagger.
 $$$$$ This work is supported by the ORS and Clarendon Fund.

Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ The Chinese Treebank (CTB) 4 is used for the experiments.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ However, the comparison is indirect because our partitions of the CTB corpus are different.

Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ Shi and Wang (2007) introduced POS information to segmentation by reranking.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ The overall tagging accuracy of our joint model was comparable to but less than the joint model of Shi and Wang (2007).

We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ Joint Word Segmentation and POS Tagging Using a Single Perceptron
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data.

Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ The Chinese Treebank (CTB) 4 is used for the experiments.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ Following Ng and Low (2004), we partition the sentences in CTB 3, ordered by sentence ID, into 10 groups evenly.

Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach.

We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Instead, a novel multiple beam search algorithm is used to do decoding efficiently.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy.

Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ However, because word segmentation and POS tagging are performed simultaneously, POS information participates in word segmentation.
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N-best reranking approach, which limits the influence of POS tagging on segmentation to the N-best list.

 $$$$$ All features are treated equally and processed together according to the linear model, regardless of whether they are from the baseline segmentor or tagger.
 $$$$$ This work is supported by the ORS and Clarendon Fund.

Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ Joint Word Segmentation and POS Tagging Using a Single Perceptron
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).

Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ Joint Word Segmentation and POS Tagging Using a Single Perceptron
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).

Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding.

 $$$$$ All features are treated equally and processed together according to the linear model, regardless of whether they are from the baseline segmentor or tagger.
 $$$$$ This work is supported by the ORS and Clarendon Fund.

We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding.

In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ 10-fold cross validation is performed to test the accuracy of the joint word segmentor and POS tagger, and to make comparisons with existing models in the literature.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ Shi and Wang (2007) also chunked the sentences before doing 10-fold cross validation, but used an uneven split.

In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ Here the category of a character is the set of tags seen on the character during training.

Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ They have been shown to improve the accuracy of a Chinese POS tagger (Tseng et al., 2005).

We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).
We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy.

For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ In particular, Ng and Low (2004) used character-based POS tagging, which prevents some important POS tagging features such as word + POS tag; Shi and Wang (2007) used an N-best reranking approach, which limits the influence of POS tagging on segmentation to the N-best list.

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ The error analysis for the development test is shown in Table 3.
