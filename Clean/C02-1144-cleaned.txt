Different measures have been proposed, which are not easy to evaluate (see (Lin and Pantel, 2002) for proposals). $$$$$ One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001).
Different measures have been proposed, which are not easy to evaluate (see (Lin and Pantel, 2002) for proposals). $$$$$ Many cluster evaluation schemes have been proposed.

NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ Concept Discovery From Text
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001).

The labeled classes are acquired in three stages $$$$$ We start with a list of m empty sets, each of which is labeled with a class in the answer key.
The labeled classes are acquired in three stages $$$$$ If an element is in a set whose class is not.

 $$$$$ Phase II

For CBC we simply used the same parameter values as reported in (Lin and Pantel, 2002). $$$$$ If the condition is violated, the committee is simply discarded.
For CBC we simply used the same parameter values as reported in (Lin and Pantel, 2002). $$$$$ We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3.

(Schutze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. $$$$$ One way to deal with these problems is to use a clustering algorithm to automatically induce semantic classes (Lin and Pantel 2001).
(Schutze, 1998) and (Lin and Pantel, 2002a, b) show that clustering methods are helpful in this area. $$$$$ Following (Lin 1998), we represent each word by a feature vector.

Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). $$$$$ We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990).
Options for identifying interesting classes include manually created methods (WordNet (Miller et al, 1990)), textual patterns (Hearst, 1992), automated clustering (Lin and Pantel, 2002), and combinations (Snow et al, 2006). $$$$$ For each cluster c, we also include wn(c).

Mutual information (MI) is an information theoric measure and has been used in many NLP tasks, including clustering words (e.g. Lin and Pantel, 2002). $$$$$ A well known problem with mutual information is that it is biased towards infrequent words/features.
Mutual information (MI) is an information theoric measure and has been used in many NLP tasks, including clustering words (e.g. Lin and Pantel, 2002). $$$$$ To compute the top similar words of a word w, we first sort w?s features according to their mutual information with w. We only compute pairwise similarities between w and the words that share a high mutual information feature with w. 4.2.

 $$$$$ Phase II

 $$$$$ Phase II

To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). $$$$$ Concept Discovery From Text
To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). $$$$$ Following (Lin 1998), we represent each word by a feature vector.

To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. $$$$$ We generated clusters from a news corpus using CBC and compared them with classes extracted from WordNet (Miller 1990).
To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. $$$$$ The sizes of the WordNet classes vary a lot.

Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002). $$$$$ We present a clustering algorithm called CBC (Cluster ing By Committee) that automatically discovers concepts from text.
Clustering by committee has also been used to discover concepts from a text by grouping terms into conceptually related clusters (Lin and Pantel, 2002). $$$$$ We presented a clustering algorithm, CBC, for automatically discovering concepts from text.
