Riezler et al (2002) report on our WSJ parsing experiments. $$$$$ We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank.
Riezler et al (2002) report on our WSJ parsing experiments. $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).

The limitation of deterministic transfer rules has been recognized in prior work (Riezler et al, 2002). $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).
The limitation of deterministic transfer rules has been recognized in prior work (Riezler et al, 2002). $$$$$ The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized.

 $$$$$ In this version of the corpus, all WSJ labels with -SBJ are retained and are restricted to phrases corresponding to SUBJ in the LFG grammar; in addition, it contains NP under VP (OBJ and OBJth in the LFG grammar), all -LGS tags (OBL-AG), all -PRD tags (XCOMP), VP under VP (XCOMP), SBAR- (COMP), and verb POS tags under VP (V in the c-structure).
 $$$$$ Under this measure, our system achieves 76.1% F-score.

Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al, 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Noun coordination, NN-compound, Adj-Mod. $$$$$ An evaluation on a gold standard of dependency relations for
Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al, 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Noun coordination, NN-compound, Adj-Mod. $$$$$ The main verb lexicon contains 9,652 verb stems and 23,525 subcategorization frame-verb stem entries; there are also lexicons for adjectives and nouns with subcategorization frames and for closed class items.

They are still reasonably popular today, as exemplified by major systems like PARC's XLE (Riezler et al, 2002). $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).
They are still reasonably popular today, as exemplified by major systems like PARC's XLE (Riezler et al, 2002). $$$$$ Facts like this can make a correct mapping of LFG f-structures to DR relations problematic.

See e.g. Riezler et al (2002) and Zhang et al (2007) for chart based parsers which can produce fragmentary analyses. $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).
See e.g. Riezler et al (2002) and Zhang et al (2007) for chart based parsers which can produce fragmentary analyses. $$$$$ To our knowledge, so far the only direct point of comparison is the parser of Carroll et al. (1999) which is also evaluated on Carroll et al.â€™s test corpus.

In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al (2002)). $$$$$ Section 2 describes the Lexical-Functional Grammar, the constraint-based parser, and the robustness techniques employed in this work.
In this project, a broad-coverage LFG grammar and parser for English was employed (see Riezler et al (2002)). $$$$$ The grammar used for this project was developed in the ParGram project (Butt et al., 1999).

Alternatively, a single input parse could be selected by stochastic models such as the one described in Riezler et al (2002). $$$$$ The evaluation measure counts the number of predicateargument relations in the f-structure of the parse selected by the stochastic model that match those in the gold standard annotation.
Alternatively, a single input parse could be selected by stochastic models such as the one described in Riezler et al (2002). $$$$$ 4 In our evaluation, we report F-scores for both types of annotation, LFG and DR, and for three types of parse selection, (i) lower bound: random choice of a parse from the set of analyses (averaged over 10 runs), (ii) upper bound: selection of the parse with the best F-score according to the annotation scheme used, and (iii) stochastic: the parse selected by the stochastic disambiguator.

Riezler et al (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. $$$$$ The WSJ labeled brackets are given LFG lexical entries which constrain both the c-structure and the f-structure of the parse.
Riezler et al (2002) describe a discriminative LFG parsing model that is trained on standard (syntax only) tree bank annotations by treating each tree as a full LFG analysis with an observed c-structure and hidden f-structure. $$$$$ A sample FRAGMENT c-structure and f-structure are shown in Fig.

For sentences out of coverage, it employs the robustness techniques (fragment parsing, 'skimming') implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. $$$$$ The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis, is tackled in our approach by an extension of a fullfledged Lexical-Functional Grammar (LFG) and a constraint-based parser with partial parsing techniques.
For sentences out of coverage, it employs the robustness techniques (fragment parsing, 'skimming') implemented in XLE and described in Riezler et al. (2002), so that 100% of our corpus sentences receive at least some sort of analysis. $$$$$ With this combination of full and partial parsing techniques we achieve 100% grammar coverage on unseen data.

Our error reduction of 51.0% also compares favorably to the 36% error reduction on English LFG parses reported in Riezler et al (2002). $$$$$ The error reduction row lists the reduction in error rate relative to the upper and lower bounds obtained by the stochastic disambiguation model.
Our error reduction of 51.0% also compares favorably to the 36% error reduction on English LFG parses reported in Riezler et al (2002). $$$$$ Both the DR and LFG annotations broadly agree in their measure of error reduction.

For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). $$$$$ We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank.
For our experiments, we used a stochastic parsing system for LFG that we trained on section 02-21 of the UPenn Wall Street Journal treebank (Marcus et al., 1993) by discriminative estimation of a conditional maximum-entropy model from partially labeled data (see Riezler et al. (2002)). $$$$$ The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994).

We follow Collins' (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). $$$$$ Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented in Riezler et al. (2000) are included in the model.
We follow Collins' (2000) approach to discriminative reranking (see also (Riezler et al., 2002)). $$$$$ Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2001).

This was done to some extent in Riezler et al (2002) to automatically generate training data for the log-linear disambiguation component of XLE. $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).
This was done to some extent in Riezler et al (2002) to automatically generate training data for the log-linear disambiguation component of XLE. $$$$$ The basic training data for our experiments are sections 02-21 of the WSJ treebank.

XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al, 2002) that works on the packed representations. $$$$$ We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear probability model (Riezler et al, 2002) that works on the packed representations. $$$$$ The XLE parser (Maxwell and Kaplan, 1993) was used to produce packed representations, specifying all possible grammar analyses of the input.

XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear (a.k.a. maximum-entropy) probability model (Riezler et al, 2002). $$$$$ We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
XLE selects the most probable analysis from the potentially large candidate set by means of a stochastic disambiguation component based on a log-linear (a.k.a. maximum-entropy) probability model (Riezler et al, 2002). $$$$$ In discriminative estimation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized.

For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al (2002). $$$$$ We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model.
For a more detailed description of the optimization problem and the feature-functions we use for stochastic LFG parsing see Riezler et al (2002). $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).

We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al, 2002), as well as an architecture for distributed data mining within this corpus, called Oceanography (Waterman, 2009). $$$$$ Any token that cannot be parsed as one of these chunks is parsed as a TOKEN chunk.
We have access to the entire English-language text of Wikipedia (about 2M pages) that was parsed using the XLE parser (Riezler et al, 2002), as well as an architecture for distributed data mining within this corpus, called Oceanography (Waterman, 2009). $$$$$ To our knowledge, so far the only direct point of comparison is the parser of Carroll et al. (1999) which is also evaluated on Carroll et al.â€™s test corpus.

The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al, 2005. $$$$$ The grammar used for this project was developed in the ParGram project (Butt et al., 1999).
The primary linguistic analysis components are the probabilistic LFG grammar for English developed at PARC (Riezler et al., 2002), and a combination of systems for frame semantic annotation: the probabilistic Shalmaneser system for frame and role annotation (Erk and Pado, 2006), and the rule-based Detour system for frame assignment (Burchardt et al, 2005. $$$$$ The DR annotation for our example sentence, obtained via a mapping from f-structures to Carroll et alâ€™s annotation scheme, is shown in Fig.

Following Pereira and Schabes' (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). $$$$$ Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).
Following Pereira and Schabes' (1992) success with partial annotations in training a model of (English) constituents generatively, their idea has been extended to discriminative estimation (Riezler et al., 2002) and also proved useful in modeling (Japanese) dependencies (Sassano, 2005). $$$$$ Furthermore, let p[f] denote the expectation of function f under distribution p. Then P(Î») can be defined for a conditional exponential model pÎ»(zIy) as: Intuitively, the goal of estimation is to find model pa'An earlier approach using partially labeled data for estimating stochastics parsers is Pereira and Schabesâ€™s (1992) work on training PCFG from partially bracketed data.
