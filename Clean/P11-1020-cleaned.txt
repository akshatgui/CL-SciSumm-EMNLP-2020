We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ Examples of this kind of data include the Multiple-Translation Chinese (MTC) Corpus 2 which consists of Chinese news stories translated into English by 11 translation agencies, and literary works with multiple translations into English (e.g.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ Asking annotators to write multiple descriptions or longer descriptions would result in more varied data but at the cost of more noise in the alignments.

The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Many of them annotated all the available videos we had.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Examples of some of the descriptions collected are shown in Figure 2.

Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ In contrast, there are no “professional paraphrasers”, with the result that there are no readily available large corpora and no consistent standards for what constitutes a high-quality paraphrase.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ The qualification process was done manually by the authors.

Figure 2 $$$$$ A screenshot of our annotation task is shown in Figure 1.
Figure 2 $$$$$ Watch and describe a short segment of a video You will be shown a segment of a video clip and asked to describe the main action/event in that segment in ONE SENTENCE.

3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ For each task, we asked the annotators to watch a very short video clip (usually less than 10 seconds long) and describe in one sentence the main action or event that occurred in the video clip We deployed the task on Amazon’s Mechanical Turk, with video segments selected from YouTube.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ Even limiting the set to descriptions produced from the Tier-2 tasks, there are still 16 descriptions on average for each video, with at least 12 descriptions for over 95% of the videos.

The STS task data includes five subtasks with text pairs from different sources $$$$$ In addition to describing a mechanism for collecting large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Microsoft Research Video Description Corpus 1.
The STS task data includes five subtasks with text pairs from different sources $$$$$ Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).

MSR video data (Chen and Dolan, 2011). $$$$$ Another application for our data is to apply it to computer vision tasks such as video retrieval.
MSR video data (Chen and Dolan, 2011). $$$$$ We introduced a data collection framework that produces highly parallel data by asking different annotators to describe the same video segments.

To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ To ensure the quality of the annotations being produced, we used a two-tiered payment system.
To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ We again used a tiered payment system to reward and retain workers who performed well.

However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ However, a lack of standard datasets and automatic evaluation metrics has impeded progress in the field.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase.

Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ Without these resources, researchers have resorted to developing their own small, ad hoc datasets (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Quirk et al., 2004; Dolan et al., 2004), and have often relied on human judgments to evaluate their results (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Bannard and Callison-Burch, 2005).
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).

Consider the following excerpts from a video description corpus (Chen and Dolan, 2011) $$$$$ In addition to describing a mechanism for collecting large-scale sentence-level paraphrases, we are also making available to the research community 85K parallel English sentences as part of the Microsoft Research Video Description Corpus 1.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011) $$$$$ Overall, 688 workers submitted at least one English description.

Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ On average, 41 descriptions were produced for each video, with at least 27 for over 95% of the videos.
Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ Even limiting the set to descriptions produced from the Tier-2 tasks, there are still 16 descriptions on average for each video, with at least 12 descriptions for over 95% of the videos.
