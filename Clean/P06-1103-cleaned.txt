Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ We use the perceptron (Rosenblatt, 1958) algorithm to train the model.
Klementiev and Roth (2006) explore the use of a perceptron-based ranking model for the purpose of finding name transliterations across comparable corpora. $$$$$ The algorithm can be naturally extended to comparable corpora of more than two languages.

 $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and EnglishKorean generative transliteration models, respectively.
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).

 $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We take advantage of dynamically growing feature space to reduce the number of supervised training examples.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We developed a linear discriminative transliteration model, and presented a method to automatically generate features.

As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.
As stated by (Klementiev and Roth, 2006), the projection of NER tags is easier in comparison to projecting other types of annotations such as POS-tags and BPC. $$$$$ The English side was tagged with a publicly available NER system based on the SNoW learning architecture (Roth, 1998), that is available on the same site.

 $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ As the iterative algorithm observes more data, it discovers and makes use of more features.
The iterative training algorithm described above is adopted from Klementiev and Roth (2006). $$$$$ Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates.

(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ We expect temporal sequence alignment to resolve many of such ambiguities.
(Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).

Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We take advantage of dynamically growing feature space to reduce the number of supervised training examples.
Our initial feature extraction scheme follows the one presented in (Klementiev and Roth, 2006), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We build a feature vector from this example in the following manner

We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). $$$$$ We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.
We evaluated our approach in two settings; first, we compared our system to a baseline system described in (Klementiev and Roth, 2006). $$$$$ Once the transliteration model was trained, we ran the algorithm to discover multi-word NEs, augmenting candidate sets of dictionary words with their translations as described in Section 3.1.

Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. $$$$$ Seeded with a small number of transliteration pairs, our algorithm discovers multi-word NEs, and takes advantage of a dictionary (if one exists) to account for translated or partially translated NEs.
Note that one of the models proposed in (Klementiev and Roth, 2006b) takes advantage of the temporal information. $$$$$ We extend our preliminary work in (Klementiev and Roth, 2006) to discover multi-word Named Entities and to take advantage of a dictionary (if one exists) to handle NEs which are partially or entirely translated.

Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. $$$$$ The model activation provides the score we use to select best transliterations on line 6.

The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles. $$$$$ Time sequence scoring is then used to rerank the list and choose the candidate best temporally aligned with the NE.
The Russian data set, originally introduced in (Klementiev and Roth, 2006b), is comprised of temporally aligned news articles. $$$$$ We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru).

 $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).
 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. $$$$$ Weakly Supervised Named Entity Transliteration And Discovery From Multilingual Comparable Corpora
For Russian, we compare to the model presented in (Klementiev and Roth, 2006b), a weakly supervised algorithm that uses both phonetic information and temporal information. $$$$$ In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a transliteration model.

We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ We score NEs similarity with a linear transliteration model.
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal co-occurrence similarity when ranking the transliteration candidates. $$$$$ In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a transliteration model.

This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version of SNoW (Roth, 1998).
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ In each iteration, we used the current transliteration model to find a list of 30 best transliteration equivalence classes for each NE.

The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). $$$$$ Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora.
The extraction proceeds either iteratively by starting from a few seed extraction rules (Collins and Singer, 1999), or by mining named entities from comparable news articles (Shinyama and Sekine, 2004) or from multilingual corpora (Klementiev and Roth, 2006). $$$$$ (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources.

More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ (Hetland, 2004) surveys recent methods for scoring time sequences for similarity.
More recently, Klementiev and Roth (2006) also use F-index (Hetland, 2004), a score using DFT, to calculate the time distribution similarity. $$$$$ We use a method called the F-index (Hetland, 2004) to implement the similarity function on line 8 of the algorithm.
