It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ The expected value for the point of divergence of scores, in terms of number of summaries in the pyramid, is 5.5.
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ First, this corresponds to the difference in PAL scores (D31041) we find when we use a different one of our three PAL annotations (see Table 1).

In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ Our method quantifies the relative importance of facts to be conveyed.
In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ Evaluation involves comparison of a peer summary (baseline, or produced by human or system) by comparing its content to a gold standard, or model.

To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ For the DUC scores there was always a single model, and no attempt to evaluate the model.

Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle.
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ This makes sense in light of the fact that a score is dominated by the higher weight SCUS that appear in a summary.

These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).
These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ SCU annotation involves two types of choices: extracting a contributor from a sentence, and assigning it to an SCU.

The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.
The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.

In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ Human-written summaries can score as low as 0.1 while machine summaries can score as high as 0.5.
In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ Here we use three DUC 2003 summary sets for which four human summaries were written.

the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).
the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ The pyramid method not only assigns a score to a summary, but also allows the investigator to find what important information is missing, and thus can be directly used to target improvements of the summarizer.

Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ Evaluating Content Selection In Summarization: The Pyramid Method
Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ The strengths of pyramid scores are that they are reliable, predictive, and diagnostic.

Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ Our analysis of summary content is based on Summarization Content Units, or SCUs and we will now proceed to define the concept.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ Our approach is to separate syntactic from semantic agreement, as in (Klavans et al., 2003).

Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ We do not attempt to represent the subsumption or implicational relations that Halteren and Teufel assign to factoids (Halteren and Teufel, 2003).
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.

This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ The label, which is subject to revision throughout the annotation process, has three functions.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ After the annotation procedure is completed, the final SCUs can be partitioned in a pyramid.

From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ Our analysis of summary content is based on Summarization Content Units, or SCUs and we will now proceed to define the concept.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ Then we present results demonstrating the need for at least five summaries per pyramid, given this corpus of 100-word summaries.

To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ The procedure used for evaluating summaries in DUC is the following: The final score is based on the content unit coverage.

They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably – (Lin and Hovy, 2002) report that humans agreed with their own prior judgment in only 82% of the cases.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.

Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries.
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ Thus scores can be computed using one, two and so on up to five reference summaries.

The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ Evaluation involves comparison of a peer summary (baseline, or produced by human or system) by comparing its content to a gold standard, or model.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ Thus scores can be computed using one, two and so on up to five reference summaries.

As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.
As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ For the DUC scores there was always a single model, and no attempt to evaluate the model.

Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.

In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ There are numerous problems with the DUC human evaluation method.
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements.
