For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ The First International Chinese Word Segmentation Bakeoff
For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.

We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. $$$$$ See (Yao, 2001; Yao, 2002) for the first and second of these; the third evaluation will be held in August 2003.
We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. $$$$$ Accuracies in the mid 80’s to mid 90’s were reported for the four systems that participated in the first evaluation, with higher scores (many in the high nineties) being reported for the second evaluation.

We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.
We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ We then used this dictionary with a simple maximum matching algorithm to segment the test corpus.

After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. $$$$$ The First International Chinese Word Segmentation Bakeoff
After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. $$$$$ It seems reasonable to treat two systems as significantly different (at the 95% confidence level), if at least one of their recall-based or precision-based confidences are different.

In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). $$$$$ Part of the problem is also that there is no single accepted segmentation standard: There are several, including the four standards used in this evaluation.
In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). $$$$$ Secondly, as we have already noted, there are at least four distinct standards in active use in the sense that large corpora are being developed according to those standards; see Section 2.1.

The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ The detailed instructions for the bakeoff can be found at http://www.sighan. org/bakeoff2003/bakeoff_instr.html (with simplified and traditional Chinese versions also available).
The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ Training material was available starting March 15, testing material was available April 22, and the results had to be returned to the SIGHAN ftp site by April 25 no later than 17:00 EDT.

Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. $$$$$ The First International Chinese Word Segmentation Bakeoff
Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. $$$$$ Chinese word segmentation is a difficult problem that has received a lot of attention in the literature; reviews of some of the various approaches can be found in (Wang et al., 1990; Wu and Tseng, 1993; Sproat and Shih, 2001).

The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ In this and subsequent tables, we list the word count for the test corpus, test recall (R), test precision (P), F score1, the out-of-vocabulary (OOV) rate for the test corpus, the recall on OOV words (R ), and the recall on in-vocabulary (R ) words.
The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ Per normal usage, OOV is defined as the set of words in the test corpus not occurring in the training corpus.2 We expect systems to do at least as well as this baseline.

Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ In this and subsequent tables, we list the word count for the test corpus, test recall (R), test precision (P), F score1, the out-of-vocabulary (OOV) rate for the test corpus, the recall on OOV words (R ), and the recall on in-vocabulary (R ) words.
Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ Figure 2 plots the recall on out-of-vocabulary words (R ) for all systems and all tracks.

In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ The First International Chinese Word Segmentation Bakeoff
In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.

A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ The First International Chinese Word Segmentation Bakeoff
A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ Without the generous contribution of these resources, this competition would not have been possible.

 $$$$$ Results were returned to participants within a couple of days of submission of the segmented test data.
 $$$$$ Finally we thank Fei Xia and Qing Ma for their work on the Second meeting of SIGHAN of which this bakeoff is a part.

This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ In the closed test, participants could only use training material from the training data for the particular corpus being testing on.
This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ Most of the training data comprise texts about Mainland China, whereas most of the testing data is about Taiwan.

No other material was allowed (Sproat and Emerson, 2003). $$$$$ Upon initial registration sites were required to declare which corpora they would be training and testing on, and whether they would be participating in the open or closed tracks (or both) on each corpus, where these were defined as follows: For the open test sites were allowed to train on the training set for a particular corpus, and in addition they could use any other material including material from other training corpora, proprietary dictionaries, material from the WWW and so forth.
No other material was allowed (Sproat and Emerson, 2003). $$$$$ No other material was allowed.

In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ We then used this dictionary with a simple maximum matching algorithm to segment the test corpus.
In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ Given the problems noted by some of the participants with some of the data, we would also like to see more consistently annotated training and test data, and test data that is more representative of what was seen in the training data.

For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). $$$$$ Links to descriptions of the corpora can be found at http://www.sighan.org/bakeoff2003/ bakeoff_instr.html; publications on specific corpora are (Huang et al., 1997) (Academia Sinica), (Xia, 1999) (Chinese Treebank); the Beijing University standard is very similar to that outlined in (GB/T 13715–92, 1993).
For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). $$$$$ Similarly ((corporate) vice president) is segmented as one word in training data but as two words ( / ) in the testing data.

We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ The First International Chinese Word Segmentation Bakeoff
We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ Language Information Sciences Research Centre, City University of Hong Kong.

The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.
The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ Accuracies in the mid 80’s to mid 90’s were reported for the four systems that participated in the first evaluation, with higher scores (many in the high nineties) being reported for the second evaluation.

In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ The First International Chinese Word Segmentation Bakeoff
In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ For example, if the system did particularly well on out-of-vocabulary words then the participants were required to explain if, for example, those results could mostly be attributed to having a good dictionary.

 $$$$$ Results were returned to participants within a couple of days of submission of the segmented test data.
 $$$$$ Finally we thank Fei Xia and Qing Ma for their work on the Second meeting of SIGHAN of which this bakeoff is a part.
