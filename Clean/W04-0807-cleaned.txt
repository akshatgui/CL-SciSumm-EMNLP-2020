This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations.
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.

In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Table 1 presents the number of words under each part of speech, and the average number of senses for each class.
In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Consequently, the training and test data sets made available for this task do not contain collocations as possible target words, but only single word units.

The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The Senseval-3 English Lexical Sample Task
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).

At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ The Senseval-3 English Lexical Sample Task
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.

 $$$$$ Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.
 $$$$$ We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.

S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ 2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.

Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.
Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.

In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ The Senseval-3 English Lexical Sample Task
In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.

We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.

These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author â€“ which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.

WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ The goal of this task was to create a framework for evaluation of systems that perform targeted Word Sense Disambiguation.
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ 27 teams participated in this word sense disambiguation task.

The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ The Senseval-3 English Lexical Sample Task
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.

Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ The Senseval-3 English Lexical Sample Task
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).

Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ The Senseval-3 English Lexical Sample Task
Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory.

In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ The Senseval-3 English Lexical Sample Task
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).

In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ The Senseval-3 English Lexical Sample Task
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.

For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ The Senseval-3 English Lexical Sample Task
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.

Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ This is a somewhat different definition of the task as compared to previous similar evaluations; the difference may have an impact on the overall performance achieved by systems participating in the task.
Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
