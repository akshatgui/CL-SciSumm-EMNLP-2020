Galley et al (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. $$$$$ What's In A Translation Rule?
Galley et al (2004) extract translation rules from a large parsed parallel corpus that extend in scope to tree fragments beyond a single node; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment. $$$$$ Our first algorithm, which has an exponential running time, cannot scale to process large corpora and extract a sufficient number of rules that a syntax-based statistical MT system would require.

In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al, 2004) and HPSG trees/forests (Wu et al, 2010). $$$$$ What's In A Translation Rule?
In terms of tree-to-string translation rule extraction, the toolkit implements the traditional maximum likelihood algorithm using PCFG trees (Galley et al, 2004) and HPSG trees/forests (Wu et al, 2010). $$$$$ Rule extraction: Algorithm 1.

In particular, we implemented the GHKM algorithm as proposed by Galley et al (2004) from word-aligned tree string pairs. $$$$$ In other words, a source word is aligned with a target word if the target word is created during the same step in which the source word is replaced.
In particular, we implemented the GHKM algorithm as proposed by Galley et al (2004) from word-aligned tree string pairs. $$$$$ Now, say that we have a source string, a target tree, and an alignment A.

 $$$$$ We suspect that such probabilistic rules could be also used in conjunction with statistical decoders, to increase the accuracy of statistical machine translation systems.
 $$$$$ This work was supported by DARPA contract N6600100-1-9814 and MURI grant N00014-00-1-0617.

For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al, 2004). $$$$$ We evaluated the coverage of our model of transformation rules with two language pairs: English-French and English-Chinese.
For the tree-to-string model, we parsed English sentences using Stanford parser and extracted rules using the GHKM algorithm (Galley et al, 2004). $$$$$ We performed experiments with two corpora, the FBIS English-Chinese Parallel Text and the Hansard FrenchEnglish corpus.We parsed the English sentences with a state-of-the-art statistical parser (Collins, 1999).

Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ In the face of these problems, we may choose among several alternatives.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive.

From word level alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. $$$$$ We propose a theory that gives formal semantics to word-level alignments defined over parallel corpora.
From word level alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. $$$$$ For the former, we present results for the three alignments: S alignments, P alignments, and the alignments computed by GIZA++.

algorithm (Galley et al, 2004) to forest-based by introducing non-deterministic mechanism. $$$$$ After all, many conventional translation systems are indeed based on syntactic transformations far more expressive than what has been proposed in syntax-based statistical MT.
algorithm (Galley et al, 2004) to forest-based by introducing non-deterministic mechanism. $$$$$ In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003).

GHKM (Galley et al, 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al, 2003). $$$$$ In addition to being motivated by rule-based systems, we also see advantages to English syntax within the statistical framework, such as marrying syntax-based translation models with syntaxbased language models (Charniak et al., 2003) and other potential benefits described by Eisner (2003).
GHKM (Galley et al, 2004) is used to generate the baseline TTS templates based on the word alignments computed using GIZA++ and different combination methods, including union and the diagonal growing heuristic (Koehn et al, 2003). $$$$$ For the former, we present results for the three alignments: S alignments, P alignments, and the alignments computed by GIZA++.

Galley et al (2004) describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and Galley et al (submitted) describe probability estimators for those rules. $$$$$ We evaluated the coverage of our model of transformation rules with two language pairs: English-French and English-Chinese.
Galley et al (2004) describe how to learn hundreds of millions of tree transformation rules from a parsed, aligned Chinese/English corpus, and Galley et al (submitted) describe probability estimators for those rules. $$$$$ To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments.

In order to test whether good translations can be generated with rules learned by Galley et al (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. $$$$$ If the same unambiguous English sentence were to appear twice in the corpus, with different Chinese translations, then it could have different learned parses.
In order to test whether good translations can be generated with rules learned by Galley et al (2004), we created DerivTool as an environment for interactively using these rules as a decoder would. $$$$$ Similarly, for each node t of T, we can define created(t, D) to be the step of derivation D during which t is created.

The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). $$$$$ For the former, we present results for the three alignments: S alignments, P alignments, and the alignments computed by GIZA++.
The grammar rules are extracted from bilingual word alignments using the GHKM algorithm (Galley et al., 2004). $$$$$ If we compare the three kinds of alignments available for the Hansard corpus, we see that much more complex transformation rules are extracted from noisy GIZA++ alignments.

We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-to-string rules according to the GHKM algorithm (Galley et al 2004). $$$$$ For the FBIS corpus (representing eight million English words), we automatically generated word-alignments using GIZA++ (Och and Ney, 2003), which we trained on a much larger data set (150 million words).
We aligned the sentence pairs using the GIZA++ toolkit (Och and Ney, 2003) and extracted tree-to-string rules according to the GHKM algorithm (Galley et al 2004). $$$$$ For the Hansard corpus, we took the human annotation of word alignment described in (Och and Ney, 2000).

Galley et al (2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below. $$$$$ Thus we have boiled down the problem of extracting complex rules to the following simple problem: find the set of minimal frontier graph fragments of a given alignment graph.
Galley et al (2004) alleviate this modeling problem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora, which we review below. $$$$$ In this section, we present some syntactic transformation rules that our system learns.

We contrast our work with (Galley et al, 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair. $$$$$ However, it is apparent that one of these derivations seems much more “wrong” than the other.
We contrast our work with (Galley et al, 2004), highlight some severe limitations of probability estimates computed from single derivations, and demonstrate that it is critical to account for many derivations for each sentence pair. $$$$$ Step 1 can be computed in a single traversal of the alignment graph.

Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al, 2004). $$$$$ Allowing more expansions logically expands the coverage of the model, until the point where it is total: transformation rules no larger than 17, 18, 23, and 43 (in number of rule expansions) respectively provide enough coverage to explain the data at 100% for each of the four cases.
Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al, 2004). $$$$$ Our rules provide a good, realistic indicator of the complexities inherent in translation.

Galley et al (2004) present one such formalism (henceforth 'GHKM'). $$$$$ Along this line, (Koehn et al., 2003) present convincing evidence that restricting phrasal translation to syntactic constituents yields poor translation performance – the ability to translate nonconstituent phrases (such as “there are”, “note that”, and “according to”) turns out to be critical and pervasive.
Galley et al (2004) present one such formalism (henceforth 'GHKM'). $$$$$ In this section, we present some syntactic transformation rules that our system learns.

Formally, transformational rules ri presented in (Galley et al, 2004) are equivalent to 1-state x Rs transducers mapping a given pattern (subtree to match in pi) to a right hand side string. $$$$$ It is certainly possible to build such rules by hand, and we have done this to formally explain a number of humantranslation examples.
Formally, transformational rules ri presented in (Galley et al, 2004) are equivalent to 1-state x Rs transducers mapping a given pattern (subtree to match in pi) to a right hand side string. $$$$$ Any subtree of this tree will be called a target subtree.

In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests. $$$$$ Our basic idea is to create transformation rules that condition on larger fragments of tree structure.
In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests. $$$$$ Multi-level reodering as the rule in the figure can prevent crossings.

Typically, by using the GHKM algorithm (Galley et al 2004), translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser. $$$$$ We use our theory to introduce a linear algorithm that can be used to derive from word-aligned, parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data.
Typically, by using the GHKM algorithm (Galley et al 2004), translation rules are learned from word-aligned bilingual texts whose source side has been parsed by using a syntactic parser. $$$$$ In other words, a source word is aligned with a target word if the target word is created during the same step in which the source word is replaced.
