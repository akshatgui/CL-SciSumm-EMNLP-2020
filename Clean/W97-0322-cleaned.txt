Once the corpus has been processed, clusters are repeatedly merged using HAC with the aver age link criteria, following (Pedersen and Bruce,1997). $$$$$ The two closest clusters are merged to form a new cluster that replaces the two merged clusters.
Once the corpus has been processed, clusters are repeatedly merged using HAC with the aver age link criteria, following (Pedersen and Bruce,1997). $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).

Here we are following (Pedersen and Bruce, 1997), who likewise took this approach to feature representation. $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
Here we are following (Pedersen and Bruce, 1997), who likewise took this approach to feature representation. $$$$$ Our feature sets are composed of various combinations of the following five types of features.

Context Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Schutze,1998). $$$$$ The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged.
Context Representations SenseClusters supports two different representations of context, first order context vectors as used by (Pedersen and Bruce, 1997) and second order context vectors as suggested by (Schutze,1998). $$$$$ The &quot;expanded&quot; sense definitions are then compared to the context of an ambiguous word, and the sensedefinition with the greatest number of word overlaps with the context is selected as correct.

But our model does have a natural preference for the most frequent sense in the thesaurus training corpus, which is a useful heuristic for word sense disambiguation (Pedersen and Bruce, 1997). $$$$$ This assumption is based on the success of the Naive Bayes model when applied to supervised word—sense disambiguation (e.g.
But our model does have a natural preference for the most frequent sense in the thesaurus training corpus, which is a useful heuristic for word sense disambiguation (Pedersen and Bruce, 1997). $$$$$ For words with skewed sense distribution, it is likely that the most frequent content words will be associated only with the dominate sense.

Previous work in word sense discrimination has shown that contexts of an ambiguous word can be effectively represented using first order (Pedersen and Bruce, 1997) or second order (Schutze, 1998) representations. $$$$$ This sample can be represented by the 4 x 4 dissimilarity matrix shown in Figure 2.
Previous work in word sense discrimination has shown that contexts of an ambiguous word can be effectively represented using first order (Pedersen and Bruce, 1997) or second order (Schutze, 1998) representations. $$$$$ In our work, the sense of an ambiguous word is represented by a feature whose value is missing.

 $$$$$ Overall, the most successful of our procedures was McQuitty's similarity analysis in combination with a high dimensional feature set.
 $$$$$ In future work, we will investigate modifications of these algorithms and feature set selection that are more effective on highly skewed sense distributions.

 $$$$$ Overall, the most successful of our procedures was McQuitty's similarity analysis in combination with a high dimensional feature set.
 $$$$$ In future work, we will investigate modifications of these algorithms and feature set selection that are more effective on highly skewed sense distributions.

(Pedersen and Bruce, 1997) and (Pedersen and Bruce,1998) propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word. $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
(Pedersen and Bruce, 1997) and (Pedersen and Bruce,1998) propose a (dis) similarity based discrimination approach that computes (dis) similarity among each pair of instances of the target word. $$$$$ Bruce, 1997a)).

(Schutze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty's method) fares well. $$$$$ Ward's and McQuitty's method are agglomerative clustering algorithms that differ primarily in how they compute the distance between clusters.
(Schutze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty's method) fares well. $$$$$ For 6 of the 13 words there was a single algorithm that was always significantly more accurate than the other two across all features.

The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative clustering. $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
The objective of this research is to extend previous work in discrimination by (Pedersen and Bruce, 1997), who developed an approach using agglomerative clustering. $$$$$ In previous unsupervised experiments with interest, using a modified version of Feature Set A, we were able to achieve an increase of 36 percentage points over the accuracy of the majority classifier when the 3 classes were evenly distributed in the sample (Pedersen and Bruce, 1997b).

We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Schutze, 1998) made binary distinctions. $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
We believe that this is an aggressive number of senses for a discrimination system to attempt, considering that (Pedersen and Bruce, 1997) experimented with 2 and 3 senses, and (Schutze, 1998) made binary distinctions. $$$$$ The data for the 12 words tagged using LDOCE senses are described in more detail in (Bruce, Wiebe, and Pedersen, 1996).

Our method of name discrimination is described in more detail in (Pedersen et al, 2005), but in general is based on an unsupervised approach to word sense discrimination introduced by (Purandare and 25 Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schutze, 1998) and (Pedersen and Bruce, 1997). $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
Our method of name discrimination is described in more detail in (Pedersen et al, 2005), but in general is based on an unsupervised approach to word sense discrimination introduced by (Purandare and 25 Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schutze, 1998) and (Pedersen and Bruce, 1997). $$$$$ (McDonald et al., 1990) apply another clustering approach to word—sense disambiguation (also see (Wilks et al., 1990)).

For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
For example, Pedersen and Bruce (1997) cluster the occurrences of an ambiguous word by constructing a vector of terms occurring in the context of the target. $$$$$ A context vector is formed for each occurrence of an ambiguous word by summing the vectors of the contextual words (the number of contextual words considered in the sum is unspecified).

An evaluation was carried out on the full 27,132 instance train+test data set using the SenseClusters evaluation methodology, which was first defined in (Pedersen and Bruce, 1997). $$$$$ (Gale, Church, and Yarowsky, 1992), (Leacock, Towel!, and Voorhees, 1993), (Mooney, 1996), (Pedersen, Bruce, and Wiebe, 1997), (Pedersen and Bruce, 1997a)).
An evaluation was carried out on the full 27,132 instance train+test data set using the SenseClusters evaluation methodology, which was first defined in (Pedersen and Bruce, 1997). $$$$$ The data for the 12 words tagged using LDOCE senses are described in more detail in (Bruce, Wiebe, and Pedersen, 1996).

 $$$$$ Overall, the most successful of our procedures was McQuitty's similarity analysis in combination with a high dimensional feature set.
 $$$$$ In future work, we will investigate modifications of these algorithms and feature set selection that are more effective on highly skewed sense distributions.

 $$$$$ Overall, the most successful of our procedures was McQuitty's similarity analysis in combination with a high dimensional feature set.
 $$$$$ In future work, we will investigate modifications of these algorithms and feature set selection that are more effective on highly skewed sense distributions.

In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. $$$$$ This paper describes an experimental comparison of three unsupervised learning algorithms that distinguish the sense of an ambiguous word in untagged text.
In (Pedersen and Bruce, 1997), they described an experimental comparison of three clustering algorithms for word sense discrimination. $$$$$ An early application of clustering to word—sense disambiguation is described in (Schiitze, 1992).
