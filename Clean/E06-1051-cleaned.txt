We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ Potentially any kernel function can work with any kernel-based algorithm.
We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.

An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. $$$$$ Finally, the Shallow Linguistic kernel It follows directly from the explicit construction of the feature space and from closure properties of kernels that KSL is a valid kernel.
An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. $$$$$ Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above.

The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature
The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.

 $$$$$ The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0

 $$$$$ The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0

A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifiFinally, Figure 5 shows the learning curve of the combined kernel KSL using the OARD evaluation methodology.
A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ The curve reaches a plateau with around 100 Medline abstracts.

 $$$$$ The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0

The former approach can produce higher performance $$$$$ On the other hand, if we use the OARD methodology, only one occurrence for each interaction has to be extracted to maximize the score.
The former approach can produce higher performance $$$$$ We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).

Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).
Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ As in (Bunescu et al., 2005; Bunescu and Mooney, 2005b), the graph points are obtained by varying the threshold on the classifiFinally, Figure 5 shows the learning curve of the combined kernel KSL using the OARD evaluation methodology.

Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).
Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).

 $$$$$ The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0

In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ PoS The PoS tag of the token.
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ Notice that KLC differs substantially from KGC as it considers the ordering of the tokens and the feature space is enriched with PoS, lemma and orthographic features.

On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information.
On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.

Giuliano et al (2006) use no syntactic information. $$$$$ Surprisingly, it outperforms most of the systems based on syntactic or semantic information, even when this information is manually annotated (i.e. the LLL challenge).
Giuliano et al (2006) use no syntactic information. $$$$$ They use composite kernels to integrate information from different syntactic sources (tokenization, sentence parsing, and deep dependency analysis) so that processing errors occurring at one level may be overcome by information from other levels.

 $$$$$ The basic idea behind kernel methods is to embed the input data into a suitable feature space F via a mapping function 0

In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ This allows us to make the classification task simpler without loosing information.
In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ The first subset does not include coreferences, while the second one includes simple cases of coreference, mainly appositions.

For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). $$$$$ For the protein-protein interaction task (AImed) we use the correct entities provided by the manual annotation.
For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). $$$$$ The first data set used in the experiments is the AImed corpus4, previously used for training protein interaction extraction systems in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).

We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. $$$$$ All experiments are conducted using 10-fold cross validation on the same data splitting used in (Bunescu et al., 2005; Bunescu and Mooney, 2005b).
We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. $$$$$ Given that we are interested here in the contribution of each kernel, we evaluated the experiments by 10-fold cross-validation on the whole training set avoiding the submission process.

The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ Exploiting Shallow Linguistic Information For Relation Extraction From Biomedical Literature
The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ In particular, we explore a kernel-based approach based solely on shallow linguistic processing, such as tokenization, sentence splitting, Part-of-Speech (PoS) tagging and lemmatization.

Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. $$$$$ In (Bunescu and Mooney, 2005b), the authors observed that a relation between two entities is generally expressed using only words that appear simultaneously in one of the following three patterns
