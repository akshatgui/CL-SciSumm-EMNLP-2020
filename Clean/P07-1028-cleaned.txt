We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Since the flexibility of similarity-based models extends to the vector space for computing similarities, one obvious remedy to the coverage problem would be the use of a less sparse vector space.
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ The most probable reason for this is the sparsity of the underlying vector space.

Our model builds on the architecture of Erk (2007). $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
Our model builds on the architecture of Erk (2007). $$$$$ In the evaluation, the similarity-model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model.

Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ Since the flexibility of similarity-based models extends to the vector space for computing similarities, one obvious remedy to the coverage problem would be the use of a less sparse vector space.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ Given a set S = Seen(rp) of seen headwords for some role rp, each similarity metric produces a set like(S) of words that have nonzero similarity to S, that is, to at least one word in S. Line (a) shows the average frequency of words in like(S).

In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ In a pseudo-disambiguation task the similarity-based model showed error rates down to 0.16, far lower than both EM-based clustering and Resnik's WordNet model.

We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ In the evaluation, the similarity-model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model.
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ In a pseudo-disambiguation task the similarity-based model showed error rates down to 0.16, far lower than both EM-based clustering and Resnik's WordNet model.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ They have been used for example for syntactic disambiguation (Hindle and Rooth, 1993), word sense disambiguation (WSD) (McCarthy and Carroll, 2003) and semantic role labeling (SRL) (Gildea and Jurafsky, 2002).
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Brockmann and Lapata (2003) perform a comparison of WordNet-based models.

Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word).
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet.

 $$$$$ This is especially relevant in view of the domain-dependence problem that SRL faces.
 $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.

Selectional preferences are computed as in Erk (2007). $$$$$ A Simple Similarity-based Model for Selectional Preferences
Selectional preferences are computed as in Erk (2007). $$$$$ The first five models are similarity-based, computed with uniform weights.

In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ A Simple Similarity-based Model for Selectional Preferences
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ We have introduced the similarity-based model for inducing selectional preferences.

Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet.
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ We use FrameNet (Baker et al., 1998), a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame.

We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005).
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ While there have been no isolated comparisons of the two generalization paradigms that we are aware of, Gildea and Jurafsky's (2002) task-based evaluation has found clusteringbased approaches to have better coverage than WordNet generalization, that is, for a given role there are more words for which they can state a preference.

The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Line (c) looks at the size of like(S).
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Since we are using a cutoff of 500 similar words computed per word in S, the size of like(S) can only vary if the same word is suggested as similar for several seen headwords in S. This way, the size of like(S) functions as an indicator of the degree of uniformity or similarity that a similarity metric “perceives” among the members of S. To facilitate comparison across frequency bands, line (c) normalizes by the size of S, showwe see that Cosine seems to “perceive” considerably less similarity among the seen headwords than any of the other metrics.

These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ Our generalization corpus is the BNC.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ In a test set of pairs (rp, w), each headword w is paired with a confounder w' chosen randomly from the BNC according to its frequency4.

Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word).
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ Our primary corpus will consist of manually semantically annotated data, and we will use semantic verb classes as predicates and semantic roles as arguments.

Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ A Simple Similarity-based Model for Selectional Preferences
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.

In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ Besides the similarity metric itself, which we discuss below, parameters of the similarity-based models include the number of seen headwords, the weighting scheme, and the number of similar words for each headword.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ So similarity-based models seem not overly sensitive to the weighting scheme used, the number of seen headwords, or the number of similar words per seen headword.

 $$$$$ This is especially relevant in view of the domain-dependence problem that SRL faces.
 $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.

 $$$$$ This is especially relevant in view of the domain-dependence problem that SRL faces.
 $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.

The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ We use FrameNet (Baker et al., 1998), a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ Like Rooth et al. (1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task.
