In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ Each Ai is a weight associated with feature Oi, and these weights are typically optimized using minimum error rate training (Och, 2003).
In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent.

Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ In Section 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang’s lack of strong results, and introduce finer grained constraints into the model.
Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3.

Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ Formally, Hiero’s translation model is a weighted synchronous contextfree grammar.

Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.'
Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ Finding the right way to balance linguistic analysis with unconstrained data-driven modeling is clearly a key challenge.

Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ He tested this conjecture by adding a soft constraint in the form of a “constituency feature”: if a synchronous rule X —* (e, f) is used in a derivation, and the span of f is a constituent in the sourcelanguage parse, then a term a, is added to the model score in expression (1).4 Unlike a hard constraint, which would simply prevent the application of rules violating syntactic boundaries, using the feature to introduce a soft constraint allows the model to boost the “goodness” for a rule if it is constitent with the source language constituency analysis, and to leave its score unchanged otherwise.
Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ For example, ONP= would denote a binary feature that matches whenever the span of f exactly covers an NP in the source-side parse tree, resulting in ANP= being added to the hypothesis score (expression (1)).

We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ In Section 3, we suggest that an insufficiently fine-grained view of constituency constraints was responsible for Chiang’s lack of strong results, and introduce finer grained constraints into the model.
We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ Using the example in Figure 1, we might want to penalize hypotheses containing rules where f is the minister gave a (and other cases, such as minister gave, minister gave a, and so forth).6 These observations suggest a finer-grained approach to the constituency feature idea, retaining the idea of soft constraints, but applying them using various soft-constraint constituency features.

By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ Hiero (Chiang, 2005; Chiang, 2007) is a hierarchical phrase-based statistical MT framework that generalizes phrase-based models by permitting phrases with gaps.

Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ Our first observation argues for distinguishing among constituent types (NP, VP, etc.).
Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ • For each constituent type, e.g.

Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ Soft Syntactic Constraints for Hierarchical Phrased-Based Translation
Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses.

Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ Whenever these latter possibilities occur, f will exactly match or cross the boundaries of some other constituent. ing constituents from the cost of crossing constituent boundaries.
Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ We accomplished this using constraints that not only distinguish among constituent types, but which also distinguish between the benefit of matching the source parse bracketing, versus the cost of using phrases that cross relevant bracketing boundaries.

Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. $$$$$ Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.'
Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. $$$$$ The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g.

Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries.
Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data.

We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). $$$$$ (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others).
We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). $$$$$ Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion.

Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ (Chiang, 2005; Chiang, 2007; Wu, 1997)).
Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ On the face of it, there are any number of possible reasons Chiang’s (2005) soft constraint did not work – including, for example, practical issues like the quality of the Chinese parses.5 However, we focus here on two conceptual issues underlying his use of source language syntactic constituents.

On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. $$$$$ Whenever these latter possibilities occur, f will exactly match or cross the boundaries of some other constituent. ing constituents from the cost of crossing constituent boundaries.
On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. $$$$$ • For each constituent type, e.g.

Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. $$$$$ For Chinese, we found that when we defined finer-grained versions of the exact-match features, there was value for some constituency types in biasing the model to favor matching the source language parse.
Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. $$$$$ Moreover, we found that there was significant value in allowing the model to be sensitive to violations (crossing boundaries) of source parses.

To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system.
To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ Although performance of XP=, XP2 and all-labels+ were similar to that of the undifferentiated constituency feature, XP+ achieved the highest gain.

Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. $$$$$ Like Chiang (2005), we find that the original, undifferentiated constituency feature (Chiang05) introduces a negligible, statistically insignificant improvement over the baseline.
Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. $$$$$ Of these, AdvP2 is also a significant improvement over the undifferentiated constituency feature (Chiang-05), with p < .01.

Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ We obtain substantial improvements in performance for translation from Chinese and Arabic to English.
Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ However, we find that several of the finer-grained constraints (IP=, VP=, VP+, QP+, and NP=) achieve statistically significant improvements over baseline (up to .74 BLEU), and the latter three also improve significantly on the undifferentiated constituency feature.

Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ • For each constituent type, e.g.
Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ Zollman and Venugopal (2006) start with a target language parser and use it to provide constraints on the extraction of hierarchical phrase pairs.
