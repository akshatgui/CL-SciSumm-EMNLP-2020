The work reported in Wu (1997), which uses an inside-outside type of training algorithm to learn statistical context free transduction. $$$$$ Our transduction model is context-free, rather than finite-state.
The work reported in Wu (1997), which uses an inside-outside type of training algorithm to learn statistical context free transduction. $$$$$ We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994).

The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997). $$$$$ The third column shows the number of all possible complete matchings between two constituents with a rank of r subconstituents each (therefore this is also the behavior for unconstrained context-free (syntax-directed) transduction grammars).
The empirical adequacy of synchronous context-free grammars of rank two (2-SCFGs) (Satta and Peserico, 2005), used in syntax based machine translation systems such as Wu (1997). $$$$$ Growth in number of legal complete subconstituent matchings for context-free (syntax-directed) transduction grammars with rank r, versus ITGs on a pair of subconstituent sequences of length r each.

In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al (2006) and Chiang (2007) are not expressive enough to do that. $$$$$ Parallel bilingual corpora have been shown to provide a rich source of constraints for statistical analysis (Brown et al. 1990; Gale and Church 1991; Gale, Church, and Yarowsky 1992; Church 1993; Brown et al.
In this paper it is shown that the synchronous grammars used in Wu (1997), Zhang et al (2006) and Chiang (2007) are not expressive enough to do that. $$$$$ The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al.

 $$$$$ Below we survey the most common constraints and discuss their relation to ITGs.
 $$$$$ I would like to thank Xuanyin Xia, Eva Wai-Man Fong, Pascale Fung, and Derick Wood, as well as an anonymous reviewer whose comments were of great value.

Bilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. $$$$$ We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994).
Bilingual Bracketing [Wu 1997] is one of the bilingual shallow parsing approaches studied for Chinese-English word alignment. $$$$$ In conjunction with automatic procedures for learning word translation lexicons, SITGs bring relatively underexploited bilingual Wu Bilingual Parsing correlations to bear on the task of extracting linguistic information for languages less studied than English.

In [Wu 1997], the Bilingual Bracketing PCFG was introduced, which can be simplified as the following production rules. $$$$$ For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' in which every production takes one of the following forms: A (Bi BO where n
In [Wu 1997], the Bilingual Bracketing PCFG was introduced, which can be simplified as the following production rules. $$$$$  3 and A 0 S. Include in G' all productions of the first six types.
In [Wu 1997], the Bilingual Bracketing PCFG was introduced, which can be simplified as the following production rules. $$$$$ Following the standard convention, we use a and b to denote probabilities for syntactic and lexical rules, respectively.

More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997]. $$$$$ In the second part, we survey a number of sample applications and extensions of bilingual parsing for segmentation, bracketing, phrasal alignment, and other parsing tasks.
More suitable ways could be bilingual chunk parsing, and refining the bracketing grammar as described in [Wu 1997]. $$$$$ The twin concepts of bilingual language modeling and bilingual parsing have been proposed.

Among the grammar formalisms successfully put into use in syntax based SMT are synchronous context-free grammars (SCFG) (Wu, 1997). $$$$$ Simple transduction grammars (as well as inversion transduction grammars) are restricted cases of ,the general class of context-free syntax-directed transduction grammars (Aho and Ullman 1969a, 1969b, 1972); however, we will avoid the term syntax-directed here, so as to de-emphasize the input-output connotation as discussed above.
Among the grammar formalisms successfully put into use in syntax based SMT are synchronous context-free grammars (SCFG) (Wu, 1997). $$$$$ The third column shows the number of all possible complete matchings between two constituents with a rank of r subconstituents each (therefore this is also the behavior for unconstrained context-free (syntax-directed) transduction grammars).

Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the sentence (Wu, 1997). $$$$$ Then for every 1 < i < N, the production probabilities are subject to the constraint that We now introduce an algorithm for parsing with stochastic ITGs that computes an optimal parse given a sentence-pair using dynamic programming.
Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6) in the length of the sentence (Wu, 1997). $$$$$ We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994).

Although this is less than the O (n6) complexity of exact ITG (In version Transduction Grammar) model (Wu, 1997), a quintic algorithm is often quite slow. $$$$$ We can use a simple transduction grammar to model the generation of bilingual sentence pairs.
Although this is less than the O (n6) complexity of exact ITG (In version Transduction Grammar) model (Wu, 1997), a quintic algorithm is often quite slow. $$$$$ For all English constituents es..t and all i, u, v such that { o<17<<11\1<v The time complexity for this constrained version of the algorithm drops from e(N3T3V3) to e(TV3).

In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrase based philosophy. $$$$$ The probability of a lexical rule A ox/y is bA(x,y) = 0.001.
In this respect it resembles Wu's bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrase based philosophy. $$$$$ We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994).

Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997). $$$$$ The constituent alignment includes a word alignment as a by-product.
Bilingual bracketing methods were used to produce a word alignment in (Wu, 1997). $$$$$ We have been using a lexicon that was automatically learned from the HKUST English-Chinese Parallel Bilingual Corpus via statistical sentence alignment (Wu 1994) and statistical Chinese word and collocation extraction (Fung and Wu 1994; Wu and Fung 1994), followed by an EM word-translation-learning procedure (Wu and Xia 1994).

We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods. $$$$$ We introduce (1) a novel stochastic inversion transduction grammar formalism for bilingual language modeling of sentence-pairs, and (2) the concept of bilingual parsing with a variety of parallel corpus analysis applications.
We present a new method that exploits a novel application of Inversion Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu 1997 [2]) for mining monolingual data to obtain tight sentence translation pairs, yielding accuracy significantly higher than previous known methods. $$$$$ We introduce a general formalism for modeling of bilingual sentence pairs, known as an inversion transduction grammar, with potential application in a variety of corpus analysis areas.

(Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. $$$$$ The constituent alignment includes a word alignment as a by-product.
(Wu, 1997) introduced a polynomial-time solution for the alignment problem based on synchronous binary trees. $$$$$ Alternative ITG parse trees for the same matching.

Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O (n6) as opposed to the monolingual O (n3) time. $$$$$ Our transduction model is context-free, rather than finite-state.
Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O (n6) as opposed to the monolingual O (n3) time. $$$$$ This is a factor of V3 more than monolingual chart parsing, but has turned out to remain quite practical for corpus analysis, where parsing need not be real-time.

The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. $$$$$ Lemma 1 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that: For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that the right-hand side of any production of G' contains either a single terminal-pair or a list of nonterminals.
The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages. $$$$$ Lemma 3 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that G' does not contain any productions of the form A B.

The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ Bracketing is another intermediate corpus annotation, useful especially when a fullcoverage grammar with which to parse a corpus is unavailable (for Chinese, an even more common situation than with English).
The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ In experiments with the minimal bracketing transduction grammar, the large majority of errors in word alignment were caused by two outside factors.

Wu (1997) demonstrated that for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities. $$$$$ Parsing, in the case of an ITG, means building matched constituents for input sentence-pairs rather than sentences.
Wu (1997) demonstrated that for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities. $$$$$ This observation holds assuming that the translation lexicon's coverage is reasonably good.

Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). $$$$$ Our transduction model is context-free, rather than finite-state.
Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). $$$$$ The usual Chinese NLP architecture first preprocesses input text through a word segmentation module (Chiang et al. 1992; Lin, Chiang, and Su 1992, 1993; Chang and Chen 1993; Wu and Tseng 1993; Sproat et al.

One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997). $$$$$ Lemma 1 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that: For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that the right-hand side of any production of G' contains either a single terminal-pair or a list of nonterminals.
One way around this difficulty is to stipulate that all rules must be binary from the outset, as in inversion-transduction grammar (ITG) (Wu, 1997). $$$$$ Lemma 3 For any inversion transduction grammar G, there exists an equivalent inversion transduction grammar G' where T(G) = T(G'), such that G' does not contain any productions of the form A B.
