 $$$$$ Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a tree cut.
 $$$$$ We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program.

 $$$$$ Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a tree cut.
 $$$$$ We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program.

After extracting the argument heads of the target slots of each verb (e.g., the intransitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using a minimum description length tree cut model (Li and Abe, 1998). $$$$$ The former is the model description length and the latter the data description length.
After extracting the argument heads of the target slots of each verb (e.g., the intransitive subject and the transitive object for the causative alternation), she then determined their selectional profiles using a minimum description length tree cut model (Li and Abe, 1998). $$$$$ The total description length L(M, S) of the tree cut model kl and the sample S observed through M is computed as the sum of the model description length L(F), parameter description length L(e I r), and data description length L(S I Note that we sometimes refer to L(F) + L(e I n as the model description length.

The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. $$$$$ We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus tree; namely, selecting a model that best explains the data from among the class of tree cut models.
The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. $$$$$ Abe and Li [1996]).

Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. $$$$$ The algorithm: Find-MDL. simple and efficient algorithm based on dynamic programming, which is guaranteed to find a model with the minimum description length.
Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. $$$$$ Hindle and Rooth (1991) proposed the use of the lexical association measure calculated based on such doubles.

W ealso plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees. $$$$$ Abe and Li [1996]).
W ealso plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees. $$$$$ (This allows us to use the same recursive algorithm, Find-MDL, in all cases.)

Our selectional preference model relies on Li and Abe (1998), applying the MDL principle to determine selectional preferences of verbs and their arguments, by means of a concept hierarchy ordered by hypernym/hyponym relations. $$$$$ Next, we tested the method of applying a default rule after applying each method.
Our selectional preference model relies on Li and Abe (1998), applying the MDL principle to determine selectional preferences of verbs and their arguments, by means of a concept hierarchy ordered by hypernym/hyponym relations. $$$$$ Abe and Li [1996]).

Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. $$$$$ A cut in a tree is any set of nodes in the tree that defines a partition of the leaf nodes, viewing each node as representing the set of all leaf nodes it dominates.
Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them. $$$$$ Abe and Li [1996]).

 $$$$$ Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a tree cut.
 $$$$$ We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program.

 $$$$$ Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a tree cut.
 $$$$$ We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program.

In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ We therefore leave this issue as a future topic, and employ a simple heuristic of equally distributing each word occurrence in the data to all of its potential word senses in our experiments.
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ Abe and Li [1996]).

Initially our project began as an application of the closely related MDL approach of Li and Abe (1998), but was hindered by sparse data. $$$$$ Note in the above algorithm that the parameter description length is calculated as An example application of Find-MDL. entire tree and when it is a proper subtree.
Initially our project began as an application of the closely related MDL approach of Li and Abe (1998), but was hindered by sparse data. $$$$$ Abe and Li [1996]).

Li and Abe (1998) used a tree cut model over WordNet, based on the principle of MinimumDescription Length (MDL). $$$$$ In generalizing values of a case frame slot using MDL, we could, in principle, calculate the description length of every possible tree cut model and output a model with the minimum description length as the generalization result, if computation time were of no concern.
Li and Abe (1998) used a tree cut model over WordNet, based on the principle of MinimumDescription Length (MDL). $$$$$ Abe and Li [1996]).

 $$$$$ Recall that P(n) is obtained by MLE, namely, by normalizing the frequencies: where f(C) denotes the total frequency of nouns in class C in the sample S. and F is a tree cut.
 $$$$$ We acknowledge the ACL for providing the ACL/DCI CD-ROM, LDC of the University of Pennsylvania for providing the Penn Treebank corpus data, and Princeton University for providing WordNet, and E. Brill and P. Resnik for providing their PP-attachment disambiguation program.

McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy (Li and Abe, 1998). $$$$$ We then give an efficient algorithm that provably obtains the optimal tree cut model for the given frequency data of a case slot, in the sense of MDL.
McCarthy determines the sense profile of a verb/slot pair using a minimum description length tree cut model over the frequency-populated hierarchy (Li and Abe, 1998). $$$$$ In generalizing values of a case frame slot using MDL, we could, in principle, calculate the description length of every possible tree cut model and output a model with the minimum description length as the generalization result, if computation time were of no concern.

We suspect the problem is two-fold, arising from the dependence of her method on tree cut models (Li and Abe, 1998). $$$$$ We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus tree; namely, selecting a model that best explains the data from among the class of tree cut models.
We suspect the problem is two-fold, arising from the dependence of her method on tree cut models (Li and Abe, 1998). $$$$$ Abe and Li [1996]).

The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. $$$$$ We have thus formalized the problem of generalizing values of a case frame slot as that of estimating a model from the class of tree cut models for some fixed thesaurus tree; namely, selecting a model that best explains the data from among the class of tree cut models.
The method addresses conceptual problems of an earlier measure proposed by McCarthy (2000), which was limited to tree cut models (Li and Abe, 1998) and failed to distinguish detailed semantic differences between them. $$$$$ Abe and Li [1996]).

The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998). $$$$$ Generalizing Case Frames Using A Thesaurus And The MDL Principle
The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus (Li and Abe, 1998). $$$$$ We proposed a new method of generalizing case frames.

This leads to the notion of "cutting" the hierarchy at one or more positions (Li and Abe, 1998). $$$$$ 11 The experimental results obtained here are better than those obtained in our preliminary experiment (Li and Abe 1995), in part because we only adopted rule (1) in the past.
This leads to the notion of "cutting" the hierarchy at one or more positions (Li and Abe, 1998). $$$$$ Abe and Li [1996]).

Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. $$$$$ The former is the model description length and the latter the data description length.
Li and Abe (1998) propose a model in which the appropriate cut c is selected according to the Minimum Description Length principle; this principle explicitly accounts for the trade-off between generalisation and accuracy by minimising a sum of model description length and data description length. $$$$$ The total description length L(M, S) of the tree cut model kl and the sample S observed through M is computed as the sum of the model description length L(F), parameter description length L(e I r), and data description length L(S I Note that we sometimes refer to L(F) + L(e I n as the model description length.
