To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ First, we generalize Marton and Resnik’s (2008) soft syntactic constraints by training all of them simultaneously; and, second, we introduce a novel structural distortion model.
To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ We then define coarse- and fine-grained versions of the structural distortion model.

As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. $$$$$ Sentence-level approximations to BLEU exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLEU computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007).
As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. $$$$$ These counts are sufficient to calculate a BLEU score, which we write as BLEU(c(e)).

An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ In a phrase-based model, reordering is performed both within phrase pairs and by the phrasereordering model.
An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach.

MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.

We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). $$$$$ The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).
We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). $$$$$ Marton and Resnik optimized their features’ weights using MERT.

The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). $$$$$ First of all, we find that MIRA is competitive with MERT when both use the baseline feature set.
The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). $$$$$ When training with MERT, the coarse-grained pair of syntax features yields a small improvement, but the finegrained syntax features do not yield any further improvement.

We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.
We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).

In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008). $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008). $$$$$ The first features we explore are based on a line of research introduced by Chiang (2005) and improved on by Marton and Resnik (2008).

 $$$$$ The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.
 $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.

The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).
The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ Here, we introduce a new oracle-translation selection method, formulating the intuition behind local updating as an optimization problem

For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types $$$$$ However, this more syntactically aware model, when tested in Chinese-English translation, did not improve translation performance.

The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). $$$$$ Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.

The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).
The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007).

Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ In this paper, we have brought together two existing lines of work

We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).
We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). $$$$$ This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.

Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ Online Large-Margin Training of Syntactic and Structural Translation Features
Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems.

Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ Recently, Marton and Resnik (2008) revisited the idea of constituency features, and succeeded in showing that finer-grained soft syntactic constraints yield substantial improvements in BLEU score for both Chinese-English and Arabic-English translation.
Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ We now describe our experiments to test MIRA and our features, the soft-syntactic constraints and the structural distortion features, on an Arabic-English translation task.

The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions.
The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ Since our loss function cannot be so decomposed, we select

In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).

 $$$$$ The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.
 $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
