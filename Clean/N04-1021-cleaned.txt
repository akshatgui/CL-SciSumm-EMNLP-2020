Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score (Och et al., 2004). $$$$$ A Smorgasbord Of Features For Statistical Machine Translation
Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a 'truly significant improvement' was the Model 1 score (Och et al., 2004). $$$$$ Our single best feature, and in fact the only single feature to produce a truly significant improvement, was the IBM Model 1 score.

In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated. $$$$$ A Smorgasbord Of Features For Statistical Machine Translation
In (Och et al, 2004), the effects of integrating syntactic structure into a state-of-the-art statistical machine translation system are investigated. $$$$$ Despite the enormous progress in machine translation (MT) due to the use of statistical techniques in recent years, state-of-the-art statistical systems often produce translations with obvious errors.

Although the improvement on the IWSLT 04 set is only moderate, the results are nevertheless comparable or better to the ones from (Och et al, 2004). $$$$$ Unfortunately, this feature function did not help to obtain better results (it actually seems to significantly hurt performance).
Although the improvement on the IWSLT 04 set is only moderate, the results are nevertheless comparable or better to the ones from (Och et al, 2004). $$$$$ Thus, we have the folAs the model is computationally expensive, we sorted the n-best list by the sentence length, and processed them from the shorter ones to the longer ones.

As a workaround, parsers can rerank the translated output of translation systems (Och et al, 2004). $$$$$ Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).
As a workaround, parsers can rerank the translated output of translation systems (Och et al, 2004). $$$$$ We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system.

Och et al (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. $$$$$ A Smorgasbord Of Features For Statistical Machine Translation
Och et al (2004) and Cherry and Quirk (2008) both use the 1-best output of a machine translation system. $$$$$ Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).

Och et al (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. $$$$$ The most straightforward way to integrate a statistical parser in the system would be the use of the (log of the) parser probability as a feature function.
Och et al (2004) also report using a parser probability normalized by the unigram probability (but not length), and did not find it effective. $$$$$ We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements.

We follow Och et al (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. $$$$$ A Smorgasbord Of Features For Statistical Machine Translation
We follow Och et al (2004) and Cherry and Quirk (2008) in evaluating our language models on their ability to distinguish the 1-best output of a machine translation system from a reference translation in a pairwise fashion. $$$$$ The goal is the translation of a text given in some source language into a target language.

A typical reranking approach to SMT (Och et al, 2004) uses a 1000 best list. $$$$$ Using this method with a 1000-best list, we obtain oracle translations that outperform the BLEU score of the human translations.
A typical reranking approach to SMT (Och et al, 2004) uses a 1000 best list. $$$$$ Here, using an 1000-best list, we obtain oracle translations with a relative human BLEU score of 88.5%.

(Och et al, 2004) describe the use of syntactic features in the rescoring step. $$$$$ We then present a selection of new features, progressing from word-level features to those based to part-of-speech tags and syntactic chunks, and then to features based on Treebank-based syntactic parses of the source and target sentences.
(Och et al, 2004) describe the use of syntactic features in the rescoring step. $$$$$ In addition to experiments with single features we also integrated multiple features using a greedy approach where we integrated at each step the feature that most improves the BLEU score.

Many solutions to the reordering problem have been proposed ,e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al,2006). $$$$$ This feature function can be thought of as a trade-off between purely word-based models, and full generative models based upon shallow syntax.
Many solutions to the reordering problem have been proposed ,e.g. syntax-based models (Chiang, 2005), lexicalized reordering (Och et al, 2004), and tree-to-string methods (Zhang et al,2006). $$$$$ A tree-to-string model is one of several syntaxbased translation models used.

(Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. $$$$$ We present results for a small selection of features at each level of syntactic representation.
(Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. $$$$$ We then present a selection of new features, progressing from word-level features to those based to part-of-speech tags and syntactic chunks, and then to features based on Treebank-based syntactic parses of the source and target sentences.

Oracle BLEU scores computed over k-best lists (Och et al, 2004) show that many high quality hypotheses are produced by first-pass SMT decoding. $$$$$ We computed the oracle translations, that is, the set of translations from our n-best list that yields the best BLEU score.1 We use the following two methods to compute the BLEU score of an oracle translation

Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual IR (Schonhofen et al, 2008) or re-score candidate translation outputs (Och et al, 2004). $$$$$ We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.
Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in cross lingual IR (Schonhofen et al, 2008) or re-score candidate translation outputs (Och et al, 2004). $$$$$ As defined by Brown et al. (1993), Model 1 gives a probability of any given translation pair, which is We used GIZA++ to train the model.

Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al, 2004). $$$$$ Chinese POS sequences are projected to English using the word alignment.
Despite the notational similarities, our approach should not be confused with projected POS models, which use source side POS tags to model reordering (Och et al, 2004). $$$$$ The Projected POS feature function was one of the strongest performing shallow syntactic feature functions, with a %BLEU score of 31.8.


A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al, 2004). $$$$$ A high-quality statistical translation system is our baseline, and we add new features to the existing set, which are then combined in a log-linear model.
A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al, 2004). $$$$$ We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.

There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). $$$$$ Word Selection This feature is based on the lexical translation probabilities p(e

Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system. $$$$$ Phrase Alignment This feature favors monotonic alignment at the phrase level.
Such an approach has been taken by Och et al (2004) for integrating sophisticated syntax-informed models in a phrase based SMT system. $$$$$ This feature function can be thought of as a trade-off between purely word-based models, and full generative models based upon shallow syntax.

This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004). $$$$$ Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).
This method is a straightforward application of the n-best re-ranking approach described in Och et al (2004). $$$$$ These n-best candidate translations are the basis for discriminative training of the model parameters and for re-ranking.

Many different feature functions were explored in (Och et al, 2004). $$$$$ Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004).
Many different feature functions were explored in (Och et al, 2004). $$$$$ We used IBM Model 1 (Brown et al., 1993) as one of the feature functions.
