Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ Alshawi et al. (2000) and Hwa et al.
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks.

Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Graça et al., 2008).
Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ A parallel corpus is word-level aligned using an alignment toolkit (Graça et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005).

In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ Link-left baselines for these corpora are much lower: 33.8% and 27.9% for Bulgarian and Spanish respectively.
In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ We have a list of 12 Bulgarian auxiliary verbs.

Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ In particular, we address challenges (1) and (2) by avoiding commitment to an entire projected parse tree in the target language during training.
Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.

While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ Alshawi et al. (2000) and Hwa et al.
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ Our work most closely relates to Hwa et al. (2005), who proposed to learn generative dependency grammars using Collins’ parser (Collins, 1999) by constructing full target parses via projected dependencies and completion/transformation rules.

However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ Dependency grammars are arguably more robust to transfer since syntactic relations between aligned words of parallel sentences are better conserved in translation than phrase structure (Fox, 2002; Hwa et al., 2005).
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ Instead, we explore formulations of both generative and discriminative probabilistic models where projected syntactic relations are constrained to hold approximately and only in expectation.

We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ To get a feel for the typical case, we used off-the-shelf parsers (McDonald et al., 2005) for English, Spanish and Bulgarian on two bitexts (Koehn, 2005; Tiedemann, 2007) and compared several measures of dependency conservation.

While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ A parallel corpus is word-level aligned using an alignment toolkit (Graça et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005).
While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ The experimental details are described in this section.

P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ In our experiments we evaluate the learned models on dependency treebanks (Nivre et al., 2007).
P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ Results for the discriminative parser are shown in the bottom panels of Figure 5.

For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ We explored two parsing models: a generative model used by several authors for unsupervised induction and a discriminative model used for fully supervised training.

Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ For Spanish, we have three rules.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ Table 3 compares the errors for different linguistic rules.

 $$$$$ Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
 $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487), by ARO MURI SUBTLE W911NF-07-1-0216 and by the European Projects AsIsKnown (FP6-028044) and LTfLL (FP7-212578).

Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ In order to evaluate our method, we a baseline inspired by Hwa et al. (2005).
Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ Table 2 shows attachment accuracy of our method and the baseline for both language pairs under several conditions.

Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ Note that this baseline is very similar to the first iteration of our model, since for a large corpus the different random choices made in different sentences tend to smooth each other out.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ As we shall see below, the discriminative parser performs even better than the generative model. u We trained our discriminative parser for 100 iterations of online EM with a Gaussian prior variance of 100.

 $$$$$ Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
 $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487), by ARO MURI SUBTLE W911NF-07-1-0216 and by the European Projects AsIsKnown (FP6-028044) and LTfLL (FP7-212578).

Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ The generative learning objective is to minimize: For discriminative estimation (Ganchev et al., 2008), we do not attempt to model the marginal distribution of x, so we simply have the two regularization terms: Note that the idea of regularizing moments is related to generalized expectation criteria algorithm of Mann and McCallum (2007), as we discuss in the related work section below.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ However, the improvement in stability makes the algorithm much more usable.

An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ Alshawi et al. (2000) and Hwa et al.
An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks.

PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. $$$$$ A parallel corpus is word-level aligned using an alignment toolkit (Graça et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005).
PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. $$$$$ The Bulgarian experiments transfer a parser from English to Bulgarian, using the OpenSubtitles corpus (Tiedemann, 2007).

The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ A parallel corpus is word-level aligned using an alignment toolkit (Graça et al., 2009) and the source (English) is parsed using a dependency parser (McDonald et al., 2005).
The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ For both corpora, we performed word alignments with the open source PostCAT (Graça et al., 2009) toolkit.

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ We also used a generative model based on dependency model with valence (Klein and Manning, 2004).
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Discriminative models outperform the generative models in the majority of cases.
