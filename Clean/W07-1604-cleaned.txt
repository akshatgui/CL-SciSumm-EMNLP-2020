More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ Izumi et al. (2003) and (2004) used errorannotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.

Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.
Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ In particular, preposition usage is one of the most difficult aspects of English grammar for non-native speakers to master.

For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ Each preposition in these essays was judged for correctness of usage by one or two human raters.
For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ Both precision and recall are low in these comparisons to the human raters.

Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ To detect the first type of error, incorrect selection, we have employed a maximum entropy (ME) model to estimate the probability of each of 34 prepositions, based on the features in their local contexts.
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ The maximum entropy model was trained with 25 contextual features.

Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ They represented the largest category, about 29%, of all the errors by 53 intermediate to advanced ESL students (Bitchener et al., 2005), and 18% of all errors reported in an intensive analysis of one Japanese writer (Murata and Ishara, 2004).
Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ Izumi et al. (2003) and (2004) used errorannotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.

(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ We present an approach that combines machine learning with rule-based filters to detect preposition errors in a corpus of ESL essays.
(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ Using more training data.

Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. $$$$$ Even though this is work in progress, we achieve precision of 0.8 with a recall of 0.3.
Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. $$$$$ That is, prediction is better for prepositions that have many examples in the training set and worse for those with fewer examples.

Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ To detect the first type of error, incorrect selection, we have employed a maximum entropy (ME) model to estimate the probability of each of 34 prepositions, based on the features in their local contexts.

The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. $$$$$ There were two other common sources of classification error

A context is represented by 25 lexical features and 4 combination features $$$$$ PHR pre is the “preceding phrase” feature that indicates whether the preposition was preceded by a noun phrase (NP) or a verb phrase (VP).
A context is represented by 25 lexical features and 4 combination features $$$$$ Many fairly common combinations of Verb+Preposition+Noun or Noun+Preposition+Noun are simply not attested, even in a sizable corpus.

Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.
Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ In English, prepositions appear in adjuncts, they mark the arguments of predicates, and they combine with other parts of speech to express new meanings.

Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ Detection of Grammatical Errors Involving Prepositions
Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ In English, prepositions appear in adjuncts, they mark the arguments of predicates, and they combine with other parts of speech to express new meanings.

We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. $$$$$ Some features had only a few values while others had many.
We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. $$$$$ Table 2 shows an example of where some of the features are derived from.

The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ Even though this is work in progress, we achieve precision of 0.8 with a recall of 0.3.

While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). $$$$$ The paper is structured as follows

For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ In addition, for both raters, precision was much higher than recall.
For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ Both precision and recall are low in these comparisons to the human raters.

Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ The maximum entropy model was trained with 25 contextual features.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ Both used POS tags and chunking information.

Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ Another difference between the training corpus and the testing corpus was that the latter contained grammatical errors.

Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ The maximum entropy model was trained with 25 contextual features.
Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ To identify extraneous preposition errors we devised two rule-based filters which were based on analysis of the development set.

Preposition errors are common among new English speakers (Chodorow et al, 2007). $$$$$ This paper presents ongoing work on the detection of preposition errors of non-native speakers of English.
Preposition errors are common among new English speakers (Chodorow et al, 2007). $$$$$ The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.
