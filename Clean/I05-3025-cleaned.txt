In ME model, the following features (Jin Kiat Low et al, 2005) are selected: a) cn (n=-2, -1, 0, 1, 2) b) cncn+1 (n=-2, -1, 0, 1) c) c-1c+1 where cn indicates the character in the left or right position n relative to the current character c0. $$$$$ C0 denotes the current character, Cn(C?n ) denotes the character n positions to the right (left) of the current character.
In ME model, the following features (Jin Kiat Low et al, 2005) are selected: a) cn (n=-2, -1, 0, 1, 2) b) cncn+1 (n=-2, -1, 0, 1) c) c-1c+1 where cn indicates the character in the left or right position n relative to the current character c0. $$$$$ When processing the current character C0 ??

The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. $$$$$ The columns R, P, and F show the recall, precision, and F mea- sure, respectively.
The current state-of-the-art segmentation software developed by (Low et al, 2005), which ranks as the best in the SIGHAN bakeoff (Emerson, 2005), attains word precision and recall of 96.9% and 96.8%, respectively, on the PKU track. $$$$$ 4 Conclusion Using a maximum entropy approach, our Chi- nese word segmenter achieves state-of-the-art ac- curacy, when evaluated on all four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff.

3) New feature templates were added, such as the templates that were used in representing numbers, dates, letters etc. (Low et al., 2005). $$$$$ Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.
3) New feature templates were added, such as the templates that were used in representing numbers, dates, letters etc. (Low et al., 2005). $$$$$ Templates (a) ?

Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2, C−1, C0, C1, C2, C−2C−1, C−1C0, C−1C1, C0C1 and C1C2. $$$$$ Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.
Those for the 4-tag set, adopted from (Xue, 2003) and (Low et al., 2005), include C−2, C−1, C0, C1, C2, C−2C−1, C−1C0, C−1C1, C0C1 and C1C2. $$$$$ T (C2) = 11243 1http://maxent.sourceforge.net/ 161 will be set to 1 (???

Third, the post processing method (Low et al, 2005) is employed to enhance the unknown word segmentation. $$$$$ After word segmentation is done by the maxi- mum entropy classifier, a post-processing step is applied to correct inconsistently segmented words made up of 3 or more characters.
Third, the post processing method (Low et al, 2005) is employed to enhance the unknown word segmentation. $$$$$ In the post-processing step, the segmentation of the characters of these consecutive words is changed so that they are segmented as a single word.

 $$$$$ Chinese word segmentation as LMR tagging.
 $$$$$ In Proceedings of the Second SIGHAN Workshop on Chinese Lan- guage Processing, pages 176?179.

We add a new feature, which also used in maximum entropy model for word segmentation task by (Low et al, 2005), to the feature templates for CRF model while keep the other features same as (Zhao et al, 2006). $$$$$ Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.
We add a new feature, which also used in maximum entropy model for word segmentation task by (Low et al, 2005), to the feature templates for CRF model while keep the other features same as (Zhao et al, 2006). $$$$$ Templates (a) ?

3) New feature templates were added, such as templates used in representing numbers, dates, letters etc. (Low et al., 2005). $$$$$ Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.
3) New feature templates were added, such as templates used in representing numbers, dates, letters etc. (Low et al., 2005). $$$$$ Templates (a) ?

Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). $$$$$ 1.3 Additional Training Corpora The presence of different standards in Chinese word segmentation limits the amount of training corpora available for the community, due to dif- ferent organizations preparing training corpora in their own standards.
Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). $$$$$ Chinese word segmentation as LMR tagging.

Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). $$$$$ 1.3 Additional Training Corpora The presence of different standards in Chinese word segmentation limits the amount of training corpora available for the community, due to dif- ferent organizations preparing training corpora in their own standards.
Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al, 2005). $$$$$ Chinese word segmentation as LMR tagging.

We use the maximum entropy segmenter of (Low et al, 2005) to segment the Chinese part of the FBIS corpus. $$$$$ 1 Chinese Word Segmenter The Chinese word segmenter we built is similar to the maximum entropy word segmenter we em- ployed in our previous work (Ng and Low, 2004).
We use the maximum entropy segmenter of (Low et al, 2005) to segment the Chinese part of the FBIS corpus. $$$$$ Use the trained word segmenter to segment another corpus D annotated in a different segmentation standard.

 $$$$$ Chinese word segmentation as LMR tagging.
 $$$$$ In Proceedings of the Second SIGHAN Workshop on Chinese Lan- guage Processing, pages 176?179.

Our Chinese word segmenter is a modification of the system described by Low et al (2005), which they entered in the 2005 Second International Chinese Word Segmentation Bakeoff. $$$$$ 1 Chinese Word Segmenter The Chinese word segmenter we built is similar to the maximum entropy word segmenter we em- ployed in our previous work (Ng and Low, 2004).
Our Chinese word segmenter is a modification of the system described by Low et al (2005), which they entered in the 2005 Second International Chinese Word Segmentation Bakeoff. $$$$$ 4 Conclusion Using a maximum entropy approach, our Chi- nese word segmenter achieves state-of-the-art ac- curacy, when evaluated on all four corpora in the open track of the Second International Chinese Word Segmentation Bakeoff.

Much of this can be attributed to the value of using an external dictionary and additional training data, as illustrated by the experiments run by Low et al (2005) with their model. $$$$$ We ad- dress the problem of OOV words in two ways: using an external dictionary containing a list of predefined words, and using additional training corpora which are not segmented according to the same segmentation standard.
Much of this can be attributed to the value of using an external dictionary and additional training data, as illustrated by the experiments run by Low et al (2005) with their model. $$$$$ Add all such characters C as additional train- ing data to the original training corpus D0, and train a new word segmenter using the en- larged training data.

It should be noted that in our testing during development, even when we strove to create a system which matched as closely as possible the one described by Low et al (2005), we were unable to achieve scores for the 2005 bakeoff data as high as their system did. $$$$$ 2 Testing During testing, the probability of a boundary tag sequence assignment t1 .
It should be noted that in our testing during development, even when we strove to create a system which matched as closely as possible the one described by Low et al (2005), we were unable to achieve scores for the 2005 bakeoff data as high as their system did. $$$$$ Our word segmenter achieved the high- est F measure for AS, CITYU, and PKU, and the second highest for MSR.

Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). $$$$$ word-based or character-based?
Especially, character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). $$$$$ Chinese word segmentation as LMR tagging.

We utilized four of the five basic feature templates suggested in (Low et al, 2005), described as follows: Cn (n=-2, -1, 0, 1, 2), CnCn+ 1 (n=-2, -1, 0, 1), Pu (C0)? T (C-2) T (C-1) T (C0) T (C1) T (C2) where C refers to a Chinese character. $$$$$ Our implementation used the opennlp maximum entropy package v2.1.0 from sourceforge.1 1.1 Basic Features The basic features of our word segmenter are similar to our previous work (Ng and Low, 2004): (a) Cn(n = ?2,?1, 0, 1, 2) (b) CnCn+1(n = ?2,?1, 0, 1) (c) C?1C1 (d) Pu(C0) (e) T (C?2)T (C?1)T (C0)T (C1)T (C2) In the above feature templates, C refers to a Chinese character.
We utilized four of the five basic feature templates suggested in (Low et al, 2005), described as follows: Cn (n=-2, -1, 0, 1, 2), CnCn+ 1 (n=-2, -1, 0, 1), Pu (C0)? T (C-2) T (C-1) T (C0) T (C1) T (C2) where C refers to a Chinese character. $$$$$ The punctuation feature, Pu(C0), checks whether C0 is a punctuation symbol (such as ??

See detail description and the example in (Low et al, 2005). $$$$$ For example, when considering the character ?#?
See detail description and the example in (Low et al, 2005). $$$$$ For example, comma ?,?

Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). $$$$$ word-based or character-based?
Especially, the character-based tagging method which was proposed by Nianwen Xue (2003) achieves great success in the second International Chinese word segmentation Bakeoff in 2005 (Low et al, 2005). $$$$$ Chinese word segmentation as LMR tagging.

Low et al (2005) introduce an external dictionary as features of a discriminative model. $$$$$ are found in the dictionary.
Low et al (2005) introduce an external dictionary as features of a discriminative model. $$$$$ Version V1 used only the basic features (Section 1.1); Version V2 used the basic features and additional features de- rived from our external dictionary (Section 1.2); Version V3 used the basic features but with ad- ditional training corpora (Section 1.3); and Ver- sion V4 is our official submitted version combin- ing basic features, external dictionary, and addi- tional training corpora.
