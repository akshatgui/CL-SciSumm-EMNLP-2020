 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ Reranking And Self-Training For Parser Adaptation
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ The trends are the same as in (McClosky et al., 2006): Adding NANC data improves parsing performance on BROWN development considerably, improving the f-score from 83.9% to 86.4%.

(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
(McClosky et al, 2006) presents a successful instance of parsing with self-training by using a re-ranker. $$$$$ Finally, the 1-best parses after reranking are combined with the WSJ training set to retrain the firststage parser.1 McClosky et al. (2006) find that the self-trained models help considerably when parsing WSJ.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
Recently there have been some improvements to the Charniak parser, use n-best re-ranking as reported in (Charniak and Johnson, 2005) and self training and re-ranking using data from the North American News corpus (NANC) and adapts much better to the Brown corpus (McClosky et al, 2006a; McClosky et al, 2006b). $$$$$ We use the Charniak and Johnson (2005) reranking parser in our experiments.

The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993) which consists of about 40,000 sentences (one million words) annotated with syntactic information.
The syntactic parser is the version that is self trained using 2,500,000 sentences from NANC, and where the starting version is trained only on WSJ data (McClosky et al, 2006b). $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.

McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
McClosky et al (2006b) used self-training and corpus weighting to adapt their parser trained on WSJ corpus to Browncorpus. $$$$$ The reranking parser used the WSJ-trained reranker model.

Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).
Recently, McClosky et al (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al, 2006b). $$$$$ Bacchiani et al. (2006) applies self-training to parser adaptation to utilize unlabeled in-domain data.

Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ The second technique is self-training â€” parsing unlabeled data and adding it to the training corpus.
Note that our approach is different from the self-training technique proposed in (McClosky et al, 2006a), although both methods belong to semi-supervised training category. $$$$$ To use the data from NANC, we use self-training (McClosky et al., 2006).

Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ We concentrate particularly on the work of (Gildea, 2001; Bacchiani et al., 2006) as they provide results which are directly comparable to those presented in this paper.
Note that the results of our model are not directly comparable with previous parsing results shown in (McClosky et al, 2006a), since the parsing accuracy is measured in terms of dependency relations while their results are f-score of the bracketings implied in the phrase structure. $$$$$ The results of this test are shown in Table 8.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-20001.
 $$$$$ We would like to thank the BLLIP team for their comments.
