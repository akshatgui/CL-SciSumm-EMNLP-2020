We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed.

In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011).
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ These results show that the high level categorical distributional model, uniting empirical data with logical form, can be implemented just like any other concrete model.

The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008).
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ Second Dataset Description The second dataset6, developed by the authors, follows the format of the (Mitchell and Lapata, 2008) dataset used for the first experiment, with the exception that the target and landmark verbs are transitive, and an object noun is provided in addition to the subject noun, hence forming a small transitive sentence.

Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ The point-wise multiplication of these vectors is defined as follows -+v O �-+ w = cai cbi ni -+ i The intuition behind having a matrix for a relational word is that any relation R on sets X and Y , i.e.
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008).

While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ To solve this problem, Coecke et al. −−→ milk.
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ In (Grefenstette et al., 2011), it is suggested that the simplified model we presented and expanded here could be evaluated in the same way as lexical semantic models, measuring compositionally built sentence vectors against a benchmark dataset such as that provided by Mitchell and Lapata (2008).

 $$$$$ One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors.
 $$$$$ Support from EPSRC grant EP/F042728/1 is gratefully acknowledged by M. Sadrzadeh.

Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words.
Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases.

Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments.

 $$$$$ One consequence of this parity is that the grammatical reductions of a pregroup grammar can be directly transformed into linear maps that act on vectors.
 $$$$$ Support from EPSRC grant EP/F042728/1 is gratefully acknowledged by M. Sadrzadeh.

Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ Experimental Support for a Categorical Compositional Distributional Model of Meaning
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ The other key difference is that they learn their matrices in a top-down fashion, i.e. by regression from the composite adjective-noun context vectors, whereas our model is bottom-up: it learns sentence/phrase meaning compositionally from the vectors of the compartments of the composites.
