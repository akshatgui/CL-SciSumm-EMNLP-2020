The segmentation model is similar to the one presented by Lee et al (2003), and obtains an accuracy of about 98%. $$$$$ Language Model Based Arabic Word Segmentation
The segmentation model is similar to the one presented by Lee et al (2003), and obtains an accuracy of about 98%. $$$$$ We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.

For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMMsegmenter (Lee et al, 2003). $$$$$ We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.
For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMMsegmenter (Lee et al, 2003). $$$$$ The baseline performances are obtained by assigning each token the most frequently occurring segmentation in the manually segmented training corpus.

The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003). $$$$$ Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions.
The Arabic data was preprocessed using an HMM segmenter that splits off attached prepositional phrases, personal pronouns, and the future marker (Lee et al, 2003). $$$$$ The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).

Lee et al (2003) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation. $$$$$ (Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.
Lee et al (2003) demonstrates a technique for segmenting Arabic text and uses it as a morphological processing step in machine translation. $$$$$ Our future work includes (i) application of the current technique to other highly inflected languages, (ii) application of the unsupervised stem acquisition technique on about 1 billion word unsegmented Arabic corpus, and (iii) adoption of a novel morphological analysis technique to handle irregular morphology, as realized in Arabic broken plurals YL+S (ktAb) 'book' vs. ��„�< (ktb) 'books'.

 $$$$$ This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6.
 $$$$$ We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit.

As in (Lee et al, 2003), we used unsupervised training data which is automatically segmented to discover previously unseen stems. $$$$$ We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.
As in (Lee et al, 2003), we used unsupervised training data which is automatically segmented to discover previously unseen stems. $$$$$ Unsupervised acquisition of new stems from an automatically segmented new corpus is a three-step process: (i) select new stem candidates on the basis of a frequency threshold, (ii) filter out new stem candidates containing a sub-string with a high likelihood of being a prefix, suffix, or prefix-suffix.

context sensitive Arabic stemmer (Lee et al 2003) to overcome the morphological complexity of Arabic. $$$$$ Language Model Based Arabic Word Segmentation
context sensitive Arabic stemmer (Lee et al 2003) to overcome the morphological complexity of Arabic. $$$$$ The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).

To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al, 2003). $$$$$ The input to the morpheme segmenter is a sequence of Arabic tokens – we use a tokenizer that looks only at white space and other punctuation, e.g. quotation marks, parentheses, period, comma, etc.
To separate the Arabic white-space delimited words into segments, we use a segmentation model similar to the one presented by (Lee et al, 2003). $$$$$ We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.

We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al, 2003). $$$$$ Language Model Based Arabic Word Segmentation
We propose in the following an extension to the aforementioned FST model, where we jointly determines not only diacritics but segmentation into affixes as described in (Lee et al, 2003). $$$$$ Their algorithm does not handle multiple affixes per word.

An Arabicsegmenter similar to (Lee et al, 2003) provides the segmentation features. $$$$$ The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).
An Arabicsegmenter similar to (Lee et al, 2003) provides the segmentation features. $$$$$ However, we first describe the segmentation algorithm.

This produces a segmentation view of the arabic source words (Lee et al., 2003). $$$$$ Language Model Based Arabic Word Segmentation
This produces a segmentation view of the arabic source words (Lee et al., 2003). $$$$$ (Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.

In (Lee et al, 2003) a statistical approach for Arabic word segmentation was presented. $$$$$ Language Model Based Arabic Word Segmentation
In (Lee et al, 2003) a statistical approach for Arabic word segmentation was presented. $$$$$ We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.

 $$$$$ This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6.
 $$$$$ We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit.

The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al, 2003). $$$$$ However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate's similarity to stems occurring in the training corpus.
The algorithm is inspired with the work on the segmentation of Arabic words (Lee et al, 2003). $$$$$ However, we first describe the segmentation algorithm.

Lee et al (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus. $$$$$ We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.
Lee et al (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus. $$$$$ Regardless of the manually segmented training corpus size, use of trigram language model probabilities reduces the word error rate of the corresponding baseline by approximately 50%.

Lee et al (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy. $$$$$ We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.
Lee et al (2003) show that the unsupervised use of the large corpus for stem identification increases accuracy. $$$$$ Interestingly, the segmenter developed from a 110K manually segmented corpus has the lowest percentage of “unknown stem” errors at 39.6% indicating that our unsupervised acquisition of new stems is working well, as well as suggesting to use a larger unsegmented corpus for unsupervised stem acquisition.

Lee et al (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. $$$$$ Language Model Based Arabic Word Segmentation
Lee et al (2003) addressed supervised word segmentation in Arabic and have some aspects similar to our approach. $$$$$ Our Arabic word segmentation system implementing the algorithm achieves around 97% segmentation accuracy on a development test corpus containing 28,449 word tokens.

As estimated by (Lee et al, 2003), we set the probability of ?u/k? to be 1E? 9. $$$$$ In that case, we use an “UNKNOWN” class in the trigram language model with the model probability given by p(UNKNOWN

We found that the value proposed by (Lee et al, 2003) for Arabic gives good results also for Hebrew. $$$$$ The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).
We found that the value proposed by (Lee et al, 2003) for Arabic gives good results also for Hebrew. $$$$$ The experimental results are shown in Table 5.

Moving on to Arabic, Lee et al (2003) describe a word segmentation system for Arabic that uses an n gram language model over morphemes. $$$$$ Language Model Based Arabic Word Segmentation
Moving on to Arabic, Lee et al (2003) describe a word segmentation system for Arabic that uses an n gram language model over morphemes. $$$$$ Given an Arabic sentence, we use a trigram language model on morphemes to segment it into a sequence of morphemes {m1, m2, ...,mn}.
