(Yarowsky and Ngai, 2001) aim at pos tagging a target language corpus using English pos tags as well as estimation of lexical priors. $$$$$ Section 4.2.2 will discuss the estimation of P(tilti-i)• The following section describes the estimation of P(tiiwi), which using Bayes rule and direct (relatively noise-free) measurement of P(w) from the French data, can be used to calculate P(wiiti) as: Inspection of the raw projected tag data shows the need for an improved estimation of P(t1w).
(Yarowsky and Ngai, 2001) aim at pos tagging a target language corpus using English pos tags as well as estimation of lexical priors. $$$$$ NNS(,), Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags.

Having said this, we follow in principle the algorithm proposed by (Yarowsky and Ngai, 2001) to estimate lexical priors. $$$$$ The major reason for estimating the lexical priors and tag sequence model separately is that a tag sequence bigram (or even trigram) model has far fewer parameters than the lexical prior model and thus can be estimated on a very conservatively chosen set of filtered, high confidence alignment data.
Having said this, we follow in principle the algorithm proposed by (Yarowsky and Ngai, 2001) to estimate lexical priors. $$$$$ After the lexical prior models have been trained (as above), sentences are also tested to identify those where the directly projected tag sequence (from the automatic alignments) is closely compatible with the estimated lexical prior probabilities for each word.

However, as was noted by (Yarowsky and Ngai, 2001), most words tend to have at most two pos. $$$$$ NNS(,), Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags.
However, as was noted by (Yarowsky and Ngai, 2001), most words tend to have at most two pos. $$$$$ Given the lower frequency of most content words, the potential risks of using these 1-to-n alignments are greater, but so are the benefits given that the 1-to-1 alignments tend to be both sparse and somewhat biased.

(Yarowsky and Ngai, 2001) propose the same algorithm as the one proposed here for their estimation of lexical priors, with the exception that they use automatic word alignments rather than our extraction algorithm for finding corresponding words. $$$$$ After the lexical prior models have been trained (as above), sentences are also tested to identify those where the directly projected tag sequence (from the automatic alignments) is closely compatible with the estimated lexical prior probabilities for each word.
(Yarowsky and Ngai, 2001) propose the same algorithm as the one proposed here for their estimation of lexical priors, with the exception that they use automatic word alignments rather than our extraction algorithm for finding corresponding words. $$$$$ 2The exception is for function words (i.e. the majority lexical prior is not a Noun, Verb, Adjective or Adverb) located in a 1-to-n alignment sequence.

As for (Yarowsky and Ngai, 2001) estimating lexical priors is merely an intermediate step, they do not report evaluation results for this step. $$$$$ The major reason for estimating the lexical priors and tag sequence model separately is that a tag sequence bigram (or even trigram) model has far fewer parameters than the lexical prior model and thus can be estimated on a very conservatively chosen set of filtered, high confidence alignment data.
As for (Yarowsky and Ngai, 2001) estimating lexical priors is merely an intermediate step, they do not report evaluation results for this step. $$$$$ Overall, these translingual projection results are quite encouraging.

Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al, 2001). $$$$$ Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora
Moreover, the success of joint bilingual learning may lend itself to many inherent multilingual NLP tasks such as POS tagging (Yarowsky and Ngai, 2001), name entity recognition (Yarowsky et al, 2001). $$$$$ Both corpora were word-aligned by the now publicly available EGYPT system (Al-Onaizan et al., 1999) and based on IBM's Model 3 statistical MT formalism (Brown et al., 1990).

Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. $$$$$ This further motivates the noise-robust training and stand-alone application of our current work.
Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways. $$$$$ Future work will focus on differential confidence weighting of sentence fragments, and iterative (E-M) re-estimation.

This can be seen as a rough approximation of Yarowsky and Ngai (2001). $$$$$ A stand-alone POS tagger applicable to new data can be used to improve statistical MT translation models, both by supporting finer translation model granularity (e.g. wind/NN modeled distinctly from wind/VB), and by serving as a source of backoff alignment probabilities for previously unseen words.
This can be seen as a rough approximation of Yarowsky and Ngai (2001). $$$$$ Thus tagging models induced from bilingual alignments can be used to improve these very alignments, and hence improve their own training source.

Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001). $$$$$ The data sets used for our projection studies both contained approximately 2 million words in each language.
Early studies of cross-lingual annotation projection were accomplished for lexically-based tasks; for example part-of-speech tagging (Yarowsky and Ngai, 2001). $$$$$ Their alignment was based on strictly word-based model variants for English and character-based model variants for Chinese, with no use of morphological analysis or stemming, POS-tagging, bracketing, outside dictionaries or any other external data source or annotation tool.'

The first to explore the idea were Yarowsky and Ngai (2001), who induced a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. $$$$$ French sides.
The first to explore the idea were Yarowsky and Ngai (2001), who induced a part-of-speech tagger for French and base noun phrase detectors for French and Chinese via transfer from English resources. $$$$$ Evaluation on noun-phrase bracketing showed 78% precision for Chinese, and 80% precision for English.

Yarowsky and Ngai (2001) were the first to propose the use of parallel texts to bootstrap the creation of taggers. $$$$$ Inducing Multilingual POS Taggers And NP Bracketers Via Robust Projection Across Aligned Corpora
Yarowsky and Ngai (2001) were the first to propose the use of parallel texts to bootstrap the creation of taggers. $$$$$ The primary exception has been in the area of parallel bilingual parsing.

 $$$$$ A stand-alone POS tagger applicable to new data can be used to improve statistical MT translation models, both by supporting finer translation model granularity (e.g. wind/NN modeled distinctly from wind/VB), and by serving as a source of backoff alignment probabilities for previously unseen words.
 $$$$$ Thus tagging models induced from bilingual alignments can be used to improve these very alignments, and hence improve their own training source.

In this case alignments such as English laws (NNS) to Frenchles (DT )lois (NNS) would be expected (Yarowsky and Ngai, 2001). $$$$$ NNS(,), Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags.
In this case alignments such as English laws (NNS) to Frenchles (DT )lois (NNS) would be expected (Yarowsky and Ngai, 2001). $$$$$ Finally, the issue arises of what to do with the 1-to-n phrasal alignment cases shown in Figure 2 (e.g. potatoes/NNS pommesINNS„ de/NNSb terre/NNS, and Laws/NNS Les/NNS„ /ois/NNSb).

 $$$$$ A stand-alone POS tagger applicable to new data can be used to improve statistical MT translation models, both by supporting finer translation model granularity (e.g. wind/NN modeled distinctly from wind/VB), and by serving as a source of backoff alignment probabilities for previously unseen words.
 $$$$$ Thus tagging models induced from bilingual alignments can be used to improve these very alignments, and hence improve their own training source.

Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001). $$$$$ The second limitation is the potential mismatch in the annotation needs of two languages; not all distinctions that may be desirable for one language (such as grammatical gender in French) are compatible or even present in a parallel language such as English.
Given that we have a parallel corpus where the German side overtly realizes T and V, this is a classical case of annotation projection (Yarowsky and Ngai, 2001). $$$$$ Temporarily excluding the case of compound alignments (e.g.

A technique known as annotation projection (Yarowsky and Ngai, 2001) provides a means to relax this resource bottleneck to some extent. $$$$$ The second limitation is the potential mismatch in the annotation needs of two languages; not all distinctions that may be desirable for one language (such as grammatical gender in French) are compatible or even present in a parallel language such as English.
A technique known as annotation projection (Yarowsky and Ngai, 2001) provides a means to relax this resource bottleneck to some extent. $$$$$ Overall, these translingual projection results are quite encouraging.

Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al, 851 2002). $$$$$ Previously, tools for automatic wordalignment of bilingual corpora were not widely available outside IBM, the research group pioneering statistical machine translation with the Candide system (Brown et al, 1990).
Previous research on resource projection attempts to address these problems by redistributing the parameter values (Yarowsky and Ngai, 2001) or by applying transformation rules (Hwa et al, 851 2002). $$$$$ Both corpora were word-aligned by the now publicly available EGYPT system (Al-Onaizan et al., 1999) and based on IBM's Model 3 statistical MT formalism (Brown et al., 1990).

Following the work of Yarowsky and Ngai (2001) we focus on the task of training a Part-of-Speech (POS) tagger, but we conduct our experiments with the more dissimilar language pair of English Chinese instead of English-French. $$$$$ The data used in our experiments are the EnglishFrench Canadian Hansards and English-Chinese Hong Kong Hansards, parallel records of parliamentary proceedings and publications.
Following the work of Yarowsky and Ngai (2001) we focus on the task of training a Part-of-Speech (POS) tagger, but we conduct our experiments with the more dissimilar language pair of English Chinese instead of English-French. $$$$$ As previously noted, the goal of this work is not to induce potential French tagset features such as grammatical gender, mood or subtle tense distinctions that do not appear in English, but to focus on the algorithm's effectiveness at accurately transferring tagging capabilities at the granularity that is present in English (or whichever projection source language used).

One method of acquiring a large corpus of automatically POS tagged Chinese data is by projection (Yarowsky and Ngai, 2001). $$$$$ Doing so salvages very large quantities of otherwise accurate tag sequence data with very little introduced noise.
One method of acquiring a large corpus of automatically POS tagged Chinese data is by projection (Yarowsky and Ngai, 2001). $$$$$ For independent evaluation data, a 120K-word hand-tagged French dataset generously provided by Universite de Montreal was used.

Following Yarowsky and Ngai (2001), we define 12 equivalence classes over the 47 Penn-English Treebank POS tags. $$$$$ NNS(,), Table 1 shows the observed frequency distributions of English tags projected onto four French words from 1-to-1 alignments, for the core N/V/J/R/I POS tags.
Following Yarowsky and Ngai (2001), we define 12 equivalence classes over the 47 Penn-English Treebank POS tags. $$$$$ The second is at the level of granularity captured in the English Penn Treebank tagset, where for example singular and plural nouns (NN and NNS) are distinguished.
