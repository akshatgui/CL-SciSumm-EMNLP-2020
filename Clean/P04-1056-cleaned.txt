We also provide the results from Bunescu and Mooney (2004) for comparison. $$$$$ However this second approach has the advantage of creating fewer potential nodes in the document factor graph, which results in faster inference.
We also provide the results from Bunescu and Mooney (2004) for comparison. $$$$$ The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).

In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. $$$$$ In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF's were found to significantly out-perform competing techniques.
In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. $$$$$ For example, in our protein-tagging task, repeated references to the same protein are common.

The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. $$$$$ Relational Markov Networks (RMN's) (Taskar et al., 2002) are a generalization of CRF's that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them.
The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. $$$$$ We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).

Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. $$$$$ We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.
Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. $$$$$ Relational Markov Networks (RMN's) (Taskar et al., 2002) are a generalization of CRF's that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them.

The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005). $$$$$ One of the current best empirical approaches to IE is conditional random fields (CRF's) (Lafferty et al., 2001).
The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005). $$$$$ The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).

 $$$$$ The set of edges is created by matching clique templates against the entire set of entities d.E.
 $$$$$ This work was partially supported by grants IIS-0117308 and IIS-0325116 from the NSF.

We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. $$$$$ We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).
We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. $$$$$ Regarding future work, a richer set of features for the local templates would likely improve performance.

Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).
Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ Another limitation is the approximate inference used by both RMN methods.

In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ Experiments on human protein tagging demonstrate the advantages of collective extraction on several annotated corpora of Medline abstracts.
In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction.

Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). $$$$$ In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF's were found to significantly out-perform competing techniques.
Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). $$$$$ Another limitation is the approximate inference used by both RMN methods.

In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).
In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).

However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. $$$$$ For example, in our protein-tagging task, repeated references to the same protein are common.
However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. $$$$$ Repeat Template (RT): If multiple entities in the same document are repetitions of the same name, their labels tend to have the same value (i.e. most of them are protein names, or most of them are not protein names).

See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ There are cases where different factor graphs may yield the same underlying RMN graph, which makes the factor graph representation preferable.
See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ The number of factor graphs for which the sum-product algorithm did not converge was non-negligible, and our approach stopped after a fix number of iterations.

 $$$$$ The set of edges is created by matching clique templates against the entire set of entities d.E.
 $$$$$ This work was partially supported by grants IIS-0117308 and IIS-0325116 from the NSF.

Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ Collective Information Extraction With Relational Markov Networks
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ As typically applied, CRF's, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation.

Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF's were found to significantly out-perform competing techniques.
Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).

Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). $$$$$ Collective Information Extraction With Relational Markov Networks
Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). $$$$$ The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).
