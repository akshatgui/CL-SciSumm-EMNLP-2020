This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser. $$$$$ Table 2 shows several examples of the system output, computed with Equation (2), in which ?+?
This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by the MINIPAR parser. $$$$$ 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu ra cy Window 1 Window 2 Window 3 Window 4 0.3 0.4 0.5 0.6 0.7 0.8 0.9 m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4 ac cu rac y Window 1 Window 2 Window 3 Window 4 Human 1

(signed integers representing positive and negative feelings) (Kim and Hovy, 2004). $$$$$ We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good?
(signed integers representing positive and negative feelings) (Kim and Hovy, 2004). $$$$$ negative.

In particular, they have been an essential ingredient for fine grained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al (2005)). $$$$$ (Wiebe et al 2002; Riloff et al 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives.
In particular, they have been an essential ingredient for fine grained sentiment analysis (e.g., Kim and Hovy (2004), Kennedy and Inkpen (2005), Wilson et al (2005)). $$$$$ The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists?positive and negative?and then to grow this by adding words obtained from WordNet (Miller et al 1993; Fellbaum et al 1993).

Kim and Hovy (2004) try to determine the final sentiment orientation of a given sentence by combining sentiment words within it. $$$$$ 2.2 Sentence Sentiment Classifier.
Kim and Hovy (2004) try to determine the final sentiment orientation of a given sentence by combining sentiment words within it. $$$$$ 3.2 Sentence Sentiment Classifier.

 $$$$$ Antonyms of negative words are added to the positive list, and synonyms to the negative one.
 $$$$$ Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort.

 $$$$$ Antonyms of negative words are added to the positive list, and synonyms to the negative one.
 $$$$$ Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort.

In separate qualitative experiments done by Pang et al (2002), 97 Wilson et al (2005) and Kim and Hovy (2004), the agreement between human judges when given a list of sentiment-bearing words is as low as 58% and no higher than 76%. $$$$$ (Wiebe et al 2002; Riloff et al 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives.
In separate qualitative experiments done by Pang et al (2002), 97 Wilson et al (2005) and Kim and Hovy (2004), the agreement between human judges when given a list of sentiment-bearing words is as low as 58% and no higher than 76%. $$$$$ The basic approach is to assemble a small amount of seed words by hand, sorted by polarity into two lists?positive and negative?and then to grow this by adding words obtained from WordNet (Miller et al 1993; Fellbaum et al 1993).

Kim and Hovy (2004) start with two lists of positive and negative seed words. $$$$$ To start the seed lists we selected verbs (23 positive and 21 negative) and adjectives (15 positive and 19 negative), adding nouns later.
Kim and Hovy (2004) start with two lists of positive and negative seed words. $$$$$ For each seed word, we extracted from WordNet its expansions and added them back into the appropriate seed lists.

Kim and Hovy (2004) found the polarity of subjective expressions. $$$$$ A more sophisticated approach would employ a parser to identify syntactic relationships between each Holder and all dependent expressions of sentiment.
Kim and Hovy (2004) found the polarity of subjective expressions. $$$$$ For this model, we also included negation words such as not and never to reverse the sentiment polarity.

 $$$$$ Antonyms of negative words are added to the positive list, and synonyms to the negative one.
 $$$$$ Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort.

The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others. $$$$$ We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good?
The system of Kim and Hovy (2004) tackles orientation detection by attributing, to each term, a positivity score and a negativity score; interestingly, terms may thus be deemed to have both a positive and a negative correlation, maybe with different degrees, and some terms may be deemed to carry a stronger positive (or negative) orientation than others. $$$$$ negative.

This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. $$$$$ We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good?
This hypothesis is confirmed by an experiment performed by Kim and Hovy (2004) on testing the agreement of two human coders at tagging words with the Positive, Negative, and Objective labels. $$$$$ Table 4 shows inter-human agreement.

Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. $$$$$ Given a new word, we use WordNet again to obtain a synonym set of the unseen word to determine how it interacts with our sentiment seed lists.
Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. $$$$$ 3.3.1 Word Sentiment Classification As mentioned, some words have both strong positive and negative sentiment.

The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). $$$$$ (implicit) In this paper we address the following challenge problem.
The lexicons are generated from manually selected seeds for a broad domain such as Health or Business, following an approach similar to (Kim and Hovy, 2004). $$$$$ In Table 5, the seed list included just a few manually selected seed words (23 positive and 21 negative verbs and 15 and 19 adjectives, repectively).

Kim and Hovy (2004), among others, have combined the two tasks, identifying subjective text and detecting its sentiment polarity. $$$$$ Identifying sentiments (the affective parts of opinions) is a challenging problem.
Kim and Hovy (2004), among others, have combined the two tasks, identifying subjective text and detecting its sentiment polarity. $$$$$ Although relatively easy task for people, detecting an opinion holder is not simple either.

Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. $$$$$ We assume synonyms of positive words are mostly positive and antonyms mostly negative, e.g., the positive word ?good?
Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. $$$$$ Antonyms of negative words are added to the positive list, and synonyms to the negative one.

However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance. There are only two prior approaches addressing word sense subjectivity or polarity classification. $$$$$ 2.1.1 Word Classification Models For word sentiment classification we developed two models.
However, Kim and Hovy (2004) and Andreevskaia and Bergler (2006) also address the classification into subjective/objective words and show this to be a potentially harder task than polarity classification with lower human agreement and automatic performance. There are only two prior approaches addressing word sense subjectivity or polarity classification. $$$$$ The classification task is defined as assigning each word to one of three categories

Kim and Hovy proposed two probabilistic models to estimate the strength of polarity (Kim and Hovy, 2004). $$$$$ Armed with such a measure, we can also assign strength of sentiment polarity to as yet unseen words.
Kim and Hovy proposed two probabilistic models to estimate the strength of polarity (Kim and Hovy, 2004). $$$$$ The absolute value of each category represents the strength of its sentiment polarity.

However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. $$$$$ Table 4 shows inter-human agreement.
However, Kim and Hovy (2004) and Andreevskaiaand Bergler (2006) show that subjectivity recognition might be the harder problem with lower human agreement and automatic performance. $$$$$ The system achieves lower agreement than humans but higher than the random process.

 $$$$$ Antonyms of negative words are added to the positive list, and synonyms to the negative one.
 $$$$$ Nonetheless, as the experiments show, encouraging results can be obtained even with relatively simple models and only a small amount of manual seeding effort.
