In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ Remarkably good performance may be achieved by identifying a single defining collocate for each class (e.g. bird and machine for the word crane), and using for seeds only those contexts containing one of these words.
In comparison, (Yarowsky, 1995) achieved 91.4% correct performance, using 1380 contexts and the dictionary definitions in training. $$$$$ Comparative performance

Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ This provides a mechanism for bootstrapping a sense tagger.
Recently, Yarowsky (1995) combined a MIlD and a corpus in a bootstrapping process. $$$$$ The details of this process are discussed in Section 7.

Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ The training sets (e.g.
Another line of research is to pick up some high-quality auto-parsed training instances from unlabeled data using bootstrapping methods, such as self-training (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and tri-training (Zhou and Li, 2005). $$$$$ This helps improve the purity of the training data.

Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).
Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. $$$$$ In general, the decision-list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below.

This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ For these words, the table below measures the claim's accuracy (when the word occurs more than once in a discourse, how often it takes on the majority sense for the discourse) and applicability (how often the word does occur more than once in a discourse).
This heuristic naturally reflects the broadly known assumption about lexical ambiguity presented in (Yarowsky, 1995), namely the one-sense-per-discourse heuristic. $$$$$ They have been displaced by more broadly applicable collocations that better partition the newly learned classes.

This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). $$$$$ The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure.
This heuristic mimes the one-sense-per collocation heuristic presented in (Yarowsky, 1995). $$$$$ The strong tendency for words to exhibit only one sense in a given collocation was observed and quantified in (Yarowsky, 1993).

These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ While not fully automatic, this approach yields rich and highly reliable seed sets with minimal work.
These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. $$$$$ This approach is least successful for senses with a complex concept space, which cannot be adequately represented by single words.

Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ Similarly, the one-sense-per-discourse constraint may also be used to correct erroneously labeled examples.
Yarowsky (1995) presented an approach that significantly reduces the amount of labeled data needed forword sense disambiguation. $$$$$ This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary.

Many of these tasks have been addressed in other fields, for example, hypothesis verification in the field of machine translation (Tran et al, 1996), sense disambiguation in speech synthesis (Yarowsky, 1995), and relation tagging in information retrieval (Marsh and Perzanowski, 1999). $$$$$ For example

Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ Unsupervised Word Sense Disambiguation Rivaling Supervised Methods
Yarowsky (1995) used both supervised and unsupervised WSD for correct phonetizitation of words in speech synthesis. $$$$$ Similarly, the one-sense-per-discourse constraint may also be used to correct erroneously labeled examples.

The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). $$$$$ In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.
The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). $$$$$ Thus the noise introduced by a few irrelevant or misleading seed words is not fatal.

This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ In a large corpus, identify all examples of the given polysemous word, storing their contexts as lines in an initially untagged training set.
This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). $$$$$ Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation.

See Yarowsky (1995) for details. $$$$$ Details are provided in (Yarowsky, to appear). encountered in other frameworks.
See Yarowsky (1995) for details. $$$$$ The details of this process are discussed in Section 7.

The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. $$$$$ The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992).
The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. $$$$$ In brief, if several instances of the polysemous word in a discourse have already been assigned SENSE-A, this sense tag may be extended to all examples in the discourse, conditional on the relative numbers and the probabilities associated with the tagged examples.

Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ Unsupervised Word Sense Disambiguation Rivaling Supervised Methods
Yarowsky (1995) first recognized that it is possible to use a small number of features for different senses to bootstrap an unsupervised word sense disambiguation system. $$$$$ For each possible sense of the word, identify a relatively small number of training examples representative of that sense.'

Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. $$$$$ A supervised algorithm based on this property is given in (Yarowsky, 1994).
Self-training (Yarowsky, 1995) is a semi supervised algorithm which has been well studied in the NLP area and gained promising result. $$$$$ She trained her fully supervised algorithm on hand-labelled sentences, applied the result to new data and added the most confidently tagged examples to the training set.

The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ Unsupervised Word Sense Disambiguation Rivaling Supervised Methods
The algorithm proposed by Yarowsky (1995) for the problem of word sense disambiguation has been cited as the origination of self-training. $$$$$ The algorithm uses these properties to incrementally identify collocations for target senses of a word, given a few seed collocations 'Note that the problem here is sense disambiguation

For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ In general, the decision-list algorithm is well suited for the task of sense disambiguation and will be used as a component of the unsupervised algorithm below.
For the fine-grained track, it achieves 2nd place after that of Tugwell and Kilgarriff (2001), which used a decision list (Yarowsky, 1995) on manually selected corpora evidence for each inventory sense, and thus is not subject to loss of distinguishability in the glosses as Lesk variants are. $$$$$ When this decision list is applied to a new test sentence, ... the loss of animal and plant species through extinction ... , the highest ranking collocation found in the target context (species) is used to classify the example as SENSE-A (a living plant).

Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ A supervised algorithm based on this property is given in (Yarowsky, 1994).
Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea,2002). $$$$$ IIf their classification begins to waver because new examples have discredited the crucial collocate, they are returned to the residual and may later be classified differently.

Self-training (Yarowsky, 1995) is a form of semi-supervised learning. $$$$$ A supervised algorithm based on this property is given in (Yarowsky, 1994).
Self-training (Yarowsky, 1995) is a form of semi-supervised learning. $$$$$ This augmentation of the training data can often form a bridge to new collocations that may not otherwise co-occur in the same nearby context with previously identified collocations.
