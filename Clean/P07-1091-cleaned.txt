Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ It should be noted that the second Chinese block “R#, HI” and its English counterpart “at the end of” are not constituents at all.
Li et al (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents. $$$$$ A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).

More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ Maximum Entropy (ME) Model, which does the binary classification whether a binary node’s children are inverted or not, based on a set of features over the SL phrases corresponding to the two children nodes.
More recently, Li et al (2007) use a maximum entropy system to learn reordering rules for binary trees (i.e., whether to keep or reorder for each node). $$$$$ The maximum entropy model has the same form as in the binary case, except that there are more classes of reordering patterns as n increases.

This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ The most notable ones are (Xia and McCord, 2004) and (Collins et al., 2005), both of which make use of linguistic syntax in the preprocessing stage.
This is one of the main problems addressed in the present work. (Li et al, 2007) use weighted n-best lists as in put for the decoder. $$$$$ A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reordering and other factors of translation.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).

In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ We do not adopt these models since a lot of subtle issues would then be introduced due to the complexity of syntax-based decoder, and the impact of syntax on reordering will be difficult to single out.
In syntax-based method, word reordering is implicitly addressed by translation rules, thus the performance is subject to parsing errors to a large extent (Zhang et al, 2007a) and the impact of syntax on reordering is difficult to single out (Li et al, 2007). $$$$$ By and large, the experiment results show that no matter what kind of reordering knowledge is used, the preprocessing of syntax-based reordering does greatly improve translation performance, and that the reordering of 3-ary nodes is crucial.

This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ A terminological remark: In the rest of the paper, we will use the terms global reordering and local reordering in place of long-distance reordering and short-distance reordering respectively.
This paper follows the term convention of global reordering and local reordering of Li et al (2007), between which the distinction is solely defined by reordering distance (whether beyond four source words) (Li et al, 2007). $$$$$ The distinction between long and short distance reordering is solely defined by distortion limit.

Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ However, there are also reorderings which do not agree with syntactic analysis.
Rules created from a syntactic parser are also utilized to form weighted n-best lists which are fed into the decoder (Li etal., 2007). $$$$$ A variety of reordered SL sentences are fed to the decoder so that the decoder can consider, to certain extent, the interaction between reordering and other factors of translation.

To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).
To overcome the problem, Li et al (2007) proposed k-best approach. $$$$$ From this point, the proposed preprocessing model correctly jump to the last phrase “Ili T ÿX/discussed”, while the baseline model fail to do so for the best translation.

A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.
A negative consequence of source order (SO) scoring as done by (Zhang et al, 2007) and (Li et al, 2007) is that they bias against the valuable phrase internal reorderings by only promoting the source sentence reordering. $$$$$ A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).

Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ Maximum Entropy (ME) Model, which does the binary classification whether a binary node’s children are inverted or not, based on a set of features over the SL phrases corresponding to the two children nodes.
Li et al (2007) modeled reordering on parse tree nodes by using a maximum entropy model with surface and syntactic features for Chinese-to-English translation. $$$$$ Our experiments are about Chinese-to-English translation.

Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ A notable approach is lexicalized reordering (Koehn et al., 2005) and (Tillmann, 2004).
Li et al (2007) used N-best reordering hypotheses to overcome the reordering ambiguity. $$$$$ (Case-sensitive) BLEU-4 (Papineni et al., 2002) is used as the evaluation metric.

Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ However, long-distance reordering is problematic in phrase-based SMT.
Syntax-based SMT is better suited to cope with long-distance dependencies, however there also are problems, some of them originated from the linguistic motivation itself, incorrect parse trees, or reordering that might involve blocks that are not constituents (Li et al, 2007). $$$$$ Therefore, while short-distance reordering is under the scope of the distance-based model, long-distance reordering is simply out of the question.

 $$$$$ To avoid this problem, we give up using rewriting patterns and design a form of reordering knowledge which can be directly applied to parse tree nodes.
 $$$$$ Shallow parsers should be tried to see if they improve the quality of reordering knowledge.

Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ Our method is inspired by previous preprocessing approaches like (Xia and McCord, 2004), (Collins et al., 2005), and (Costa-juss`a and Fonollosa, 2006), which split translation into two stages: where a sentence of the source language (SL), S, is first reordered with respect to the word order of the target language (TL), and then the reordered SL sentence S' is translated as a TL sentence T by monotonous translation.
Li et al (2007) used a parser to get the syntactic tree of the source language sentence. $$$$$ The GIGAWORD corpus is used for training language model.
