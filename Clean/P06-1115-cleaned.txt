KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mappingphrases of natural language (NL) sentences to semantic concepts in a meaning representation lan guage (MRL).
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ We compared our system?s performance with the following existing systems

For details please refer to (Kate and Mooney, 2006). $$$$$ Previous work on learning semantic parsers either employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta tistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).
For details please refer to (Kate and Mooney, 2006). $$$$$ We compared our system?s performance with the following existing systems

Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ The more the two strings share, the greater the similarity score will be.
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ ulate this type of noise by substituting a word in the corpus by another word, w, with probability ped(w)?P (w), where p is a parameter, ed(w) isw?s edit distance (Levenshtein, 1966) from the original word and P (w) is w?s probability proportional toits word frequency.

We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ 2.2 Most Probable Semantic Derivation.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ The results are also similar on the GEOQUERY corpus.

We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ Using String-Kernels For Learning Semantic Parsers
We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ KRISP does semantic parsing using the notion of a semantic derivation of an NL sentence.

We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ Previous work on learning semantic parsers either employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta tistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ We compared our system?s performance with the following existing systems

KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers.
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ One difference, however, is that their strings are over characters while our strings are over words.

KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ In the next subsection we will de scribe how KRISP obtains these probabilities using string-kernel based SVM classifiers.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ We compared our system?s performance with the following existing systems

The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ The productions of the formal MRL grammar are treated like semantic concepts.
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ We compared our system?s performance with the following existing systems

To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ The productions of the formal MRL grammar are treated like semantic concepts.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ We compared our system?s performance with the following existing systems

We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ Previous work on learning semantic parsers either employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta tistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ KRISP does semantic parsing using the notion of a semantic derivation of an NL sentence.

KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ In contrast, kernel methods allow a convenient mechanism to implicitly work with a potentially infinite number of features which can robustly capture these range of contexts even when the data is noisy.Our system, KRISP (Kernel-based Robust Interpretation for Semantic Parsing), takes NL sentences paired with their formal meaning representations as training data.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ A learn ing system for semantic parsing is given a trainingcorpus of NL sentences paired with their respec tive MRs from which it has to induce a semantic parser which can map novel NL sentences to their correct MRs. Figure 1 shows an example of an NL sentence and its MR from the CLANG domain.

Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.
Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.

 $$$$$ Instead of non-terminals, productions are shown in the nodes to emphasize the role of productions in semantic derivations.
 $$$$$ AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.
