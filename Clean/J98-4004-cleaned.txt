Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ The value of 1.4 can diverge from f, just like the other estimates.
Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ In order to better understand just why the parent annotation transform performs so much better than the other transforms, transformation/detransformation experiments were performed in which the parent annotation transform was performed selectively either on all nodes with a given category label, or all nodes with a given category label and parent category label.

Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ The theory of PCFGs is described elsewhere (e.g., Charniak [1993]), so it is only summarized here.
Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ Charniak and Carroll (1994) describe this transformation as adding &quot;pseudo context-sensitivity&quot; to the language model because the distribution of expansions of a node depends on nonlocal context, viz,, the category of its parent.3 This nonlocal information is sufficient to distinguish the upper and lower NPs in the structures considered here.

See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. $$$$$ The labeled precision and recall scores for the Flatten and Parent transforms differed significantly from each other and also from the Id transform at the 0.01 level, while neither the NP-VP nor the N'-V' transform differed significantly from each other or the Id transform at the 0.1 level.
See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. $$$$$ The rows labeled NP attachments and VP attachments provide the number of times the following tree schema, which As expected, the PCFGs induced from the output of the Flatten transform and Parent transform significantly improve precision and recall over the original treebank PCFG (i.e., the PCFG induced from the output of the Id transform).

First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ Only subtrees consisting of a parent node labeled NP whose first child is also labeled NP are affected by this transformation.
First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ In order to better understand just why the parent annotation transform performs so much better than the other transforms, transformation/detransformation experiments were performed in which the parent annotation transform was performed selectively either on all nodes with a given category label, or all nodes with a given category label and parent category label.

 $$$$$ For example, suppose our corpora only contain two trees, both of which have yields V Det N P Det N, are always analyzed as a VP with a direct object NP and a PP, and differ only as to whether the PP modifies the NP or the VP.
 $$$$$ SBR-9720368 and SBR-9812169.

These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ Both the Flatten and Parent transforms induced PCFGs that have substantially more productions than the original treebank grammar, perhaps reflecting the fact that they encode more contextual information than the original treebank grammar, albeit in different ways.
These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ Thus it is possible that the performance gains achieved by the parent annotation transform have little to do with PP attachment.

Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ Parent appends to each nonroot nonterminal node's label its parent's category.
Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ Both the Flatten and Parent transforms induced PCFGs that have substantially more productions than the original treebank grammar, perhaps reflecting the fact that they encode more contextual information than the original treebank grammar, albeit in different ways.

It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). $$$$$ It is well known that natural language exhibits dependencies that context-free grammars (CFGs) cannot describe (Culy 1985; Shieber 1985).
It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). $$$$$ This paper has presented theoretical and empirical evidence that the choice of tree representation can make a significant difference to the performance of a PCFG-based parsing system.

Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ The previous subsection showed that inserting additional nodes into the tree structure can result in a PCFG language model that better models the distribution of trees in the training corpus.
Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ For example, the node labels used in the PCFG induced from trees produced by applying the parent transform are pairs of categories from the original Penn II WSJ tree bank, and so the labeled precision and recall measures obtained by comparing the parse trees obtained using this PCFG with the trees from the tree bank would be close to zero.

 $$$$$ For example, suppose our corpora only contain two trees, both of which have yields V Det N P Det N, are always analyzed as a VP with a direct object NP and a PP, and differ only as to whether the PP modifies the NP or the VP.
 $$$$$ SBR-9720368 and SBR-9812169.

Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ More specifically, this paper studies the effect of varying the tree structure representation of PP modification from both a theoretical and an empirical point of view.
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ The approach developed here overcomes this problem by applying an additional tree transformation step that converts the parse trees produced using the PCFG back to the Penn II tree representations, and compares these trees to the held-out test trees using the labeled precision and recall trees.

 $$$$$ For example, suppose our corpora only contain two trees, both of which have yields V Det N P Det N, are always analyzed as a VP with a direct object NP and a PP, and differ only as to whether the PP modifies the NP or the VP.
 $$$$$ SBR-9720368 and SBR-9812169.

The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ (Note that this is not a criticism of the use of this representation in a treebank, but of modeling such a representation with a PCFG).
The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ It is straightforward to use the PCFG estimation techniques described in Section 2 to estimate PCFGs from the result of applying these transformations to sections 221 of the Penn II WSJ corpus.

In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ PCFG Models Of Linguistic Tree Representations
In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ What makes a tree representation a good choice for PCFG modeling seems to be quite different to what makes it a good choice for a representation of a linguistic theory.

Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. $$$$$ The &quot;parent annotation&quot; transform discussed below, which appends the category of a parent node onto the label of all of its nonterminal children as sketched in Figure 2, has just this effect.
Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. $$$$$ Parent appends to each nonroot nonterminal node's label its parent's category.

To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). $$$$$ The Chomsky adjunction representations are motivated within the theoretical framework of Transformational Grammar, which explicitly argues for nonlocal, indeed, non-context-free, dependencies.
To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). $$$$$ For example, I am currently using this methodology to study the interaction between tree structure and a &quot;slash category&quot; node labeling in tree representations with empty categories (Gazdar et al. 1985).

Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ Thus a PCFG language model induced from the simple two-tree corpus above can underestimate the relative frequency of NP attachment by a factor of more than 3.

A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ The previous section presented theoretical evidence that varying the tree representations used to estimate a PCFG language model can have a noticeable impact on that model's performance.
A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ While the resulting parse trees can be compared to the trees in the test corpus using the precision and recall measures described above, the results would not be meaningful as the parse trees reflect a different tree representation to that used in the test corpus, and thus are not directly comparable with the test corpus trees.

As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ More specifically, this paper studies the effect of varying the tree structure representation of PP modification from both a theoretical and an empirical point of view.
As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ The effect of this transformation is to produce trees of the kind discussed in Section 4.4.

Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998). $$$$$ For example, it might be that several different Penn II tree representations can correspond to a single parse tree, as is the case with a parser producing flattened tree representations.
Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998). $$$$$ A tree transformation/detransformation methodology for empirically evaluating the effect of different tree representations on parsing systems was developed in this paper.
