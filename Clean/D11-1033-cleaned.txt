In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LM and Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ We apply this criterion for the first time to the task of selecting training data for machine translation systems.

Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010).
Axelrod et al (2011) improved the perplexity based approach and proposed bilingual cross entropy difference as a ranking function with in- and general-domain language models. $$$$$ Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.

 $$$$$ We furthermore extend this idea for MT-specific purposes.
 $$$$$ The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.

This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ The conventional wisdom is that more data is better; the larger the training corpus, the more accurate the model can be.
This would empirically provide more accurate lexical probabilities, and thus better match the translation task at hand (Axelrod et al., 2011). $$$$$ This would empirically provide more accurate lexical probabilities, and thus better target the task at hand.

Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy.
Axelrod et al (2011) proposed a bilingual cross-entropy difference to select data from parallel corpus for domain adaptation which captures the contextual information slightly, and outperformed monolingual cross-entropy difference (Moore and Lewis, 2010), which first shows the advantage of bilingual data selection. $$$$$ We consider three methods for extracting domaintargeted parallel data from a general corpus

We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).
We experimented with two different types of sub sampling techniques - Model 1, similar to that used by Schwenk et al (2011), and modified Moore Lewis (Axelrod et al, 2011) - for the language pairs es-en, en-es, fr-en and en-fr. $$$$$ The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010).

 $$$$$ We furthermore extend this idea for MT-specific purposes.
 $$$$$ The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.

 $$$$$ We furthermore extend this idea for MT-specific purposes.
 $$$$$ The maximum size of a useful general-domain corpus is now limited only by the availability of data, rather than by how large a translation model can be fit into memory at once.
