Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).
Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ portion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al, 1999).

In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ The parser described in this paper is similar to that of Yamada and Matsumoto (2003) in that it uses a deterministic parsing algorithm in combination with a classifier induced from a treebank.
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.

It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ Deterministicmethods for dependency parsing have now been ap plied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).

Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ 3 Memory-Based Learning.
Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ Memory-based learning and problem solving is based on two fundamental principles

To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP.
To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.

Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ This also means that the time complexity of the algorithm used here is linearin the size of the input, while the algorithm of Ya mada and Matsumoto is quadratic in the worst case.
Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ words that are analyzed as such (Yamada and Matsumoto, 2003).

For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ 3 Memory-Based Learning.
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).

We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).
We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).

 $$$$$ TR.POS . . .
 $$$$$ The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.

The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).

This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ n onto the stack, giving the configuration ?n

Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).
Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ Thesesettings are the result of extensive experiments partially reported in Nivre et al (2004).

Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the token on top of the stack, n is the next input token, w is any word, and r, r?
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ The two central elements in any configuration are the token on top of the stack (T) and the next input token(N), the tokens which may be connected by a de pendency arc in the next configuration.

Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ Deterministic Dependency Parsing Of English Text
Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004).

We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ On the other hand, the best available parsers trained on thePenn Treebank, those of Collins (1997) and Charniak (2000), use statistical models for disambigua tion that make crucial use of dependency relations.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.

That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003).The parsing algorithm used here was first de fined for unlabeled dependency parsing in Nivre (2003) and subsequently extended to labeled graphsin Nivre et al (2004).
That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP.

The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ Deterministicmethods for dependency parsing have now been ap plied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004).
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ For English, the interest in dependency parsing has been weaker than for other languages.

Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ whose domain is a finite space of parser states, which are abstractions over configurations.
Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ Finally, we use a lookahead of three tokens, considering only their parts-of-speech.

Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ For this purpose we define a number of features that can be used to define different models of parser state.
Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ Figure 2 illustrates the features that are used to define parser states in the present study.

There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.
There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.
