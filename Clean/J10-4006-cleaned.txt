We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ In our experiments we do not make use of the contexts of the target word pairs that are provided with the test set.

Several approaches have investigated the above mentioned problem $$$$$ Distributional Memory

Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Distributional Memory

Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.

More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ As in structured DSMs, we adopt word–link–word tuples as the most suitable way to capture distributional facts.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.

The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ These terms were selected based on their frequency in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000 most frequent verbs and adjectives), augmenting the list with lemmas that we found in various standard test sets, such as the TOEFL and SAT lists.

We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word– link–word tuples.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ This is essentially the same task as the TOEFL, but applied to word pairs instead of words.

In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Bootstrapped confidence intervals are obtained as described in Section 6.1.3.

 $$$$$ For example, there is a sbj intr−1 link between an intransitive verb and its subject

Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.

We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ In all cases, we compare the performance of several DM implementations to state-of-the-art results.
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ In many of the experiments herein, DM is not only compared to the results available in the literature, but also to our implementation of state-of-the-art DSMs.

LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ automated generation of commonsense concept descriptions in terms of intuitively salient properties

A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Distributional Memory

Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ See Baroni et al. (2010) for the full list.

we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ The weights assigned to the tuples by the scoring function σ are given by Local Mutual Information (LMI) computed on the raw corpus-derived word–link–word cooccurrence counts.

The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ TypeDM is again our best model.

Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ We refer to it as the centroid of the vectors.

Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ Coherent with this approach, we make our best DM model (TypeDM) publicly available from http

The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ See Baroni et al. (2010) for the full list.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus 1998) consists of 100 noun–verb pairs rated by 36 subjects.

Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ In all cases, we compare the performance of several DM implementations to state-of-the-art results.
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
