We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ In our experiments we do not make use of the contexts of the target word pairs that are provided with the test set.

Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. $$$$$ Distributional Memory: A General Framework for Corpus-Based Semantics
Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. $$$$$ A general framework for distributional semantics should satisfy the following two requirements: (1) representing corpus-derived data in such a way as to capture aspects of meaning that have so far been modeled with different, prima facie incompatible data structures; (2) using this common representation to address a large battery of semantic experiments, achieving a performance at least comparable to that of state-of-art, taskspecific DSMs.

Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Distributional Memory: A General Framework for Corpus-Based Semantics
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.

Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.

More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ As in structured DSMs, we adopt word–link–word tuples as the most suitable way to capture distributional facts.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ The labeled tensor is nothing other than a formalization of distributional data extracted in the word–link–word–score format, which is customary in many structured DSMs.

The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ These terms were selected based on their frequency in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000 most frequent verbs and adjectives), augmenting the list with lemmas that we found in various standard test sets, such as the TOEFL and SAT lists.

We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word– link–word tuples.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ This is essentially the same task as the TOEFL, but applied to word pairs instead of words.

In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Bootstrapped confidence intervals are obtained as described in Section 6.1.3.

 $$$$$ For example, there is a sbj intr−1 link between an intransitive verb and its subject: (talk, sbj intr−1, soldier).
 $$$$$ This separation is in line with what is commonly assumed in cognitive science and formal linguistics, and we hope it will contribute to make corpus-based modeling a core part of the ongoing study of semantic knowledge in humans and machines.

Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.

We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ In all cases, we compare the performance of several DM implementations to state-of-the-art results.
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ In many of the experiments herein, DM is not only compared to the results available in the literature, but also to our implementation of state-of-the-art DSMs.

LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ automated generation of commonsense concept descriptions in terms of intuitively salient properties: a dog is a mammal, it barks, it has a tail, and so forth (Almuhareb 2006; Baroni and Lenci 2008; Baroni, Evert, and Lenci 2008; Baroni et al. 2010).

A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Distributional Memory: A General Framework for Corpus-Based Semantics
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.

Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ See Baroni et al. (2010) for the full list.

we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ The weights assigned to the tuples by the scoring function σ are given by Local Mutual Information (LMI) computed on the raw corpus-derived word–link–word cooccurrence counts.

The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ TypeDM is again our best model.

Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ We refer to it as the centroid of the vectors.

Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ Coherent with this approach, we make our best DM model (TypeDM) publicly available from http://clic.cimec.unitn.it/dm.
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ TypeDM is again our best model.

The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ See Baroni et al. (2010) for the full list.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ The McRae data set (McRae, Spivey-Knowlton, and Tanenhaus 1998) consists of 100 noun–verb pairs rated by 36 subjects.

Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ In all cases, we compare the performance of several DM implementations to state-of-the-art results.
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).
