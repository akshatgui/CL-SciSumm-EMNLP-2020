In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ The problem of estimating parameters for log-linear models is not new.

The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ Table 1 summarizes the basic properties of these corpora.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ Table 2 describes the properties of the features used for each corpus.

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992).
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible.

For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).
For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.

To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ This is an instance of over fitting, and it can be addressed, as is customary, by adding a regularization term that promotes small values of 0 to the objective function.
To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ This suggests another possible objective function: choose 0 to maximize the number Co (c3) of times the maximum likelihood parse (under 0) is in fact the correct parse, in the training corpus.

The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well.

It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ This paper described a log-linear model for SUBGs and evaluated two estimators for such models.

A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ Such infinite parameter values indicate that the model treats pseudo-maximal features categorically; i.e., any parse with a non-maximal feature value is assigned a zero conditional probability.
A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ Of course, a feature which is pseudo-maximal over the training corpus is not necessarily pseudo-maximal for all yields.

For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ For example, the adjunct attachment ambiguity in (1) results in alternative syntactic structures which use the same productions the same number of times in each derivation, so a model with only production features would necessarily assign them the same likelihood.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.

Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {位1 ... 位m}, is defined as. $$$$$ The derivation trees of the PCFG can be mapped onto a set containing all of the SUBG's syntactic analyses.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {位1 ... 位m}, is defined as. $$$$$ Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis.

As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).

As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).

 $$$$$ There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).
 $$$$$ In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992).
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.

L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ This motivates an alternative strategy involving a data-based estimate of E9(f3): where y(w) is the yield belonging to the syntactic analysis w, and yi = y(wi) is the yield belonging to the i'th sample in the training corpus.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ Of course (6) is no longer the gradient of the likelihood function, but fortunately it is (exactly) the gradient of (the log of) another criterion: Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields.

When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ The problem of estimating parameters for log-linear models is not new.

 $$$$$ There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).
 $$$$$ In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.

The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ This is an instance of over fitting, and it can be addressed, as is customary, by adding a regularization term that promotes small values of 0 to the objective function.
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).

As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ Of course (6) is no longer the gradient of the likelihood function, but fortunately it is (exactly) the gradient of (the log of) another criterion: Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ As for pseudo-likelihood: So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields.

This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ A common choice is to add a quadratic to the log-likelihood, which corresponds to multiplying the likelihood itself by a normal distribution.
