For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification.
For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ As we noted in Section 5, we are able to significantly outperform basic structural correspondence learning (Blitzer et al., 2006).
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006).

For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification.
For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.
We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ Finally we note that while Blitzer et al. (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al.

Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ Sections 2-5 focused on how to adapt to a target domain when you had a labeled source dataset.
Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ Finally we note that while Blitzer et al. (2006) did combine SCL with labeled target domain data, they only compared using the label of SCL or non-SCL source classifiers as features, following the work of Florian et al.

We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which we have used in our results so far.
We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ When it is 0, the two domains are indistinguishable using a linear classifier.
In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006).

However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006).
However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ We propose solutions to these two questions and evaluate them on a corpus of reviews for four different types of products from Amazon: books, DVDs, electronics, and kitchen appliances2.
We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for four different product types: books, DVDs, electronics and kitchen appliances.

We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline.
We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances.
We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ This is the case for adapting from kitchen appliances to books.

Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ When we report results with SCL and SCL-MI, we require that pivots occur in more than five documents in each domain.
Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ The relative reduction in error due to adaptation of SCL-MI for this test is 90.8%.

To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ Second, we evaluate the A-distance (Ben-David et al., 2006) between domains as measure of the loss due to adaptation from one to the other.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ Second, we evaluate the A-distance (Ben-David et al., 2006) between domains as measure of the loss due to adaptation from one to the other.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.
Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ We chose pivot features using not only common frequency among domains but also mutual information with the source labels.

In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ Section 4 gives results for SCL and the mutual information method for selecting pivot features.
In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ When we report results with SCL and SCL-MI, we require that pivots occur in more than five documents in each domain.

Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ A supervised classifier trained on book reviews cannot assign weight to the kitchen features in the second row of table 2.
Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ We stress that our method improves a supervised baseline.

Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). $$$$$ First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification.
Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ Structural correspondence learning reduces the error due to transfer by 21%.
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.

We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ First, we show how to extend the recently proposed structural correspondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification.
We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline.
