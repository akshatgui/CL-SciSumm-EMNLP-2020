In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.
In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ We extracted from GovTrack all available transcripts of U.S. floor debates in the House of Representatives for the year 2005 (3268 pages of transcripts in total), together with voting records for all roll-call votes during that year.

votes on the bill under discussion (Thomas et al, 2006). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
votes on the bill under discussion (Thomas et al, 2006). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ Or, we could even attempt to model relationships between topics or concepts, in a kind of extension of collaborative filtering.

Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. $$$$$ The enhanced accuracies are obtained via a fairly primitive automatically-acquired “agreement detector” and a conceptually simple method for integrating isolated-document and agreement-based information.
Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. $$$$$ Moreover, and crucially, it is very clear that using agreement information, encoded as preferences within our graph-based approach rather than as hard constraints, yields substantial improvements on both the development and test set; this, we believe, is our most important finding.

The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). $$$$$ Agrawal et al. (2003)).
The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. $$$$$ In particular, since we treat each individual speech within a debate as a single “document”, we are considering a version of document-level sentiment-polarity classification, namely, automatically distinguishing between positive and negative documents (Das and Chen, 2001; Pang et al., 2002; Turney, 2002; Dave et al., 2003).
However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). $$$$$ Get Out The Vote

For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http $$$$$ This section outlines the main steps of the process by which we created our corpus (download site

In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ Each speech segment was labeled by the vote (“yea” or “nay”) cast for the proposed bill by the person who uttered the speech segment.
In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ Using relationship information Applying an SVM to classify each speech segment in isolation leads to clear improvements over the two baseline methods, as demonstrated in Table 4.

 $$$$$ In this section, we discuss the specific classification framework that we adopt and the set of mechanisms that we propose for modeling specific types of relationships.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity.

Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. $$$$$ Get Out The Vote

Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. $$$$$ Get Out The Vote

The first baseline is based on the work of (Thomas et al 2006). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
The first baseline is based on the work of (Thomas et al 2006). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. $$$$$ These labels are then used to train an SVM classifier, the output of which is subsequently used to create weights on agreement links in the test set as follows.
We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. $$$$$ Detecting agreement We used a simple method to learn to identify cross-speaker references indicating agreement.

Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). $$$$$ Previous sentiment-analysis work in different domains has considered inter-document similarity (Agarwal and Bhattacharyya, 2005; Pang and Lee, 2005; Goldberg and Zhu, 2006) or explicit inter-document references in the form of hyperlinks (Agrawal et al., 2003).

Stances in online debates $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Stances in online debates $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).

Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ Politically-oriented text Sentiment analysis has specifically been proposed as a key enabling technology in eRulemaking, allowing the automatic analysis of the opinions that people submit (Shulman et al., 2005; Cardie et al., 2006; Kwon et al., 2006).
Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).
