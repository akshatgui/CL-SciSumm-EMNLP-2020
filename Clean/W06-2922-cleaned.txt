Bick (2006) used the lowercased FORM if the LEMMA is not available, Corston-Oliver and Aue (2006) a prefix and Attardi (2006) a stem derived by a rule-based system for Danish, German and Swedish. $$$$$ LEMMA was used in features whenever available, otherwise the FORM was used.
Bick (2006) used the lowercased FORM if the LEMMA is not available, Corston-Oliver and Aue (2006) a prefix and Attardi (2006) a stem derived by a rule-based system for Danish, German and Swedish. $$$$$ For Danish, German and Swedish the Snowball stemmer (Porter 2001) was used to generate a value for LEMMA.

Use only some components, e.g. Bick (2006) uses only case, mood and pronoun subclass and Attardi (2006) uses only gender, number, person and case. $$$$$ FEATS were used to extract a single token combining gender, number, person and case, through a language specific algorithm.
Use only some components, e.g. Bick (2006) uses only case, mood and pronoun subclass and Attardi (2006) uses only gender, number, person and case. $$$$$ At each step the parser uses classifiers trained on treebank data in order to predict which action to perform and which dependency label to assign given the current configuration.

The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. $$$$$ Recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004).
The most efficient parsers are greedy transition-based parsers, which only explore a single derivation for each input and relies on a locally trained classifier for predicting the next parser action given a compact representation of the derivation history, as pioneered by Yamada and Matsumoto (2003), Nivre (2003), Attardi (2006), and others. $$$$$ The parser by Yamada and Matsumoto (2003) used the following actions

Speech tagger described in Dell Orletta (2009) and dependency parsed by the DeSR parser (Attardi,2006) using Support Vector Machine as learning algorithm. $$$$$ Using Memory Based Learning increases considerably the parsing time, while as expected learning time is quite shorter.
Speech tagger described in Dell Orletta (2009) and dependency parsed by the DeSR parser (Attardi,2006) using Support Vector Machine as learning algorithm. $$$$$ I also tried alternative machine learning algorithms, including SVM, Winnow, Voted Perceptron.

This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser. $$$$$ These parsers are based on learning the correct sequence of Shift/Reduce actions used to construct the dependency tree.
This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shift/Reduce parser. $$$$$ The parser constructs dependency trees employing a deterministic bottom-up algorithm which performs Shift/Reduce actions while analyzing input sentences in left-to-right order.

Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006) .Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. $$$$$ Recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004).
Transition based parsers typically have a linear or quadratic complexity (Attardi, 2006) .Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst case quadratic complexity and an expected linear parsing time. $$$$$ Using Memory Based Learning increases considerably the parsing time, while as expected learning time is quite shorter.

 $$$$$ LEMMA was used in features whenever available, otherwise the FORM was used.
 $$$$$ The following treebanks were used for training the parser

However, other non-projective parsers such as (Attardi, 2006) follow a constructive approach and can be analysed deductively. $$$$$ Experiments With A Multilanguage Non-Projective Dependency Parser
However, other non-projective parsers such as (Attardi, 2006) follow a constructive approach and can be analysed deductively. $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

However, the goal of that transition is different from ours (selecting between projective and non projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition can not be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. $$$$$ The overall parsing algorithm is an inductive statistical parser, which extends the approach by Yamada and Matsumoto (2003), by adding six new reduce actions for handling non-projective relations and also performs dependency labeling.
However, the goal of that transition is different from ours (selecting between projective and non projective parsing, rather than building some arcs in advance) and the approach is specific to one algorithm while ours is generic for example, the LEFT ARC transition can not be added to the arc-standard and arc-eager parsers, or to extensions of those like the ones by Attardi (2006) or Nivre (2009), because these already have it. $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

Non-projective transitions that create dependency arcs between non-contiguous nodes have been used in the transition-based parser by Attardi (2006). $$$$$ Experiments With A Multilanguage Non-Projective Dependency Parser
Non-projective transitions that create dependency arcs between non-contiguous nodes have been used in the transition-based parser by Attardi (2006). $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser. $$$$$ Parsing is deterministic and proceeds bottom-up.
DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser. $$$$$ The parser constructs dependency trees employing a deterministic bottom-up algorithm which performs Shift/Reduce actions while analyzing input sentences in left-to-right order.

This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency. $$$$$ Experiments With A Multilanguage Non-Projective Dependency Parser
This idea is demonstrated by Attardi (2006), who proposes a transition system whose individual transitions can deal with non-projective dependencies only to a limited extent, depending on the distance in the stack of the nodes involved in the newly constructed dependency. $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. $$$$$ The high error rate of J (adjective) is expected, mainly due to coordination problems.
The reported coverage in Attardi (2006) is already very high when the system is restricted to transitions of degree two or three. $$$$$ The error of R (preposition) is also relatively high.

Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. $$$$$ No additional resources are used.
Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). We now turn to describe our variant of the transition system of Attardi (2006), which is equivalent to the original system restricted to transitions of degree two. $$$$$ Shift/Reduce, one to decide which Reduce action and a third one to choose the dependency in case of Left/Right action For details on the CoNLL-X shared task and the measurements see (Buchholz, et al. 2006).

Table 1 $$$$$ Finally, I extended the repertoire of actions used by the parser, in order to handle non-projective relations.
Table 1 $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

We turn next to describe the equivalence between our system and the system in Attardi (2006). $$$$$ Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 166–170, New York City, June 2006. c�2006 Association for Computational Linguistics The parser is modular

While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher degree transitions. $$$$$ For example, the parameter PosFeatures determines for which tokens the POS tag will be included in the context, PosLeftChi7dren determines how many left outermost children of a token to consider, PastActions tells how many previous actions to include as features.
While in the previous sections we have described a tabular method for the transition system of Attardi (2006) restricted to transitions of degree up to two, it is possible to generalize the model to include higher degree transitions. $$$$$ Prepositions are problematic, but their error rate is higher than expected since they are, in terms of surface order, rather regular and close to the noun.

We build upon DeSR, the shift-reduce parser described in (Attardi, 2006). $$$$$ These parsers are based on learning the correct sequence of Shift/Reduce actions used to construct the dependency tree.
We build upon DeSR, the shift-reduce parser described in (Attardi, 2006). $$$$$ Shift/Reduce, one to decide which Reduce action and a third one to choose the dependency in case of Left/Right action For details on the CoNLL-X shared task and the measurements see (Buchholz, et al. 2006).

(Attardi, 2006)) have been introduced for handling non-projective dependency trees $$$$$ Experiments With A Multilanguage Non-Projective Dependency Parser
(Attardi, 2006)) have been introduced for handling non-projective dependency trees $$$$$ For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.

ULISSE was tested against the output of two really different data-driven parsers $$$$$ Using Maximum Entropy (Berger, et al. 1996) classifiers I built a parser that achieves a throughput of over 200 sentences per second, with a small loss in accuracy of about 23 %.
ULISSE was tested against the output of two really different data-driven parsers $$$$$ The following treebanks were used for training the parser
