We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ A methodology for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co—occurrence features extracted from a different sized window of context.

Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)).
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ The size and range of the left window of context is indicated along the horizontal margin in Tables 3 and 4 while the right window size and range is shown along the vertical margin.

It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ Similar differences in training and testing methodology exist among the other studies.
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ A preliminary version of this paper appears in (Pedersen, 2000).

The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ This paper presents a corpus—based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that performs disambiguation via a majority vote.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ In this paper ensemble disambiguation is based on a simple majority vote of the nine member classifiers.

The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)).
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998).

In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)).
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ This is discussed in the context of combining part—of—speech taggers in (Brill and Wu, 1998).

They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998).
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).

Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ A Simple Approach To Building Ensembles Of Naive Bayesian Classifiers For Word Sense Disambiguation
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble.

It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ This paper presents a corpus—based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that performs disambiguation via a majority vote.
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ The single most accurate classifier for interest is Naive_Bayes(4,1), which attains accuracy of 86% while the ensemble approach reaches 89%.

Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ Word sense disambiguation is often cast as a problem in supervised learning, where a disambiguator is induced from a corpus of manually sense—tagged text using methods from statistics or machine learning.
Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm.

The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ A methodology for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co—occurrence features extracted from a different sized window of context.

Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ The lesson taken from these results was that an ensemble should consist of classifiers that represent as differently sized windows of context as possible; this reduces the impact of redundant errors made by classifiers that represent very similarly sized windows of context.

Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ However, a preliminary study found that the accuracy of a Naive Bayesian ensemble using a weighted vote was poor.

Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ Experimental results disambiguating these words with an ensemble of Naive Bayesian classifiers are shown to rival previously published results.
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ These experiments use the same sense-tagged corpora for interest and line as previous studies.
