This score is used, for instance, in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Champollion (Smadja et al, 1996). $$$$$ To evaluate Champollion, we used a collocation compiler, XTRACT (Smadja 1993), to automatically produce several lists of source (English) collocations.
This score is used, for instance, in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Champollion (Smadja et al, 1996). $$$$$ We use XTRACT (Smadja and McKeown 1990; Smadja 1991a; Smadja 1993), a tool we developed previously, to identify collocations in the source language (task 1).

Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al, 1996). $$$$$ His evaluation shows 68% precision and 64% recall.
Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al, 1996). $$$$$ But even if we artificially impose some lower ceiling for m—say three times the length of the source language collocation—we run into the second, and more severe, problem of the exhaustive approach.

This highlights the need for finding multi-word translation correspondences. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed flexible collocations (Smadja et al, 1996). $$$$$ Kupiec (1993) describes a technique for finding noun phrase correspondences in bilingual corpora using several stages.
This highlights the need for finding multi-word translation correspondences. Previous works that focus on multi-word translation correspondences from parallel corpora include noun phrase correspondences (Kupiec, 1993), fixed flexible collocations (Smadja et al, 1996). $$$$$ Evaluation has not been completed for the remaining correspondences-4900 distinct English noun phrases.

Our preliminary finding supports the work on collocation by Smadja et al (1996) in that gapped sequences are also an important class of multi-word translations. $$$$$ One of the major differences between their work and ours is that, like van der Eijk and Kupiec, they only handle translation of uninterrupted sequences of words; they do not handle the broader class of flexible collocations.
Our preliminary finding supports the work on collocation by Smadja et al (1996) in that gapped sequences are also an important class of multi-word translations. $$$$$ The error is due to the fact that the French word important did not pass the first step of the algorithm as its Dice coefficient with important factor was too low.

Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system. $$$$$ To evaluate Champollion, we used a collocation compiler, XTRACT (Smadja 1993), to automatically produce several lists of source (English) collocations.
Smadja et al (1996) limits to find French translation of English collocation identified by his Xtract system. $$$$$ For example, a tagger for French would allow us to run XTRACT on the French part of the corpus, and thus to translate from either French or English as input.

The relationship between pointwise Mutual Information and the Dice coefficient is also discussed in (Smadja et al, 1996). $$$$$ On the other hand, in computational linguistics, information-theoretic measures such as mutual information are widely used (e.g., Bahl et al. 1986; Church and Hanks 1990; Church et al.
The relationship between pointwise Mutual Information and the Dice coefficient is also discussed in (Smadja et al, 1996). $$$$$ Specific mutual information falls somewhere in between the Dice coefficient and average mutual information: it is not completely symmetric but neither does it ignore 0-0 matches.

A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al,1996). $$$$$ Translating Collocations For Bilingual Lexicons: A Statistical Approach
A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al,1996). $$$$$ The algorithm we use is based on statistical methods and produces p-word translations of collocations in which not be the same.

 $$$$$ Finally, another criterion for selecting a similarity measure is its suitability for testing for a particular outcome, where outcome is determined by the application.
 $$$$$ We also thank Ofer Wainberg for his excellent work on improving the efficiency of Champollion and for adding the preposition extension, and Ken Church and AT&T Bell Laboratories for providing us with a prealigned Hansards corpus.

Commonly used association measures are the Mutual Information (Fano, 1961) and the Dice factor (Smadja et al 1996). $$$$$ On the other hand, in computational linguistics, information-theoretic measures such as mutual information are widely used (e.g., Bahl et al. 1986; Church and Hanks 1990; Church et al.
Commonly used association measures are the Mutual Information (Fano, 1961) and the Dice factor (Smadja et al 1996). $$$$$ In our early work on Champollion (Smadja 1992), we used specific mutual information (SI) as a correlation metric.

 $$$$$ Finally, another criterion for selecting a similarity measure is its suitability for testing for a particular outcome, where outcome is determined by the application.
 $$$$$ We also thank Ofer Wainberg for his excellent work on improving the efficiency of Champollion and for adding the preposition extension, and Ken Church and AT&T Bell Laboratories for providing us with a prealigned Hansards corpus.

Our method is similar to that of Smadja et al (1996), except that we incorporate lexical-level information into the association-based method. $$$$$ Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains.
Our method is similar to that of Smadja et al (1996), except that we incorporate lexical-level information into the association-based method. $$$$$ Our goal is to provide a tool for compiling bilingual lexical information above the word level in multiple languages, for different domains.

Our MWE translation extraction method is similar to the two-phase approach proposed by Smadja et al (1996). $$$$$ To achieve efficient processing of the corpus database, Champollion is implemented in two phases: the preparation phase and the actual translation phase.
Our MWE translation extraction method is similar to the two-phase approach proposed by Smadja et al (1996). $$$$$ At the translation phase, only the indices are accessed.

As noted by Smadja et al (1996), this two-step approach drastically reduces the search space. $$$$$ Stage 1—Step 1: Initialization of the work space.
As noted by Smadja et al (1996), this two-step approach drastically reduces the search space. $$$$$ Various thresholds are used in Champollion's algorithm to reduce the search space.

However, translations of collocated context words in the source word sequence create noisy candidate words, which might cause incorrect extraction of target translations by naive statistical correlation measures, such as the Dice coefficient used by Smadja et al (1996). $$$$$ Then, Termight identifies candidate translations for each occurrence of a source term by using the word alignment to find the first and last target positions aligned with any words of the source terms.
However, translations of collocated context words in the source word sequence create noisy candidate words, which might cause incorrect extraction of target translations by naive statistical correlation measures, such as the Dice coefficient used by Smadja et al (1996). $$$$$ Let 131 be the number of translations with i words that are examined by Champollion, and Si the number of these translations that actually survive the thresholds and will be used to generate the candidate translations with i +1 words.

Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. $$$$$ 1993).
Bound-length N-gram correspondences include (Kupiec, 1993) where NP recognizers are used to extract translation units and (Smadja et al, 1996) which uses the Extract system to extract collocations. $$$$$ Ultimately, such techniques would be more useful than those currently used, because they would be able to extract knowledge from noisy data.

Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al, 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al, 1996) and induction of NLP-tools (Kuhn, 2004). $$$$$ The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al.
Aligning parallel text, i.e. automatically setting the sentences or words in one text into correspondence with their equivalents in a translation, is a very useful preprocessing step for a range of applications, including but not limited to machine translation (Brown et al, 1993), cross-language information retrieval (Hiemstra, 1996), dictionary creation (Smadja et al, 1996) and induction of NLP-tools (Kuhn, 2004). $$$$$ The most obvious are machine translation and machine-assisted human translation, but other multilingual applications, including information retrieval, summarization, and computational lexicography, also require access to bilingual lexicons.

Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)). $$$$$ For example, .
Estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. the Dice coefficient (Smadja et al, 1996)). $$$$$ On the other hand, in computational linguistics, information-theoretic measures such as mutual information are widely used (e.g., Bahl et al. 1986; Church and Hanks 1990; Church et al.

Smadja et al (1996) proposed a statistical association measure of the Dice coefficient to deal with the problem of collocation translation. $$$$$ For each run of Champollion, and for each input collocation, we took the final set of candidate translations of different lengths produced by Champollion (with the intermediate stages driven by the Dice coefficient) and compared the results obtained using both the Dice coefficient and SI at the last stage for selecting the proposed translation.
Smadja et al (1996) proposed a statistical association measure of the Dice coefficient to deal with the problem of collocation translation. $$$$$ Otherwise, the entry with the highest Dice coefficient with the source collocation is selected as the translation.

Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word (Smadja, 1992; Smadja et al, 1996). $$$$$ The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al.
Some methods make alignment suggestions at an intermediate level between sentence and word 271 and word (Smadja, 1992; Smadja et al, 1996). $$$$$ We use XTRACT (Smadja and McKeown 1990; Smadja 1991a; Smadja 1993), a tool we developed previously, to identify collocations in the source language (task 1).
