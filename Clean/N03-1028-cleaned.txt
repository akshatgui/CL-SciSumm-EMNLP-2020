Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999). $$$$$ The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001).
Many machine learning techniques have been successfully applied to chunking tasks, such as Regularized Winnow (Zhang et al, 2001), SVMs (Kudo and Matsumoto, 2001), CRFs (Sha and Pereira, 2003), Maximum Entropy Model (Collins, 2002), Memory Based Learning (Sang, 2002) and SNoW (Munoz et al., 1999). $$$$$ We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002).

For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. $$$$$ For our highest F score, we used the complete feature set, around 3.8 million in the CoNLL training set, which contains all the features whose predicate is on at least once in the training set.
For chunking, we follow Sha and Pereira (2003) for the set of features, including token and POS information. $$$$$ Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features.

the perceptron performance is comparable to that of Conditional Random Field models (ShaandPereira, 2003). $$$$$ Shallow Parsing With Conditional Random Fields
the perceptron performance is comparable to that of Conditional Random Field models (ShaandPereira, 2003). $$$$$ We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.

The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. $$$$$ The standard evaluation metrics for a chunker are precision P (fraction of output chunks that exactly match the reference chunks), recall R (fraction of reference chunks returned by the chunker), and their harmonic mean, the F1 score F1 = 2 * P * R/(P + R) (which we call just F score in what follows).
The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. $$$$$ Taku Kudo provided the output of his SVM chunker for the significance test.

In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set. $$$$$ Table 1 summarizes the feature set.
In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set. $$$$$ For our highest F score, we used the complete feature set, around 3.8 million in the CoNLL training set, which contains all the features whose predicate is on at least once in the training set.

This evaluation was also used in (Sha and Pereira, 2003). $$$$$ GIS, CG, and L-BFGS were used to train CRFs and MEMMs.
This evaluation was also used in (Sha and Pereira, 2003). $$$$$ The relative slowness of iterative scaling is also documented in a recent evaluation of training methods for maximum-entropy classification (Malouf, 2002).

The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. $$$$$ That is, the label at position i is yi = where ci is the chunk tag of word i, one of B, or I.
The second-order encoding used in our NER experiments is the same as that described in (Sha and Pereira, 2003) except removing IOB-tag of previous position label. $$$$$ For any label y = c'c, c(y) = c is the corresponding chunk tag.

CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking (Sha and Pereira, 2003). $$$$$ We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.
CRFs have been applied with impressive empirical results to the tasks of noun phrase chunking (Sha and Pereira, 2003). $$$$$ In a longer version of this work we will also describe shallow parsing results for other phrase types.

In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003). $$$$$ In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing.
In recent years discriminative probabilistic model shave been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003). $$$$$ There is no reason why the same techniques cannot be used equally successfully for the other types or for other related tasks, such as POS tagging or named-entity recognition.

Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). $$$$$ The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing.
Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). $$$$$ In a longer version of this work we will also describe shallow parsing results for other phrase types.

The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. $$$$$ For comparisons with other reported results we use F score.
The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. $$$$$ We believe that the superior convergence rate of preconditioned CG is due to the use of approximate second-order information.

Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. $$$$$ We report F scores for comparison with previous work, but we also give statistical significance estimates using McNemar’s test for those methods that we evaluated directly.
Although not directly comparable, Sha and Pereira (2003) report almost the same level of accuracy (94.38%) on noun phrase recognition, using a much smaller training set. $$$$$ Zhang et al. (2002) reported a higher F score (94.38%) with generalized winnow using additional linguistic features that were not available to us.


For more information on current training methods for CRFs, see Sha and Pereira (2003). $$$$$ We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.
For more information on current training methods for CRFs, see Sha and Pereira (2003). $$$$$ Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.

 $$$$$ In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian.
 $$$$$ Taku Kudo provided the output of his SVM chunker for the significance test.

The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). $$$$$ Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search direction.
The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). $$$$$ However in our case the Hessian has the form

Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003). $$$$$ Conjugate-gradient (CG) methods have been shown to be very effective in linear and non-linear optimization (Shewchuk, 1994).
Although this non-concavity prevents efficient global maximization of equation (3), it still allows us to incorporate incomplete an notations using gradient ascent iterations (Sha and Pereira, 2003). $$$$$ Instead of searching along the gradient, conjugate gradient searches along a carefully chosen linear combination of the gradient and the previous search direction.

Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003). $$$$$ Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features.
Chunking approach in this paper is closely similar to the work of Sha and Pereira (2003). $$$$$ These models combine the best features of generative finite-state models and discriminative (log-)linear classifiers, and do NP chunking as well as or better than “ad hoc” classifier combinations, which were the most accurate approach until now.

In the field of English text chunking (Sha and Pereira, 2003), the step 1, 3, and 4 have been studied sufficiently, whereas the step 2, how to select optimal feature template subset efficiently, will be the main topic of this paper. $$$$$ Most previous work used two main machine-learning approaches to sequence labeling.
In the field of English text chunking (Sha and Pereira, 2003), the step 1, 3, and 4 have been studied sufficiently, whereas the step 2, how to select optimal feature template subset efficiently, will be the main topic of this paper. $$$$$ )F(Y , x) can be computed efficiently using a variant of the forward-backward algorithm.

 $$$$$ In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian.
 $$$$$ Taku Kudo provided the output of his SVM chunker for the significance test.
