Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results. $$$$$ At this point we have a preliminary clustering — no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties.
Preliminary experiments with tags derived automatically using distributional clustering (Clark, 2000), have shown essentially the same results. $$$$$ I am currently applying this approach to the induction of phrase structure rules, and preliminary experiments have shown encouraging results.

It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus. $$$$$ Finch and Chater (1992), (1995) and Schiitze (1993), (1997) use a set of features derived from the co-occurrence statistics of common words together with standard clustering and information extraction techniques.
It has often been proposed that children might make use of information about the contextual distribution of usage of words to induce the parts-of-speech of their native language (e.g. Maratsos and Chalkley, 1980), and work by, e.g., Redington, Chater and Finch (1998) and Clark (2000), showed that parts-of-speech can indeed be induced by clustering together words that are used in similar contexts in a corpus. $$$$$ The work of Chater and Finch can be seen as similar to the work presented here given an independence assumption.

Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. $$$$$ Inducing Syntactic Categories By Context Distribution Clustering
Schutze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters. $$$$$ Appendix A shows the five most frequent words in a clustering with 77 clusters.

Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. $$$$$ We can then measure the similarity of words by the similarity of their context distributions, using the Kullback-Leibler (KL) divergence as a distance function.
Firstly, each word is annotated with a distributional similarity tag, from a distributional similarity model (Clark, 2000) trained on 100 million words from the British National Corpus and English Gigaword corpus. $$$$$ I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).

Our work builds on two older part-of-speech inducers word clustering algorithms of Clark (2000) and Brown et al (1992) that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al, 2010). $$$$$ Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.
Our work builds on two older part-of-speech inducers word clustering algorithms of Clark (2000) and Brown et al (1992) that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al, 2010). $$$$$ A minimum of this function can be found using the EM algorithm(Dempster et al., 1977).

 $$$$$ If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words

Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence. $$$$$ We can then measure the similarity of words by the similarity of their context distributions, using the Kullback-Leibler (KL) divergence as a distance function.
Our main purely unsupervised results are with a flat clustering (Clark, 2000) that groups words having similar context distributions, according to Kullback Leibler divergence. $$$$$ Unfortunately it is not possible to cluster based directly on the context distributions for two reasons

 $$$$$ If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words

We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well. $$$$$ Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.
We found that Brown et al's (1992) older information-theoretic approach, which does not explicitly address the problems of rare and ambiguous words (Clark, 2000) and was designed to induce large numbers of plausible syntactic and semantic clusters, can perform just as well. $$$$$ The new algorithm currently does not use information about the orthography of the word, an important source of information.

 $$$$$ If the context is restricted to the word on either side, I can define the context distribution to be a distribution over all ordered pairs of words

Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV. $$$$$ Brown et al. (1992) use a very large amount of data, and a well-founded information theoretic model to induce large numbers of plausible semantic and syntactic clusters.
Unsupervised word clustering techniques of Brown et al (1992) and Clark (2000) are well-suited to dependency parsing with the DMV. $$$$$ In summary, the new method avoids the limitations of other approaches, and is better suited to integration into a complete unsupervised language acquisition system.

We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. $$$$$ I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).
We trained the model described in (Clark, 2000), with code downloaded from his website, on several hundred million words from the British national corpus, and the English Gigaword corpus. $$$$$ For two CLAWS tags, AJO (adjective) and NN1(singular common noun) that occur frequently among rare words in the corpus, I selected all of the words that occurred n times in the corpus, and at least half the time had that CLAWS tag.

Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. $$$$$ Previous techniques give good results, but fail to cope well with ambiguity or rare words.
Clark (2000) also builds distributional profiles, introducing an iterative clustering method to better handle ambiguity and rare words. $$$$$ At this point we have a preliminary clustering — no very rare words will be included, and some common words will also not be assigned, because they are ambiguous or have idiosyncratic distributional properties.

Clark (2000) reports results on a corpus containing 12 million terms, Schutze (1993) on one containing 25 million terms, and Brown, et al (1992) on one containing 365 million terms. $$$$$ I used 12 million words of the British National Corpus as training data, and ran this algorithm with various numbers of clusters (77, 100 and 150).
Clark (2000) reports results on a corpus containing 12 million terms, Schutze (1993) on one containing 25 million terms, and Brown, et al (1992) on one containing 365 million terms. $$$$$ In absolute terms the perplexities are rather high; I deliberately chose a rather crude model without backing off and only the minimum amount of smoothing, which I felt might sharpen the contrast.

Clark (2000) presentsa framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. $$$$$ Ambiguity can be handled naturally within this framework.
Clark (2000) presentsa framework which in principle should accommodate lexical ambiguity using mixtures, but includes no evidence that it does so. $$$$$ There are often several local minima — in practice this does not seem to be a major problem.

We trained a variant of our system without gold part-of-speech tags, using the unsupervised word clusters (Clark, 2000) computed by Finkel and Manning (2009). $$$$$ Both of these problems can be overcome in the normal way by using clusters

This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets. $$$$$ I therefore chose to use an objective statistical measure, the perplexity of a very simple finite state model, to compare the tags generated with this clustering technique against the BNC tags, which uses the CLAWS-4 tag set (Leech et al., 1994) which had 76 tags.
This approach is taken by Clark (2000), where the perplexity of a finite-state model is used to compare different category sets. $$$$$ As can be seen, the perplexity is lower with the model trained on data tagged with the new algorithm.

To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster. $$$$$ I then sort the words by the divergence from the cluster that is closest to them, and select the best as being the members of the cluster for the next iteration.
To overcome this problem, Clark (2000) proposes a bootstrapping approach, in which he (1) clusters the most distributionally reliable words, and then (2) incrementally augments each cluster with words that are distributionally similar to those already in the cluster. $$$$$ Each cluster is represented by the most frequent member of the cluster.

It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schutze (1995) and Clark (2000)). $$$$$ This paper addresses the issue of the automatic induction of syntactic categories from unannotated corpora.
It is perhaps not immediately clear why morphological information would play a crucial role in the induction process, especially since the distributional approach has achieved considerable success for English POS induction (see Lamb (1961), Schutze (1995) and Clark (2000)). $$$$$ I am currently applying this approach to the induction of phrase structure rules, and preliminary experiments have shown encouraging results.

In fact, Jardino and Adda (1994), Schutze (1997) and Clark (2000) have attempted to address the ambiguity problem to a certain extent. $$$$$ Ambiguity can be handled naturally within this framework.
In fact, Jardino and Adda (1994), Schutze (1997) and Clark (2000) have attempted to address the ambiguity problem to a certain extent. $$$$$ There are often several local minima — in practice this does not seem to be a major problem.
