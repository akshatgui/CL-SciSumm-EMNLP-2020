We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ On the contrary, the MaxEnt-based reordering model is not limited by this constraint since it is based on features of phrase, not phrase itself.

Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section.

In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ The flat and distortion reordering models (Row 3 and 4) show similar performance with Pharaoh.
In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ It can be said that their decoder did not violate the ITG constraints but not that it observed the ITG.

As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.

The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ In this paper we presented a MaxEnt-based phrase reordering model for SMT.
The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ On the contrary, the MaxEnt-based reordering model is not limited by this constraint since it is based on features of phrase, not phrase itself.

Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.

Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ Many systems use very simple models to reorder phrases 1.
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.

For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ Finally, a maximum entropy classifier will be trained on the features.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ Why do we use the first words as features?

Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ Finally, a maximum entropy classifier will be trained on the features.

The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ Maximum Entropy Based Phrase Reordering Model For Statistical Machine Translation
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation.

Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ In this paper, we build a maximum entropy based classification model as the reordering model.
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section.

In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule. $$$$$ Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order.
In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule. $$$$$ Given a source sentence c, firstly we initiate the chart with phrases from phrase translation table by applying the lexical rule.

 $$$$$ So the phrasal reordering is totally dependent on the language model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

 $$$$$ So the phrasal reordering is totally dependent on the language model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

 $$$$$ So the phrasal reordering is totally dependent on the language model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ Beyond the MaxEnt-based reordering model, another feature deserving attention in our system is the CKY style decoder which observes the ITG.

 $$$$$ So the phrasal reordering is totally dependent on the language model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

 $$$$$ So the phrasal reordering is totally dependent on the language model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ In this paper we presented a MaxEnt-based phrase reordering model for SMT.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ Traditional distortion/flat-based SMT translation systems are good for learning phrase translation pairs, but learn nothing for phrasal reorderings from real-world data.

The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Later, the straight rule (1) merges two consecutive blocks into a single larger block in the straight order; while the inverted rule (2) merges them in the inverted order.
The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ The third one is a flat reordering model, which assigns probabilities for the straight and inverted order.
