On the other hand, as Abney (1997) points out, the context-sensitive dependencies that unification-based constraints introduce render the relative frequency estimator suboptimal. $$$$$ Even if the language described by the grammar of interest—that is, the set of possible trees—is context-free, there may well be context-sensitive statistical dependencies.
On the other hand, as Abney (1997) points out, the context-sensitive dependencies that unification-based constraints introduce render the relative frequency estimator suboptimal. $$$$$ Random fields can be readily applied to capture such statistical dependencies whether or not L(G) is context-sensitive.

Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework. $$$$$ The models of interest are known as random fields.
Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework. $$$$$ In this framework, a model consists of: (1) An AV grammar G whose purpose is to define a set of dags L(G).

Abney (1997) proposes a gradient ascent, based upon a Monte Carlo procedure for estimating E0 (fj). $$$$$ The procedure for adjusting field weights has much the same structure as the procedure for choosing initial weights.
Abney (1997) proposes a gradient ascent, based upon a Monte Carlo procedure for estimating E0 (fj). $$$$$ The resulting Newton iteration is: The estimation procedure is: procedure Adjust Weights (ai, , an) begin until the field converges do

Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). $$$$$ Stochastic Attribute-Value Grammars
Later, AVG were enriched with a statistical component (Abney, 1997): stochastic AVG (SAVG). $$$$$ Before the advent of statistical methods, regular and context-free grammars were considered too inexpressive for serious consideration, and even now the reliance on stochastic versions of the less-expressive grammars is often seen as an expedient necessitated by the lack of an adequate stochastic version of attribute-value grammars.

As Abney (1997) shows, we can not use relatively simple techniques such as relative frequencies to obtain a model for estimating derivation probabilities in attribute-value grammars. $$$$$ Stochastic Attribute-Value Grammars
As Abney (1997) shows, we can not use relatively simple techniques such as relative frequencies to obtain a model for estimating derivation probabilities in attribute-value grammars. $$$$$ It is not immediately obvious how to use the ITS algorithm to equip attribute-value grammars with probabilities.

PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). $$$$$ The models of interest are known as random fields.
PCFG-based models can only approximate LFG and similar constraint-based formalisms (Abney, 1997). $$$$$ We can impose such a constraint by means of an attribute-value grammar.

Simple PCFG based models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney,1997). $$$$$ The models of interest are known as random fields.
Simple PCFG based models, while effective and computationally efficient, can only provide approximations to LFG and similar constraint-based formalisms (Abney,1997). $$$$$ We can impose such a constraint by means of an attribute-value grammar.

 $$$$$ The answer is of course that qi and /5 are probability distributions over L(Gi), but not all of L(G1) appears in the corpus.
 $$$$$ All responsibility for flaws and errors of course remains with me.

Unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible UBG readings do not sum to the value 1, a problem which is discussed intensively by Eisele (1994), Abney (1997). $$$$$ Eisele (1994) takes a logic-programming approach to constraint grammars.
Unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible UBG readings do not sum to the value 1, a problem which is discussed intensively by Eisele (1994), Abney (1997). $$$$$ Eisele recognizes that this problem arises only where there are context dependencies.

We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars. $$$$$ Stochastic Attribute-Value Grammars
We would like the parsing model to include long-range dependencies, but this introduces problems for generative parsing models similar to those described by Abney (1997) for attribute-value grammars. $$$$$ The standard parsing techniques can be readily adapted to the random-field models to be discussed below, so I simply refer the reader to the literature.

For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). $$$$$ A point of terminology: I will use the term grammar to refer to an unweighted grammar, be it a context-free grammar or attribute-value grammar.
For example, when we apply a unification-based grammar, LPCFG-like modeling results in an inconsistent probability model because the model assigns probabilities to parsing results not allowed by the grammar (Abney, 1997). $$$$$ Then: In parsing, we use the probability distribution qi (x) defined by model M1 to disambiguate: the grammar assigns some set of trees {xi, , x,,} to a sentence a, and we choose that tree xi that has greatest probability qi (x,).

In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). $$$$$ For purpose of illustration, take feature 1 to have weight )(31 = v--2- and feature 2 to have weight 02 = 3/2.
In particular, when we apply Feature-Based LTAG (FBLTAG), the above probability is no longer consistent because of the non-local constraints caused by feature unification (Abney, 1997). $$$$$ The aim of feature selection is to choose a feature that reduces this divergence as much as possible.

Such constraints are known 83 to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). $$$$$ The models of interest are known as random fields.
Such constraints are known 83 to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997). $$$$$ We estimate weights using the ERF method: we estimate the weight of a rule as the relative frequency of the rule in the training corpus, among rules with the same left-hand side.

As is well known (Abney, 1997), DAG-like dependencies can not in general be modeled with a generative approach of the kind taken here. $$$$$ The models of interest are known as random fields.
As is well known (Abney, 1997), DAG-like dependencies can not in general be modeled with a generative approach of the kind taken here. $$$$$ But it can be taken as a useful first approximation.

Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations. $$$$$ By parameter estimation we mean determining values for the weights 0.
Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations. $$$$$ There are two important differences.

Abney (1997) pointed out that the non-context free dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log linear models for defining probability distributions over the parses of a unification grammar. $$$$$ Markov chains are stochastic processes corresponding to regular grammars and random branching processes are stochastic processes corresponding to context-free grammars.
Abney (1997) pointed out that the non-context free dependencies of a unification grammar require stochastic models more general than Probabilistic Context-Free Grammars (PCFGs) and Markov Branching Processes, and proposed the use of log linear models for defining probability distributions over the parses of a unification grammar. $$$$$ Throughout we will use the following stochastic context-free grammar for illustrative purposes.

The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al, 1999). $$$$$ Statistical dependencies under the model of Mark et al. (1992).
The following section reviews stochastic unification grammars (Abney, 1997) and the statistical quantities required for efficiently estimating such grammars from parsed training data (Johnson et al, 1999). $$$$$ The approach described here assumes complete data (a parsed training corpus).

(Abney, 1997) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. $$$$$ As probability distributions, qi and /3 should have the same total mass, namely, one.
(Abney, 1997) and has the advantage of elegantly bypassing the issue of loosing probability mass to failed derivations due to unification failures. $$$$$ (Renormalization is necessary because of the failed derivations.)

 $$$$$ The answer is of course that qi and /5 are probability distributions over L(Gi), but not all of L(G1) appears in the corpus.
 $$$$$ All responsibility for flaws and errors of course remains with me.

Abney gives fuller details (Abney, 1997). $$$$$ (See Appendix 1 for details.)
Abney gives fuller details (Abney, 1997). $$$$$ This is the real justification for equation (3), and the reader is referred to Della Pietra, Della Pietra, and Lafferty (1995) for details.
