This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al, 1993) and the DSO collection (Ng & Lee, 1996). $$$$$ The result can be viewed either as a collection of passages in which words have been tagged syntactically and semanti- eally, or as a lexicon in which illustrative sentences can be found for many definitions.
This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al, 1993) and the DSO collection (Ng & Lee, 1996). $$$$$ At the present time, the correla- tion of a lexical meaning with examples in which a word is used to express that meaning must be done by hand.

Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al, 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. $$$$$ The targeted approach starts with the lexicon: target a polysemous word, extract all sentences from the corpus in which that word occurs, categorize the instances and write definitions for each sense, and create a pointer between each instance of the word and its appropriate sense in the lexicon; then target another word and repeat he process.
Typically, word frequency distributions are estimated with respect to a sense-tagged corpus such as SemCor (Miller et al, 1993), a 220,000 word corpus tagged with WordNet (Fellbaum, 1998) senses. $$$$$ text, along with the WordNet synsets for all of the senses of that word (in the appropriate part of speech).

We base our experiments on SemCor (Miller et al, 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. $$$$$ QUERYING THE TAGGED TEXT A program to query the semantically tagged database has also been written: p rsent  (print sentences) allows a user to retrieve sentences by entering the base form of a word and its semantic tag.
We base our experiments on SemCor (Miller et al, 1993), a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers. $$$$$ 306 Implementation f this program requires the creation of a "master list" of semantically tagged words.

The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ This program is useful to the lexicographers, who are intimately familiar with WordNet semantic tags and who use it to find sample sentences.
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ Miller, G. A. and Fellbaum, C. Semantic networks of English.

In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al, 1993) and their manual translations into Romanian. $$$$$ The Brown Corpus is the text and WordNet is the lexicon.
In the experiments reported in this section, we use a parallel corpus consisting of 107 documents from the SemCor corpus (Miller et al, 1993) and their manual translations into Romanian. $$$$$ We are now finishing a first installment of semantically tagged text consisting of 100 passages from the Brown Corpus; as soon as that much has been completed and satisfactorily cleaned up, we plan to make it, and the corresponding WordNet database, available to other labora- tories that also have permission to use the Brown Corpus.

Existing hand annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ This alone is a powerful improvement over p rsent .
Existing hand annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ Miller, G. A. and Fellbaum, C. Semantic networks of English.

To train the classifiers for the all-words task we just used Semcor (Miller et al, 1993). $$$$$ Since collocations are less polysemous than are individual words, their inclusion in WordNet promises to simplify the task of sense resolution.
To train the classifiers for the all-words task we just used Semcor (Miller et al, 1993). $$$$$ By itself, WordNet does not provide topical groupings of words that can be used for sense resolution.

This is roughly comparable with most frequent sense figures in standard annotated corpora such as Semcor (Miller et al, 1993) and the Senseval/Semeval data sets, which suggests that diversity may not play a major role in the current Google ranking algorithm. $$$$$ By providing them with the appropriate sense of an unfamiliar word, they are spared the task of selecting a sense from the several alternatives listed in a standard ic- tionary.
This is roughly comparable with most frequent sense figures in standard annotated corpora such as Semcor (Miller et al, 1993) and the Senseval/Semeval data sets, which suggests that diversity may not play a major role in the current Google ranking algorithm. $$$$$ ), WordNet: An on-line lexical data- base.

Cor text collection (Miller et al, 1993), a subset of the Brown corpus manually tagged with WordNet senses (37,176 sentences in 352 newspaper articles). $$$$$ The Brown Corpus is the text and WordNet is the lexicon.
Cor text collection (Miller et al, 1993), a subset of the Brown corpus manually tagged with WordNet senses (37,176 sentences in 352 newspaper articles). $$$$$ text, along with the WordNet synsets for all of the senses of that word (in the appropriate part of speech).

To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al, 1993). $$$$$ The task of semantic tagging has provided a useful stimulus to improve both coverage and precision.
To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al, 1993). $$$$$ CONTEXT: A TAGGING INTERFACE The task of semantically tagging a text by hand is notori- ously tedious, but the tedium can be reduced with an appropriate user interface.

The Semcor corpus (Miller et al, 1993), a fraction of the Brown corpus (Kucera and Francis, 1967) which has been manually annotated with Wordnet synset labels. $$$$$ The Brown Corpus is the text and WordNet is the lexicon.
The Semcor corpus (Miller et al, 1993), a fraction of the Brown corpus (Kucera and Francis, 1967) which has been manually annotated with Wordnet synset labels. $$$$$ THE BROWN CORPUS The textual component of our universal semantic oncor- dance is taken from the Brown Corpus [3, 4].

Most of current all-words generic supervised WSD systems take SemCor (Miller et al, 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. $$$$$ At the present time, the correla- tion of a lexical meaning with examples in which a word is used to express that meaning must be done by hand.
Most of current all-words generic supervised WSD systems take SemCor (Miller et al, 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. $$$$$ A new semantic tag must then be inserted by the tagger.

This includes sense ranks in WordNet, SemCor statistics (Miller et al, 1993), and similarity scores and rankings in Lin's resources. $$$$$ WordNet contains only uninflected (or base) forms of words, so the interface to WordNet includes raorphy, a morpho- logical analyzer that is applied to input strings to generate the base forms.
This includes sense ranks in WordNet, SemCor statistics (Miller et al, 1993), and similarity scores and rankings in Lin's resources. $$$$$ APPL ICAT IONS Our reasons for building this universal semantic oncor- dance were to test and improve the coverage of WordNet and to develop resources for developing and testing pro- cedures for the automatic sense resolution in context.

Unless specified otherwise, we use WordNet 1.7.1 (Milleret al, 1990) and the associated sense annotated SemCor corpus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). $$$$$ The Brown Corpus is the text and WordNet is the lexicon.
Unless specified otherwise, we use WordNet 1.7.1 (Milleret al, 1990) and the associated sense annotated SemCor corpus (Miller et al, 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). $$$$$ Additional search keys can be specified to find words that co-occur in sentences.

ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al, 1993). $$$$$ The Brown Corpus is the text and WordNet is the lexicon.
ImCor dataset by associating images from the Corel database with text from the SemCor corpus (Miller et al, 1993). $$$$$ The WordNet database is constantly growing and changing.

Coarse-grained English All-Words LexPar and SynWSD were trained on an 1 million words corpus comprising the George Orwell's 1984 novel and the SemCor corpus (Miller et al, 1993). $$$$$ Miller, G. A. and Fellbaum, C. Semantic networks of English.
Coarse-grained English All-Words LexPar and SynWSD were trained on an 1 million words corpus comprising the George Orwell's 1984 novel and the SemCor corpus (Miller et al, 1993). $$$$$ New York: Scribners, 1984.

Then, we present a detailed comparison of their performance on SemCor (Miller et al, 1993). $$$$$ At the present time, the correla- tion of a lexical meaning with examples in which a word is used to express that meaning must be done by hand.
Then, we present a detailed comparison of their performance on SemCor (Miller et al, 1993). $$$$$ Ku~era, H. and Francis, W. N. Computational nalysis of present-day American English.

A Times and SemCor corpora (Milleretal., 1993), and used to generate a training corpus, with manually-annotated positive and negative examples of part-whole relations. $$$$$ At the present time, the correla- tion of a lexical meaning with examples in which a word is used to express that meaning must be done by hand.
A Times and SemCor corpora (Milleretal., 1993), and used to generate a training corpus, with manually-annotated positive and negative examples of part-whole relations. $$$$$ The semantic relations among open-class words include: synonymy and antonymy (which are semantic relations between words and which are found in all four syntactic categories); hyponymy and hypernymy (which are semantic relations between concepts and which organize nouns into a categorical hierarchy); meronymy and holonymy (which represent part-whole relations among noun concepts); and troponymy (manner relations) and entailment relations between verb concepts.

We are using a subset of the SemCor texts (Miller et al, 1993) - five randomly selected files covering different topics: news, sports, entertainment, law, and debates - as well as the data set provided for the English all words task during SENSEVAL-2. $$$$$ The task of semantic tagging has provided a useful stimulus to improve both coverage and precision.
We are using a subset of the SemCor texts (Miller et al, 1993) - five randomly selected files covering different topics: news, sports, entertainment, law, and debates - as well as the data set provided for the English all words task during SENSEVAL-2. $$$$$ Miller, G. A. and Fellbaum, C. Semantic networks of English.

We contrast the performance of first sense heuristics i) from SemCor (Miller et al, 1993) and ii) derived automatically from the BNC following (McCarthy et al, 2004) and also iii) an upper-bound first sense heuristic extracted from the test data. $$$$$ Manual semantic tagging is tedious; it should be done automatically as soon as it is possible to resolve word senses in context automatically.
We contrast the performance of first sense heuristics i) from SemCor (Miller et al, 1993) and ii) derived automatically from the BNC following (McCarthy et al, 2004) and also iii) an upper-bound first sense heuristic extracted from the test data. $$$$$ Search only for a specific sense.
