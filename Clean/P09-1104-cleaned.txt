This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005).

In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ Wu (1997)’s inversion transduction grammar (ITG) is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments.

Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ (2006), we assume the score a of a potential alignment a) decomposes as where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively.
Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ Null productions are also a source of double counting, as there are many possible orders in which to attach null alignments to a bitext cell; we address this by adapting the grammar to force a null attachment order.

Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ The set of such ITG alignments, AITG, are a strict subset of A1-1 (Wu, 1997).

To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ In this work, we investigate large-scale, discriminative ITG word alignment.
To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006).

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ For example, we include the average Dice of all the cells in a block.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ For example, we include the average Dice of all the cells in a block.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ For example, we include the average Dice of all the cells in a block.

However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ Indeed, blocks are the primary reason for gold alignments being outside the space of one-to-one ITG alignments.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ We evaluate our proposed alignments (a) against hand-annotated alignments, which are marked with sure (s) and possible (p) alignments.

MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ We instead use a variant of MIRA similar to Chiang et al. (2008).

We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs.

 $$$$$ We will use BITG to refer to this block ITG variant and ABITG to refer to the alignment family, which is neither contained in nor contains A1-1.
 $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.

This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ Our supervised ITG model gave a 1.1 BLEU increase over GIZA++.

This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ Table 1 illustrates results for the Hansards data set.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005).

The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ All in all, our discriminatively trained, block ITG models produce alignments which exhibit the best AER on the NIST 2002 Chinese-English alignment data set.
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ The right-hand three columns in Table 2 present supervised results on our Chinese English data set using block features.

When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al (2009). $$$$$ No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005).
When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al (2009). $$$$$ We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features.

Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Indeed, blocks are the primary reason for gold alignments being outside the space of one-to-one ITG alignments.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Then, When a* is an ITG alignment (i.e., m* is 0), M(a*) consists only of alignments which have all the sure alignments in a*, but may have some subset of the possible alignments in a*.

We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs.

Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ We opt instead to maximize the probability of the set of alignments M(a*) which achieve the same optimal in-class loss.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005).

We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models.
