More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ We now define a kernel function for dependency trees.
More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ See Zelenko et al. (2003) for a proof that K is kernel function.

Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ We represent each relation instance as an augmented dependency tree.

This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004) $$$$$ Table 4 shows that precision is adequate, but recall is low.
This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004) $$$$$ While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ We define a slightly more general version of the kernel described by Zelenko et al. (2003).
Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. $$$$$ String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003).
Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. $$$$$ See Zelenko et al. (2003) for a proof that K is kernel function.

For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.
For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ For example, we can increase the importance of two nodes having the same Wordnet hypernym2.

Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004). $$$$$ The features used to represent each node are shown in Table 3.
Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004). $$$$$ These results are shown in Table 6.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ Our algorithm is similar to that described by Zelenko et al. (2003).
The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ See Zelenko et al. (2003) for a proof that K is kernel function.

To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ The relation-argument feature specifies whether an entity is the first or second argument in a relation.
To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ Formally, a relation instance is a dependency tree T with nodes It0 ... tn}.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ We represent each relation instance as an augmented dependency tree.

In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ For each entity pair, we create an augmented dependency tree (described below) representing this instance.
In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ Here, we detect, but do not classify, relations.

 $$$$$ To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.
 $$$$$ It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.

This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ Dependency Tree Kernels For Relation Extraction
This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ We describe a relation extraction technique based on kernel methods.

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ Dependency Tree Kernels For Relation Extraction
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ We describe a relation extraction technique based on kernel methods.

Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ Dependency Tree Kernels For Relation Extraction
Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ We describe a relation extraction technique based on kernel methods.
