In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ As in the case of vocabulary features, the presence of more relations will lead to overall lower probabilities so we also consider the number of discourse relations (F14) and the log likelihood combined with the number of relations as features.
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008).

In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ We have investigated which linguistic features correlate best with readability judgments.

We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ The correlations between readability and syntactic features is shown in Table 3.

Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ Revisiting Readability

Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ Revisiting Readability

 $$$$$ The correlations are positive

When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ Text coherence is defined as the ease with which a person (tacitly assumed to be a competent language user) understands a text.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written.

 $$$$$ The correlations are positive

Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005).
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994).

Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ (Barzilay and Lapata, 2008) found that articles written for adults tended to contain many more entities than articles written for children.

This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ Our study is novel in the use of gold-standard discourse features for predicting readability and the simultaneous analysis of various readability factors.
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ A more general and principled approach to using vocabulary information for readability decisions has been the use of language models.

Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written.
Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008).

To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994).
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ We randomly selected thirty articles from the Wall Street Journal corpus that was used in both the Penn Treebank and the Penn Discourse Treebank.1 Each article was read by at least three college students, each of whom was given unlimited time to read the texts and perform the ratings.2 Subjects were asked the following questions

Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ Thus for all subsequent analysis, we will use only the first question (“On a scale of 1 to 5, how well written is this text?”).
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ We use the squared multiple correlation coefficient (R2) to assess the effectiveness of predictions.

Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ The vocabulary features we used are article likelihood estimated from a language model from WSJ (F5), and article likelihood according to a unigram language model from NEWS (F6).
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ We computed another language model which is over discourse relations instead of words.

Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994).
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008).

Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ We use the Brown Coherence Toolkit6 to compute entity grids (Barzilay and Lapata, 2008) for each article.
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.

Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ Syntactic complexity is an obvious factor

These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ (Barzilay and Lapata, 2008) found that articles written for adults tended to contain many more entities than articles written for children.
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ We use the Brown Coherence Toolkit6 to compute entity grids (Barzilay and Lapata, 2008) for each article.

An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ We tested the average number of characters per word, average number of words per sentence, maximum number of words per sentence, and article length (F7).3 Article length (F7) was the only significant baseline factor, with correlation of -0.37.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
