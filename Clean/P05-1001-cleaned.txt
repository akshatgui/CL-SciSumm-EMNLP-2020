Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ In the first step, we train m predictors independently.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ We choose this task because the original intention of this shared task was to test the effectiveness of semi-supervised learning methods.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ For semi-supervised learning, the idea is to create many auxiliary prediction problems (relevant to the task) from unlabeled data so that we can learn the shared structureO(useful for the task) using the ASO algorithm.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

For a more complete description, see (Ando and Zhang, 2005a). $$$$$ The formal derivation can be found in (Ando and Zhang, 2004).
For a more complete description, see (Ando and Zhang, 2005a). $$$$$ See (Ando and Zhang, 2004) for the precise formulation.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ One binary classification problem can be created for each possible word value (e.g., “IBM”, “he”, “get”,••J.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ From a c-way classification problem, c!=(c — k)! binary prediction problems can be created.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. $$$$$ •Relevancy: auxiliary problems should be related to the target problem.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. $$$$$ Hence, many auxiliary problems can be obtained using this idea.

This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ See (Ando and Zhang, 2004) for the precise formulation.
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ Moreover, the auxiliary problems used in our experiments are merely possible examples.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

 $$$$$ Structural learning provides a framework for carrying out possible new ideas.
 $$$$$ Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.

To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e. what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.
To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.

Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ The formal derivation can be found in (Ando and Zhang, 2004).
Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ See (Ando and Zhang, 2004) for the precise formulation.

We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ See (Ando and Zhang, 2004) for the precise formulation.
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ In all settings (including baseline methods), the loss function is a modification of the Huber’s robust loss for regression: L(p, y) = max (0,1 — py)2 if py
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$  —1; and —4py otherwise; with square regularization (A=10-4).

For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ The final classifier for the target task is in the form of (1), a linear predictor for structural learning.

Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ Ex 3.2 Predict the top-k choices of the classifier.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ See (Ando and Zhang, 2004) for the precise formulation.

As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ The formal derivation can be found in (Ando and Zhang, 2004).
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ See (Ando and Zhang, 2004) for the precise formulation.
