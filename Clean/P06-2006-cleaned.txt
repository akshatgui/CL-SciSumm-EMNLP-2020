Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ There are also issues of incompatible tokenization and lemmatization between the systems and of differing syntactic annotation of similar information, which lead to problems mapping between our GR output and the current DepBank.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ There are similar issues with other DepBank features and relations.

Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ Evaluating The Accuracy Of An Unlexicalized Statistical Parser On The PARC DepBank
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ Kaplan et al. (2004) compare the accuracy and speed of the PARC XLE Parser to Collins’ Model 3 parser.

To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003).

This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ However, until recently, no WSJ data has been carefully annotated to support relational evaluation.
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG).

Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.
Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ The system has been used to analyse about 150 million words of English text drawn primarily from the PTB, TREC, BNC, and Reuters RCV1 datasets in connection with a variety of projects.

The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ Second, tokens are part-ofspeech and punctuation (PoS) tagged using a 1storder Hidden Markov Model (HMM) utilizing a lexicon of just over 50K words and an unknown word handling module.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006).

The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006).
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ We have kept the GR representation simpler and more readable by suppressing lemmatization, token numbering and PoS tags, but have left the DepBank annotations unmodified.

We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ They compare performance of a grammatically cut-down and complete version of the XLE parser to the publically available version of Collins’ parser.
We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ Thus, we have made no use of the PTB itself and only limited use of WSJ text.

For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003).

The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG).
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ The output of the parser can be displayed as syntactic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents.

 $$$$$ There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of features, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categories.
 $$$$$ We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (classes of) relations and features is both more informative and facilitates system comparisons.

Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ This allows comparison of overall accuracy on modifiers with, for instance overall accuracy on arguments.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ However, the performance of the system, as measured by microraveraged F1-score on GR extraction alone, has declined by 2.7% over the held-out Susanne data, so even the unlexicalized parser is by no means domain-independent. than this since some of the test sentences are elliptical or fragmentary, but in many cases are recognized as single complete constituents.

 $$$$$ There are currently 676 phrase structure rule schemata, 15 feature propagation rules, 30 default feature value rules, 22 category expansion rules and 41 feature types which together define 1124 compiled phrase structure rules in which categories are represented as sets of features, that is, attribute-value pairs, possibly with variable values, possibly bound between mother and one or more daughter categories.
 $$$$$ We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (classes of) relations and features is both more informative and facilitates system comparisons.

C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ DepBank facilitates comparison of PCFG-like statistical parsers developed from the PTB with other parsers whose output is not designed to yield PTB-style trees, using an evaluation which is closer to the protypical parsing task of recovering predicate-argument structure.

In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006).
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ However, in this paper we focus exclusively on the baseline unlexicalized system.

Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.
Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ Thus we reannotated the DepBank sentences with GRs using our current system, and then corrected and extended this annotation utilizing a software tool to highlight differences between the extant annotations and our own.2 This exercise, though time-consuming, uncovered problems in both annotations, and yields a doubly-annotated and potentially more valuable resource in which annotation disagreements over complex attachment decisions, for instance, can be inspected.

For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.
For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ King et al. (2003) describe the PARC 700 Dependency Bank (hereinafter DepBank), which consists of 700 WSJ sentences randomly drawn from section 23.

The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003).
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ There are similar issues with other DepBank features and relations.

The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ The full system can be extended in a variety of ways – for example, by pruning PoS tags but allowing multiple tag possibilities per word as input to the parser, by incorporating lexical subcategorization into parse selection, by computing GR weights based on the proportion and probability of the n-best analyses yielding them, and so forth – broadly trading accuracy and greater domaindependence against speed and reduced sensitivity to domain-specific lexical behaviour (Briscoe and Carroll, 2002; Carroll and Briscoe, 2002; Watson et al., 2005; Watson, 2006).
The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ Kiefer et al., 1999).

The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ The output of the parser can be displayed as syntactic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents.
The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ The few features that cannot be computed from GRs and CLAWS tags directly, such as stmt type, could be computed from the derivation tree.
