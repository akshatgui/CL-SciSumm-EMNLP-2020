More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner(2006) for machine translation.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ Here, following Smith and Eisner (2006), we usea weighted, quasi-synchronous dependency grammar.

Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ Augmented with the q.-side dependency la bel.child-parent Question parent-child pair align respectively to answer child-parent pair.
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ same node Question parent-child pair align to the same answer-word.siblings Question parent-child pair align to sib lings in the answer.

Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA
Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ Here, following Smith and Eisner (2006), we usea weighted, quasi-synchronous dependency grammar.

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner(2006) for machine translation.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Cui et al.

 $$$$$ The player knows the answer (or at least thinks he knows the answer) and must quickly turn it into a question.2 The question-answer pairs used on Jeopardy!
 $$$$$ This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164.

We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ To evaluate our model, we conducted experiments using Text REtrieval Conference (TREC) 8?13 QA dataset.8 5.1 Experimental Setup.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ Cui et al.

Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ Because we are interested in augmenting the QG with additional lexical-semantic knowledge, we also estimate pkid by mixing the base model with a model that exploits WordNet (Miller et al, 1990) lexical-semantic relations.
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ Both results are reported.

Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ Any algorithm will get the same performance on these questions, and thereforeobscures the evaluation results.
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ 5.3 Results.

 $$$$$ The player knows the answer (or at least thinks he knows the answer) and must quickly turn it into a question.2 The question-answer pairs used on Jeopardy!
 $$$$$ This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164.

Experiments were conducted to evaluate tree edit models for three tasks $$$$$ Our task is the same as Pun yakanok et al (2004) and Cui et al (2005), where we search for single-sentence answers to factoid questions.
Experiments were conducted to evaluate tree edit models for three tasks $$$$$ Cui et al.

 $$$$$ The player knows the answer (or at least thinks he knows the answer) and must quickly turn it into a question.2 The question-answer pairs used on Jeopardy!
 $$$$$ This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164.

For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ Our task is the same as Pun yakanok et al (2004) and Cui et al (2005), where we search for single-sentence answers to factoid questions.
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ We used the questions in TREC 8?12 for training and set aside TREC 13 questions for development(84 questions) and testing (100 questions).

The experimental setup is the same as in Wang et al (2007). $$$$$ To evaluate our model, we conducted experiments using Text REtrieval Conference (TREC) 8?13 QA dataset.8 5.1 Experimental Setup.
The experimental setup is the same as in Wang et al (2007). $$$$$ Cui et al.

We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ 5.2 Baseline Systems.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ Both results are reported.

The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ Both results are reported.
The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ 5.3 Results.

This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ Our task is the same as Pun yakanok et al (2004) and Cui et al (2005), where we search for single-sentence answers to factoid questions.
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ Cui et al.

 $$$$$ The player knows the answer (or at least thinks he knows the answer) and must quickly turn it into a question.2 The question-answer pairs used on Jeopardy!
 $$$$$ This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164.

In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner(2006) for machine translation.
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ translation.

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ To model the syntactic transformation process, re searchers in these fields?especially in machine translation?have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al, 2005;Galley et al, 2006; Smith and Eisner, 2006, in ter alia).
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ translation.

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Here, following Smith and Eisner (2006), we usea weighted, quasi-synchronous dependency grammar.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ This model is very sim ilar to Smith and Eisner (2006).
