It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.
It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows $$$$$ The interesting point is that a search with the ITG constraints cannot generate a word-reordering that contains one of these two subsequences.

A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). $$$$$ Afterwards, we will describe the IBM constraints.
A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). $$$$$ We used a single-word based search with IBM Model 4.

(Zens and Ney, 2003) did comparative study over different reordering constraints. $$$$$ A Comparative Study On Reordering Constraints In Statistical Machine Translation
(Zens and Ney, 2003) did comparative study over different reordering constraints. $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.

6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ 5 the results for the Verbmobil task are shown.
6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ We see that the results on this task are similar.

Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). $$$$$ The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997).
Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). $$$$$ In this section, we will describe the IBM constraints (Berger et al., 1996).

xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ If arbitrary wordreorderings are permitted, the search problem is NP-hard.
xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible.

 $$$$$ In the beginning, all source positions are uncovered.
 $$$$$ Future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.

Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. $$$$$ 4 shows the results for the Verbmobil task and for the Canadian Hansards task.
Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. $$$$$ It contains the results for both translation directions German-English (S—*T) and English-German (T—*S) for the Verbmobil task and French-English (S—*T) and English-French (T—*S) for the Canadian Hansards task, respectively.

ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.
ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). $$$$$ In the following, we will call these the ITG constraints.

For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). $$$$$ This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.
For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). $$$$$ For a more detailed analysis, subjective judgments by test persons are necessary.

Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.
Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ Afterwards, we will describe the IBM constraints.

Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ The second constraints are the IBM constraints (Berger et al., 1996).
Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ In this section, we will describe the IBM constraints (Berger et al., 1996).

The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar $$$$$ This representation is equivalent to the parse trees of the simple grammar in (Wu, 1997).
The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar $$$$$ We used a single-word based search with IBM Model 4.

Some of the properties of these alignments are studied in (Zens and Ney, 2003). $$$$$ The evaluation consists of two parts

A comparison of both methods can be found in Zens and Ney (2003). $$$$$ This comparison includes a theoretical discussion on the permitted number of reorderings for each of these constraints.
A comparison of both methods can be found in Zens and Ney (2003). $$$$$ These methods cannot be applied to the CYK-style search for the ITG constraints.

A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ Afterwards, we will describe the IBM constraints.
A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ We have described the ITG constraints in detail and compared them to the IBM constraints.

Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ It contains the results for both translation directions German-English (S—*T) and English-German (T—*S) for the Verbmobil task and French-English (S—*T) and English-French (T—*S) for the Canadian Hansards task, respectively.
Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ For the Canadian Hansards task, the baseline ITG constraints yield a worse coverage than the IBM constraints.

Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ Obviously, these productions are not in the normal form of an ITG, but with the method described in (Wu, 1997), they can be normalized.
Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ The ITG constraints were introduced in (Wu, 1995).

This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ The first constraints are based on inversion transduction grammars (ITG) (Wu, 1995; Wu, 1997).
This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ In (Vilar, 1998) a model similar to Wu’s method was considered.

 $$$$$ In the beginning, all source positions are uncovered.
 $$$$$ Future work will include the automatic extraction of the bilingual grammar as well as the use of this grammar for the translation process.
