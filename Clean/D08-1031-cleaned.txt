 $$$$$ Descriptions of the methods for computing these features are described next.
 $$$$$ This work is partly supported by NSF grant SoD-HCER-0613885 and a grant from Boeing.

To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). $$$$$ Understanding the Value of Features for Coreference Resolution
To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)). $$$$$ This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features.

Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient - it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident. $$$$$ We do not use any other gold annotated input at evaluation time.
Bengtson and Roth (2008) simply discard twinless CEs, but this solution is likely too lenient - it doles no punishment for mistakes on twinless annotated or extracted CEs and it would be tricked, for example, by a system that extracts only the CEs about which it is most confident. $$$$$ In Section 6 experiments we do not use any gold annotated input and do not assume mention types or boundaries are given.

 $$$$$ Descriptions of the methods for computing these features are described next.
 $$$$$ This work is partly supported by NSF grant SoD-HCER-0613885 and a grant from Boeing.

 $$$$$ Descriptions of the methods for computing these features are described next.
 $$$$$ This work is partly supported by NSF grant SoD-HCER-0613885 and a grant from Boeing.

Inaddition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. $$$$$ Our results show that a pairwise model with strong features outperforms a state-of-the-art system with a more complex model.
Inaddition, the architecture and system components of Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. $$$$$ We described and evaluated a state-of-the-art coreference system based on a pairwise model and strong features.

 $$$$$ Descriptions of the methods for computing these features are described next.
 $$$$$ This work is partly supported by NSF grant SoD-HCER-0613885 and a grant from Boeing.

Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. $$$$$ Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b).
Building on elements of the coreference system described in Bengtson and Roth (2008), we design an end-to-end system (Sec. 2) that identifies candidate mentions and then applies one of two inference protocols, Best-Link and All-Link (Sec. 2.3), to disambiguate and cluster them. $$$$$ The results of our end-to-end coreference system are shown in Table 7.

Illinois-Coref follows the architecture used in Bengtson and Roth (2008). $$$$$ We suggest that our system can be used as a baseline for the development of more complex models – which may have less impact when a more robust set of features is used.
Illinois-Coref follows the architecture used in Bengtson and Roth (2008). $$$$$ Distances have been used in e.g.

For the ACE 2004 coreference task, a good performance in mention detection is typically achieved by training a classifier e.g., (Bengtson and Roth, 2008). $$$$$ The ACE training data provides the equivalence classes for mentions.
For the ACE 2004 coreference task, a good performance in mention detection is typically achieved by training a classifier e.g., (Bengtson and Roth, 2008). $$$$$ We use the official ACE 2004 English training data (NIST, 2004).

We use the same features as Bengtson and Roth (2008), with the knowledge extracted from the OntoNotes-4.0 annotation. $$$$$ We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 2007).
We use the same features as Bengtson and Roth (2008), with the knowledge extracted from the OntoNotes-4.0 annotation. $$$$$ This type of example may require some additional world knowledge or deeper comprehension of the document.

Although its strategy is simple, Bengtson and Roth (2008) show that with a careful design, it can achieve highly competitive performance. $$$$$ This paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features.
Although its strategy is simple, Bengtson and Roth (2008) show that with a careful design, it can achieve highly competitive performance. $$$$$ This paper introduces a rather simple but stateof-the-art system, which we intend to be used as a strong baseline to evaluate the impact of structural innovations.

For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008). $$$$$ To enable the system to learn such patterns, we treat the presence or absence of each pair of final head nouns, one from each mention of an example, as a feature.
For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008). $$$$$ These features are boolean: For any given pair, a feature is active if that pair describes the example.

For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008). $$$$$ These features are boolean: For any given pair, a feature is active if that pair describes the example.
For an empirical evaluation of the contribution of a subset of these features to the mention-pair model, see Bengtson and Roth (2008). $$$$$ Although this evaluation is lenient, given that the mention detection component performs at over 90% F1, we believe it provides a realistic measure for the performance of the end-to-end system and focuses the evaluation on the coreference component.

 $$$$$ Descriptions of the methods for computing these features are described next.
 $$$$$ This work is partly supported by NSF grant SoD-HCER-0613885 and a grant from Boeing.

We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. $$$$$ Understanding the Value of Features for Coreference Resolution
We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. $$$$$ We described and evaluated a state-of-the-art coreference system based on a pairwise model and strong features.

Note that we solve the above Best-Link inference using an efficient algorithm (Bengtson and Roth, 2008) which runs in time quadratic in the number of mentions. $$$$$ Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b).
Note that we solve the above Best-Link inference using an efficient algorithm (Bengtson and Roth, 2008) which runs in time quadratic in the number of mentions. $$$$$ However, we are not aware of any system using the number of compatible mentions as a distance.

The baseline system applies the strategy in (Bengtson and Roth, 2008, Section 2.2) to learn the pairwise scoring functions using the Averaged Perceptron algorithm. $$$$$ We learn the pairwise coreference function using an averaged perceptron learning algorithm (Freund and Schapire, 1998) – we use the regularized version in Learning Based Java2 (Rizzolo and Roth, 2007).
The baseline system applies the strategy in (Bengtson and Roth, 2008, Section 2.2) to learn the pairwise scoring functions using the Averaged Perceptron algorithm. $$$$$ We train a regularized average perceptron using examples selected as described in Section 2.2.1.

Among them the mention pair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the state of-the-art performance (Bengtson and Roth, 2008). $$$$$ Our results show that a pairwise model with strong features outperforms a state-of-the-art system with a more complex model.
Among them the mention pair model (McCarthy and Lehnert, 1995) is one of the most influential ones and can achieve the state of-the-art performance (Bengtson and Roth, 2008). $$$$$ We described and evaluated a state-of-the-art coreference system based on a pairwise model and strong features.

Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. $$$$$ Best-Link was shown to outperform Closest-Link in an experiment by Ng and Cardie (2002b).
Best-first clustering has been previously studied by Ng and Cardie (2002) and Bengtson and Roth (2008) and found to be effective. $$$$$ This is similar to the technique of Ng and Cardie (2002b).
