See (Gildea and Jurafsky, 2000) for some promising initial work in applying statistical techniques to the FrameNet database to automatically label frame elements. $$$$$ We apply statistical techniques that have been successful for these tasks, including probabilistic parsing and statistical classification.
See (Gildea and Jurafsky, 2000) for some promising initial work in applying statistical techniques to the FrameNet database to automatically label frame elements. $$$$$ The FrameNet database defines a tagset of semantic roles called frame elements, and includes roughly 50,000 sentences from the British National Corpus which have been hand-labeled with these frame elements.

While a machine learning approach is used in (Gildea and Jurafsky, 2000) to determine general semantic roles, we used a simple rule-based traversal of the parse tree instead, which could also reliably determine the generic agent and patient role of a sentence, and this suffices for our current purpose. $$$$$ Identifying these roles, for example, could allow a system to determine that in the sentence &quot;The first one crashed&quot; the subject is the vehicle, but in the sentence &quot;The first one crashed it&quot; the subject is the agent, which would help in information extraction in this domain.
While a machine learning approach is used in (Gildea and Jurafsky, 2000) to determine general semantic roles, we used a simple rule-based traversal of the parse tree instead, which could also reliably determine the generic agent and patient role of a sentence, and this suffices for our current purpose. $$$$$ Historically, two types of semantic roles have been studied: abstract roles such as AGENT and PATiENT, and roles specific to individual verbs such as EATER and EATEN for &quot;eat&quot;.

Many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles. $$$$$ Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge.
Many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2000), showed that lexical and syntactic information is very useful for predicate argument recognition tasks, such as semantic roles. $$$$$ More recently, a domain independent system has been trained on general function tags such as MANNER and TEMpoRAL by Blaheta and Charniak (2000).

Gildea and Jurafsky (2000, 2002) describe a statistical approach for semantic role labelling using data collected from FrameNet. $$$$$ Our approach to semantic analysis is to treat the problem of semantic role labeling like the similar problems of parsing, part of speech tagging, and word sense disambiguation.
Gildea and Jurafsky (2000, 2002) describe a statistical approach for semantic role labelling using data collected from FrameNet. $$$$$ Probabilities for each possible semantic role r are then computed from the features.

(Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles. $$$$$ The next section describes the set of frame elements/semantic roles used by our system.
(Pado et al., 2008) describe an unsupervised approach that, like ours, uses verbal argument patterns to deduce deverbal patterns, though the resulting labels are semantic roles used in SLR tasks (cf. (Gildea and Jurafsky, 2000)) rather than syntactic roles. $$$$$ Rather, examples are chosen to illustrate typical usage patterns for each word.

 $$$$$ The probability computation will be described in the next section; the features include: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).
 $$$$$ We plan to continue this work by integrating semantic role identification with parsing, by bootstrapping the system on larger, and more representative, amounts of data, and by attempting to generalize from the set of predicates chosen by FrameNet for annotation to general text.

Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). $$$$$ Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al., 1998).
Starting with Gildea and Jurafsky (2000), a number of studies have developed (almost exclusively statistical) models of this task, e.g. Thompson et al. (2003) and Fleischman et al. (2003). $$$$$ For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as &quot;Atlanta&quot; filled a semantic slot such as DESTiNATioN in a semantic frame for air travel.

Since direct assignment of role labels to instances fails due to the preponderance of unlabelled instances, which make up 86.7% of all instances, we follow Gildea and Jurafsky (2000) in splitting the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. $$$$$ Probabilities for each possible semantic role r are then computed from the features.
Since direct assignment of role labels to instances fails due to the preponderance of unlabelled instances, which make up 86.7% of all instances, we follow Gildea and Jurafsky (2000) in splitting the task into two sequential subtasks: first, argument recognition decides for each instance whether it bears a semantic role or not; then, argument labelling assigns a label to instances recognised as role-bearers. $$$$$ The clustered head word alone correctly classified 79.7% of the cases where the head word was in the vocabulary used for clustering; 97.9% of instances of nominal head words were in the vocabulary.

There has been some related work on using the frame of FrameNet for reasoning (Chang et al, 2002) and also on the automatic annotation of English texts with regard to the relevant frames (Gildea and Jurafsky, 2000) and frame elements. $$$$$ Our statistical algorithms are trained on a hand-labeled dataset: the FrameNet database (Baker et al., 1998).
There has been some related work on using the frame of FrameNet for reasoning (Chang et al, 2002) and also on the automatic annotation of English texts with regard to the relevant frames (Gildea and Jurafsky, 2000) and frame elements. $$$$$ The experiment was performed using only frame elements with a noun as head word.

To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier. $$$$$ Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge.
To our knowledge, Gildea and Jurafsky (2000) is the only work that uses FrameNet to build a statistical semantic classifier. $$$$$ The system is a statistical one, based on training a classifier on a labeled training set, and testing on an unlabeled test set.

Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence. $$$$$ Roughly 5% of the examples were identified as passive uses.
Gildea and Jurafsky (2000) describe a system that uses completely syntactic features to classify the Frame Elements in a sentence. $$$$$ Experiments were conducted using features similar to those described above to identify constituents in a sentence's parse tree that were likely to be frame elements.

We extend Gildea and Jurafsky (2000)'s initial effort in three ways. $$$$$ Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge.
We extend Gildea and Jurafsky (2000)'s initial effort in three ways. $$$$$ Although we expect our features to interact in various ways, the data are too sparse to calculate probabilities directly on the full set of features.

Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000). $$$$$ Example sentences are shown in Table 1.
Training (32,251 sentences), development (3,491 sentences), and held out test sets (3,398 sentences) were generated from the June 2002 FrameNet release following the divisions used in Gildea and Jurafsky (2000). $$$$$ Results for this system on test data, held out during development of the system, are shown in Table correct, without use of any of the syntactic features.

Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). $$$$$ Since the parser used assigns each constituent a head word as an integral part of the parsing model, we were able to read the head words of the constituents from the parser output.
Due to data sparsity issues, we do not calculate this model directly, but rather, model various feature combinations as described in Gildea and Jurafsky (2000). $$$$$ Although we expect our features to interact in various ways, the data are too sparse to calculate probabilities directly on the full set of features.

Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. $$$$$ The system is a statistical one, based on training a classifier on a labeled training set, and testing on an unlabeled test set.
Gildea and Jurafsky (2000) use 36995 training, 4000 development, and 3865 test sentences. $$$$$ Results for this system on test data, held out during development of the system, are shown in Table correct, without use of any of the syntactic features.

As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000). $$$$$ The system is a statistical one, based on training a classifier on a labeled training set, and testing on an unlabeled test set.
As a further analysis, we have examined the performance of our base ME model on the same test set as that used in Gildea and Jurafsky (2000). $$$$$ For our experiments, we divided the FrameNet corpus as follows: one-tenth of the annotated sentences for each target word were reserved as a test set, and another one-tenth were set aside as a tuning set for developing our system.

Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. $$$$$ The probability computation will be described in the next section; the features include: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).
Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. $$$$$ A system using grammatical function, along with the head word, phrase type, and target word, but no passive information, scored 79.2%.

(Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. $$$$$ The FrameNet project proposes roles at an intermediate level, that of the semantic frame.
(Gildea and Jurafsky, 2000) proposed a statistical approach based on FrameNet I data for annotation of semantic roles. $$$$$ The preliminary version of the FrameNet corpus used for our experiments contained 67 frames from 12 general semantic domains chosen for annotation.

The features used for training the labeler are a subset of the features used by Gildea and Jurafsky (2000), Xue and Palmer (2004), and Pradhan et al (2004). $$$$$ Various lexical and syntactic features are derived from parse trees and used to derive statistical classifiers from hand-annotated training data.
The features used for training the labeler are a subset of the features used by Gildea and Jurafsky (2000), Xue and Palmer (2004), and Pradhan et al (2004). $$$$$ Lexical statistics computed on constituent head words were found to be the most important of the features used.
