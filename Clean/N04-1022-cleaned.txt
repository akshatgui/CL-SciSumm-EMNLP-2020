Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics.

This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ This is measured through Bayes-Risk : The expectation is taken under the true distribution that describes translations of human quality.

Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ Our goal is to find the decoder that has the best performance over all translations.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ We therefore use statistical translation models (Och, 2002) to approximate the distribution . tion 3) on the -best List is implemented as and .

For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ We report the performance of the MBR decoders on a Chinese-to-English translation task.
For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ We finally report the performance of MBR decoders optimized for each loss function.

Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).

We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ In contrast, the maximum likelihood techniques that underlie the decision processes of most current MT systems do not take into account these application specific goals.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ Given a loss function and a distribution, it is well known that the decision rule that minimizes the BayesRisk is given by (Bickel and Doksum, 1977; Goel and Byrne, 2000): We shall refer to the decoder given by this equation as the Minimum Bayes-Risk (MBR) decoder.

We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ Our goal is to find the decoder that has the best performance over all translations.
We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ Each Chinese sentence in this set has four reference translations.

Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We present results under the Bitree loss function as an example of incorporating linguistic information into a loss function; we have not yet measured its correlation with human assessments of translation quality.

For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics.

To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.

With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ The 1000-best lists were then rescored using the different translation loss functions described in Section 2.

Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ In practice, we will consider the space of translations to be an -best list of translation alternatives generated under a baseline translation model.

Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004). $$$$$ Minimum Bayes-Risk Decoding For Statistical Machine Translation
Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
