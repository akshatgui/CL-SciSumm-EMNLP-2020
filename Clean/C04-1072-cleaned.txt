In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ In this paper, we introduce a new evaluation method, ORANGE, for evaluating automatic machine translation evaluation metrics automatically without extra human involvement other than using a set of reference translations.
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ (Oracle1 Ranking for Gisting Evaluation) and the smaller the ratio is, the better the automatic metric is. There are several advantages of the proposed ORANGE evaluation method

This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ We adopt this assumption and add one more assumption that automatic translations are usually worst than their reference translations.
This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. $$$$$ From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.
156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).
ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores.

BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. $$$$$ Therefore, Y1 would be ranked higher than Y2 using WLCS.
BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. $$$$$ We call the smoothed BLEU

Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).
Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ We call the smoothed BLEU

Stemming is enabled (Lin and Och, 2004a). $$$$$ For example, a statistical machine translation system such as ISI?s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence.
Stemming is enabled (Lin and Och, 2004a). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Turian et al (2003) introduced General Text Matcher (GTM) based on accuracy measures such as recall, precision, and F-measure.
Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Another possible function family is the polynomial family of the form k?

BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. $$$$$ From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.
BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. $$$$$ We call the smoothed BLEU

We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). $$$$$ correlation coefficients are computed according to different automatic evaluation methods vs. human assigned adequacy and fluency.
We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).
Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ In order to compute BLEU at sentence level, we apply the following smoothing technique

As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ The other potential problem for correlation analysis of human vs. automatic framework is that high corpus-level correlation might not translate to high sentence-level correlation.
As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided.

The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided.
The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). $$$$$ Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided.
As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

They obtained similar results in both cases (Lin and Och, 2004a). $$$$$ Table 7 shows the results.
They obtained similar results in both cases (Lin and Och, 2004a). $$$$$ We then check the portion of the cases where machine translations are as good as the human translations.

While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ Turian et al (2003) introduced General Text Matcher (GTM) based on accuracy measures such as recall, precision, and F-measure.
While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 respectively.

The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).
The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences.

As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. $$$$$ From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.
As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. $$$$$ ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).

Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. $$$$$ Statistics Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.
Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. $$$$$ the gunman police killed each sentence has C(4,2)2 = 6 skip-bigrams.

Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y). $$$$$ Skip-bigram cooccurrence statistics measure the overlap of skip bigrams between a candidate translation and a set of reference translations.
Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y). $$$$$ sentences.
