Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.

In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ In including an estimate of the outside probability, our technique is related to A* methods for monolingual parsing (Klein and Manning, 2003), although our estimate is not guaranteed to be lower than complete outside probabity assigned by ITG.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern.

The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al. (1996).
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ λL1,L2 is the probability of any word alignment template for a pair of L1word source string and L2-word target string, which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths, following Brown et al. (1993).

 $$$$$ Since we are only lexicalizing rather than bilexicalizing the rules, the non-head constituents need to be lexicalized using head generation rules so that the top-down generation process can proceed in all branches.
 $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.

Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs

We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ In this experiment, we didn’t apply the pruning techniques for the lexicalized ITG.
We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.

In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ Zhang and Gildea (2004) found ITG to outperform the tree-to-string model for word-level alignment, as measured against human gold-standard alignments.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ The interaction between lexical information and word order also explains the higher performance of IBM model 4 over IBM model 3 for alignment.

Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ This can be also be thought of as squeezing together the four outside corners, creating a new cell whose probability is estimated using IBM Model 1.
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ The performance of the full model of unlexicalized ITG is compared with the pruned model of lexicalized ITG using more training data and evaluation data.

Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ Fortunately, we can exploit the recursive nature of the cells.

For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ We use IBM Model 1 as our estimate of both the inside and outside probabilities.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.

Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ We use IBM Model 1 as our estimate of both the inside and outside probabilities.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ λL1,L2 is the probability of any word alignment template for a pair of L1word source string and L2-word target string, which we model as a uniform distribution of word-forword alignment patterns after a Poisson distribution of target string’s possible lengths, following Brown et al. (1993).

Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10−5 and the number k for the top-k pruning as 25.

Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ We prune cells whose probability is lower than a fixed ratio below the best cell for the same source substring.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.

Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ It differs in that the head words are chosen through EM rather than deterministic rules.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Once the cells have been scored, there can be many ways of pruning.

The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ Hence, the algorithm takes just O(n4) steps to compute the figure of merit for all cells in the chart.

Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ Figure 3(a) displays the tic-tac-toe pattern for the inside and outside components of a particular cell.
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ Hence, the algorithm takes just O(n4) steps to compute the figure of merit for all cells in the chart.
