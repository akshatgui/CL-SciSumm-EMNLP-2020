Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ 4 yields parameters that are optimal with respect to translation quality.

The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ Table 1 provides some statistics on the training, development and test corpus used.
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ The development corpus was used to optimize the parameters of the log-linear model.

 $$$$$ In this paper, we use the following methods

We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002).
We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ The use of log-linear models for statistical machine translation was suggested by Papineni et al. (1997) and Och and Ney (2002).

 $$$$$ In this paper, we use the following methods

Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ The basic feature functions of our model are identical to the alignment template approach (Och and Ney, 2002).

In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ Minimum Error Rate Training In Statistical Machine Translation
In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ Which error rate should be optimized during training?

The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ Minimum Error Rate Training In Statistical Machine Translation
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ Which error rate should be optimized during training?

Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ The basic feature functions were trained using the training corpus.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ Between BLEU and NIST, the differences are more moderate, but by optimizing on NIST, we still obtain a large improvement when measured with NIST compared to optimizing on BLEU.

We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ Minimum Error Rate Training In Statistical Machine Translation
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ Which error rate should be optimized during training?

Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ In this framework, we have a set of feature functions .
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ Note, that the resulting objective function might still have local optima, which makes the optimization hard compared to using the objective function of Eq.

The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ BLEU score

All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ Minimum Error Rate Training In Statistical Machine Translation
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ Which error rate should be optimized during training?

 $$$$$ In this paper, we use the following methods

We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ Minimum Error Rate Training In Statistical Machine Translation
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.

The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ Minimum Error Rate Training In Statistical Machine Translation
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002).

The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ This can happen because with the modified model scaling factors the -best list can change significantly and can include sentences not in the existing-best list.
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ Which error rate should be optimized during training?

In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ We use .
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ In addition to the feature functions described in (Och and Ney, 2002), our system includes a phrase penalty (the number of alignment templates used) and special alignment features.

We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ Minimum Error Rate Training In Statistical Machine Translation
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.

The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ In this framework, we have a set of feature functions .
The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ 5) using a log-linear model (Eq.
