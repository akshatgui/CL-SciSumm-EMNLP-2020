Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. $$$$$ We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. $$$$$ We c ? 2008.

As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. $$$$$ We c ? 2008.
As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. $$$$$ In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense.

Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. $$$$$ Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features.
Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. $$$$$ Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.

The best results of 92.15% are reported by De Felice and Pulman (2008). $$$$$ We c ? 2008.
The best results of 92.15% are reported by De Felice and Pulman (2008). $$$$$ The best reported results to date on determiner selection are those in Turner and Charniak (2007).

In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English
In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ We c ? 2008.

 $$$$$ We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.
 $$$$$ Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.

T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ We c ? 2008.
T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be.

On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ 170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)).

On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. $$$$$ We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. $$$$$ We c ? 2008.

Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. $$$$$ We c ? 2008.
Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. $$$$$ Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter.

 $$$$$ We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.
 $$$$$ Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.
