Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ Our extension to the model in Cohen et al. (2008) follows naturally after we have defined the shared LN distribution.

To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly.

Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ We provide a variational EM algorithm for inference.
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ In our experiments, we use this variational EM algorithm on a training set, and then use the normal experts’ means to get a point estimate for θ, the grammar weights.

 $$$$$ 2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ We define a shared logistic normal distribution with N “experts” over a collection of K multinomial distributions.

 $$$$$ 2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

 $$$$$ 2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ In §3, we present our model and a variational inference algorithm for it.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.

While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y).
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.

While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ Parts of speech are matched through the single coarse tagset (footnote 4).

Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ Bold marks best overall accuracy per length bound, and † marks figures that are not significantly worse (binomial sign test, p < 0.05).
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.

The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM).
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ A tree y is defined by a pair of functions yleft and yright (both 10, 1, 2,..., n} __+ 2{1,2,...,n}) that map each word to its sets of left and right dependents, respectively.

These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN.
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Smith’s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.

Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ This performance measure is also known as attachment accuracy.
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.

 $$$$$ 2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Following this observation, we compare four different settings in our experiments (all SLN settings include one normal expert for each multinomial on its own, equivalent to the regular LN setting from Cohen et al. ): bilities corresponding to a verbal parent (any parent, using the coarse tags of Cohen et al., 2008).

Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. $$$$$ Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. $$$$$ Our experiments found that these improvements do not hold on longer sentences.

Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ Our variational inference algorithm is derived similarly to that of Cohen et al. (2008).
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ For the covariance matrices, we follow the setting in Cohen et al. (2008) in our experiments also described in §3.1.

The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ The performance on English improved significantly in the bilingual setting, achieving highest performance with TIEV&N.

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ The baselines include right attachment (where each word is attached to the word to its right), MLE via EM (Klein and Manning, 2004), and empirical Bayes with Dirichlet and LN priors (Cohen et al., 2008).
