The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ Building A Sense Tagged Corpus With Open Mind Word Expert
The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ For each word with sense tagged data created with Open Mind Word Expert, a test corpus will be built by trained human taggers, starting with examples extracted from the corpus mentioned in Section 3.1.

The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). $$$$$ Building A Sense Tagged Corpus With Open Mind Word Expert
The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). $$$$$ Open Mind Word Expert is a newly born project that follows the Open Mind initiative (Stork, 1999).

Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). $$$$$ Building A Sense Tagged Corpus With Open Mind Word Expert
Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). $$$$$ For each word with sense tagged data created with Open Mind Word Expert, a test corpus will be built by trained human taggers, starting with examples extracted from the corpus mentioned in Section 3.1.

Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ Building A Sense Tagged Corpus With Open Mind Word Expert
Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ The tagging process is usually done by trained lexicographers, and consequently is quite expensive, limiting the size of such corpora to a handful of tagged texts.

The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ Building A Sense Tagged Corpus With Open Mind Word Expert
The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ For each word with sense tagged data created with Open Mind Word Expert, a test corpus will be built by trained human taggers, starting with examples extracted from the corpus mentioned in Section 3.1.

Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.
Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ The tagging will be collected over the Web from volunteer contributors.

Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ Open Mind Word Expert is a Web-based interface where users can tag words with their WordNet senses.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ Indeed, the tagging accuracy measured on the set where both COBALT and STAFS assign the same label is 82.5%, a figure that is close to the 85.5% inter-annotator agreement measured for the SENSEVAL-2 nouns (Kilgarriff, 2002).

The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ That is, for each ambiguous word for which we want to build a sense tagged corpus, users are presented with a set of natural language (English) sentences that include an instance of the ambiguous word.
The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ The two classifiers are trained on a relatively small corpus of tagged data, which is formed either with (1) Senseval training examples, in the case of Senseval words, or (2) examples obtained with the Open Mind Word Expert system itself, when no other training data is available.

For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet senses.
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ According to (Dagan et al., 1995), there are two main types of active learning.

We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. $$$$$ Initially, example sentences are extracted from a large textual corpus.
We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. $$$$$ The system treats every training example as a set of soft constraints on the sense of the word of interest.

Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ This paper introduces Open Mind Word Expert, a Web-based system that aims at creating large sense tagged corpora with the help of Web users.
Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ These are the examples that are presented to the users for tagging in Stage 2.

Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.
Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ In Open Mind Word Expert, these are the instances that are presented to the users for tagging in the active learning stage.

Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002). $$$$$ One of the first large scale hand tagging efforts is reported in (Miller et al., 1993), where a subset of the Brown corpus was tagged with WordNet senses.
Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002). $$$$$ The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora.
