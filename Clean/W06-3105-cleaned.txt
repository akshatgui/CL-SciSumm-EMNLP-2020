DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. $$$$$ In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.
DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. $$$$$ The generative process modeled has four steps

DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. $$$$$ The generative process modeled has four steps

This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair.

140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. $$$$$ At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability.
140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. $$$$$ We tested on the first 1,000 unique sentences of length 5 to 15 in the corpus and trained on sentences of length 1 to 60 starting after the first 10,000.

These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. $$$$$ While in some cases, such as idiomatic vs. literal translations, two segmentations may be in true competition, we show that the most common result is for different segmentations to be recruited for different examples, overfitting the training data and overly determinizing the phrase translation estimates.
These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. $$$$$ We show that estimates are overly determinized because segmentations are used in unintuitive ways for the sake of data likelihood.

As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ We first propose a simple generative, phrase-based model and verify that its estimates are inferior to those given by surface statistics.
As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).

In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.
In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.

Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. $$$$$ The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word.
Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. $$$$$ Unlike previous attempts to train a similar model (Marcu and Wong, 2002), we allow information from a word-alignment model to inform our approximation.

We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ Thus, after training, the parameters of the phrase translation model φEM can be used directly for decoding.

The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.
The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.

While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ In practice, however, this approach has not led to an improvement in BLEU.

thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. $$$$$ Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993).
thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. $$$$$ We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).

For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ The generative process we modeled produces a phrase-aligned English sentence from a French sentence where the former is a translation of the latter.
For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ The learned parameters from this model will be used to translate sentences from English to French.

This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). $$$$$ The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.

Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ A maximum phrase length of three was used for all experiments.
Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ As more probability mass is reserved for fewer translations, many of the alternative translations under φH are assigned prohibitively small probabilities.

DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ This training loop necessitates approximation because summing over all possible segmentations and alignments for each sentence is intractable, requiring time exponential in the length of the sentences.
DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).

The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006). $$$$$ The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.

 $$$$$ However, the results in figure 1 of the following section show that OEM trained on twice as much data as OH still underperforms the heuristic, indicating a larger issue than decreased training set size.
 $$$$$ A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.

If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.
If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ To test the relative performance of OEM and OH, we evaluated each using an end-to-end translation system from English to French.

This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ The ambiguous process of translation can be modeled either by the latent segmentation variable or the phrase translation probabilities.
