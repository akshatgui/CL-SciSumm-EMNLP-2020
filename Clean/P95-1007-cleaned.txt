A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). $$$$$ Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable.
A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). $$$$$ It has the effect of dividing the evidence from a training instance across all possible categories for the words.

As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. $$$$$ Comparisons are made across five dimensions: While Hindle and Rooth (1993) use a partial parser to acquire training data, such machinery appears unnecessary for noun compounds.
As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. $$$$$ At the very least, this information can be applied in broad coverage parsing to assist in the control of search.

Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. $$$$$ It uses what I will call the DEPENDENCY MODEL.
Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. $$$$$ As can be seen, the dependency model is more accurate than the adjacency model.

This is an extension of left branch preference in Lauer (1995). $$$$$ Another significant difference between the models is the predictions they make about the proportion 'Lauer and Dras (1994) give a formal construction motivating the algorithm given in Lauer (1994). of left and right-branching compounds.
This is an extension of left branch preference in Lauer (1995). $$$$$ This will conceal any preference given by the parameters involving 11.

 $$$$$ Given a three word compound, a search is conducted elsewhere in the corpus for each of the two possible subcomponents.
 $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.

Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.68 Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). $$$$$ In contrast, the adjacency model appears to predict a proportion of 50%.
Lauer (1995): adjacency 68.90 Lauer (1995): dependency 77.50 Best Altavista 78.68 Lauer (1995): tuned 80.70 Upper bound 81.50 Table 9: Performance comparison with the literature for compound bracketing 1993). $$$$$ In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia.

Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. $$$$$ Two types of training scheme are explored in this study, both unsupervised.
Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. $$$$$ Lauer and Dras (1994) suggest two improvements to the method used above.

Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. $$$$$ A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way.2 Because the corpus is not tagged or parsed, a somewhat conservative strategy of looking for unambiguous sequences of nouns was used.
Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. $$$$$ In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia.

Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ Two types of training scheme are explored in this study, both unsupervised.
Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ For the adjacency model, when the given compound is w1w2w3, we can estimate this ratio as: In both cases, we sum over all possible categories for the words in the compound.

Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ A test set of syntactically ambiguous noun compounds was extracted from our 8 million word Grolier's encyclopedia corpus in the following way.2 Because the corpus is not tagged or parsed, a somewhat conservative strategy of looking for unambiguous sequences of nouns was used.
Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ In the case of the pattern training scheme, the difference between 68.9% for adjacency and 77.5% for dependency is statistically significant at the 5% level (p = 0.0316), demonstrating the superiority of the dependency model, at least for the compounds within Grolier's encyclopedia.

The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ The remaining compounds were assigned either a left-branching or right-branching analysis.
The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one.

Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ A similar architecture may be applied to noun compounds.
Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate.

Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ This paper explores the application of corpus statistics (Charniak, 1993) to noun compound parsing (other computational problems are addressed in Arens et al, 1987; Vanderwende, 1993 and Sproat, 1994).
Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ The simplest of these is reported in Pustejovsky et al (1993).

We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauers set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. $$$$$ Corpus Statistics Meet The Noun Compound: Some Empirical Results
We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets: Lauers set of 8 prepositions (Lauer, 1995) and our list of 22 semantic relations. $$$$$ Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model.

On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ Corpus Statistics Meet The Noun Compound: Some Empirical Results
On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing.

They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). $$$$$ Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable.
They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). $$$$$ This is derived from the idea that parse trees capture the structure of semantic relationships within a noun compound.'

(Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. $$$$$ There are at least four existing corpus-based algorithms proposed for syntactically analysing noun compounds.
(Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps: adjacency and dependency. $$$$$ The experiments above demonstrate a number of important points.

(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ In Lauer (1994), the degree of acceptability is again provided by statistical measures over a corpus.
(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ The equations presented above for the dependency model differ from those developed in Lauer and Dras (1994) in one way.

(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ As can be seen, the dependency model is more accurate than the adjacency model.
(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ While these changes are motivated by the dependency model, I have also applied them to the adjacency model for comparison.

This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ All consecutive sequences of these words were extracted, and the three word sequences used to form the test set.
This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model.
