Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ Sentence Compression Beyond Word Deletion
Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ There are many rewrite operations that could compress a sentence, besides deletion, including reordering, substitution, and inser tion.

 $$$$$ ?he, he?
 $$$$$ Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.

For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ We hope that some of the work described here might be of relevance to other gen eration tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999).
For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ (3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details).

Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ (3) where the subscripts s and m denote slack and margin rescaling, which are different formulations of the training problem (see Tsochantaridis et al (2005) and Taskar et al (2003) for details).
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ In this section we present our extensions of Cohnand Lapata?s (2007) model.

Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Sentence Compression Beyond Word Deletion
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Firstly, the synchronous grammar provides expressive power to model consistent syntactic effects such as reordering, changes in nonterminal categories and lexical substitution.

Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.
Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline.

Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. $$$$$ Instead, we use Cohn and Lapata?s (2007) extractive model as a baseline.
Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. $$$$$ We may, however, want to re tain the meaning of the sequence while rendering the sentence shorter, and this is precisely what our model can achieve, e.g., by allowing substitutions.As far as grammaticality is concerned, our abstractive model is numerically better than the extrac tive baseline but the difference is not statistically significant.

Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ sentences were compressed automatically by our model and the baseline.
Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ Deletions accounted for 67% of rewrite operations, substitutions for 27%, and insertions for 6%.

We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). $$$$$ Each grammar rule is assigned a weight, and these weights are learnt in discriminative training.
We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). $$$$$ Training The model is trained using SVM struct , a large margin method for structured output problems (Joachims, 2005; Tsochantaridis et al, 2005).

The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. $$$$$ Therefore, in this paper we consider sentence compression from a more general perspective and generate abstracts rather than extracts.
The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. $$$$$ From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).

In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). $$$$$ All our experiments used the data from this annotator.
In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). $$$$$ From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).

For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). $$$$$ Sentence Compression Beyond Word Deletion
For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). $$$$$ Unfortunately,none of these data sources are suited to our problem since they have been produced with a single rewriting operation, namely word deletion.

Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ Sentence Compression Beyond Word Deletion
Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ The future of the nation is in your hands.

 $$$$$ ?he, he?
 $$$$$ Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.

Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003).
Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.

The first abstractive compression method was proposed by Cohn and Lapata (2008). $$$$$ Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.
The first abstractive compression method was proposed by Cohn and Lapata (2008). $$$$$ Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results.

The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.
The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences.

We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ Cohn and Lapata (2007) extract a STSG froma parsed, word-aligned corpus of source and target sentences.
We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ Our materials thus con sisted of 90 (30 ? 3) source-target sentences.

This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). $$$$$ We then present a tree-to-tree transducer capable of transforming an input parse tree into a compressed parse tree.
This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). $$$$$ Cohn and Lapata apply this model to ex tractive compression with state-of-the-art results.

 $$$$$ ?he, he?
 $$$$$ Special thanks to Phil Blunsom, James Clarke and Miles Osborne for their insightful suggestions.
