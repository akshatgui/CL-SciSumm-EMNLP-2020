See (Brown et al, 1993) or (Germann et al, 2001) for a detailed discussion of this translation model and a description of its parameters. $$$$$ Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.
See (Brown et al, 1993) or (Germann et al, 2001) for a detailed discussion of this translation model and a description of its parameters. $$$$$ The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.

The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details. $$$$$ Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.
The decoding algorithm that we use is a greedy one - see (Germann et al, 2001) for details. $$$$$ The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.

As discussed by Germann et al (2001), the word-for-word gloss is constructed by aligning each French word fL with its most likely English translation efk. $$$$$ If two French words align to the same English word, then that English word is said to have a fertility of two.
As discussed by Germann et al (2001), the word-for-word gloss is constructed by aligning each French word fL with its most likely English translation efk. $$$$$ The gloss is constructed by aligning each French word f with its most likely English translation ef(ef argmaxt(ef)).

Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. $$$$$ Return to the second step (pop).
Similarly to the work by Germann et al (2001), their decoder is deterministic and explores the entire neighbourhood of a state in order to identify the most promising step. $$$$$ The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.

A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001). $$$$$ Fast Decoding And Optimal Decoding For Machine Translation
A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al, 2001). $$$$$ Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?

For instance, Germann et al (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference. $$$$$ Fast Decoding And Optimal Decoding For Machine Translation
For instance, Germann et al (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference. $$$$$ Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?

This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al, 2001), that are modeled using a standard TSP formulation. $$$$$ Fast Decoding And Optimal Decoding For Machine Translation
This feature distinguishes our task from other decoding problems, such as decoding in machine translation (Germann et al, 2001), that are modeled using a standard TSP formulation. $$$$$ Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).

We also included in the figure the performance of an IBM Model 4 word based translation system (M4), which uses a greedy decoder [Germann et al, 2001]. $$$$$ In this paper, we work with IBM Model 4, which revolves around the notion of a word alignment over a pair of sentences (see Figure 1).
We also included in the figure the performance of an IBM Model 4 word based translation system (M4), which uses a greedy decoder [Germann et al, 2001]. $$$$$ Even when the greedy decoder uses an optimized-forspeed set of operations in which at most one word is translated, moved, or inserted at a time and at most 3-word-long segments are swapped—which is labeled “greedy” in Table 2—the translation accuracy is affected only slightly.

(Germann et al, 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. $$$$$ The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).
(Germann et al, 2001) presents a greedy approach to search for the translation that is most likely according to previously learned statitistical models. $$$$$ Greedyand greedyare greedy decoders optimized for speed. models.

The system uses models GIZA++ and ISI ReWrite decoder (Germann et al., 2001). $$$$$ The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.
The system uses models GIZA++ and ISI ReWrite decoder (Germann et al., 2001). $$$$$ Greedyand greedyare greedy decoders optimized for speed. models.

As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al, 2001). $$$$$ There are several sets of decisions to be made.
As a point of comparison, we also trained an IBM-4 translation model with the GIZA++ toolkit (Och and Ney, 2000), using the combined bi-phrase building and training sets, and translated the test set using the ReWrite decoder (Germann et al, 2001). $$$$$ Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).

Och et al (2001) and Germann et al (2001) both implemented optimal decoders and benchmarked approximative algorithms against them. $$$$$ This paper reports on measurements of speed, search errors, and translation quality in the context of a traditional stack decoder (Jelinek, 1969; Brown et al., 1995) and two new decoders.
Och et al (2001) and Germann et al (2001) both implemented optimal decoders and benchmarked approximative algorithms against them. $$$$$ Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).

Germann et al (2001) compare translations obtained by a multi-stack decoder and a greedy hill-climbing algorithm against those produced by an optimal integer programming decoder that treats decoding as a variant of the traveling-salesman problem. $$$$$ In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders

While acceptably fast for the kind of evaluation used in Germann et al (2001), namely sentences of up to 20 words, its speed becomes an issue for more realistic applications. $$$$$ Fast Decoding And Optimal Decoding For Machine Translation
While acceptably fast for the kind of evaluation used in Germann et al (2001), namely sentences of up to 20 words, its speed becomes an issue for more realistic applications. $$$$$ In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.

In this subsection we recapitulate the greedy hill climbing algorithm presented in Germann et al (2001). $$$$$ Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).
In this subsection we recapitulate the greedy hill climbing algorithm presented in Germann et al (2001). $$$$$ The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.

Thirdly, the results of our experiments with randomized searches show that greedy decoding does not perform as well on longer sentences as one might conclude from the findings in Germann et al (2001). $$$$$ In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.
Thirdly, the results of our experiments with randomized searches show that greedy decoding does not perform as well on longer sentences as one might conclude from the findings in Germann et al (2001). $$$$$ The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.

In this paper, we have analyzed the complexity of the greedy decoding algorithm originally presented in Germann et al (2001) and presented improvements that drastically reduce the decoder's complexity and speed to practically linear time. $$$$$ In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders

Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. $$$$$ In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders

 $$$$$ After heads and non-heads are placed, NULL-generated words are permuted into the remaining vacant slots randomly.
 $$$$$ This work was supported by DARPA-ITO grant N66001-00-1-9814.

The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001). $$$$$ First, we can consider only certain English words as candidates for zero-fertility, namely words which both occur frequently and have a high probability of being assigned frequency zero.
The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001). $$$$$ TranslateAndInsert iterates over alignments, where is the size of the list of words with high probability of having fertility 0 (1024 words in our implementation).
