On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ It can be expected that this clue will work best with parallel corpora, second-best with comparable corpora, and somewhat worse with unrelated corpora.
On the other hand, the alternative approach using comparable or unrelated text corpora were studied by Rapp (1999) and Fung et al (1998). $$$$$ On the one hand, their task was more difficult because they worked on a pair of unrelated languages (English/Japanese) using smaller corpora and a random selection of test words, many of which were multi-word terms.

For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ Table 1 shows the results for 20 of the 100 German test words.
For instance, good results are obtained from large corpora several million words for which the accuracy of the proposed translation is between 76% (Fung, 1998) and 89% (Rapp, 1999) for the first 20 candidates. $$$$$ This was true in 89 cases.5 For comparison, Fung & McKeown (1997) report an accuracy of about 30% when only the top candidate is counted.

This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ We translate all known words in this vector to the target language.
This can be accomplished as in Rapp (1999) and Schafer and Yarowsky (2002) by creating bag-of-words context vectors around both the source and target language words and then projecting the source vectors into the (English) target space via the current small translation dictionary. $$$$$ With the resulting vector, we now perform a similarity computation to all vectors in the co-occurrence matrix of the target language.

Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung & Yee, 1998; Fung & McKeown, 1997).
Here, a standard technique of estimating bilingual term correspondences from com parable corpora (e.g., Fung and Yee (1998) and Rapp (1999)) is employed. $$$$$ Another approach, as conducted by Fung & Yee (1998), would be to consider all possible translations listed in the lexicon and to give them equal (or possibly descending) weight.

For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ The third clue is generally limited to the identification of word pairs with similar spelling.
For example, Rapp (1999) filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while Fung and Yee (1998) restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words. $$$$$ For all other pairs, it is usually used in combination with the first clue.

The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ It can be expected that this clue will work best with parallel corpora, second-best with comparable corpora, and somewhat worse with unrelated corpora.
The approach we investigate for identifying term translations in comparable corpora is similar to (Rapp, 1999) and many others. $$$$$ However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required.

Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation.
Complex linguistic tools such as terminological extractors (Daille and Morin,2005), parsers (Yu and Tsujii, 2009) or lemma tizers (Rapp, 1999) are sometimes used. $$$$$ (According to Lezius, Rapp, & Wettler, 1998, 93% of the tokens of a German text had only one lemma.)

Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts.
Context length can be based on a number of units, for instance 3 sentences (Daille and Morin, 2005), windows of 3 (Rapp, 1999) or 25 words (Prochasson et al, 2009), etc. $$$$$ Instead, we combine the four vectors of length n into a single vector of length 4n.

As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ To determine the English translation of an unknown German word, the association vector of the German word is computed and compared to all association vectors in the English association matrix.
As already noted, most authors use the log-likelihood ratio to measure the association between collocates; some, like (Rapp, 1999), informally compare the performance of a small number of association measures, or combine the results obtained with different association measures (Daille and Morin, 2005). $$$$$ It must be noted, however, that the other authors applied their similarity measures directly to the (log of the) co-occurrence vectors, whereas we applied the measures to the association vectors based on the log-likelihood ratio.

Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ It can be expected that this clue will work best with parallel corpora, second-best with comparable corpora, and somewhat worse with unrelated corpora.
Expand the dictionary of step 3 using comparable corpora as proposed in a study by Rapp (1999). $$$$$ It is further assumed that there is a small dictionary available at the beginning, and that our aim is to expand this base lexicon.

From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ However, this is a relatively rare case.
From a previous pilot study (Rapp, 1999) it can be expected that this methodology achieves an accuracy in the order of 70%, which means that only a relatively modest amount of manual post editing is required. $$$$$ 4 This means that alternative translations of a word were not considered.

(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts.
(Rapp, 1999) and (Koehn and Knight, 2002) extract new word translations from non-parallel corpus. $$$$$ The validity of the co-occurrence clue is obvious for parallel corpora, but — as empirically shown by Rapp — it also holds for non-parallel corpora.

 $$$$$ Since our corpora are very large, to save disk space and processing time we decided to remove all function words from the texts.
 $$$$$ I thank Manfred Wettler, Gisela Zunker-Rapp, Wolfgang Lezius, and Anita Todd for their support of this work.

To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ It must be noted, however, that the other authors applied their similarity measures directly to the (log of the) co-occurrence vectors, whereas we applied the measures to the association vectors based on the log-likelihood ratio.
To identify the context terms CT (WS) of a source word WS, as in (Rapp, 1999), we use log likelihood ratio (LL) Dunning (1993). $$$$$ According to our observations, estimates based on the log-likelihood ratio are generally more reliable across different corpora and languages.

Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ Automatic Identification Of Word Translations From Unrelated English And German Corpora
Using large, unrelated English and German corpora (with 163m and 135mwords) and a small German-English bilingual dictionary (with 22k entires), Rapp (1999) demonstrated that reasonably accurate translations could be learned for 100 German nouns that were not contained in the seed bilingual dictionary. $$$$$ Our German/English base lexicon is derived from the Collins Gem German Dictionary with about 22,300 entries.

We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ Using our source-language corpus, we compute a co-occurrence vector for this word.
We extend the vector space approach of Rapp (1999) to compute similarity between phrases in the source and target languages. $$$$$ The vector with the highest similarity is considered to be the translation of our source-language word.

 $$$$$ Since our corpora are very large, to save disk space and processing time we decided to remove all function words from the texts.
 $$$$$ I thank Manfred Wettler, Gisela Zunker-Rapp, Wolfgang Lezius, and Anita Todd for their support of this work.

One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now.
One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional co-locational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). $$$$$ Another approach, as conducted by Fung & Yee (1998), would be to consider all possible translations listed in the lexicon and to give them equal (or possibly descending) weight.

Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ They were based on mutual information (Church & Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning, 1993).
Some successful combinations are cos CP (Schuetze and Pedersen, 1997), Lin PMI (Lin, 1998), City LL (Rapp, 1999), and Jensen Shannon divergence of conditional probabilities (JSD CP). $$$$$ It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others.

The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ For counting word co-occurrences, in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size.
The counts can be collected in positional (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). $$$$$ However, the computational methods described below are in the same way applicable to window sizes of any length with or without consideration of word order.
