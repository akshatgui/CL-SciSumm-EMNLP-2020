MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability
MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).

Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance.
Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ Therefore, we must include multiple optimizer replications.

We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ To determine whether an experimental manipulation results in a statistically reliable difference for an evaluation metric, we use a stratified approximate randomization (AR) test.
We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ Therefore, we must include multiple optimizer replications.

Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). $$$$$ Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure.
Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). $$$$$ The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times.

In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).
In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ The Chinese2004), and to determine the significance of MT re- English systems were optimized 300 times, and the sults (Och, 2003; Koehn, 2004; Zhang et al., 2004; German-English systems were optimized 50 times.

Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ Dyer et al., 2010).
Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ In this simulation, we randomly selected n optimizer outputs from our large pool and ran the AR test to determine the significance; we repeated this procedure 250 times.

Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).
Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).

Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).
Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance.

We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure.
We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).

We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability
We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ This can the standard deviation over 5 MERT runs when each be done by replication of the optimization procedure of several metrics was used as the objective function. with different starting conditions (e.g., by running 4 Experiments MERT many times).

The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). $$$$$ To compute ssel, assume we have n independent optimization runs which produced weight vectors that were used to translate a test set n times.
The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). $$$$$ Based on the average performance of the systems reported in Table 1, we expect significance over a large enough number of independent trials.

In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ In this paper, we consider how to make such experiments more statistically reliable.
In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).

All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).
All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ The p-values reported are the pvalues at the edges of the 95% confidence interval (CI) according to AR seen in the 250 simulated comparison scenarios.

While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ However, it could be that a particu- “experimental” system (System B), which previous lar sample is on the “low” side of the distribution research has suggested will perform better. over optimizer outcomes (i.e., it results in relatively The first system pair contrasts a baseline phrasepoorer scores on the test set) or on the “high” side. based system (Moses) and experimental hierarchiThe danger here is obvious: a high baseline result cal phrase-based system (Hiero), which were conpaired with a low experimental result could lead to a structed from the Chinese-English BTEC corpus useful experimental manipulation being incorrectly (0.7M words), the later of which was decoded with identified as useless.
While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ However, with only a single test corpus, we may have unreliable results because of idiosyncrasies in the test set.

We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ Cer et al. (2008) Results are reported using BLEU (Papineni et al., 2002), METEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006).
We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ Also, since metric scores (such as BLEU) are in general not comparable across test sets, we stratify, exchanging only hypotheses that correspond to the same sentence.
