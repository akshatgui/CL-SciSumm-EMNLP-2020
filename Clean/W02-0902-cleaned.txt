Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. $$$$$ The similarity and frequency clues, however, seem to be too imprecise to pinpoint the search to the correct translations.

Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. $$$$$ Some of this work has been re-implemented and is freely available for research purposes [AlOnaizan et al., 1999].
Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. $$$$$ Verbs, adjectives, adverbs and other part of speech may be tackled in a similar way.

Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ Still, these words – often called cognates – maintain a very similar spelling.

This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001].
This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ Intuitively it is obvious that pairs of words that are similar in one language should have translations that are similar in the other language.

Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. $$$$$ Roughly speaking, SMT divides the task of translation into two steps

Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ The important point here is that we have generated a similarity matrix, which we will use now to find new translation word pairs.
Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ We are trying to build a one-to-one GermanEnglish translation lexicon for the use in a machine translation system.

The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). $$$$$ In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001].
The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). $$$$$ The counts are then normalized by a using the tf/idf method which is often used in information retrieval [Jones, 1979].

In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002) $$$$$ For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001].
In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002) $$$$$ Clearly, a seed lexicon to bootstrap these methods is needed.

However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. $$$$$ Or

In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.
In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.

Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. $$$$$ This section will describe clues that enable us to find translations of words of the two monolingual corpora.
Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. $$$$$ One strategy is to define two words as similar if they occur in a similar context.

Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ When checking them against a benchmark lexicon, we found these mappings to be 88% correct.
Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ We add this word pair to the lexicon, and drop word pairs that include either the German and English word from further search.

Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ This measurement is called longest common subsequence ratio [Melamed, 1995].
Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ In related work, string edit distance (or, Levenshtein distance) has been used [Mann and Yarowski, 2001].

However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. $$$$$ Obviously, the vast majority of word pairs can not be collected this way, since their spelling shows no resemblance at all.
However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. $$$$$ We add this word pair to the lexicon, and drop word pairs that include either the German and English word from further search.

Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ This idea has already been investigated in earlier work.
Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ In an experiment where we identified the best lexical entries using a very large parallel corpus, we could achieve 89% accuracy on this test corpus. many correct lexicon entries where added (Entries), and how well the resulting translation lexicon performs compared to the actual wordlevel translations in a parallel corpus (Corpus).

 $$$$$ Obviously, the vast majority of word pairs can not be collected this way, since their spelling shows no resemblance at all.
 $$$$$ These may be learned using the described methods.

Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. $$$$$ Tiedemann [1999] explores the automatic construction of a string similarity measure that learns which letter changes occur more likely between cognates of two languages.
Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. $$$$$ The similarity of two context vectors a = (ai) and b = (bi) is then defined by

Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Recently, there has been a surge in research in machine translation that is based on empirical methods.
Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Clearly, a seed lexicon to bootstrap these methods is needed.

Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ As already mentioned, there are some wellestablished transformation rules for the adoption of words from a foreign language.
Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ Intuitively it is obvious that pairs of words that are similar in one language should have translations that are similar in the other language.

The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. $$$$$ For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001].
The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. $$$$$ Their approach to constructing and comparing context vectors differs significantly from methods discussed in the previous section.
