Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ We then obtain statistics on the parts-ofspeech of words in the corpus.
Here, we follow an approach introduced by Koehn and Knight (2003): First, we collect frequency statistics over words in our training corpus. $$$$$ We introduced various methods to split compound words into parts.

Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ Compounding of words is common in a number of languages (German, Dutch, Finnish, Greek, etc.).
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ An evaluation of full sentences is expected to show similar results.

Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ Compounded words are a challenge for NLP applications such as machine translation (MT).
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ We do not want to break up a compound into parts that are prepositions or determiners, but only content words: nouns, adverbs, adjectives, and verbs.

Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We introduce methods to learn splitting rules from monolingual and parallel corpora.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We then obtain statistics on the parts-ofspeech of words in the corpus.

We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words.
We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ 7.3 Translation Quality with Phrase Based Machine Translation Compound words violate the bias for one-to-one word correspondences of word based SMT systems.

In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ One source of information about word correspondence is a parallel corpus: text in a foreign language, accompanied by translations into English.
In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ Its main remaining source of error is the lack of training data.

We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ Such a translation only occurs when Grund is used as the first part of a compound.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ We introduced various methods to split compound words into parts.

To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ This insight leads us to define a splitting metric based on word frequency.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ If the word has not been seen before, we use the frequency method as a back-off.

The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Empirical Methods For Compound Splitting
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].

Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available.

We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ This approach requires a translation lexicon.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ If multiple biggest splits are possible, the one with the highest frequency score is taken. frequency based: split into most frequent words, as described in Section 4 using parallel: split guided by splitting knowledge from a parallel corpus, as described in Section 5 using parallel and POS: as previous, with an additional restriction on the POS of split parts, as described in Section 6 Since we developed our methods to improve on this metric, it comes as no surprise that the most sophisticated method that employs splitting knowledge from a parallel corpus and information about POS tags proves to be superior with 99.1% accuracy.

Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ While the linguistic properties of compounds are widely studied [Langer, 1998], there has been only limited work on empirical methods to split up compounds for specific applications.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ The columns in this table mean: correct split: words that should be split and were split correctly correct non: words that should not be split and were not wrong not: words that should be split but were not wrong faulty split: words that should be split, were split, but wrongly (either too much or too little) wrong split: words that should not be split, but were precision: (correct split) / (correct split + wrong faulty split + wrong superfluous split) recall: (correct split) / (correct split + wrong faulty split + wrong not split) accuracy: (correct) / (correct + wrong) To briefly review the methods: raw: unprocessed data with no splits eager: biggest split, i.e., the split into as many parts as possible.

Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts): Since this metric is purely defined in terms of German word frequencies, there is not necessarily a relationship between the selected option and correspondence to English words.
Taken from (Koehn and Knight, 2003): S= split, pi= part, n= number of parts. $$$$$ The columns in this table mean: correct split: words that should be split and were split correctly correct non: words that should not be split and were not wrong not: words that should be split but were not wrong faulty split: words that should be split, were split, but wrongly (either too much or too little) wrong split: words that should not be split, but were precision: (correct split) / (correct split + wrong faulty split + wrong superfluous split) recall: (correct split) / (correct split + wrong faulty split + wrong not split) accuracy: (correct) / (correct + wrong) To briefly review the methods: raw: unprocessed data with no splits eager: biggest split, i.e., the split into as many parts as possible.

The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ Given this gold standard, we can evaluate the splits proposed by the methods.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ For each German NP/PP, we have a English translation.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Recall that our first objective is to break up German words into parts that have a one-to-one translation correspondence to English words.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ As fillers we allow s and es when splitting German words, which covers almost all cases.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ To summarize: We try to cover the entire length of the compound with known words and fillers between words.

We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ We introduced various methods to split compound words into parts.
