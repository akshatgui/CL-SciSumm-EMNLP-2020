Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ We then obtain statistics on the parts-ofspeech of words in the corpus.
Here, we follow an approach introduced by Koehn and Knight (2003) $$$$$ We introduced various methods to split compound words into parts.

Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ Compounding of words is common in a number of languages (German, Dutch, Finnish, Greek, etc.).
Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and UrduHindi (Lehal, 2010). $$$$$ An evaluation of full sentences is expected to show similar results.

Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ Compounded words are a challenge for NLP applications such as machine translation (MT).
Correctsplitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). $$$$$ We do not want to break up a compound into parts that are prepositions or determiners, but only content words

Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We introduce methods to learn splitting rules from monolingual and parallel corpora.
Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. $$$$$ We then obtain statistics on the parts-ofspeech of words in the corpus.

We split large words based on word frequencies to tackle the problem of word compounds in German (Koehn and Knight, 2003). $$$$$ Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts)

In order to reduce the source vocabulary size for the German-English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). $$$$$ One source of information about word correspondence is a parallel corpus

We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ Such a translation only occurs when Grund is used as the first part of a compound.
We used the frequency-based segmentation algorithm initially introduced in (Koehn and Knight, 2003) to handle compounding. $$$$$ We introduced various methods to split compound words into parts.

To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ This insight leads us to define a splitting metric based on word frequency.
To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. $$$$$ If the word has not been seen before, we use the frequency method as a back-off.

The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Empirical Methods For Compound Splitting
The empirical approach of Koehn and Knight (2003) splits German compounds into words found in a training corpus. $$$$$ Known words are words that exist in a training corpus, in our case the European parliament proceedings consisting of 20 million words of German [Koehn, 2002].

Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ Larson et al. [2000] propose a data-driven method that combines compound splitting and word recombination for speech recognition.
Popovic et al. (2006) compared the approach of Nie? en and Ney (2000) with the corpus-driven splitting of Koehn and Knight (2003) in terms of performance on an SMT task. $$$$$ First, we measured the impact on a word based statistical machine translation system, the widely studied IBM Model 4 [Brown et al., 1990], for which training tools [Al-Onaizan et al., 19991 and decoders [Germann et al., 2001] are freely available.

We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ This approach requires a translation lexicon.
We briefly introduce the computational morphology SMOR (section 3.1) and the corpus driven approach of Koehn and Knight (2003) (section 3.2), before we present our hybrid approach that combines the benefits of both in section 3.3. $$$$$ If multiple biggest splits are possible, the one with the highest frequency score is taken. frequency based

Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ While the linguistic properties of compounds are widely studied [Langer, 1998], there has been only limited work on empirical methods to split up compounds for specific applications.
Koehn and Knight (2003) describe a method requiring no linguistically motivated morphological analysis to split compounds. $$$$$ The columns in this table mean

Taken from (Koehn and Knight, 2003) $$$$$ Given the count of words in the corpus, we pick the split S with the highest geometric mean of word frequencies of its parts pi (n being the number of parts)

The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.
The one-to-one correspondence gold standard (Koehn and Knight, 2003) indicates only compounds that were translated compositionally by a human translator. $$$$$ Given this gold standard, we can evaluate the splits proposed by the methods.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ For each German NP/PP, we have a English translation.
Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. $$$$$ Recall that our first objective is to break up German words into parts that have a one-to-one translation correspondence to English words.

 $$$$$ An algorithm to break up words in such a manner could be implemented using dynamic programming, but since computational complexity is not a problem, we employ an exhaustive recursive search.
 $$$$$ Future machine translation models that are sensitive to such linguistic clues might benefit even more.

Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ As fillers we allow s and es when splitting German words, which covers almost all cases.
Koehn and Knight (2003) used a fixed set of two known fillers s and es for handling German compounds. $$$$$ To summarize

We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ One reason for this is that the system recovers more easily from words that are split too much than from words that are not split up sufficiently.
We split German compound words (mostly nouns), based on the frequency of the words in the potential decompositions (Koehn and Knight, 2003a). $$$$$ We introduced various methods to split compound words into parts.
