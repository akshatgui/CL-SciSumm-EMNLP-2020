It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ Use of comparable corpora and minimal use of knowledge resources.

MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Pairwise MSA can be extended efficiently to multiple sequences via the iterative pairwise alignment, a polynomial-time method commonly used in computational biology (Durbin et al., 1998).3 The results can be represented in an intuitive form via a word lattice (see Figure 3), which compactly represents (n-gram) structural similarities between the cluster’s sentences.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.

Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ This is accomplished by applying hierarchical complete-link clustering to the sentences using a similarity metric based on word n-gram overlap ( ).
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ We presented an approach for generating sentence level paraphrases, a task not addressed previously.

We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ Pairwise MSA takes two sentences and a scoring function giving the similarity between words; it determines the highest-scoring way to perform insertions, deletions, and changes to transform one of the sentences into the other.

See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ The choice of 50% is not arbitrary — it can be proved using the pigeonhole principle that our strictmajority criterion imposes a unique linear ordering of the backbone nodes that respects the word ordering within the sentences, thus guaranteeing at least a degree of wellformedness and avoiding the problem of how to order backbone nodes occurring on parallel “branches” of the lattice.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ Agreement according to the Kappa statistic was 0.6.

Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ Learning To Paraphrase

This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ Pairwise MSA can be extended efficiently to multiple sequences via the iterative pairwise alignment, a polynomial-time method commonly used in computational biology (Durbin et al., 1998).3 The results can be represented in an intuitive form via a word lattice (see Figure 3), which compactly represents (n-gram) structural similarities between the cluster’s sentences.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.

Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ However, extraction methods are not easily extended to generation methods.

In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ Learning To Paraphrase

Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ Learning To Paraphrase

Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ Rather, we use two comparable corpora, in our case, collections of articles produced by two different newswire agencies about the same events.

Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ First, working on each of the comparable corpora separately, we compute lattices — compact graph-based representations — to find commonalities within (automatically derived) groups of structurally similar sentences.
Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ Our first step is to cluster sentences into groups from which to learn useful patterns; for the multiple-sequence techniques we will use, this means that the sentences within clusters should describe similar events and have similar structure, as in the sentences of Figure 2.

The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ See Bangalore et al. (2002) and Barzilay and Lee (2002) for other uses of such data.
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ All evaluations involved judgments by native speakers of English who were not familiar with the paraphrasing systems under consideration.

Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ We are not aware of another system generating sentence-level paraphrases.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.

While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ One might initially suppose that sentence-level paraphrasing is simply the result of word-for-word or phraseby-phrase substitution applied in a domain- and contextindependent fashion.

There exist many different string similarity measures $$$$$ This is accomplished by applying hierarchical complete-link clustering to the sentences using a similarity metric based on word n-gram overlap ( ).
There exist many different string similarity measures $$$$$ To compare the set of argument values of two lattices, we simply count their word overlap, giving double weight to proper names and numbers and discarding auxiliaries (we purposely ignore order because paraphrases can consist of word re-orderings).

Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ The use of related corpora is key

Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ Once we have identified the backbone nodes as points of strong commonality, the next step is to identify the regions of variability (or, in lattice terms, many parallel disjoint paths) between them as (probably) corresponding to the arguments of the propositions that the sentences represent.

Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ We are grateful to many people for helping us in this work.

Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ In terms of agreement, the Kappa value (measuring pairwise agreement discounting chance occurrences9) on the common set was 0.54, which corresponds to moderate agreement (Landis and Koch, 1977).
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ Agreement according to the Kappa statistic was 0.6.
