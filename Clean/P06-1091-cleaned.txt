 $$$$$ 2.
 $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ This block set is used to decode training sentence to obtain block orientation sequences that are used in the discriminative parameter training.
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ 1 used in our model.

It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The block set is generated using a phrase-pair selection algorithm similar to (Koehn et al., 2003; Al-Onaizan et al., 2004), which includes some heuristic filtering to mal statement here.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993).

Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The block set is generated using a phrase-pair selection algorithm similar to (Koehn et al., 2003; Al-Onaizan et al., 2004), which includes some heuristic filtering to mal statement here.
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The ’SWAP’ re-ordering uses the same features as the monotone models plus additional orientation-based and distortionBLEU on the training data ( sentences) and the MT03 test data (670 sentences). based features.

Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting.
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ This step does not require the use of a decoder.

The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches 1.
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting.

Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper.
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ Although the training algorithm can handle realvalued features as used in (Och, 2003; Tillmann and Zhang, 2005) the current paper intentionally excludes them.

Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ These features correspond to the use of a language model, but the weights for theses features are trained on the parallel training data only.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ The block set is generated using a phrase-pair selection algorithm similar to (Koehn et al., 2003; Al-Onaizan et al., 2004), which includes some heuristic filtering to mal statement here.

 $$$$$ 2.
 $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

 $$$$$ 2.
 $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting.
Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ Using this feature representation and the loss function in Eq.

Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ A block is a pair of phrases which are translations of each other.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ In the training data where target translations are given, a BLEU score can be calculated for each against the target translations.

 $$$$$ 2.
 $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ This block set is used to decode training sentence to obtain block orientation sequences that are used in the discriminative parameter training.
For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ We refer to this formulation as ’costMargin’ (cost-sensitive margin) method

This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ A Discriminative Global Training Algorithm For Statistical MT
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ We are able to achieve comparable performance to (Tillmann and Zhang, 2005).

This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ Starting with a simple model, the training data is decoded multiple times

Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ For the results with word-based features, the decoder still generates phrase-to-phrase translations, but all the scoring is done on the word level.

This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ We are able to achieve comparable performance to (Tillmann and Zhang, 2005).
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993).
