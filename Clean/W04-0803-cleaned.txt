To date PropBank and FrameNet are the two main resources in English for training semantic role labelling systems, as in the CoNLL-2004 shared task (Carreras and Marquez, 2004) and SENSEVAL-3 (Litkowski, 2004). $$$$$ Senseval-3 Task

Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see Litkowski, 2004 for more details). $$$$$ The Senseval-3 task used 8,002 of these sentences selected randomly from 40 frames (also selected randomly) having at least 370 annotations (out of the 100 frames having the most annotations).1 Participants were provided with a training set that identified, for each of the 40 frames, the lexical unit identification number (which equates to a file name) and a sentence identification name.
Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see Litkowski, 2004 for more details). $$$$$ In some cases, they used all frames as a basis for training and in others, they 5The diversity of frame elements in the test data has not yet been investigated, so the assertion that this task is more difficult is based solely on the general expansion of FrameNet. employed novel grouping methods based on the similarities among different frames.

Although the strict measures are more interesting, we include these figures for comparison with the systems participating in the Senseval-3 Restricted task (Litkowski, 2004). $$$$$ Senseval-3 Task

This task is a more advanced and realistic version of the Automatic Semantic Role Labeling task of Senseval-3 (Litkowski, 2004). $$$$$ Senseval-3 Task

Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). $$$$$ Senseval-3 Task

The same problem was again highlighted by the results obtained with and without the frame information in the Senseval-3 competition (Litkowski,2004) of FrameNet (Johnson et al, 2003) role labeling task. $$$$$ The FrameNet project (Johnson et al., 2003) has put together a body of hand-labeled data and the Gildea and Jurafsky study has put together a set of suitable metrics for evaluating the performance of an automatic system.
The same problem was again highlighted by the results obtained with and without the frame information in the Senseval-3 competition (Litkowski,2004) of FrameNet (Johnson et al, 2003) role labeling task. $$$$$ FrameNet recognizes the permissibility of “conceptually salient” frame elements that have not been instantiated in a sentence; these are called null instantiations (see Johnson et al. for a fuller description).

Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details). $$$$$ The Senseval-3 task used 8,002 of these sentences selected randomly from 40 frames (also selected randomly) having at least 370 annotations (out of the 100 frames having the most annotations).1 Participants were provided with a training set that identified, for each of the 40 frames, the lexical unit identification number (which equates to a file name) and a sentence identification name.
Also from these 321 frames only 100 were considered to have enough training data and were used in Senseval-3 (see (Litkowski, 2004) for more details). $$$$$ In some cases, they used all frames as a basis for training and in others, they 5The diversity of frame elements in the test data has not yet been investigated, so the assertion that this task is more difficult is based solely on the general expansion of FrameNet. employed novel grouping methods based on the similarities among different frames.

This task has been the subject of a previous Senseval task (Automatic Semantic Role Labeling, Litkowski (2004)) and two shared tasks on semantic role labeling in the Conference on Natural Language Learning (Carreras Marquez (2004) and Carreras Marquez (2005)). $$$$$ Senseval-3 Task

 $$$$$ FrameNet Explorer provides several facilities for examining the FrameNet data

In addition to the CoNLL-2004 shared task, another evaluation exercise was conducted in the Senseval-3 workshop (Litkowski, 2004). $$$$$ Senseval-3 Task

Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Mrquez, 2005). $$$$$ The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky.
Beginning with work by Gildea and Jurafsky (2002), there has been a large interest in semantic role labelling, as evidenced by its adoption as a shared task in the Senseval-III competition (FrameNet data, Litkowski, 2004) and at the CoNLL-2004 and 2005 conference (PropBank data, Carreras and Mrquez, 2005). $$$$$ The data for this task is a sample of the FrameNet hand-annotated data.
