The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Clark and Curran (2004) shows that this supertagging method results in a highly efficient parser.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Our system demonstrates that accurate and efficient wide-coverage CCG parsing is feasible.

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The dependency structures considered in this paper are described in detail in Clark et al. (2002) and Clark and Curran (2003).
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The same parser is used to produce the packed charts.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Parsing The WSJ Using CCG And Log-Linear Models
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.

The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The average number of categories assigned to each word is determined by a parameter in the supertagger.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ For the first set of experiments, we used a setting which assigns 1.7 categories on average per word.

However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ The dependency structures considered in this paper are described in detail in Clark et al. (2002) and Clark and Curran (2003).
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ Let L, be the number of correct dependencies in 7r with respect to a gold standard dependency structure G; then the dependency structure, 7rmax, which maximises the expected recall rate is

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ (See Clark and Curran (2004) for a description of the Eisner constraints.)
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ Clark and Curran (2004) shows that this supertagging method results in a highly efficient parser.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ For the dependency model, the highest scoring dependency structure is required.

Clark and Curran (2004b) describes the CCG parser. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
Clark and Curran (2004b) describes the CCG parser. $$$$$ Our normal-form parser significantly outperforms the parser of Clark et al. (2002) and produces results at least as good as the current state-of-the-art for CCG parsing.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ We used 45 machines of a 64-node Beowulf cluster to estimate the dependency model, with an average memory usage of approximately 550 MB for each machine.

We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ The packed charts perform a number of roles

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi

We applied the same normal-form restrictions used in Clark and Curran (2004b) $$$$$ For the normal-form model we were able to reduce the size of the charts considerably by applying two types of restriction to the parser

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ We used 45 machines of a 64-node Beowulf cluster to estimate the dependency model, with an average memory usage of approximately 550 MB for each machine.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ Initially we tried the parallel version of GIS described in Clark and Curran (2003) to perform the estimation, running over the Beowulf cluster.

Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ We had hoped that by modelling the predicate-argument dependencies produced by the parser, rather than local rule dependencies, we would improve performance.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ However, using the predicate-argument dependencies in the normal-form model instead of, or in addition to, the local rule dependencies, has not led to an improvement in parsing accuracy.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi
