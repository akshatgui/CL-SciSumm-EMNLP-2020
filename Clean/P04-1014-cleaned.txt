The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Clark and Curran (2004) shows that this supertagging method results in a highly efficient parser.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Our system demonstrates that accurate and efficient wide-coverage CCG parsing is feasible.

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The dependency structures considered in this paper are described in detail in Clark et al. (2002) and Clark and Curran (2003).
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The same parser is used to produce the packed charts.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Parsing The WSJ Using CCG And Log-Linear Models
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.

The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The average number of categories assigned to each word is determined by a parameter in the supertagger.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ For the first set of experiments, we used a setting which assigns 1.7 categories on average per word.

However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ The dependency structures considered in this paper are described in detail in Clark et al. (2002) and Clark and Curran (2003).
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ Let L, be the number of correct dependencies in 7r with respect to a gold standard dependency structure G; then the dependency structure, 7rmax, which maximises the expected recall rate is: LP LR UP UR cat where S is the sentence for gold standard dependency structure G and i ranges over the dependency structures for S. This expression can be expanded further: The final score for a dependency structure  is a sum of the scores for each dependency  in ; and the score for a dependency  is the sum of the probabilities of those derivations producing .

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ (See Clark and Curran (2004) for a description of the Eisner constraints.)
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ Clark and Curran (2004) shows that this supertagging method results in a highly efficient parser.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ For the dependency model, the highest scoring dependency structure is required.

Clark and Curran (2004b) describes the CCG parser. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
Clark and Curran (2004b) describes the CCG parser. $$$$$ Our normal-form parser significantly outperforms the parser of Clark et al. (2002) and produces results at least as good as the current state-of-the-art for CCG parsing.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ We used 45 machines of a 64-node Beowulf cluster to estimate the dependency model, with an average memory usage of approximately 550 MB for each machine.

We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ In Clark and Curran (2003) we argued for the use of log-linear parsing models for CCG.
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ The packed charts perform a number of roles: they are a compact representation of a very large number of CCG derivations; they allow recovery of the highest scoring parse or dependency structure without enumerating all derivations; and they represent an instance of what Miyao and Tsujii (2002) call a feature forest, which is used to efficiently estimate a log-linear model.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.

We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ For the normal-form model we were able to reduce the size of the charts considerably by applying two types of restriction to the parser: first, categories can only combine if they appear together in a rule instantiation in sections 2–21 of CCGbank; and second, we apply the normal-form restrictions described in Eisner (1996).
We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ The difference is due largely to the normal-form constraints used by the normal-form parser.

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ We used 45 machines of a 64-node Beowulf cluster to estimate the dependency model, with an average memory usage of approximately 550 MB for each machine.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ Initially we tried the parallel version of GIS described in Clark and Curran (2003) to perform the estimation, running over the Beowulf cluster.

Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ We had hoped that by modelling the predicate-argument dependencies produced by the parser, rather than local rule dependencies, we would improve performance.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ However, using the predicate-argument dependencies in the normal-form model instead of, or in addition to, the local rule dependencies, has not led to an improvement in parsing accuracy.

 $$$$$ The components of the gradient vecThe first two terms in (5) are expectations of feature fi: the first expectation is over all derivations leading to each gold standard dependency structure; the second is over all derivations for each sentence in the training data.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
