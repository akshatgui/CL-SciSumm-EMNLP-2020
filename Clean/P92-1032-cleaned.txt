Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ See Gale et al. (to appear) for further details.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ We originally designed the experiment in Gale et al. (1992) to test the hypothesis that multiple uses of a polysemous word tend to have the same sense within a common discourse.

Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Fortunately, we have found in (Gale et al., 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions.
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ We originally designed the experiment in Gale et al. (1992) to test the hypothesis that multiple uses of a polysemous word tend to have the same sense within a common discourse.

 $$$$$ Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures.
 $$$$$ Incidentally, most part of speech algorithms are currently performing at or near the limit of our ability to measure performance, indicating that there may be room for refining the experimental conditions along similar lines to what we have done here, in order to improve the dynamic range of the evaluation.

The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ See Gale et al. (to appear) for further details.
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ Fortunately, we have found in (Gale et al., 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions.

(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ See Gale et al. (to appear) for further details.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ We originally designed the experiment in Gale et al. (1992) to test the hypothesis that multiple uses of a polysemous word tend to have the same sense within a common discourse.

This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ The availability of massive lexicographic databases offers a promising route to overcoming the knowledge acquisition bottleneck.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ See Gale et al. (to appear) for further details.

To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ Estimating Upper And Lower Bounds On The Performance Of Word-Sense Disambiguation Programs
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ In order to address this state of affairs, we decided to try to establish upper and lower bounds on the level of performance that we could expect to obtain.

The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ See Gale et al. (to appear) for further details.
The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ Fortunately, we have found in (Gale et al., 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions.
