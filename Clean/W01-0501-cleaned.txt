Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. $$$$$ This case study of co-training for natural language learning addresses the scalability question using the task of base noun phrase identification.
Similarly, in a case study on co-training for natural language tasks, Pierce and Cardie (2001) find that the degradation in data quality from automatic labelling prevents these systems from performing comparably to their fully-supervised counterparts. $$$$$ Cardie et al. (2000)) systems.

Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
Pierce and Cardie (2001) have shown, however, that for tasks which require large numbers of labeled examples such as most NLP tasks co training might be inadequate because it tends to generate noisy data. $$$$$ In particular, many natural language learning tasks contrast sharply with the classification tasks previously studied in conjunction with co-training in that they require hundreds of thousands, rather than hundreds, of training examples.

We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. $$$$$ This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
We observed the same pattern in co-training; its accuracy peaked after two iterations (85.1%) and then performance degraded drastically (68% after five iterations) due in part to an increase in mislabeled data in the training set (as previously observed in (Pierce and Cardie, 2001)) and in part because the data skew is not controlled for. $$$$$ A straightforward solution to this problem is to have a human anized, as co-training achieves 95.03% accuracy, just 0.14% away from the goal, after 600 iterations (and reaches 95.12% after 800 iterations).

For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). $$$$$ Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g.
For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)). $$$$$ We leave evaluation of this possibility to future work.

The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001). $$$$$ Co-Training.

 $$$$$ Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.
 $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.

The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ This procedure preserves the distribution of labels in the labeled data as instances are labeled and added.
The drop in F-measure is potentially due to the pollution of the labeled data by mislabeled instances (Pierce and Cardie, 2001). $$$$$ For initial labeled data, the first L instances of the training data are given their correct labels.

However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. $$$$$ This study examines the learning behavior of co-training on natural language processing tasks that typically require large numbers of training instances to achieve usable performance levels.
However, it has been shown (Pierce and Cardie, 2001) that semi-supervised learning is brittle for NLP tasks where typically large amounts of high quality annotations are needed to train appropriate classifiers. $$$$$ On one hand, the goal of weakly supervised learning is to bootstrap a classifier from small amounts of labeled data and large amounts of unlabeled data, often by automatically labeling some of the unlabeled data.

The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ Corrected Co-Training.
The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (PierceandCardie, 2001). $$$$$ This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation.

For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ Cardie et al. (2000)) systems.
For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). $$$$$ Argamon et al. (1999)), and memory-based learning (e.g.

The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
The fact that no improvement was obtained agrees with previous observations that classifiers that are too accurate can not be improved with bootstrapping (Pierce and Cardie,2001). $$$$$ For example, with L = 200 the co-training classifiers appear not to be accurate enough to sustain co-training, while with L = 1000, they are too accurate, in the sense that co-training contributes very little accuracy before the labeled data deteriorates (Figure 5).

A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ For this task, they suggest the following two views: (1) the words contained in the text of the page; for example, research interests or publications; (2) the words contained in links pointing to the page; for example, my advisor.
A comparative analysis of words that benefit from basic/smoothed co-training with global parameter settings, versus words with little or no improvement obtained through bootstrapping reveals several observations: (1) Words with accurate basic classifiers can not be improved through co-training, which agrees with previous observations (Pierce and Cardie, 2001). $$$$$ Words to the left and right of the focus word are included for context.

Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. $$$$$ Collins and Singer (1999) were concerned that the CT algorithm does not strongly enforce the requirement that hypothesis functions should be compatible with the unlabeled data.
Algorithms such as co-training (Blum and Mitchell, 1998) (Collins and Singer, 1999) (Pierce and Cardie, 2001) and the Yarowsky algorithm (Yarowsky, 1995) make assumptions about the data that permit such an approach. $$$$$ Yarowsky (1995), Riloff and Jones (1999)), suggesting that the rewards of understanding and dealing with this issue may be significant.

The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
The table further indicates that co-training for machine translation suffers the same problem reported in Pierce and Cardie (2001): gains above the accuracy of the initial corpus are achieved, but decline as after acer tain number of machine translations are added to the training set. $$$$$ Co-Training.

Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ Co-Training.
Pierce and Cardie (2001) showed that the quality of the automatically labeled training data is crucial for co-training to perform well because too many tagging errors prevent a high performing model from being learned. $$$$$ Furthermore, our experiments support the hypothesis that labeled data quality is a crucial issue for co-training.

To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. $$$$$ We address this problem with a moderately supervised variant, corrected co-training, that employs a human annotator to correct the errors made during bootstrapping.
To address the problem of data pollution by tagging errors, Pierce and Cardie (2001) propose corrected co-training. $$$$$ Corrected Co-Training.

Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ This enables bootstrapping from a small set of labeled training data via a large set of unlabeled data.
Thus, these bootstrapping methods will presumably not find the most useful unlabeled examples but require a human to review data points of limited training utility (Pierce and Cardie, 2001). $$$$$ Finally, we note that the accuracy of automatically accumulated training data is an important issue for many bootstrapping learning methods (e.g.

 $$$$$ Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.
 $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 9454149, 0081334, and 0074896.

In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ Cardie and Pierce (1998)), memory-based sequence learning (e.g.
In their experiments on NP identifiers, Pierce and Cardie (2001) observed a similar effect. $$$$$ To apply co-training, the base NP classification task must first be factored into views.

Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Base noun phrase identification is a crucial component of systems that employ partial syntactic analysis, including information retrieval (e.g.
Co-training has been applied to a number of NLP applications ,including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Cardie et al. (2000)) systems.
