For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ Also present in Table 1 are the results of training Brill's tagger on the same data.

The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ The experiments presented in Section 4 include ICA in the training time and performance comparisonsï¿½.
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).

We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ The following table shows the above sentence with the assigned chunk tags: and the part-of-speech tags were generated by Brill's tagger (Brill, 1995).

A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).
A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ Following Ramshaw & Marcus' (1999) work in base noun phrase chunking, each word is assigned a chunk tag corresponding to the phrase to which it belongs .

Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The problem is cast as a classification task, and the sentence is reduced to a 4-tuple containing the preposition and the non-inflected base forms of the head words of the verb phrase VP and the two noun phrases NP1 and NP2.

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The experiment was performed with the part-ofspeech data set.

In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ The rules are then applied sequentially to the evaluation set in the order they were learned.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches.

With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ The results of this tagger are presented to provide a performance comparison with a widely used tagger.

The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ Transformation Based Learning In The Fast Lane
The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.

This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.

The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).
The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ The templates used to generate rules are similar to the ones used by Brill and Resnik (1994) and some include WordNet features.

We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ Transformation Based Learning In The Fast Lane
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.

For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ The results of this tagger are presented to provide a performance comparison with a widely used tagger.

Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ The results of this tagger are presented to provide a performance comparison with a widely used tagger.

With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).

Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration.

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The experiment was performed with the part-ofspeech data set.

Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ Transformation Based Learning In The Fast Lane
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ Again, the ICA algorithm learns the rules very fast, but has a slightly lower performance than the other two TBL systems.

We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.

Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ Ramshaw & Marcus (1994) attempted to reduce the training time of the algorithm by making the update process more efficient.
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster.
