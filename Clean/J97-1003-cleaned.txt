The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. $$$$$ The algorithm has three parts: tokenization into terms and sentence-sized units, determination of a score for each sentence-sized unit, and detection of the subtopic boundaries, which are assumed to occur at the largest valleys in the graph that results from plotting sentence-units against scores.
The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. $$$$$ In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation.

This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. $$$$$ The lexical score is computed as follows.
This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. $$$$$ Any attempt to make an absolute cutoff, even one normalized for the length of the document, is problematic since there should be some relationship between the structure and style of the text and the number of segments assigned to it.

Foltz et al's (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). $$$$$ Because the goal is to partition texts into contiguous, nonoverlapping subtopic segments, I call the general approach TextTiling (Hearst, 1993, 1994a, 1994b).1 Subtopic discussions are assumed to occur within the scope of one or more overarching main topics, which span the length of the text.
Foltz et al's (1998) approach is in line with the earlier TextTiling method that identifies subtopic structure in text (Hearst, 1997). $$$$$ In a line of work we call Mixed-Media access (Chen et al. 1994), textual subtopic structure is being integrated with other media types, such as images and speech.

Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). $$$$$ This section presents comparisons of the results of the algorithm against human judgments and against article boundaries.
Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). $$$$$ As mentioned above, Hearst (1995) and Hearst and Plaunt (1993) show how to use TextTiles in information retrieval tasks, although this work does not show whether or not the results of these algorithms produce better performance than the results of some other segmentation strategy would.

Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al, 2003). $$$$$ The results of Hearst and Plaunt (1993), Salton, Allan, and Buckley (1993) and Moffat et al. (1994) suggest that it is the nature of the intermediate size of the passages that matters.
Many researchers have attempted to make use of cue phrases (Hirschberg and Litman, 1993), especially for segmentation both in prose (Hearst, 1997) and conversation (Galley et al, 2003). $$$$$ The use of discourse cues for detection of segment boundaries and other discourse purposes has been extensively researched, although predominantly on spoken text (see Hirschberg and Litman [19931 for a summary of six research groups' treatments of 64 cue words).

The best known algorithm based on this idea is TextTiling (Hearst, 1997). $$$$$ Hearst (1993) posited that this argument should also apply to determining which words best distinguish one subtopic from another.
The best known algorithm based on this idea is TextTiling (Hearst, 1997). $$$$$ This variation of the TextTiling algorithm is explored and evaluated in Hearst (1994b).

reference segmentation from a coder should not be trusted, given that inter-annotator agreement is often reported to be rather poor (Hearst, 1997, p. 54). $$$$$ Results for this approach are reported in Section 6.
reference segmentation from a coder should not be trusted, given that inter-annotator agreement is often reported to be rather poor (Hearst, 1997, p. 54). $$$$$ Results for this approach are reported in Section 6.

(Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). $$$$$ Note that the depth scores are based only on relative score information, ignoring absolute values.
(Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). $$$$$ The kappa coefficients found in Isard and Carletta (1995) ranged from .43 to .68 for four coders placing transaction boundaries, and those found in (Rose 1995) ranged from .65 to .90 for four coders segmenting sentences.

Pairwise mean kappa scores were calculated by comparing a coder's segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53-54). $$$$$ (pp.
Pairwise mean kappa scores were calculated by comparing a coder's segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53-54). $$$$$ There are several ways to evaluate a segmentation algorithm, including comparing its segmentation against that of human judges, comparing its segmentation against author-specified orthographic information, and comparing its segmentation against other automated segmentation strategies in terms of how they effect the outcome of some computational task.

Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. $$$$$ There are several ways to evaluate a segmentation algorithm, including comparing its segmentation against that of human judges, comparing its segmentation against author-specified orthographic information, and comparing its segmentation against other automated segmentation strategies in terms of how they effect the outcome of some computational task.
Coders often disagree in segmentation tasks (Hearst, 1997, p. 56), making it improbable that a single, correct, reference segmentation could be identified from human codings. $$$$$ As mentioned above, Hearst (1995) and Hearst and Plaunt (1993) show how to use TextTiles in information retrieval tasks, although this work does not show whether or not the results of these algorithms produce better performance than the results of some other segmentation strategy would.

This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). $$$$$ Reynar (1994) describes an algorithm similar to that of Hearst (1993) and Hearst and Plaunt (1993) with a difference in the way in which the size of the blocks of adjacent regions are chosen.
This category choice is similar to those chosen by Hearst (1997, p. 53), who computed chance agreement in terms of the probability that coders would say that a segment boundary exists (segt), and the probability that they would not (unsegt). $$$$$ According to Carletta (1996), K measures pairwise agreement among a set of coders making category judgments, correcting for expected chance agreement as follows: where P(A) is the proportion of times that the coders agree and P(E) is the proportion of times that they would be expected to agree by chance.

The best known algorithm based on this idea is TextTiling (Hearst, 1997). $$$$$ Hearst (1993) posited that this argument should also apply to determining which words best distinguish one subtopic from another.
The best known algorithm based on this idea is TextTiling (Hearst, 1997). $$$$$ This variation of the TextTiling algorithm is explored and evaluated in Hearst (1994b).

Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarisation to anaphora resolution (Hearst, 1997). $$$$$ Although most discourse segmentation work is done at a finer granularity than that suggested here, multi-paragraph segmentation has many potential applications.
Automatic segmentation of discourse forms the basis for many applications, from information retrieval and text summarisation to anaphora resolution (Hearst, 1997). $$$$$ There are also potential applications in some other areas, such as text summarization.

While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). $$$$$ In contrast, TextTiling has the goal of identifying major subtopic boundaries, attempting only a linear segmentation.
While agreement among annotators regarding linear segmentation has been found to be higher than 80% (Hearst, 1997), with respect to hierarchical segmentation it has been observed to be as low as 60% (Flammia and Zue, 1995). $$$$$ There are several ways to evaluate a segmentation algorithm, including comparing its segmentation against that of human judges, comparing its segmentation against author-specified orthographic information, and comparing its segmentation against other automated segmentation strategies in terms of how they effect the outcome of some computational task.

As in (Hearst, 1997), we segment each document into several 'mini-documents', each one devoted to a single topic, and then to perform location and topic-based clustering over the (now larger) set of mini-documents. $$$$$ This approach, called TileBars, allows the user to make informed decisions about which documents and which passages of those documents to view, based on the distributional behavior of the query terms in the documents.
As in (Hearst, 1997), we segment each document into several 'mini-documents', each one devoted to a single topic, and then to perform location and topic-based clustering over the (now larger) set of mini-documents. $$$$$ When examining documents of all lengths, he finds that relevant documents tend to have more TextTiles than nonrelevant ones (95% significant by a Mann Whitney test).

So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al, 1998)). $$$$$ This is not so much a change in setting or character as a change in subject matter.
So far, it has been used mainly in the context of automatic text segmentation, where a change in vocabulary is often the mark of topic change (Hearst, 1997), and, to a lesser extent, in discourse studies (see, e.g., (Foltz et al, 1998)). $$$$$ This phenomenon occurs in some of the other texts as well, but to a much lesser extent.

In this study, we use Galley et al's (2003) LCSeg algorithm, a variant of TextTiling (Hearst, 1997). $$$$$ The results are that the small, artificial multi-paragraph groupings seemed to perform better than the author-supplied sectioning information (which usually consisted of many more paragraphs than Moffet et al. 's subdivision algorithm or TextTiling would create).
In this study, we use Galley et al's (2003) LCSeg algorithm, a variant of TextTiling (Hearst, 1997). $$$$$ This variation of the TextTiling algorithm is explored and evaluated in Hearst (1994b).

To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. $$$$$ This includes author-determined segments, marked orthographically (paragraphs, sections, and chapters) (Hearst and Plaunt 1993; Salton, Allan, and Buckley 1993; Moffat et al. 1994) and/or automatically derived units of text, including fixed-length blocks (Hearst and Flaunt 1993; Callan 1994), segments motivated by subtopic structure (TextTiles) (Hearst and Plaunt 1993), or segments motivated by properties of the query (Mittendorf and Schaible 1994).
To compute a baseline, we follow Kan (2003) and Hearst (1997) in using Monte Carlo simulated segments. $$$$$ Hearst and Plaunt (1993), in some early passage-based retrieval experiments, report improved results using passages over full-text documents, but do not find a significant difference between using motivated subtopic segments and arbitrarily chosen block lengths that approximated the average subtopic segment length.

Compared to the task of segmenting expository texts reported in Hearst (1997) with a 39.1% chance of each paragraph end being a target topic boundary, the chance of each speaker turn being a top-level or sub-topic boundary in our ICSI corpus is just 2.2% and 0.69%. $$$$$ 69-70) After many pages of attempting to pin the concept down, they suggest, as one alternative, investigating topic-shift markers instead: It has been suggested ... that instead of undertaking the difficult task of attempting to define 'what a topic is', we should concentrate on describing what we recognize as topic shift.
Compared to the task of segmenting expository texts reported in Hearst (1997) with a 39.1% chance of each paragraph end being a target topic boundary, the chance of each speaker turn being a top-level or sub-topic boundary in our ICSI corpus is just 2.2% and 0.69%. $$$$$ Thus the expected chance agreement P(E) is .524 (since P(Boundary) = .391 and P(Nonboundary) ----- .609, (.3912 + .6092) = .524).

For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. $$$$$ Stoddard (1991) creates cohesion maps by assigning to each word a location on a twodimensional grid corresponding to the word's position in the text.
For example, lexical cohesion-based algorithms, such as LCSEG (Galley et al, 2003), or its word frequency-based predecessor TextTile (Hearst, 1997) capture topic shifts by modeling the similarity of word repetition in adjacent windows. $$$$$ For example, the algorithms described here should prove useful for topic-based segmentation of video transcripts (Christel et al. 1995).
