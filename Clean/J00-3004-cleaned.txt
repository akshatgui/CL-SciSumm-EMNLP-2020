Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result. $$$$$ In our work we use the prediction by partial matching (PPM) symbolwise compression scheme (Cleary and Witten 1984), which has become a benchmark in the compression community.
Using a statistical model called prediction by partial matching (PPM), Teahan et al (2000) reported a significantly better result. $$$$$ For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).

Teahan (Teahan et al, 2000) has successfully applied escape method D to segment Chinese text. $$$$$ It is trained on a corpus of presegmented text, and when applied to new text, interpolates word boundaries so as to maximize the compression obtained.
Teahan (Teahan et al, 2000) has successfully applied escape method D to segment Chinese text. $$$$$ For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).

Treating each character individually as in (Teahan et al, 2000) requires a large amount of training data in order to calculate all the probabilities in the tables, as well as a large amount of table space and time to lookup data from the tables. $$$$$ Good performance can be acquired using simple rules only if the training corpus is large enough.
Treating each character individually as in (Teahan et al, 2000) requires a large amount of training data in order to calculate all the probabilities in the tables, as well as a large amount of table space and time to lookup data from the tables. $$$$$ 5.6 Effect of the Amount of Training Data For the Rocling corpus, we experimented with different amounts of training data.

Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). $$$$$ Hockenmaier and Brew (1998) present an algorithm, based on Palmer's (1997) experiments, that applies a symbolic machine learning technique—transformation-based error-driven learning (Brill 1995)—to the problem of Chinese word segmentation.
Machine learning approaches are more desirable and have been successful in both unsupervised learning (Peng and Schuur mans, 2001) and supervised learning (Teahan et al, 2000). $$$$$ Using a set of rule templates and four distinct initial-state annotators, Palmer concludes that the learning technique works well.

For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge. $$$$$ Several different algorithms have been proposed, which, generally speaking, can be classified into dictionary-based and statistical-based methods, although other techniques that involve more linguistic information, such as syntactic and semantic knowledge, have been reported in the natural language processing literature.
For example, the N-gram generative language modeling based approach of Teahan et al (2000) does not use domain knowledge. $$$$$ PPM is an n-gram approach that uses finite-context models of characters, where the previous few (say three) characters predict the upcoming one.

In addition, compared to simple models like n-gram language models (Teahan et al, 2000), another shortcoming of CRF-based segmenters is that it requires significantly longer training time. $$$$$ Ponte and Croft (1996) introduce two models for word segmentation

This approach reduces to the cross-entropy/compression-based approach of (Teahan et al 2000). $$$$$ A Compression-Based Algorithm For Chinese Word Segmentation
This approach reduces to the cross-entropy/compression-based approach of (Teahan et al 2000). $$$$$ This approach means that we can capitalize on existing research in text compression to create good models for word segmentation.

Second, the probabilistic models used in these methods (e.g. Teahan et al, 2000) are trained on a segmented corpus which is not always available. $$$$$ For this experiment (first reported by Teahan et al. [19981), PPM was trained on the million-word Brown corpus (Kucera and Francis 1967).
Second, the probabilistic models used in these methods (e.g. Teahan et al, 2000) are trained on a segmented corpus which is not always available. $$$$$ For order 3 models, most words are segmented with the same error rate as for order 5 models, though some words are missed when order 2 models are used.

This is in contrast to supervised word segmentation algorithms (e.g., Teahan et al, 2000), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese. $$$$$ Chinese is written without using spaces or other word delimiters.
This is in contrast to supervised word segmentation algorithms (e.g., Teahan et al, 2000), which are typically used for segmenting text in documents written in languages that do not put spaces between their words like Chinese. $$$$$ Chinese is written without using spaces or other word delimiters.
