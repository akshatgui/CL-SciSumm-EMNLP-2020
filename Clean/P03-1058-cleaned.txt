To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). $$$$$ We evaluated our approach on all the nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001), which used the WordNet 1.7 sense inventory (Miller, 1990).
To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). $$$$$ We evaluated our approach to word sense disambiguation on all the 29 nouns in the English lexical sample task of SENSEVAL-2 (Edmonds and Cotton, 2001; Kilgarriff 2001).

To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ We then used our approach of parallel text alignment described in the last section to obtain the training examples from the English side of the parallel texts.
To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ If there are insufficient training examples for some sense of w from the parallel texts, then we just used as many parallel text training examples as we could find for that sense.

Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ This annotated corpus then serves as the training material for a learning algorithm.
Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ The research most similar to ours is the work of Diab and Resnik (2002).

For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. $$$$$ In this step, parallel texts are first sentence-aligned and then word-aligned.
For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. $$$$$ It is a fundamental problem in natural language processing (NLP), and the ability to disambiguate word sense accurately is important for applications like machine translation, information retrieval, etc.
There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. $$$$$ For the 7 nouns that are lumped into one sense (i.e., they are all translated into one Chinese word), we do not perform WSD on these words.
For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. $$$$$ (iii) Truly ambiguous word

Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ In this step, parallel texts are first sentence-aligned and then word-aligned.
Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data. $$$$$ In this paper, we address the above issues and report our findings, exploiting the English-Chinese parallel corpora in Table 2 for word sense disambiguation.
Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data. $$$$$ In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.

 $$$$$ We rely on two sources to decide on the sense classes of w

Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ In this step, parallel texts are first sentence-aligned and then word-aligned.
Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ In the output of GIZA++, each English word token is aligned to some Chinese word token.

For example, Ng et al (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. $$$$$ For example, six English-Chinese parallel corpora are now available from Linguistic Data Consortium.
For example, Ng et al (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. $$$$$ After word alignment, each 3-sentence context in English containing an occurrence of the noun w that is aligned to a selected Chinese translation then forms a training example.

Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. $$$$$ Given a word-aligned parallel corpus, the different translations in a target language serve as the “sense-tags” of an ambiguous word in the source language.
Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. $$$$$ By defining sense distinction in terms of different target translations, the outcome of word sense disambiguation of a source language word is the selection of a target word, which directly corresponds to word selection in machine translation.

Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). $$$$$ Our previous research demonstrated that such an approach leads to a state-of-the-art WSD program with good performance.
Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). $$$$$ Our present work can be similarly extended beyond bilingual corpora to multilingual corpora.

For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ In this step, parallel texts are first sentence-aligned and then word-aligned.
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. $$$$$ The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999).
Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. $$$$$ However, large-scale, good-quality parallel corpora have recently become available.

Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. $$$$$ The parallel text alignment approach works well for nature and sense, among these nouns.
Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ Next, we randomly selected 10 sets of training examples from the parallel corpora, such that the number of training examples in each sense followed the official training set of w. (When there were insufficient training examples for a sense, we just used as many as we could find from the parallel corpora.)
To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ In this step, parallel texts are first sentence-aligned and then word-aligned.
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ Next, we randomly selected 10 sets of training examples from the parallel corpora, such that the number of training examples in each sense followed the official training set of w. (When there were insufficient training examples for a sense, we just used as many as we could find from the parallel corpora.)
To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.

As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. $$$$$ Two senses are lumped together if they are translated in the same way in Chinese.
As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. $$$$$ For example, sense 1 and 7 of channel are both translated as “频道” in Chinese, so these two senses are lumped together.
