Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ On the other hand, Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ In this section we compare their performance for English part-ofspeech tagging.

We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ An explicit sampler represents and samples the HMM parameters 0 and 0 in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled.

All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007).
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t

Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ First, the sampler can either be pointwise or blocked.
Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.

The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ This is done by first computing the parameters 0* and 0* of a proposal HMM using (7). scribed above to produce a proposal state sequence t* for the words in the sentence.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ Finally, we use a Metropolis-Hastings accept-reject step to decide whether to update the current state sequence for the sentence with the proposal t*, or whether to keep the current state sequence.

[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ We can partially address this by cross-validation.
[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ We call the accuracy of the resulting tagging the crossvalidation accuracy.

 $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.
 $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.

We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. $$$$$ Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation ofInformation (VI) described by Meilˇa (2003) as an evaluation of an unsupervised tagging.
We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. $$$$$ However as Goldwater (p.c.) points out, this may not be an ideal evaluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI.

M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ Goldwater and Griffiths (2007) proposed an information-theoretic measure known as the Variation ofInformation (VI) described by Meilˇa (2003) as an evaluation of an unsupervised tagging.
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags.

To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ We call the accuracy of the resulting tagging the crossvalidation accuracy.
To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ Variational Bayes converges faster than all of the other estimators we examined here.

 $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.
 $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.

We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.

They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).

We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). $$$$$ As Johnson et al. (2007) explains, samples of the HMM parameters 0 and 0 can be obtained using (5) if required.
We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). $$$$$ But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used.

For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ We can partially address this by cross-validation.
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).

 $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.
 $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.

 $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.
 $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.

Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.

Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank.

Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy. $$$$$ One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.
Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy. $$$$$ We call the accuracy of the resulting tagging the crossvalidation accuracy.
