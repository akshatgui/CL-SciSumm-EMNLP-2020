Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types.

Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). $$$$$ F. Determine the temporal relation between two events where one event syntactically dominates the other event.
Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). $$$$$ All corpora include event and timex annotation.

We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ The distribution over the six languages was very uneven: sixteen systems for English, two for Spanish and one for English and Spanish.

Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ Task participants could choose to either do all tasks, focus on the time expression task, focus on the event task, or focus on the four temporal relation tasks.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ However, we had expected that for TempEval-2 the systems would score better on task C since we added the restriction that the event and time expression had to be syntactically adjacent.

We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ A comparison with the Tempeval-1 results from Semeval-2007 may be of interest.

We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ SemEval-2010 Task 13: TempEval-2
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.

This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ SemEval-2010 Task 13: TempEval-2
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.

There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ However, addressing this aim is beyond the scope of an evaluation challenge and a more modest approach is appropriate.
There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.

In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ The English data sets were based on TimeBank (Pustejovsky et al., 2003; Boguraev et al., 2007), a hand-built gold standard of annotated texts using the TimeML markup scheme.4 However, all event annotation was reviewed to make sure that the annotation complied with the latest guidelines and all temporal relations were added according to the Tempeval-2 relation tasks, using the specified relation types.

TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ A comparison with the Tempeval-1 results from Semeval-2007 may be of interest.

The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ The 2007 SemEval task, TempEval-1 (Verhagen et al., 2007; Verhagen et al., 2009), was an initial evaluation exercise based on three limited temporal ordering and anchoring tasks that were considered realistic both from the perspective of assembling resources for development and testing and from the perspective of developing systems capable of addressing the tasks.1 TempEval-2 is based on TempEval-1, but is more elaborate in two respects: (i) it is a multilingual task, and (ii) it consists of six subtasks rather than three.
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ The results are very similar except for task D, but if we take a away the one outlier (the NCSUjoint score of 0.21) then the average becomes 0.78 with a standard deviation of 0.05.
