See Ng and Cardie (2002) for a detailed description of the features. $$$$$ We use the C4.5 decision tree induction system (Quinlan, 1993) to train a classifier that, given a description of two NPs in a document, NP✂ and NP✄ , decides whether or not they are coreferent.
See Ng and Cardie (2002) for a detailed description of the features. $$$$$ (See the Hand-selected Features block in Table 2.)

We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ Best-first clustering.
We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ Rather than a right-to-left search from each anaphoric NP for the first coreferent NP, we hypothesized that a right-to-left search for a highly likely antecedent might offer more precise, if not generally better coreference chains.

The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. $$$$$ This paper presents an NP coreference system that investigates two types of extensions to the Soon et al. corpus-based approach.
The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. $$$$$ Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.

One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ First, we find that performance drops significantly when using the full feature set, even though the learning algorithms investigated have built-in feature selection mechanisms.
One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ In addition, we include four new semantic features to allow finer-grained semantic compatibility tests.

In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). $$$$$ Best-first clustering.
In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). $$$$$ We plan to continue investigations along these lines, developing, for example, a true best-first clustering coreference framework and exploring a “supervised clustering” approach to the problem.

We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ The MUC-6 corpus produces a training set of 26455 instances (5.4% positive) from 4381 NPs and a test set of 28443 instances (5.2% positive) from 4565 NPs.
We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ For the MUC-7 corpus, we obtain a training set of 35895 instances (4.4% positive) from 5270 NPs and a test set of 22699 instances (3.9% positive) from 3558 NPs.

Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ Second, in an attempt to understand whether incorporating additional knowledge can improve the performance of a corpus-based coreference resolution system, we expand the Soon et al. feature set from 12 features to an arguably deeper set of 53.
Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ Each training instance represents the two NPs under consideration and consists of the 12 Soon et al. features, which are described in Table 1.

Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. $$$$$ Noun phrase coreference resolution refers to the problem of determining which noun phrases (NPs) refer to each real-world entity mentioned in a document.
Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. $$$$$ We hypothesized, however, that splitting this feature into several primitive features, depending on the type of NP, might give the learning algorithm additional flexibility in creating coreference rules.

Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ As a result, we modify the coreference clustering algorithm to select as the antecedent of NP✄ the NP with the highest coreference likelihood value from among preceding NPs with coreference class values above 0.5.

Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ We evaluate the Duplicated Soon Baseline system using the standard MUC-6 (1995) and MUC7 (1998) coreference corpora, training the coreference classifier on the 30 “dry run” texts, and applying the coreference resolution algorithm on the 20–30 “formal evaluation” texts.
Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ For instance, the CONTAINS PN feature effectively disallows coreference between NPs that contain distinct proper names but are not themselves proper names (e.g.

Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Because sources of linguistic information in a learning-based system are represented as features, we can, in contrast, incorporate them selectively rather than as universal hard constraints.
Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).

In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ This rule covers 38 examples, but has 18 exceptions.

It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.
It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).

In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ Aone and Bennett (1995), McCarthy and Lehnert (1995)).
In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ Each training instance represents the two NPs under consideration and consists of the 12 Soon et al. features, which are described in Table 1.

Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. $$$$$ Machine learning approaches to this problem have been reasonably successful, operating primarily by recasting the problem as a classification task (e.g.
Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. $$$$$ We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.

At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.
At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ Sidner (1979), Harabagiu et al. (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.

Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).
Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ Sidner (1979), Harabagiu et al. (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.

Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ Overall, the learning framework and linguistic knowledge source modifications boost performance of Soon’s learning-based coreference resolution approach from an F-measure of 62.6 to 70.4, and from 60.4 to 63.4 for the MUC-6 and MUC-7 data sets, respectively.
Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ Two final features make use of an in-house naive pronoun resolution algorithm (PRO RESOLVE) and a rule-based coreference resolution system (RULE RESOLVE), each of which relies on the original and expanded feature sets described above.

We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. $$$$$ Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.
We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. $$$$$ In particular, our tree makes use of many of the features that are not present in the original Soon feature set.

We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). $$$$$ Best-first clustering.
We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). $$$$$ We plan to continue investigations along these lines, developing, for example, a true best-first clustering coreference framework and exploring a “supervised clustering” approach to the problem.
