Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ Re-Evaluation The Role Of Bleu In Machine Translation Research
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.

Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. $$$$$ Because Bleu is precision based, and because recall is difficult to formulate over multiple reference translations, a brevity penalty is introduced to compensate for the possibility of proposing highprecision hypothesis translations which are too short.
Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. $$$$$ Therefore omitting content-bearing lexical items does not carry a greater penalty than omitting function words.

This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality.

Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ The primary reason that Bleu is viewed as a useful stand-in for manual evaluation is that it has been shown to correlate with human judgments of translation quality.
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ However, at today’s levels the amount of variation that Bleu admits is unacceptably high.

Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ This serves as another significant counter-example to Bleu’s correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do.

Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.

This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ Figure 1 gives a scatterplot of each of the hypothesis translations produced by the second best Bleu system from the 2005 NIST MT Evaluation.
This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ Appropriate uses for Bleu include tracking broad, incremental changes to a single system, comparing systems which employ similar translation strategies (such as comparing phrase-based statistical machine translation systems with other phrase-based statistical machine translation systems), and using Bleu as an objective function to optimize the values of parameters such as feature weights in log linear translation models, until a better metric has been proposed.

This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does.

It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.
It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus.

The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexical variants.
The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.

Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.
Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.

We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006). $$$$$ In order to allow some amount of variant order in phrases, Bleu places no explicit constraints on the order that matching n-grams occur in.
We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006). $$$$$ Here we denote bigram mismatches for the hypothesis translation given in Table 1 with vertical bars: Appeared calm

 $$$$$ For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.
 $$$$$ The authors are grateful to Amittai Axelrod, Frank Keller, Beata Kouchnir, Jean Senellart, and Matthew Stone for their feedback on drafts of this paper, and to Systran for providing translations of the Europarl test set.

 $$$$$ For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.
 $$$$$ The authors are grateful to Amittai Axelrod, Frank Keller, Beata Kouchnir, Jean Senellart, and Matthew Stone for their feedback on drafts of this paper, and to Systran for providing translations of the Europarl test set.

Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems.
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ It is also therefore possible to have a higher Bleu score without any genuine improvement in translation quality.

For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ This serves as another significant counter-example to Bleu’s correlation with human judgments of translation quality, and further increases the concern that Bleu may not be appropriate for comparing systems which employ different translation strategies.

Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.

In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ The rationale behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive.
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al., 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.

Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ All these uses of Bleu are predicated on the assumption that it correlates with human judgments of translation quality, which has been shown to hold in many cases (Doddington, 2002; Coughlin, 2003).
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ Bleu’s inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases.

For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ The rationale behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ Papineni et al. (2002) showed that Bleu correlated with human judgments in its rankings of five Chinese-to-English machine translation systems, and in its ability to distinguish between human and machine translations.
