Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date. $$$$$ By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task.
Moreover, including predictions of bi-directional IBM Model 4 and model of Liang et al (2006) as features, we achieve an absolute AER of 3.8 on the English French Hansards alignment task - the best AER result published on this task to date. $$$$$ To our knowledge, this is the lowest published unsupervised AER result, and it is competitive with supervised approaches.

By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall. $$$$$ Intersection eliminates the spurious alignments, but at the expense of recall.
By also including as features the posteriors of the model of Liang et al (2006), we achieve AER of 3.8, and 96.7/95.5 precision/recall. $$$$$ We maintain both high precision and recall.

Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. $$$$$ Alignment By Agreement
Liang et al (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. $$$$$ We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.

Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006). $$$$$ Alignment By Agreement
Training is performed using the agreement-based learning method which encourages the directional models to overlap (Liang et al, 2006). $$$$$ One can classify these six models into two groups: sequence-based models (models 1, 2, and HMM) and fertility-based models (models 3, 4, and 5).1 Whereas the sequence-based models are tractable and easily implemented, the more accurate fertility-based models are intractable and thus require approximation methods which are difficult to implement.

Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006). $$$$$ We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models.
Concerning the former, we trained an unsupervised model with the Berkeley aligner, an implementation of the symmetric word-alignment model described by Liang et al (2006). $$$$$ We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.

The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments. $$$$$ .. , c(4), c(> 5).3 We use three sets of distortion parameters, one for transitioning into the first state, one for transitioning out of the last state, and one for all other transitions.
The state-of-the-art unsupervised Berkeley aligner (Liang et al, 2006) with default setting is used to construct word alignments. $$$$$ We have described an efficient and fully unsupervised method of producing state-of-the-art word alignments.

We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5. $$$$$ Then we used the validation set and the first 100 sentences of the test set as our development set to tune our models.
We used version two of the Berkeley alignment model (Liang et al, 2006), with the posterior threshold set at 0.5. $$$$$ For these results, the threshold used for posterior decoding was tuned on the development set.

The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5. $$$$$ The E→F model gives 0 probability to any many-to-one alignments and the F→E model gives 0 probability to any one-to-many alignments.
The reordering metrics require alignments which were created using the Berkeley word alignment package version 1.1 (Liang et al., 2006), with the posterior probability to being 0.5. $$$$$ Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035.

As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations. $$$$$ Each model was trained for 5 iterations, using the same training regimen as in Och and Ney (2003). and joint training across different size training sets and different models, evaluated on the development set.
As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al (2006), and then trained our model for 5 EM iterations. $$$$$ We also observed that jointly trained HMMs converged very quickly—in 5 iterations—and did not exhibit overfitting with increased iterations.

Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. $$$$$ Each z can be thought of as an element in the set of generalized alignments, where any subset of word pairs may be aligned (Och and Ney, 2003).
Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al, 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. $$$$$ Following past work, we initialized the translation probabilities of model 1 uniformly over word pairs that occur together in some sentence pair.

They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006). $$$$$ By jointly training two simple HMM models, we obtain 4.9% AER on the standard English-French Hansards task.
They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al (2006). $$$$$ On the other hand, if the edge (i+2, j+2) were included, that penalty would be mitigated.

We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work. $$$$$ The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).
We used two well studied unsupervised aligners, GIZA++ (Och and Ney, 2003) and HMM (Liang et al, 2006) and one supervised aligner, ITG (Haghighi et al, 2009) as representatives in this work. $$$$$ We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.

We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009). $$$$$ The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).
We used three aligners in this work: GIZA++ (Och and Ney, 2003), jointly trained HMM (Liang et al, 2006), and ITG (Haghighi et al, 2009). $$$$$ We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation.

The HMM aligner used in this work was due to Liang et al (2006). $$$$$ The classic approaches to unsupervised word alignment are based on IBM models 1–5 (Brown et al., 1994) and the HMM model (Ney and Vogel, 1996) (see Och and Ney (2003) for a systematic comparison).
The HMM aligner used in this work was due to Liang et al (2006). $$$$$ Typically, the Viterbi alignment argmaxz p(z

Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006). $$$$$ Word alignment is an important component of a complete statistical machine translation pipeline (Koehn et al., 2003).
Second, an increase in AER does not necessarily imply an improvement in translation quality (Liang et al., 2006) and vice-versa (Vilar et al, 2006). $$$$$ While AER is only a weak indicator of final translation quality in many current translation systems, we hope that more accurate alignments can eventually lead to improvements in the end-to-end translation process.

As suggested by Liang et al (2006), we can group the distortion parameters into a few buckets. $$$$$ The distortion parameters pd(aj = i0

As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data. $$$$$ We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sentences, a validation set of 37 sentences, and a test set of 447 sentences.
As an additional experiment, we tested the Cross EM aligner (Liang et al, 2006) from the Berkeley Aligner package on the MSR data. $$$$$ We trained models 1, 2, and HMM on the Hansards data.

Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences. $$$$$ We tested our approach on the English-French Hansards data from the NAACL 2003 Shared Task, which includes a training set of 1.1 million sentences, a validation set of 37 sentences, and a test set of 447 sentences.
Exceptions where discriminative SMT has been used on large training data are Liang et al (2006a) who trained 1.5 million features on 67,000 sentences. $$$$$ We trained models 1, 2, and HMM on the Hansards data.

Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a). $$$$$ Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035.
Training data for discriminative learning are prepared by comparing a 100-best list of translations against a single reference using smoothed per sentence BLEU (Liang et al, 2006a). $$$$$ By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051.

We performed word alignment using a cross EM word aligner (Liang et al, 2006). $$$$$ Word alignment is an important component of a complete statistical machine translation pipeline (Koehn et al., 2003).
We performed word alignment using a cross EM word aligner (Liang et al, 2006). $$$$$ In our case, we would have to sum over the set of alignments where each word in English is aligned to at most one word in French and each word in French is aligned to at most one word in English.
