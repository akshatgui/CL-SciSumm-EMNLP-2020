Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.
Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).
An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et alâ€™s (2006) and can easily exploit and expand on previous research in phrase-based machine translation.

This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ We are not too concerned though because, in practice, we decode using a larger set of submodels (feature functions).
This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. $$$$$ The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.
Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). $$$$$ In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.
Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ Our translation models rely upon and naturally exploit submodels (feature functions) that have been initially developed in phrase-based systems for choosing target translations of source language phrases, and use new, syntax-based translation and target language submodels for assembling target phrases into well-formed, grammatical outputs.
We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ To obtain the non-lexicalized xRS rules, we compute the set of all minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules.

Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006).
Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.
(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

Inside the Moses toolkit, three different statistical approaches have been implemented $$$$$ SPMT

Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). $$$$$ To test our hypothesis, we modify our rule extraction algorithm so that for every foreign phrase FZ , we extract not only a minimally syntactified, lexicalized xRS rule, but also one composed rule.
Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). $$$$$ The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.

Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.
Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). $$$$$ Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.
Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). $$$$$ On a 1 to 5 quality scale, the difference between the phrase-based and syntax-based systems was, on average, between 0.2 and 0.3 points.

Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).
Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ Also, the SPMT models yield significantly fewer rules that the model of Galley et al. In contrast with the model proposed by Chiang, the SPMT models introduced in this paper are fully grounded in syntax; this makes them good candidates for exploring the impact that syntaxbased language models could have on translation performance.

As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.
As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ To obtain the non-lexicalized xRS rules, we compute the set of all minimal rules (lexicalized and non-lexicalized) by applying the algorithm proposed by Galley et al. (2006) and then remove the lexicalized rules.
In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ In contrast, in the work of Galley et al. (2004; 2006), a rule is defined to be minimal when it is necessary in order to explain a (7r, F, A) tuple.
In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ Our intuition is that composed rules that involve the application of more than two minimal rules are not reliable.

Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).
Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ The xRS formalism utilized by Galley et al. (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases.

The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ We compare these two probabilities across the submodels and we scale all model probabilities to be compatible with those of Model 2 composed.
The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ As test data, we used the 2003 NIST test set.

Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al., 2006) when significantly different engines compete for producing the best outputs.
Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).

We build two translation systems $$$$$ Because our approach uses translation rules with Syntactified target language Phrases (see Figure 1), we call it SPMT.
We build two translation systems $$$$$ The SPMT models are similar to the models proposed by Chiang (2005) and Galley et al. (2006).
