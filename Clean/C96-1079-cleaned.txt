In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed. $$$$$ This paper looks briefly at the history of these Conferences and then examines the considerations which led to the structure of MUC-6} The Message Understanding Conferences were initiated by NOSC to assess and to foster research on the automated analysis of military messages containing textual information.
In the case of English, partially motivated by Message Understanding Conferences (MUCs) (Grishman and Sundheim, 1996), a number of coreference resolution methods have been proposed. $$$$$ Although called "conferences", the distinguishing characteristic of the MUCs are not the conferences themselves, but the evaluations to which participants must submit in order to be permitted to attend the conference.

A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC 6 (1995) (Grishman and Sundheim, 1996). $$$$$ By MUC-2 (1989), the task had crystalized as one of template filling.
A condition of attending the MUC workshops was participation in a required evaluation (bakeoff) task of filling slots in templates about events, and began (after an exploratory MUC-1 in 1987) with MUC-2 in 1989, followed by MUC-3 (1991), MUC-4 (1992), MUC-5 (1993) and MUC 6 (1995) (Grishman and Sundheim, 1996). $$$$$ For MUC-2, the template had 10 slots.

Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). $$$$$ Message Unders tand ing  Conference  - 6: A Br ie f  H is tory Ralph Grishman Dept.
Discourse references have been the subject of attention in both the Message Understanding Conference (Grishman and Sundheim, 1996) and the Automatic Content Extraction program (Strassel et al., 2008). $$$$$ encouraging work on "deeper understanding" aTipster is a U.S. Govermnent program of research and development in the areas of inibrmation retrieval and information extraction.

There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). $$$$$ DARPA has a number of information science and technol- ogy programs which are driven in large part, by regular evaluations.
There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). $$$$$ The joint venture task re- quired 11 templates with a total of 47 slots for the output double tile number of slots defined for MUC-4  and the task documentation was over 40 pages long.

The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al, 1999), and ontology population (Suchanek et al, 2007). $$$$$ One re- ceives a description of a class of events to be iden- tiffed in the text; for each of these events one must fill a template with information about the event.
The automated construction of semantically typed lexicons (terms classified into their appropriate semantic class) from unstructured text is of great importance for various kinds of information extraction (Grishman and Sundheim, 1996), question answering (Moldovan et al, 1999), and ontology population (Suchanek et al, 2007). $$$$$ A C{]REF tag has mt ID ai;l;ri

The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). $$$$$ One re- ceives a description of a class of events to be iden- tiffed in the text; for each of these events one must fill a template with information about the event.
The topic of the sixth MUC (MUC-6) was management succession events (Grishman and Sundheim, 1996). $$$$$ Tile scenario involved changes in corporate executive management per- sonnel.

Figure 2 is a simplified event from the the MUC-6 evaluation similar to one described by Grishman and Sundheim (1996). $$$$$ The template has slots for information about the event, such as the type of event, the agent, the time and place, the effect, etc.
Figure 2 is a simplified event from the the MUC-6 evaluation similar to one described by Grishman and Sundheim (1996). $$$$$ Figure 2: Sample coreference annotation.

As demonstrated by prior work (Grishman and Sundheim, 1996), grammar-based IE systems can be effective in many scenarios. $$$$$ The goal for scenar io  templates  mini- MUC - -  was to demonstrate that effective infor- mation extraction systems could be created in a few weeks.
As demonstrated by prior work (Grishman and Sundheim, 1996), grammar-based IE systems can be effective in many scenarios. $$$$$ Although it is difficult to meaningfully compare results on differ- ent scenarios, the scores obtained by most systems after a few weeks (40% to 50% recall, 60% to 70% precision) were comparable to the best scores ob- tained in prior MUCs.

When it was introduced, in the 6th Message Understanding Conference (Grishman and Sundheim, 1996), the named entity recognition task comprised three entity identification and labeling subtasks: ENAMEX (proper names and acronyms designating persons, locations, and organizations), TIMEX (absolute temporal terms) and NUMEX (numeric expressions, monetary expressions, and percentages). $$$$$ To meet this goal the con> mittce developed the "named entity" task, which t(asically involves identifying the names of all the people, organizations, and geographic locations in a text.
When it was introduced, in the 6th Message Understanding Conference (Grishman and Sundheim, 1996), the named entity recognition task comprised three entity identification and labeling subtasks: ENAMEX (proper names and acronyms designating persons, locations, and organizations), TIMEX (absolute temporal terms) and NUMEX (numeric expressions, monetary expressions, and percentages). $$$$$ To meet this goal, we decided that the infbrmation extraction task for MUC-6 wouhl have to involve a relatively simple template, more like MUC-2 than MUC-5; this was duhbed "mini- 467 Mr. <ENAMEX TYPE="PERSON">Dooner</ENAMEX> met with <ENAMEX:TYPE="PERSON">Martin Puris</ENAMEX>, president and chief executive officer of <ENAMEX TYPE="ORGANIZATION">Ammirati & Puris</ENAMEX>, about <ENAMEX TYPE="ORGANIZATION">McCann</ENAMEX>~s acquiring the agency with billings of <NUMEX TYPE="MONEY">$400 million</NUMEX>, but nothing has materialized.

The NER task was introduced with the 6th Message UnderstandingConference (MUC) in 1995 (Grishman and Sundheim, 1996). $$$$$ Message Unders tand ing  Conference  - 6: A Br ie f  H is tory Ralph Grishman Dept.
The NER task was introduced with the 6th Message UnderstandingConference (MUC) in 1995 (Grishman and Sundheim, 1996). $$$$$ MUC-6 introduced sev- eral innovations over prior MUCs, most notably in the range of different asks for which evaluations were conducted.

The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996). $$$$$ MUC-6 introduced sev- eral innovations over prior MUCs, most notably in the range of different asks for which evaluations were conducted.
The problem of name recognition and classification has been intensively studied since 1995, when it was introduced as part of the MUC 6 Evaluation (Grishman and Sundheim, 1996). $$$$$ The second MUC also worked out the details of the primary evaluation measures, recall and pre- cision.

 $$$$$ 5 Portability The second goal was 1;o focus on portability in the inibrmation extraction task the ability to rapidly retarget a system to extract; information about a different class of events.
 $$$$$ The PERSON and ORGANIZATION templates are the "temt)late lement" templates, which are invariant across scenarios.

In a previous attempt to define predicate-argument structure, Semeval, the effort was abandoned because so many constructs would require detailed attention and resolution, and because most information-extraction systems did not generate full predicate-argument structures (most likely because the task did not require it) (Grishmanand Sundheim, 1996). $$$$$ Systems did particularly poorly in identifying descriptions; the highest-scoring system had 38% recall and 51% precision for descriptions.
In a previous attempt to define predicate-argument structure, Semeval, the effort was abandoned because so many constructs would require detailed attention and resolution, and because most information-extraction systems did not generate full predicate-argument structures (most likely because the task did not require it) (Grishmanand Sundheim, 1996). $$$$$ Whether this is true, and just what the hard problems are, will require more extensive analysis of the results of MUC-6.

Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ The old-style MUC information extraction task, based on a description of a particular (:lass of events (a "scenario") was called the "scenario template" task.
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ A C{]REF tag has mt ID ai;l;ri

 $$$$$ 5 Portability The second goal was 1;o focus on portability in the inibrmation extraction task the ability to rapidly retarget a system to extract; information about a different class of events.
 $$$$$ The PERSON and ORGANIZATION templates are the "temt)late lement" templates, which are invariant across scenarios.

We used 6 categories of entity types, which are the major categories defined in the MUC (Grishman and Sundheim 1996) or ACE project (ACE homepage). $$$$$ The tag ENAMEX ("entity name expression") is used for both people and organiza- tion names; the tag NUNEX ( "numer ic  expression") is used for currency and I)ercentages.
We used 6 categories of entity types, which are the major categories defined in the MUC (Grishman and Sundheim 1996) or ACE project (ACE homepage). $$$$$ A major  role o[ the.

For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. $$$$$ For each executive post; one generates a SUCCESSION_EVENT template, which contains refl~rences to the ORGANIZATION template for the organization i volved, and the IN_AND OUT template for the activity involving that post (if an article describes a person leaving and a per- son start;ing the same job, there will be two IN_AND_OUT templates).
For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. $$$$$ The PERSON and ORGANIZATION templates are the "temt)late lement" templates, which are invariant across scenarios.

Diversity IE traditionally targets a selected event type (Grishman and Sundheim, 1996). $$$$$ The template has slots for information about the event, such as the type of event, the agent, the time and place, the effect, etc.
Diversity IE traditionally targets a selected event type (Grishman and Sundheim, 1996). $$$$$ To meet this goal, we decided that the infbrmation extraction task for MUC-6 wouhl have to involve a relatively simple template, more like MUC-2 than MUC-5; this was duhbed "mini- 467 Mr. <ENAMEX TYPE="PERSON">Dooner</ENAMEX> met with <ENAMEX:TYPE="PERSON">Martin Puris</ENAMEX>, president and chief executive officer of <ENAMEX TYPE="ORGANIZATION">Ammirati & Puris</ENAMEX>, about <ENAMEX TYPE="ORGANIZATION">McCann</ENAMEX>~s acquiring the agency with billings of <NUMEX TYPE="MONEY">$400 million</NUMEX>, but nothing has materialized.
