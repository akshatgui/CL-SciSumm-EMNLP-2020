 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

A previous implementation of the IMS system, NUS-PT (Chan et al, 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine grained task, respectively. $$$$$ In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task.
A previous implementation of the IMS system, NUS-PT (Chan et al, 2007b), participated in SemEval-2007 English all-words tasks and ranked first and second in the coarse-grained and fine grained task, respectively. $$$$$ Hereafter, we will use symbols to represent the Chinese and English words in the rule: c1, c2, and c3 will represent the Chinese words “4”, “�”, and “YI” respectively.

For example, (Chan et al, 2007) trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier. $$$$$ Using the WSD classifier described in Section 2, we classified the words in each Chinese source sentence to be translated.
For example, (Chan et al, 2007) trained a discriminative model for WSD using local but also across-sentence unigram collocations of words in order to refine phrase pair selection dynamically by incorporating scores from the WSD classifier. $$$$$ For example, for the Chinese sentence “.. .

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

Chan et al (2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models. $$$$$ For our the decoder could weigh the additional alternative experiments, we use the SVM implementation of translations against its own.
Chan et al (2007) use an SVM based classifier for disambiguating word senses which are directly incorporated in the decoder through additional features that are part of the log-linear combination of models. $$$$$ For local 2005) did not use a state-of-the-art MT system, collocations, we use 3 features, w_1w+1, w_1, and while the experiments in (Vickrey et al., 2005) were w+1, where w_1 (w+1) is the token immediately to not done using a full-fledged MT system and the the left (right) of the current ambiguous word ocevaluation was not on how well each source sentence currence w. For parts-of-speech, we use 3 features, was translated as a whole.

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (Chan et al, 2007). $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.
A similar approach has been tried in the word-sense disambiguation (WSD) domain where local but also across-sentence unigram collocations of words are used to refine phrase pair selection dynamically by incorporating scores from the WSD classifier (Chan et al, 2007). $$$$$ Pharaoh, a state-of-the-art phrase-based MT sys- 2 Word Sense Disambiguation tem (Koehn et al., 2003).

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

It has long been believed that being able to detect the correct sense of a word in a given context - performing word sense disambiguation (WSD) - will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al, 2007) and summarization (Elhadad et al., 1997). $$$$$ Word Sense Disambiguation Improves Statistical Machine Translation
It has long been believed that being able to detect the correct sense of a word in a given context - performing word sense disambiguation (WSD) - will lead to improved performance of systems tackling high end applications such as machine translation (Chan et al, 2007) and summarization (Elhadad et al., 1997). $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.

To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6). $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.
To show the effect of our framework, we globally train millions of word level context features motivated by word sense disambiguation (Chan et al, 2007) together with the features used in traditional SMT system (Section 6). $$$$$ To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems.

We use a set of word context features motivated by word sense disambiguation (Chan et al, 2007) to test scalability. $$$$$ Word Sense Disambiguation Improves Statistical Machine Translation
We use a set of word context features motivated by word sense disambiguation (Chan et al, 2007) to test scalability. $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.

Independent of these lexical substitution tasks, the connection between word senses and word translation has been explored in Chan et al (2007) and Carpuat and Wu (2007), who predict the probabilities of a target word being translated as an item in a sense inventory, where the sense inventory is a list of possible translations. $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.
Independent of these lexical substitution tasks, the connection between word senses and word translation has been explored in Chan et al (2007) and Carpuat and Wu (2007), who predict the probabilities of a target word being translated as an item in a sense inventory, where the sense inventory is a list of possible translations. $$$$$ Hence, unlike traditional WSD where the sense classes are tied to a specific sense inventory, our “senses” here consist of the English phrases extracted as translations for each Chinese phrase.

Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ In this paper, we successfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-artWSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system.

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

Chan et al (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement. $$$$$ Furthermore, the improvement is statistically significant.
Chan et al (2007) incorporated a WSD system into the hierarchical SMT system, Hiero (Chiang, 2005), and reported statistically significant improvement. $$$$$ We have shown that WSD improves the translation performance of a state-of-the-art hierarchical phrase-based statistical MT system and this improvement is statistically significant.

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al, 2007). $$$$$ Word Sense Disambiguation Improves Statistical Machine Translation
This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al, 2007). $$$$$ A hierarchical phrase-based model for statistical machine translation.

See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT. $$$$$ Word sense disambiguation (WSD) is the task of determining the correct meaning or sense of a word in context.
See (Chan et al, 2007) for the relevance of word sense disambiguation and (Chiang et al, 2009) for the role of prepositions in MT. $$$$$ Pharaoh, a state-of-the-art phrase-based MT sys- 2 Word Sense Disambiguation tem (Koehn et al., 2003).

 $$$$$ A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed.
 $$$$$ David Chiang was partially supported under the GALE program of the Defense Advanced Research Projects Agency, contract HR0011-06-C0022.

We note that the best performing system (Chan et al, 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity. $$$$$ In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task.
We note that the best performing system (Chan et al, 2007b) of this task achieved a relatively high accuracy of 82.5%, highlighting the importance of having an appropriate level of sense granularity. $$$$$ We obtain accuracy that ing two additional features into the MT model which compares favorably to the best participating system operate on the existing rules of the grammar, with- in the task (Carpuat et al., 2004). out introducing competing rules.
