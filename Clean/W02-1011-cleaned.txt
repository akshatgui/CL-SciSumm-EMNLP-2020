For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer. $$$$$ Our data source was the Internet Movie Database (IMDb) archive of the rec.arts.movies.reviews newsgroup.3 We selected only reviews where the author rating was expressed either with stars or some numerical value (other conventions varied too widely to allow for automatic processing).
For example, (Pang et al, 2002) collected reviews from a movie database and rated them as positive, negative, or neutral based on the rating (e.g., number of stars) given by the reviewer. $$$$$ Ratings were automatically extracted and converted into one of three categories: positive, negative, or neutral.

It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al, 2002). $$$$$ For the work described in this paper, we concentrated only on discriminating between positive and negative sentiment.
It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al, 2002). $$$$$ Our aim in this work was to examine whether it suffices to treat sentiment classification simply as a special case of topic-based categorization (with the two “topics” being positive sentiment and negative sentiment), or whether special sentiment-categorization methods need to be developed.

Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al, 2002), e.g., favorable or unfavorable, positive or negative. $$$$$ Our aim in this work was to examine whether it suffices to treat sentiment classification simply as a special case of topic-based categorization (with the two “topics” being positive sentiment and negative sentiment), or whether special sentiment-categorization methods need to be developed.
Sentiment classification is a special task of text classification whose objective is to classify a text according to the sentimental polarities of opinions it contains (Pang et al, 2002), e.g., favorable or unfavorable, positive or negative. $$$$$ Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.

For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ We used documents from the movie-review corpus described in Section 3.
For evaluation, we use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets that contain reviews of four different types of product from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ (Nigam et al., 1999).

Automatic identification of subjective content of ten relies on word indicators, such as unigrams (Pang et al, 2002) or predetermined sentiment lexica (Wilson et al, 2005). $$$$$ Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).
Automatic identification of subjective content of ten relies on word indicators, such as unigrams (Pang et al, 2002) or predetermined sentiment lexica (Wilson et al, 2005). $$$$$ (Nigam et al., 1999).

 $$$$$ We applied these procedures to uniformlydistributed data, so that the random-choice baseline result would be 50%.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation.

For the purpose of this work, we follow the definition of Pang et al (2002) and Turney (2002) and consider a binary classification task for output labels as positive and negative. $$$$$ We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative.
For the purpose of this work, we follow the definition of Pang et al (2002) and Turney (2002) and consider a binary classification task for output labels as positive and negative. $$$$$ For the work described in this paper, we concentrated only on discriminating between positive and negative sentiment.

The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al, 2002). $$$$$ Most previous research on sentiment-based classification has been at least partially knowledge-based.
The low recall for adjective POS based synsets can be detrimental to classification since adjectives are known to express direct sentiment (Pang et al, 2002). $$$$$ Since adjectives have been a focus of previous work in sentiment detection (Hatzivassiloglou and Wiebe, 2000; Turney, 2002)13, we looked at the performance of using adjectives alone.

We choose to use SVM since it performs the best for sentiment classification (Pang et al, 2002). $$$$$ Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.
We choose to use SVM since it performs the best for sentiment classification (Pang et al, 2002). $$$$$ (Nigam et al., 1999).

Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. $$$$$ MaxEnt feature/class functions Fi,c only reflects the presence or absence of a feature, rather than directly incorporating feature frequency.
Following standard practice in sentiment analysis (Pang et al, 2002), the input to SVMlight consisted of normalized presence-of-feature (rather than frequency-of-feature) vectors. $$$$$ In order to investigate whether reliance on frequency information could account for the higher accuracies of Naive Bayes and SVMs, we binarized the document vectors, setting ni(d) to 1 if and only feature fi appears in d, and reran Naive Bayes and SVMlight on these new vectors.11 As can be seen from line (2) of Figure 3, better performance (much better performance for SVMs) is achieved by accounting only for feature presence, not feature frequency.

Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)). $$$$$ Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).
Much of this research explores sentiment classification, a text categorization task in which the goal is to classify a document as having positive or negative polarity (e.g., Das and Chen (2001), Pang et al (2002), Turney (2002), Dave et al (2003), Pang and Lee (2004)). $$$$$ (Nigam et al., 1999).

Each has proven to be effective in previous sentiment analysis studies (Pang et al, 2002), so as this experiment is rooted in sentiment classification, these methods were also assumed to perform well in this cross-discourse setting. $$$$$ However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.
Each has proven to be effective in previous sentiment analysis studies (Pang et al, 2002), so as this experiment is rooted in sentiment classification, these methods were also assumed to perform well in this cross-discourse setting. $$$$$ The philosophies behind these three algorithms are quite different, but each has been shown to be effective in previous text categorization studies.

These results support experiments carried out for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but differs from sentiment classification results from Pang et al (2002) that suggest that term-based models perform better than the frequency-based alternative. $$$$$ However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization.
These results support experiments carried out for topic based classification using Bayesianclassifiers by McCallum and Nigam (1998), but differs from sentiment classification results from Pang et al (2002) that suggest that term-based models perform better than the frequency-based alternative. $$$$$ Most previous research on sentiment-based classification has been at least partially knowledge-based.

Pang et al (2002) applied these classifiers to the movie review domain, which produced good results. $$$$$ We used documents from the movie-review corpus described in Section 3.
Pang et al (2002) applied these classifiers to the movie review domain, which produced good results. $$$$$ The results produced via machine learning techniques are quite good in comparison to the humangenerated baselines discussed in Section 4.

 $$$$$ We applied these procedures to uniformlydistributed data, so that the random-choice baseline result would be 50%.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation.

Previouswork has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). $$$$$ Thumbs Up? Sentiment Classification Using Machine Learning Techniques
Previouswork has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). $$$$$ Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines.

Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. $$$$$ Another, more related area of research is that of determining the genre of texts; subjective genres, such as “editorial”, are often one of the possible categories (Karlgren and Cutting, 1994; Kessler et al., 1997; Finn et al., 2002).
Pang et al (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. $$$$$ (Turney (2002) makes a similar point, noting that for reviews, “the whole is not necessarily the sum of the parts”.)

Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al, 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case. $$$$$ Fi,c is a feature/class function for feature fi and class c, defined as follows:6 class c. The parameter values are set so as to maximize the entropy of the induced distribution (hence the classifier’s name) subject to the constraint that the expected values of the feature/class functions with respect to the model are equal to their expected values with respect to the training data: the underlying philosophy is that we should choose the model making the fewest assumptions about the data while still remaining consistent with it, which makes intuitive sense.
Although the discriminative model (e.g., SVM) is proven to be more effective on unigrams (Pang et al, 2002) for its ability of capturing the complexity of more relevant features, WR features are more inclined to work better in the generative model (e.g., NB) since the feature independence assumption holds well in this case. $$$$$ Hence, if context is in fact important, as our intuitions suggest, bigrams are not effective at capturing it in our setting.

We do this because we already have enough training data, so there is no need to resort to cross-validation (Pang et al, 2002). $$$$$ All results reported below, as well as the baseline results from Section 4, are the average three-fold cross-validation results on this data (of course, the baseline algorithms had no parameters to tune).
We do this because we already have enough training data, so there is no need to resort to cross-validation (Pang et al, 2002). $$$$$ (Nigam et al., 1999).

Applications of text categorization, such as sentiment classification (Pang et al, 2002), are now required to run on multiple languages. $$$$$ Sentiment classification would also be helpful in business intelligence applications (e.g.
Applications of text categorization, such as sentiment classification (Pang et al, 2002), are now required to run on multiple languages. $$$$$ Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.
