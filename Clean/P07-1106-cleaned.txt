On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ Chinese Segmentation with a Word-Based Perceptron Algorithm
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ In comparison, our system is based on a single perceptron model.

 $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
 $$$$$ We thank the anonymous reviewers for their insightful comments.

Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ Chinese Segmentation with a Word-Based Perceptron Algorithm
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.

 $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
 $$$$$ We thank the anonymous reviewers for their insightful comments.

(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.

All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ Furtherfor decoding. more, our approach provides an example of the poSeveral discriminatively trained models have re- tential of search-based discriminative training methcently been applied to the CWS problem.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.

It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Table 4 shows the accuracy with various features from the model removed.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.

In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.

 $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
 $$$$$ We thank the anonymous reviewers for their insightful comments.

More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.

We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ In comparison, our system is based on a single perceptron model.

We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff).
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ 80% of the sentences (3813) were randomly chosen for training and the rest (985 sentences) were used as development testing data.

 $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
 $$$$$ We thank the anonymous reviewers for their insightful comments.

For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ These results demonstrate the imalgorithms such as the Viterbi algorithm can be used portance of word-based features for CWS.
For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ Figure 2 gives the decoding algorithm.

This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ Two sets of experiments were conducted.
This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.

 $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
 $$$$$ We thank the anonymous reviewers for their insightful comments.

For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ Hence we are Boolean valued vectors containing the indicator use a beam-search decoder during training and test- features for one element in the sequence. ing; our idea is similar to that of Collins and Roark Denote the global feature vector for segmented (2004) who used a beam-search decoder as part of sentence y with 4b(y) E Rd, where d is the total a perceptron parsing model.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ Figure 2 gives the decoding algorithm.

The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al. (2006) for comparison.

We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.

We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ Chinese Segmentation with a Word-Based Perceptron Algorithm
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ There is no fixed standard for Chinese word segmentation.
