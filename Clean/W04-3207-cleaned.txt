Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ We might take the projection idea a step farther.
Finally, the work that is the closest to ours in spirit is the idea of joint estimation (Smith and Smith, 2004). $$$$$ In the spirit of joint optimization, we might have also combined the Korean morphology and syntax models into one inference task.

More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ We model the sequence of morphemes and their tags as a log-linear trigram model.
More recently, Smith and Smith (2004) proposed to merge an English parser, a word alignment model, and a Korean PCFG parser trained from a small number of Korean parse trees under a unified log linear model. $$$$$ The first is the Korean PCFG trained on the small KTB training sets, as described in §3.3.

We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ This inference task is carried out by a bilingual parser.
We know of only one study which evaluates these bilingual grammar formalisms on the task of grammar induction itself (Smith and Smith, 2004). $$$$$ While both of these formalisms require bilingual grammar rules, Eisner (2003) describes an algorithm for learning tree substitution grammars from unaligned trees.

(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ Unlabeled precision and recall show continued improvement with more Korean training data.
(Smith and Smith, 2004) tried to build a Korean parser by bilingual approach with English, and achieved labeled precision/recall around 40% for Korean. $$$$$ The bilingual parser without the English head span filter gives a small recall improvement on average at similar precision.

Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ In our bilingual parser, the English and Korean parses are mediated through word-to-word translational correspondence links.
Smith and Smith (2004) explore syntactic projection further by proposing an English-Korean bilingual parser integrated with a word translation model. $$$$$ We can describe this effect as a filtering of Korean constituents by the English model and word alignments.

Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ Consider the problem of parsing a language L for which annotated resources like treebanks are scarce.
Smith and Smith (2004 ) consider a similar setting for parsing both English and Korean, but instead of learning a joint model, they consider a fixed combination of two parsers and a word aligner. $$$$$ A second filter we consider is to keep only the (remaining) bilingual constituents corresponding to an English head word’s maximal span.

 $$$$$ 6 When we combine the PCFG with the other models to do joint bilingual parsing, we simply use the logs of the PCFG probabilities as if they were log-linear weights.
 $$$$$ This work was supported under a National Science Foundation Graduate Research Fellowship and a Fannie and John Hertz Foundation Fellowship.

Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ Following Hwa et al. (2002), we looked at dependency links in the true English parses from the KTB where both the dependent and the head were linked to words on the Korean side using the intersection alignment.
Another distinct body of work addresses the problem of parser bootstrapping based on syntactic dependency projection (e.g. Hwa et al 2002), often using approaches based in synchronous parsing (e.g. Smith and Smith, 2004). $$$$$ Fox (2002) has considered English and French, and Hwa et al. (2002) investigate Chinese and English.

The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ In order to experiment easily with different algorithms, we implemented all the morphological disambiguation and parsing models in this paper in Dyna, a new language for weighted dynamic programming (Eisner et al., 2004).
The lattice-conditional estimation approach was first used by Kudo et al (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. $$$$$ Improved English parsers—such as Collins’ models—have also been implemented in Dyna, the dynamic programming framework used here (Eisner et al., 2004).

Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ The feature templates used by our model are shown in Table 2.
Similar results were presented by Smith and Smith (2004), using a similar estimation strategy with a model that included far more feature templates. $$$$$ The first feature corresponds to the fully-described child-generation event; others are similar but less informative.

Smith and Smith (2004) applied factored estimation to a bilingual weighted grammar, driven by data limitations. $$$$$ Bilingual Parsing With Factored Estimation

In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ Since the IBM models only hypothesize one-to-many alignments from target to source, we trained using each side of the bitext as source and target in turn.
In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al (2010). $$$$$ The output of our bilingual parser contains three types of constituents
