Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ Experiments showed that putting a large weight on the model trained on labeled data performs best.

In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ Under source sentence.

A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ Semi-supervised learning has been previously applied to improve word alignments.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ Self-training for SMT was proposed in (Ueffing, 2006).

Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ The proposed method adapts the trained models to the style and domain of the new input.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ A comparison with Table 4 shows that transductive learning on the development set and test corpora, adapting the system to their domain and style, is more effective in improving the SMT system than the use of additional source language data.

Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ For details, see (Ueffing and Ney, 2007).
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ Self-training for SMT was proposed in (Ueffing, 2006).

In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ We provide a basic description here; for a lected sentence pairs are replaced in each iteration, detailed description see (Ueffing et al., 2007). and only the original bilingual training data, L, is The models (or features) which are employed by kept fixed throughout the algorithm.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ For details, see (Ueffing and Ney, 2007).

Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006).
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ Since re-training the full phrase tables is not feasible here, a (small) additional phrase table, specific to U, was trained and plugged into the SMT system as an additional model.

We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ In one set of experiments, we used the 100K and 150K training sentences filtered according to n-gram coverage over the test set.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ In (CallisonBurch et al., 2004), a generative model for word alignment is trained using unsupervised learning on parallel text.

To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: which we implemented follows the approaches suggested in (Blatz et al., 2003; Ueffing and Ney, 2007): The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities, Levenshtein-based word posterior probabilities, and a target language model score.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ These K sampled translations and their associated source sentences make up the additional training data Ti.

The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ The SMT system we applied in our experiments is PORTAGE.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ Self-training for SMT was proposed in (Ueffing, 2006).

From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Transductive learning for statistical machine translation
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Self-training for SMT was proposed in (Ueffing, 2006).
