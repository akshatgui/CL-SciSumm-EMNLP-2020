This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). $$$$$ As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models.

Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ Before the training starts, we apply the non-compositional constraints over the pruned bitext space to further constrain the space of phrase pairs.
Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns.

It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. $$$$$ There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. $$$$$ Our pruning differs from Zhang and Gildea (2005) in two major ways.

As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ The sole difference between EM and VB with a sparse prior Î± is that the raw fractional counts c are replaced by exp(Ïˆ(c + Î±)), an operation that resembles smoothing.
As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ For small values of Î± the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more likely ones.

The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. $$$$$ We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases.

Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time.
Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area.

 $$$$$ Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space.
 $$$$$ The last author was supported by NSF IIS-0546554.

 $$$$$ Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space.
 $$$$$ The last author was supported by NSF IIS-0546554.

We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ In the end, a Viterbi pass for the phrasal ITG is executed to produce the non-compositional phrasal alignments.

Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ From this alignment, phrase pairs are extracted in the usual manner, and a phrase-based translation system is trained.
Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004).

We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). $$$$$ First of all, we do not have to run a GIZA++ aligner.

Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing
Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007).

The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs.
The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ C is our unique pre-terminal for generating terminal multi-word pairs: We parameterize our probabilistic model in the manner of a PCFG: we associate a multinomial distribution with each nonterminal, where each outcome in this distribution corresponds to an expansion of that nonterminal.

Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ 500 Chinese-English pairs from this set were manually aligned and used as a gold standard.

Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging.
Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ As pointed out by Johnson (2007), in effect this expression adds to c a small value that asymptotically approaches Î± â€” 0.5 as c approaches oc, and 0 as c approaches 0.

Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.
Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method.

Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.
Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ We chose Variational Bayes, for its procedural similarity to EM and ease of implementation.

In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ Bayesian Learning of Non-Compositional Phrases with Synchronous Parsing
In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.

Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ This linear time algorithm allows us to compute span pruning in O(n3) time.
Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).

Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior.
Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ Second, Dirichlet distributions with small, non-zero parameters place more probability mass on multinomials on the edges or faces of the probability simplex, distributions with fewer non-zero parameters.
