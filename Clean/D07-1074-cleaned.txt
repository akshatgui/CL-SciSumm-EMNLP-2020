We perform term disambiguation on each document using an entity extractor (Cucerzan, 2007). $$$$$ Entity-based cross- document coreferencing using the vector space model.
We perform term disambiguation on each document using an entity extractor (Cucerzan, 2007). $$$$$ Using Encyclopedic Knowledge for Named Entity Disambiguation.

More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan's work. $$$$$ Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented sys- tem shows high disambiguation accuracy on both news stories and Wikipedia articles.
More similarity features were added by Cucerzan (2007) who realized that topical coherence between a candidate entity and other entities in the context will improve NED accuracy and by Milne and Witten (2008) who built on Cucerzan's work. $$$$$ They employed several of the disambiguation resources discussed in this paper (Wikipedia entity pages, redirection pages, categories, and hyperlinks) and built a context- article cosine similarity model and an SVM based on a taxonomy kernel.

Cucerzan (2007) employed context vectors consisting of phrases and categories extracted from Wikipedia. $$$$$ categories in the Wikipedia category tree.
Cucerzan (2007) employed context vectors consisting of phrases and categories extracted from Wikipedia. $$$$$ They evaluated their models for person name disambiguation over 110, 540, and 2,847 categories, reporting accuracies between 55.4% and 84.8% on (55-word context, entity) pairs extracted from Wikipedia, depending on the model and the development/test data employed.

This entity disambiguation data set was introduced by Cucerzan (2007). $$$$$ 708?716, Prague, June 2007. c?2007 Association for Computational Linguistics Large-Scale Named Entity Disambiguation Based on Wikipedia Data  Silviu Cucerzan Microsoft Research One Microsoft Way, Redmond, WA 98052, USA silviu@microsoft.com  Abstract This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results.
This entity disambiguation data set was introduced by Cucerzan (2007). $$$$$ (e.g., ?Texas (disambiguation)?

we implemented the approach brought by Cucerzan (2007) based on our best understanding. $$$$$ 708?716, Prague, June 2007. c?2007 Association for Computational Linguistics Large-Scale Named Entity Disambiguation Based on Wikipedia Data  Silviu Cucerzan Microsoft Research One Microsoft Way, Redmond, WA 98052, USA silviu@microsoft.com  Abstract This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results.
we implemented the approach brought by Cucerzan (2007) based on our best understanding. $$$$$ However, the brute force approach we tried ?

e. $$$$$ is the surface form that is displayed (also referred to as the anchor text of the link).
e. $$$$$ Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.

To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts. $$$$$ Mann and Yarowsky (2003) ad- dressed the task of clustering the Web search re- sults for a set of ambiguous personal names by employing a rich feature space of biographic facts obtained via bootstrapped extraction patterns.
To model these characteristics, Bunescu and Pasca (2006) and Cucerzan (2007) incorporate information from Wikipedia articles, Artiles et al (2007) use Webpage content, Mann and Yarowsky (2003) extract biographic facts. $$$$$ Mann, G. S. and D. Yarowsky.

 $$$$$ is the surface form that is displayed (also referred to as the anchor text of the link).
 $$$$$ Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.

Cucerzan (2007) disambiguated the names by combining the BOW model with the Wikipedia category information. $$$$$ We aug- ment the Wikipedia category information with in- formation automatically extracted from Wikipedia list pages and use it in conjunction with the context information in a vectorial model that employs a novel disambiguation method.
Cucerzan (2007) disambiguated the names by combining the BOW model with the Wikipedia category information. $$$$$ The superimposed tooltips show how several of the surface forms were disambiguated based on the context and category agreement method.

Cucerzan (2007) and Bunescu (2006) used Wikipedia's category information in the disambiguation process. $$$$$ 3 Information Extraction from Wikipedia We discuss now the extraction of entities and the three main types of disambiguation clues (entity surface forms, category tags, and contexts) used by the implemented system.
Cucerzan (2007) and Bunescu (2006) used Wikipedia's category information in the disambiguation process. $$$$$ 3.2 Category Information All articles that are titled ?List of [?]?

 $$$$$ is the surface form that is displayed (also referred to as the anchor text of the link).
 $$$$$ Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.

Cucerzan (2007), by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Pasca (2006). $$$$$ is used to refer to more than twenty different named entities in Wikipedia.
Cucerzan (2007), by contrast to the above, used Wikipedia primarily for Named Entity Disambiguation, following the path of Bunescu and Pasca (2006). $$$$$ Using Encyclopedic Knowledge for Named Entity Disambiguation.

As in our paper, and unlike the above mentioned works, Cucerzan (2007) made use of the explicit Category information found within Wikipedia. $$$$$ We aug- ment the Wikipedia category information with in- formation automatically extracted from Wikipedia list pages and use it in conjunction with the context information in a vectorial model that employs a novel disambiguation method.
As in our paper, and unlike the above mentioned works, Cucerzan (2007) made use of the explicit Category information found within Wikipedia. $$$$$ 3.2 Category Information All articles that are titled ?List of [?]?

Cucerzan (2007) did not make use of the Category information in identifying the class of a given entity. $$$$$ However, both studies targeted the clus- tering of all mentions of an entity across a given document collection rather than the mapping of these mentions to a given reference list of entities.
Cucerzan (2007) did not make use of the Category information in identifying the class of a given entity. $$$$$ 3.2 Category Information All articles that are titled ?List of [?]?

 $$$$$ is the surface form that is displayed (also referred to as the anchor text of the link).
 $$$$$ Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.

Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. $$$$$ 1 Introduction and Related Work The ability to identify the named entities (such as people and locations) has been established as an important task in several areas, including topic de- tection and tracking, machine translation, and in- formation retrieval.
Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. $$$$$ 7 Conclusions and Potential Impact We presented a large scale named entity disam- biguation system that employs a huge amount of information automatically extracted from Wikipe- dia over a space of more than 1.4 million entities.

Previous work by Bunescu and Pasca (2006) and Cucerzan (2007) aims to link entity mentions to their corresponding topic pages in Wikipedia but the authors differ in their approaches. $$$$$ When processing the Wikipedia collection, we distinguish among four types of articles: entity pages, redirecting pages, disambiguation pages, and list pages.
Previous work by Bunescu and Pasca (2006) and Cucerzan (2007) aims to link entity mentions to their corresponding topic pages in Wikipedia but the authors differ in their approaches. $$$$$ 3.1 Surface Form to Entity Mappings There are four sources that we use to extract entity surface forms: the titles of entity pages, the titles of redirecting pages, the disambiguation pages, and the references to entity pages in other Wikipedia articles.

We evaluated our system on two datasets: the Text Analysis Conference (TAC) track on Knowl edge Base Population (TAC-KBP) (McNamee and Dang, 2009) and the newswire data used by Cucerzan (2007) (Microsoft News Data). $$$$$ 709 2 The Disambiguation Paradigm We present in this section an overview of the pro- posed disambiguation model and the world knowl- edge data employed in the instantiation of the model discussed in this paper.
We evaluated our system on two datasets: the Text Analysis Conference (TAC) track on Knowl edge Base Population (TAC-KBP) (McNamee and Dang, 2009) and the newswire data used by Cucerzan (2007) (Microsoft News Data). $$$$$ In tests on both real news data and Wikipedia text, the system obtained accuracies exceeding 91% and 88%.

We downloaded the evaluation data used in Cucerzan (2007): 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia. $$$$$ 6 Evaluation We used as development data for building the de- scribed system the Wikipedia collection as of April 2, 2006 and a set of 100 news stories on a diverse range of topics.
We downloaded the evaluation data used in Cucerzan (2007): 20 news stories from MSNBC with 642 entity mentions manually linked to Wikipedia and another 113 mentions not having any corresponding link to Wikipedia. $$$$$ 6.2 News Stories We downloaded the top two stories in the ten MSNBC news categories (Business, U.S.

 $$$$$ is the surface form that is displayed (also referred to as the anchor text of the link).
 $$$$$ Journal of the Ameri- can Society for Information Science and Technology, 45(9):645-655.
