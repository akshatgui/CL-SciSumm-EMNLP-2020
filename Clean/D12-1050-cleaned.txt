Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001).
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ Socher et al. (2011a) and Socher et al.

Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.
Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.

The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008).
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Socher et al. (2011a) and Socher et al.

Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ A Comparison of Vector-based Representations for Semantic Composition
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ Table 3 summarizes the performance of the various models on the phrase similarity dataset.

 $$$$$ Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·)

Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues $$$$$ For each of our three vector sources and three different compositional methods, we create the following features

Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ Socher et al. (2011a) and Socher et al.
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).
