We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ We have seen that there is a large amount of variation in the neighbours selected by different measures andtherefore the choice of measure in a given appli cation is likely to be important.

Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).

Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ 3.2 Results.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ low freq.
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ However, having discussed a connection between frequency and distributional generality, we might also expect to find that the frequency of the hypernymic term is greater than that of the hyponymicterm.

Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004). $$$$$ Characterising Measures Of Lexical Distributional Similarity
Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004). $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

 $$$$$ c P (c

Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Previous work on the evaluation of dis tributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be orientedtowards a particular task such as language mod elling (Dagan et al, 1999; Lee, 1999).

Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ This supports the idea of a three-waylinking between distributional generality, rela tive frequency and semantic generality.

Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ The complete set of 2000 nouns (WScomp) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$  500), and WSlow contains the nouns ranked 3001-4000 (frequency ? 100).
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ Thus, a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set.

Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ Characterising Measures Of Lexical Distributional Similarity
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). $$$$$ An increase in distance correlates with a decrease in similarity.
Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). $$$$$ 4.1 Measuring bias.

As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ The cosine measure (Salton and McGill, 1983) returns the cosine of the angle between two vectors.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ To do this, we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.

of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ The k nearest neighbours of a target word w are the k words for which similarity with w is greatest.

Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Another is the ?-skew diver gence measure, which uses the p distribution tosmooth the q distribution.
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.

Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ The first approach is not ideal since it assumes that the goal of distributional similarity methods is topredict semantic similarity and that the semantic resource used is a valid gold standard.
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ This will make it possible to predict in advanceof any experimental evaluation which distributional similarity measures might be most appro priate for a particular application.

Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better.
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).
