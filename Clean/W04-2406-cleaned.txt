 $$$$$ In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.
 $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.

The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ Then we describe our approach to the evaluation of unsupervised word sense discrimination.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ (Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.

Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ (Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).

(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ We employ two different representations of the context in which a target word occurs.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ The second order experiments in this paper use two different types of features, co–occurrences and bigrams, defined as they are in the first order experiments.

This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ This suggests that the second order context vectors (SC) have an advantage over the first order vectors for small training data as is found among the 24 SENSEVAL-2 words.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ When given smaller amounts of data like SENSEVAL-2, second order context vectors and a hybrid clustering method like Repeated Bisections perform better.

A detailed description can be found in Purandare and Pedersen (2004). $$$$$ This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).
A detailed description can be found in Purandare and Pedersen (2004). $$$$$ What follows is a concise description of each configuration.

The method is described in Purandare and Pedersen (2004). $$$$$ (Sch¨utze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty’s method) fares well.
The method is described in Purandare and Pedersen (2004). $$$$$ As such this is a hybrid method that combines a hierarchical divisive approach with partitioning.

Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ The data is particularly challenging for unsupervised algorithms due to the large number of fine grained senses, generally 8 to 12 per word.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ This table shows the number of words for which an experiment performed better than the the majority class, broken down by part of speech.

 $$$$$ In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.
 $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Table 1 shows that the top 3 experiments for each of the mixed-words are all second order vectors (SC).

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ Second order context vectors are designed to identify such relationships, in that exact matching is not required, but rather words that occur in similar contexts will have similar vectors.

Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ This paper also proposes and evaluates several extensions to these techniques.
Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ (Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.

First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ Pedersen and Bruce use a small number of local features that include co–occurrence and part of speech information near the target word.
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ Note that PB1 and SC1 use co–occurrence features, while PB3 and SC3 rely on bigram features.

(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ This paper also proposes and evaluates several extensions to these techniques.

Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ While there has been some previous work in sense discrimination (e.g., (Sch¨utze, 1992), (Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998), (Sch¨utze, 1998), (Fukumoto and Suzuki, 1999)), by comparison it is much less than that devoted to word sense disambiguation, which is the process of assigning a meaning to a word from a predefined set of possibilities.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ He selects features based on their frequency counts or log-likelihood ratios in this corpus.

In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ From this, we observe that with both first order and second order context vectors Repeated Bisections is more effective than UPGMA.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Second order context vectors succeed in such cases because they find indirect second order co– occurrences of feature words and hence describe the context more extensively than the first order representations.

Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ (Pedersen and Bruce, 1997) and (Pedersen and Bruce, 1998) propose a (dis)similarity based discrimination approach that computes (dis)similarity among each pair of instances of the target word.
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).

These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ The small volume of data combined with large number of possible senses leads to very small set of examples for most of the senses.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ And when the number of senses is greater than the number of clusters, some senses will not be assigned to any cluster.

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ These mix-words were created in order to provide data that included both fine grained and coarse grained distinctions.

Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ This paper presents a systematic comparison of discrimination techniques suggested by Pedersen and Bruce ((Pedersen and Bruce, 1997), (Pedersen and Bruce, 1998)) and by Sch¨utze ((Sch¨utze, 1992), (Sch¨utze, 1998)).
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ Sch¨utze reduces the dimensionality of this feature space using Singular Value Decomposition (SVD), which is also employed by related techniques such as Latent Semantic Indexing (Deerwester et al., 1990) and Latent Semantic Analysis (Landauer et al., 1998).
