IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). $$$$$ An alignment Ã¢ for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model.
IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). $$$$$ So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.

The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. $$$$$ In this paper, we present and compare various single-word based alignment models for statistical machine translation.
We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. $$$$$ For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.

The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. $$$$$ As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. $$$$$ This allows an automatic evaluation, once a reference alignment has been produced.

The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).

The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). $$$$$ The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .

Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). $$$$$ Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).

The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b). $$$$$ We present different methods to combine alignments.
The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b). $$$$$ Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments.

Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ In (statistical) alignment models Pr(fil , aflef), a 'hidden' alignment a is introduced which describes a mapping from source word fi to a target word Ca,.
Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .

The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ So far, no well established evaluation criterion exists in the literature for these alignment models.
In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .

We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). $$$$$ As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. $$$$$ As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. $$$$$ We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.

We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.

 $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
 $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).

Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. $$$$$ We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

This demonstrates that we are competitive with the methods described in (Och and Ney, 2000). $$$$$ We present different methods to combine alignments.
This demonstrates that we are competitive with the methods described in (Och and Ney, 2000). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

 $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
 $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).

The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004). $$$$$ In this paper, we present and compare various single-word based alignment models for statistical machine translation.
The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004). $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
