 $$$$$ Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.
 $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%.
Instead, we build a log-linear CRF-based dependency parser, which is similar to the CRF based constituent parser of Finkel et al (2008). $$$$$ The most recent similar research is (Petrov et al., 2007).

Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ We found no difference in parser performance between using stochastic gradient descent (SGD), and the more common, but significantly slower, L-BFGS.
Finkel et al (2008) show that SGD achieves optimal test performance with far fewer iterations than other optimization routines such as L-BFGS. $$$$$ In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time.

Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ This function, Lˆ is designed to approximate the true function L based off a small subset of the training data represented by Db.
Therefore, we propose a simple corpus-weighting strategy, as shown in Algorithm 1, where Db i, k is the subset of training data used in kth update and b is the batch size; k is the update step, which is adjusted following the simulated annealing procedure (Finkel et al, 2008). $$$$$ Here b, the batch size, means that Db is created by drawing b training examples, with replacement, from the training set D. With this notation we can express the stochastic evaluation of the function as Lˆ (Db;e).

 $$$$$ Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.
 $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ The most recent similar research is (Petrov et al., 2007).
We found that the SGD parameters described by Finkel et al (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. $$$$$ We also show that the use of SGD for training CRFs performs as well as L-BFGS in a fraction of the time.

Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Lastly, we iterate through the data multiple times, so if we can compute this information just once, we will save time on all subsequent iterations on that sentence.
Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al, 2008). $$$$$ Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.

We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ This paper extends the third thread of work, where joint inference via dynamic programming algorithms is used to train models and to attempt to find the globally best parse.
We directly compare asynchronous algorithms with multiprocessor synchronous mini-batch algorithms (e.g., Finkel et al, 2008) and traditional batch algorithms. $$$$$ We found that an initial gain of η0 = 0.1 and batch size between 15 and 30 was optimal for this application.

Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ This is further supported by Johnson (2001), who saw no parsing gains when switching from generative to discriminative training, and by Petrov et al. (2007) who saw only small gains of around 0.7% for their final model when switching training methods.
Finkel et al (2008) used this approach to speed up training of a log-linear model for parsing. $$$$$ Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.

As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ Unlike (Taskar et al., 2004), our algorithm has the advantage of being easily parallelized (see footnote 7 in their paper).
As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al, 2008). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.

Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ In this work, we provide just such a framework for training a feature-rich discriminative parser.
Finkel et al (2008) incorporated rich local features into a tree CRF model and built a competitive parser. $$$$$ For both WSJ15 and WSJ40, we trained a generative model; a discriminative model, which used lexicon features, but no grammar features other than the rules themselves; and a feature-based model which had access to all features.

Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ In our experiments SGD converged to a lower objective function value than L-BFGS, however it required far fewer iterations (see Figure 2) and achieved comparable test set performance to L-BFGS in a fraction of the time.
Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Taskar et al. (2004) took a large margin approach to discriminative learning, but achieved only small gains.

 $$$$$ Note that there exists a grammar in this class which is weakly equivalent with any arbitrary CFG.
 $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM, by the Disruptive Technology Office (DTO) Phase III Program for Advanced Question Answering for Intelligence (AQUAINT) through Broad Agency Announcement (BAA) N61339-06R-0034, and by a Scottish Enterprise EdinburghStanford Link grant (R37588), as part of the EASIE project.

The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ Our parsing model is based on a conditional random field model, however, unlike previous TreeCRF work, e.g., (Cohn and Blunsom, 2005; Jousse et al., 2006), we do not assume a particular tree structure, and instead find the most likely structure and labeling.
The model can be used for tasks like syntactic parsing (Finkel et al, 2008) and semantic role labeling (Cohn and Blunsom, 2005). $$$$$ Because they were focusing on grammar splitting they, like (Johnson, 2001), did not employ any features, and, like (Taskar et al., 2004), they saw only small gains from switching from generative to discriminative training.

In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004).
In an important contrast, Koo et al (2008) smooth the sparseness of lexical features in a discriminative dependency parser by using cluster based word-senses as intermediate abstractions in addition to POS tags (also see Finkel et al (2008)). $$$$$ The most recent similar research is (Petrov et al., 2007).

Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ Stochastic optimization methods have proven to be extremely efficient for the training of models involving computationally expensive objective functions like those encountered with our task (Vishwanathan et al., 2006) and, in fact, the on-line backpropagation learning used in the neural network parser of Henderson (2004) is a form of stochastic gradient descent.
Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008). $$$$$ Utilization of stochastic optimization routines requires the implementation of a stochastic objective function.

Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ We then define a conditional probability distribution over entire trees, using the standard CRF distribution, shown in (1).
Recently, CRF using tree structures were used in (Finkel et al, 2008) in the case of parsing. $$$$$ The partition function Zs, which makes the probability of all possible parses sum to unity, is defined over all structures as well as all labelings of those structures.

Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ For example, in (Lafferty et al., 2001), when switching from a generatively trained hidden Markov model (HMM) to a discriminatively trained, linear chain, conditional random field (CRF) for part-of-speech tagging, their error drops from 5.7% to 5.6%.
Recently, there has been an explosion of interest in Conditional Random Fields (CRFs) (Lafferty et al., 2001) for solving structured output classification problems, with many successful applications in NLP including syntactic parsing (Finkel et al2008), syntactic chunking (Sha and Pereira, 2003) and discourse chunking (Ghosh et al2011) in Penn Discourse Treebank (Prasad et al2008). $$$$$ For all experiments, we trained and tested on the Penn treebank (PTB) (Marcus et al., 1993).

One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ The stochastic gradient, �L (D(i) b ;e), is then simply the derivative of the stochastic function value.
One sees this clear trend in the supervised NLP literature examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al, 2005), exponentiated gradient algorithms (Collins et al, 2008), stochastic gradient for constituency parsing (Finkel et al, 2008). $$$$$ More recent is the work of (Turian and Melamed, 2006; Turian et al., 2007), which improved both the training time and accuracy of (Taskar et al., 2004).

Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ In Figure 3 we show for an example from section 22 the parse trees produced by our generative model and our feature-based discriminative model, and the correct parse.
Once we have converted our sentences into parse trees, we train a discriminative constituency parser similar to that of (Finkelet al, 2008). $$$$$ We have presented a new, feature-rich, dynamic programming based discriminative parser which is simpler, more effective, and faster to train and test than previous work, giving us new state-of-the-art performance when training and testing on sentences of length < 15 and the first results for such a parser trained and tested on sentences of length < 40.
