(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ Such systematic mismatches should be learned by the model, and used during translation.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ Several researchers have tried putting “more syntax” into translation models: like us, they use statistical versions of synchronous grammars, which generate source and target sentences in parallel and so describe their correspondence.4 This approach offers four features absent from IBM-style models: (1) a recursive phrase-based translation, (2) a syntax-based language model, (3) the ability to condition a word’s translation on the translation of syntactically related words, and (4) polynomial-time optimal alignment and decoding (Knight, 1999).

(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ Our methods work on either dependency trees (as shown) or phrase-structure trees.

Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001).

For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ STSG is simply a version of synchronous treeadjoining grammar or STAG (Shieber and Schabes, 1990) that lacks the adjunction operation.

In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ The tree parsing algorithm resembles bottom-up chart parsing under the derivation CFG.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ The algorithm is almost as before.

 $$$$$ Third, our version of the STSG formalism is more flexible than previous versions.
 $$$$$ This algorithm is essentially alignment to an unknown tree T2; we do not loop over its nodes c2, but choose t2 freely.

 $$$$$ Third, our version of the STSG formalism is more flexible than previous versions.
 $$$$$ This algorithm is essentially alignment to an unknown tree T2; we do not loop over its nodes c2, but choose t2 freely.

A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ This means that a sentence and its translation must have isomorphic syntax trees, although they may have different numbers of surface words if null words a are allowed in one or both languages.

To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ When would learned tree-to-tree mappings be useful?

If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ When would learned tree-to-tree mappings be useful?
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ As before, a derived tree pair T has the same form as an elementary tree pair.

Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ Third, our version of the STSG formalism is more flexible than previous versions.

We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ Learning Non-Isomorphic Tree Mappings For Machine Translation
We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999).

A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ For a fixed grammar, the runtime and space are only O(n) for a tree of n nodes.

More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ We make the quite natural proposal of using a synchronous tree substitution grammar (STSG).
More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ A synchronous TSG consists of a set of elementary tree pairs.

Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ Learning Non-Isomorphic Tree Mappings For Machine Translation
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ Obviously, in MT, when one has parsers for both the source and target language.

 $$$$$ Third, our version of the STSG formalism is more flexible than previous versions.
 $$$$$ This algorithm is essentially alignment to an unknown tree T2; we do not loop over its nodes c2, but choose t2 freely.

We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ Our approach is to reconstruct all possible derivations, using dynamic programming to decompose the tree pair into aligned pairs of elementary trees in all possible ways.
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ The tree parsing algorithm resembles bottom-up chart parsing under the derivation CFG.

Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ Learning Non-Isomorphic Tree Mappings For Machine Translation
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ When would learned tree-to-tree mappings be useful?

Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ There is a natural analogy between (probabilistic) TSGs and (probabilistic) CFGs.
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ One can also apply agenda-based parsing strategies.

As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ But the input is a tree rather than a string, and the chart is indexed by nodes of the input tree rather than spans of the input string:5 The β values are inside probabilities.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ As before, a derived tree pair T has the same form as an elementary tree pair.
