For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).
For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ In the field of Natural Language Processing, many successes have been reported.

There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ Let s = c1c2 · · · cm be a sequence of Japanese characters, t = t1t2 · · · tm be a sequence of Japanese character types 3 associated with each character, and yi ∈ {+1, −1}, (i = (1, 2,...,m− 1)) be a boundary marker.
There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ Note that we distinguish the relative position of each character and character type.

We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. $$$$$ In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).


 $$$$$ The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.
 $$$$$ µd ¶µ

Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. $$$$$ We use a TRIE to efficiently store the set Q.
Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. $$$$$ Although there are many implementations for TRIE, we use a Double-Array (Aoe, 1989) in our task.

PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. $$$$$ This is a naive extension of Inverted Indexing in Information Retrieval.
PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. $$$$$ One is PKI (Polynomial Kernel Inverted), which is an extension of Inverted Index.

PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). $$$$$ Then, we introduced two fast classification algorithms for this kernel.

Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ The experiments using a Cubic Kernel are suitable to evaluate the effectiveness of the basket mining approach applied in the PKE, since a Cubic Kernel projects the original feature space F into F3 space, which is too large to be handled only using a naive exhaustive method.
Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ PKE takes a basket mining approach to enumerating effective feature combinations more efficiently than their exhaustive method.

We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. $$$$$ These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.

We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).
We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ Kernel methods (e.g., Support Vector Machines (Vapnik, 1995)) attract a great deal of attention recently.

Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. $$$$$ Kernel-based learning (e.g., Support Vector Machines) has been successfully applied to many hard problems in Natural Language Processing (NLP).
Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. $$$$$ These methods are applicable not only to the NLP tasks but also to general machine learning tasks where training and test examples are represented in a binary vector.

We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. $$$$$ In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).

readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. $$$$$ In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).

The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ In our mining task, we can give a real number to minimum support, since each transaction (support example Xj) has possibly non-integer frequency Cdyjαj.
The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ In our experiments, we used the same settings as (Kudo and Matsumoto, 2002).

This result conforms to the results reported in (Kudo and Matsumoto, 2003). $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
This result conforms to the results reported in (Kudo and Matsumoto, 2003). $$$$$ The result can be obtained by merging these two results.

 $$$$$ The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.
 $$$$$ µd ¶µ

In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space. $$$$$ In order to build PKE, we extend the PrefixSpan (Pei et al., 2001), an efficient Basket Mining algorithm, to enumerate effective feature combinations from a set of support examples.
In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space. $$$$$ There are many subset mining algorithms proposed, however, we will focus on the PrefixSpan algorithm, which is an efficient algorithm for sequential pattern mining, originally proposed by (Pei et al., 2001).

 $$$$$ The use of a polynomial kernel allows such feature expansion without loss of generality or an increase in computational costs, since the Polynomial Kernel of degree d implicitly maps the original feature space F into Fd space.
 $$$$$ µd ¶µ

We use the following tools for syntactic processing $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
We use the following tools for syntactic processing $$$$$ In previous research, we presented a state-of-the-art SVMs-based Japanese dependency parser (Kudo and Matsumoto, 2002).
