(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ The resulting clusterings are then used in training partially class-based language models.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.

We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ The clusterings generated in each iteration as well as the initial clustering are stored as the set of words in each cluster, the total number of occurrences of each cluster in the training corpus, and the list of words preceeding each cluster.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ We then added these models as additional features to the log linear model of the Arabic-English machine translation system.

However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ We use a predictive class bigram model as given in Eq.
However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ For all models used in our experiments, both wordand class-based, the smoothing method used was Stupid Backoff (Brants et al., 2007).

By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens).
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ While the greedy non-distributed exchange algorithm is guaranteed to converge as each exchange increases the log likelihood of the assumed bigram model, this is not necessarily true for the distributed exchange algorithm.

While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data

We use the following fairly standard features in our tagger $$$$$ This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data

We use the following features for our tagger $$$$$ This then leads to the following optimization criterion, where N(w) and N(c) denote the number of occurrences of a word w or a class c in the training data and N(c, d) denotes the number of occurrences of some word in class c followed by a word in class d in the training data

Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ Our experiments showed that this also seems to be the case for the distributed exchange algorithm.
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ The weights are trained using minimum error rate training (Och, 2003) with BLEU score as the objective function.

To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ The experiments presented show that predictive class-based models trained using the obtained word classifications can improve the quality of a state-ofthe-art machine translation system as indicated by the BLEU score in both translation tasks.

However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ Thus the algorithm scales linearly in the number of classes.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ Given a sentence f in the source language, the machine translation problem is to automatically produce a translation eï¿½ in the target language.
