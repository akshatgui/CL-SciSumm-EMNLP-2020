Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.

Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.
Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ We also analyse the sub-sequence filtering scheme directly by creating a BF with only 3-grams and a BF containing both 2-grams and 3-grams and comparing their actual error rates when presented with 3-grams that are all known to be negatives.

As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. $$$$$ Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. $$$$$ Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.

There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.

We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. $$$$$ We presented 500K negatives to the filter and recorded the frequency of overestimation errors of each size.
We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. $$$$$ As shown in Section 3.1, the probability of overestimating an item’s frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.

We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.

Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ This is known as a one-sided error.
Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.

Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ Bits in the filter can, therefore, be shared by distinct items allowing significant space savings but introducing a non-zero probability of false positives at test time.

However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.

Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.

Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ The error rate Err is, This implies that, unlike a conventional lossless data structure, the model’s accuracy depends on other components in system and how it is queried.
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.

Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.
Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ By testing for 2-grams prior to querying for the 3-grams, we can avoid performing some queries that may otherwise have incurred errors using the fact that a 3-gram cannot be present if one of its constituent 2-grams is absent.

We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.

All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ We use the French-English section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003) and the English Gigaword Corpus (LDC2003T05; GW) for additional language modelling.
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ Our baseline LM and other comparison models are conventional n-gram models smoothed using modified Kneser-Ney and built using the SRILM Toolkit (Stolcke, 2002); as is standard practice these models drop entries for n-grams of size 3 and above when the corresponding discounted count is less than 1.

RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ The base 2 quantisation performs worse for smaller amounts of memory, possibly due to the larger set of events it is required to store.
RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).

For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system.
For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ This does not use any frequency information.

The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ A standard BF can implement a Boolean ‘language model’ test: have we seen some fragment of language before?
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ The baseline language model, EP-KN-3, is a trigram model trained on the English portion of the parallel corpus.

RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.

Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.

There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.
