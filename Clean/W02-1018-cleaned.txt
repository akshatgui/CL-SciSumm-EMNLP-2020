posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.
posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.

Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ This yields conditional probability distributions and which we use for decoding.
Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ This yields conditional probability distributions and which we use for decoding.

In (Marcu and Wong, 2002), a joint probability phrase model is presented. $$$$$ 2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.
In (Marcu and Wong, 2002), a joint probability phrase model is presented. $$$$$ Figure 2

The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).
The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ Figure 2

The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).
The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).

 $$$$$ We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.
 $$$$$ This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.

Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.
Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.

A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. $$$$$ Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.
A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. $$$$$ As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the data.

A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. $$$$$ A Phrase-Based Joint Probability Model For Statistical Machine Translation
A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. $$$$$ Figure 2

Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ The first iterations estimate the alignment probabilities using Model 1.
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ The first iterations estimate the alignment probabilities using Model 1.

The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ A Phrase-Based Joint Probability Model For Statistical Machine Translation
The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.

For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).
For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).

Marcu and Wong (2002) propose a joint probability model. $$$$$ 2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.
Marcu and Wong (2002) propose a joint probability model. $$$$$ Figure 2

However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ 3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.
However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.

Indeed, Marcu and Wong (2002) conjectures that none exist. $$$$$ We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).
Indeed, Marcu and Wong (2002) conjectures that none exist. $$$$$ We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).

 $$$$$ We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.
 $$$$$ This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.

For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ The sentences in the corpus were at most 20 words long.
For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ The sentences in the corpus were at most 20 words long.

Marcu and Wong (2002) describes an approximation to O. $$$$$ However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.
Marcu and Wong (2002) describes an approximation to O. $$$$$ However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.


The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ A Phrase-Based Joint Probability Model For Statistical Machine Translation
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.
