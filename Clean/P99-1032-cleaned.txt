Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). $$$$$ As applied here, the observed variables are the classifications assigned by the judges.
Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). $$$$$ Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges.

Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). $$$$$ Goodman's procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995).
Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective.
Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ Such sentences may be either subjective or objective.
We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ We apply these models to three data configurations: one with two categories (subjective and objective with no certainty ratings), one with four categories (subjective and objective with coarse-grained certainty ratings, as shown in Table 1), and one with eight categories (subjective and objective with fine-grained certainty ratings).

Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. $$$$$ Our goal is to correct correlated disagreements automatically.
Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. $$$$$ We use the latent class model to correct symmetric disagreements that appear to result from bias.

Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). $$$$$ We also present the results of a probabilistic classifier developed on the resulting annotations.
Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). $$$$$ The results of the experiments are very promising.

Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). $$$$$ In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features.
Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ Let nj denote the number of sentences that judge 1 classifies as i and judge 2 classifies as j, and let be the probability that a randomly selected sentence is categorized as i by judge 1 and j by judge 2.
In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ In contrast, agreement among the other judges noticeably improves.

Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not.
Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). $$$$$ Then, the latent class model is: (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} .
While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). $$$$$ The strong performance of the classifier and its consistency with the judges demonstrate the value of this approach to developing gold-standard tags.

Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective.
Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. $$$$$ The definitions of the categories in our coding manual are intention-based: &quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective.
Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. $$$$$ The speaker, in these cases, is being used as a reliable source of information.&quot; Following are examples of subjective and objective sentences: In sentence 4, there is no uncertainty or evaluation expressed toward the speaking event.

Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ We also present the results of a probabilistic classifier developed on the resulting annotations.
Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ They are used to guide the revision of the coding manual, resulting in improved Kappa scores, and they serve as a gold standard for developing a probabilistic classifier.

Again, our feature set is richer than Wiebe et al (1999). $$$$$ Thus, it is used to define the bias-corrected tags for the second data set as well.
Again, our feature set is richer than Wiebe et al (1999). $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ In related work (Wiebe et al., in preparation), we found that article types, such as announcement and opinion piece, are significantly correlated with the subjective and objective classification.
Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ On each fold, one set is used for testing, and the other nine are used for training.

 $$$$$ Relative bias is one aspect of agreement among judges.
 $$$$$ We are grateful to Matthew T. Bell and Richard A. Wiebe for participating in the annotation study, and to the anonymous reviewers for their comments and suggestions.

According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. $$$$$ From the coding manual: &quot;Subjective speech-event (and private-state) sentences are used to communicate the speaker's evaluations, opinions, emotions, and speculations.
According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. $$$$$ A second corpus is annotated by the same four judges according to the new coding manual.

One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). $$$$$ A second corpus is annotated by the same four judges according to the new coding manual.
One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). $$$$$ Two disjoint corpora are used in steps 2 and 5, both consisting of complete articles taken from the Wall Street Journal Treebank Corpus (Marcus et al., 1993).

Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). $$$$$ This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975).
Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). $$$$$ The results of the second tagging experiment are analyzed using the methods described in section 3, and bias-corrected tags are produced for the second data set.

During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999). $$$$$ Linguistic categorizations usually do not cover all instances perfectly.
During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999). $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).
