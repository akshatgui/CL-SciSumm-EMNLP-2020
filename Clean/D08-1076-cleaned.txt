As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ The performance drop in iteration 1 is also attributed to overfitting the candidate repository.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ The reduced performance for N-best MERT is a consequence of the performance drop in the first iteration which causes the final weights to be far off from the initial parameter set.

Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ For a description on how to generate lattices, see (Ueffing et al., 2002).
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).

While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ Lattice-based Minimum Error Rate Training for Statistical Machine Translation
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).

A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ To develop the algorithm for computing the upper envelope of all translation hypotheses that are encoded in a phrase lattice, we first consider a node vPVf with some incoming and outgoing arcs: Each path that starts at the source node s and ends in v defines a partial translation hypothesis which can be represented as a line (cf.
A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ The memory efficiency of the suggested algorithm results from the following theorem which provides a novel upper bound for the number of cost minimizing paths in a directed acyclic graph with arcspecific affine cost functions.

Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ We now assume that the upper envelope for these partial translation hypotheses is known.

Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ By construction, the upper envelope consists of at most K line segments.

Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on Cs and preventing the optimization procedure from stopping in a poor local optimum.
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on confusion networks (Rosti et al., 2007; Macherey and Och, 2007).

The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).

Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ The memory efficiency of the suggested algorithm results from the following theorem which provides a novel upper bound for the number of cost minimizing paths in a directed acyclic graph with arcspecific affine cost functions.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ This convex hull is again a convex polygon which consists of at most N1 + N2 edges, and therefore, the number of cost minimizing paths in G1' (and thus also in G) is upper bounded by NiN2.� ❑ Corollary: The upper envelope for a phrase lattice Gf~p = (Vf, Efq) consists of at most

We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT.
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.

Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ Section 7 reports on experiments conducted on the NIST 2008 translation tasks.
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ While only the aren task shows improvements on the development data, lattice MERT provides consistent gains over N-best MERT on all three blind test sets.

Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ While this update scheme provides a ranking of the feature functions according to their discriminative power (each iteration picks the feature function for which changing the corresponding weight yields the highest gain), it does not take possible correlations between the feature functions into account.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).

In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.

In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ Lattice-based Minimum Error Rate Training for Statistical Machine Translation
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.

To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.

To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.
To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.

Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.
Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ Candidate translations in MERT are typically represented as N-best lists which contain the N most probable translation hypotheses.

But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ A weakness of the Downhill Simplex algorithm is, however, its decreasing robustness for optimization problems in more than 10 dimensions.

Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.

If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ In this section, the algorithm for computing the upper envelope on N-best lists is extended to phrase lattices.
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ In (Zens et al., 2007), the MERT criterion is optimized on N-best lists using the Downhill Simplex algorithm (Press et al., 2007, p. 503).
