Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours. $$$$$ Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.
Matsuzaki et al (2005) independently introduce a similar approach and present empirical results that rival ours. $$$$$ It is in contrast with our approach where (approximated) posterior probability is optimized.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

These scores are the same as the variational rule scores of Matsuzaki et al (2005). $$$$$ We use to denote the rule probability of rule and to denote the probability with which is generated as a root node.
These scores are the same as the variational rule scores of Matsuzaki et al (2005). $$$$$ The parsing performances were measured using F scores of the parse trees that were obtained by re-ranking of 1000-best parses by a PCFG.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

Matsuzaki et al (2005) introduced a model for such learning $$$$$ This paper defines a generative model of parse trees that we call PCFG with latent annotations (PCFG-LA).
Matsuzaki et al (2005) introduced a model for such learning $$$$$ PCFG-LA is a generative probabilistic model of parse trees.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],. $$$$$ In the definition below, denotes the non-terminal label of the-th node.
Just as Collins manually split the S nonterminal label into S and SG for sentences with and without subjects, Matsuzaki et al (2005) split S into S [1], S [2],. $$$$$ In contrast, our method induces all parameters automatically, except that manually written head-rules are used in binarization.

Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005). $$$$$ The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).
Before extracting the backbone PCFG and running the constrained inside-outside (EM) training algorithm, we preprocessed the Treebank using center-parent binarization Matsuzaki et al (2005). $$$$$ A model created using CENTER-PARENT with was used throughout this experiment.

Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data. $$$$$ We used sections 2 through 20 of the Penn WSJ corpus as training data and section 21 as heldout data.
Matsuzaki et al (2005) used a markovized grammar to get a better unannotated parse forest during decoding, but they did not markovize the training data. $$$$$ We obtained an observable grammar for each model by reading off grammar rules from the binarized training trees.

Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find. $$$$$ Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.
Matsuzaki et al (2005) note that the best annotated parse is in fact NP-hard to find. $$$$$ Because exact inference with a PCFG-LA, i.e., selection of the most probable parse, is NP-hard, we are forced to use some approximation of it.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank. $$$$$ To see the degree of dependency of trained models on initializations, four instances of the same model were trained with different initial values of parameters.3 The model used in this experiment was created by CENTER-PARENT binarization and was set to 16.
'Basic' models are trained on a non-markovized tree bank (as in Matsuzaki et al (2005)); all others are trained on a markovized tree bank. $$$$$ For each binarization method, PCFG-LA models with different numbers of latent annotation symbols, , and , were trained.

With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing. $$$$$ The relationships between the number of parameters in the models and their parsing performances are shown in Figure 7.
With these techniques we reach a parsing accuracy similar to Matsuzaki et al (2005), but with an order of magnitude less parameters, resulting in more efficient parsing. $$$$$ The relationships between the average parse time and parsing performance using the three parsing methods described in Section 3 are shown in Figure 8.

We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc. $$$$$ This model is an extension of PCFG models in which non-terminal symbols are annotated with latent variables.
We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al, 2005), or secondary linksMij (not constrainedby TREE/PTREE) that augment the parse with representations of control, binding, etc. $$$$$ The data points were made by varying configurable parameters of each method, which control the number of candidate parses.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

 $$$$$ The first method simply limits the number of candidate parse trees compared in Eq.
 $$$$$ It suggests that certain types of information used in those lexicalized 5Actually, the number of parses contained in the packed forest is more than 1 million for over half of the test sentences when = and , while the number of parses for which the first method can compute the exact probability in a comparable time (around 4 sec) is only about 300. parsers are hard to be learned by our approach.

These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). $$$$$ The algorithm is a special variant of the inside-outside algorithm of Pereira and Schabes (1992).
These expectations can be easily computed from the inside/outside scores, similarly as in the maximum bracket recall algorithm of Goodman (1996), or in the variational approximation of Matsuzaki et al (2005). $$$$$ Once we have computed and , the parse tree that maximizes is found using a Viterbi algorithm, as in PCFG parsing.

The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions. $$$$$ In this model, an observed parse tree is considered as an incomplete data, and the correplete data) and observed tree (incomplete data). sponding complete data is a tree with latent annotations.
The tree bank data is right-binarized (Matsuzaki et al, 2005) to construct grammars with only unary and binary productions. $$$$$ If node has a right sibling , let be the mother node of.

Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework. $$$$$ Note that models created using different binarization methods have different numbers of parameters for the same .
Later, automated methods for nonterminal refinement were introduced, first splitting all categories equally (Matsuzaki et al, 2005), and later refining nonterminals to different degrees (Petrov et al,2006) in a split-merge EM framework. $$$$$ The different lines for the second and the third methods correspond to different values of .

The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005). $$$$$ Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.
The resulting memory limitations alone can prevent the practical learning of highly split grammars (Matsuzaki et al, 2005). $$$$$ However, both the memory size and the training time are more than linear in , and the training time for the largest ( ) models was about 15 hours for the models created using CENTER-PARENT, CENTER-HEAD, and LEFT and about 20 hours for the model created using RIGHT.
