We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ The toolkit also implements suffix-array grammar extraction and minimum error rate training.
We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ The toolkit also implements suffix-array grammar extraction (Lopez, 2007) and minimum error rate training (Och, 2003).

Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. $$$$$ Joshua

We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al.
We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ In addition to the distributed LM mentioned above, we implement three local n-gram language models.

We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ More details on the MERT method and the implementation can be found in Zaidan (2009).4
We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ More details will be provided in Li et al. (2009b).

Our hybrid machine translation system combines translation output from $$$$$ Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al.
Our hybrid machine translation system combines translation output from $$$$$ In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation.

Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. $$$$$ Joshua

A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. $$$$$ The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a).
A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. $$$$$ More details will be provided in Li et al. (2009b).

However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models $$$$$ In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation.
However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models $$$$$ Hierarchical phrase-based translation requires a translation grammar extracted from a parallel corpus, where grammar rules include associated feature values.

Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). $$$$$ Chart parsing

We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ Moreover, each component can be treated as a stand-alone tool and does not rely on the rest of the toolkit we provide.
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ Grammar formalism

Joshua (Li et al, 2009) $$$$$ More details will be provided in Li et al. (2009b).
Joshua (Li et al, 2009) $$$$$ It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b)

The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ Johsuaâ€™s MERT module optimizes parameter weights so as to maximize performance on a development set as measuered by an automatic evaluation metric, such as Bleu.
The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ More details on the MERT method and the implementation can be found in Zaidan (2009).4

Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. $$$$$ Joshua

The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a).
The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ Specifically, we follow Callison-Burch et al. (2005; Lopez (2007) and use a source language suffix array to extract only those rules which will actually be used in translating a particular set of test sentences.

We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. $$$$$ Joshua

We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ The parsing and pruning algorithms are carefully implemented with dynamic programming strategies, and efficient data structures are used to minimize overhead.
We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ More details will be provided in Li et al. (2009b).


We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). $$$$$ Joshua

We use Joshua, a Java-based open source implementation of the hierarchical decoder (Li et al, 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrase model.perl script packed with Moses2 (Koehn et al., 2007). $$$$$ Joshua

In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ Grammar formalism
