Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ Inducing Probabilistic CCG Grammars from Logical Form with Higher-Order Unification
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.

Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ The approach uses higher-order unification to define the space of possible grammars in a language- and representation-independent manner, paired with an algorithm that learns a probabilistic parsing model.

Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ For example, Kate & Mooney (2006) present a method (KRISP) that extends an existing SVM learning algorithm to recover logical representations.

Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Figure 2 presents the unification-based learning algorithm, UBL.

The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ These parses closely resemble the types of analyses that will be possible under the grammars we learn in the experiments described in Section 8.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ Discussion The alternation between refining the lexicon and updating the parameters drives the learning process.

 $$$$$ The sum over parses in Eq.
 $$$$$ Zettlemoyer was supported by a US NSF International Research Fellowship.

An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ For future work, we are interested in exploring the generality of the approach while extending it to new understanding problems.

For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).

WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.

As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 1996, 2000).
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)

We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir Meaning: answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008).

 $$$$$ The sum over parses in Eq.
 $$$$$ Zettlemoyer was supported by a US NSF International Research Fellowship.

In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ Section 5 describes how we estimate the parameters of a probabilistic parsing model and how this parsing model can be used to guide the selection of items to add to the lexicon.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.

This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). $$$$$ The new syntactic category for g is determined based on its type, T(g).
This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). $$$$$ For example, there is no distinct syntactic category N for nouns (which have semantic type he, ti).

Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ For this representation, lexical items such as: can be used to construct the desired output.

For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ These algorithms are all language independent but representation specific.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.

Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1.

Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ Figure 2 presents the unification-based learning algorithm, UBL.

We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. $$$$$ Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).
We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. $$$$$ Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.

The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.
