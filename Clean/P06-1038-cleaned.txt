 $$$$$ Again, that paper uses syntactic information.
 $$$$$ We are planning an evaluation according to this measure after improving the merge stage.

In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. $$$$$ Instead of running the algorithm of this section on the whole corpus, we divide the corpus into windows of equal size (see Section 5 for size determination) and perform the category discovery algorithm of this section on each window independently.
In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. $$$$$ The table below gives some statistics.

To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). $$$$$ We now use them in order to discover categories.
To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). $$$$$ We have presented a novel method for patternbased discovery of lexical semantic categories.

It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). $$$$$ Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.
It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). $$$$$ Our algorithm clearly outperforms k-means, which outperforms random.

Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words
Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ In this paper we require that meta-patterns obey the following constraints: (1) at most 4 words; (2) exactly two content words; (3) no two consecutive CWs.

Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. $$$$$ Now define a meta-pattern as any sequence of HFWs and CWs.
Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. $$$$$ Additional patterns include ‘from x to y’, ‘x and/or y’ (punctuation is treated here as white space), ‘x and a y’, and ‘neither x nor y’.

To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). $$$$$ Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words
To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). $$$$$ In this paper we require that meta-patterns obey the following constraints: (1) at most 4 words; (2) exactly two content words; (3) no two consecutive CWs.

We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. $$$$$ Our approach is based on patterns, and utilizes the following stages: We performed a thorough evaluation on two English corpora (the BNC and a 68GB web corpus) and on a 33GB Russian corpus, and a sanity-check test on smaller Danish, Irish and Portuguese corpora.
We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. $$$$$ Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes those categories in order to automatically discover additional patterns.

Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ Efficient Unsupervised Discovery Of Word Categories Using Symmetric Patterns And High Frequency Words
Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.

Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.
Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ It is the first pattern-based lexical acquisition method that is fully unsupervised, requiring no corpus annotation or manually provided patterns or words.

 $$$$$ Again, that paper uses syntactic information.
 $$$$$ We are planning an evaluation according to this measure after improving the merge stage.

Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ We will eventually combine data from several patterns and from different corpus windows (Section 4.)
Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ In the automatic part, we followed as closely as possible the methodology and data used in previous work, so that meaningful comparisons could be made.

The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. $$$$$ Our approach is based on patterns, and utilizes the following stages: We performed a thorough evaluation on two English corpora (the BNC and a 68GB web corpus) and on a 33GB Russian corpus, and a sanity-check test on smaller Danish, Irish and Portuguese corpora.
The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. $$$$$ The Russian corpus was assembled from many web sites and carefully filtered for duplicates, to yield 33GB and 4G words.

All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ We used 5%.)
All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ The (uncorrected) precision is the only metric given in (Widdows and Dorow, 2002), who reported 82% (for the BNC.)

We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ We performed in-depth evaluation on two languages, English and Russian, using three corpora, two for English and one for Russian.
We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small.

We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ We use the three measures as follows.
We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ The evaluation method is as follows.

It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ A lexical category is a set of words that share a significant aspect of their meaning, e.g., sets of words denoting vehicles, types of food, tool names, etc.
It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ A word can obviously belong to more than a single category.

Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.
Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ Note that a pair of nodes connected by a symmetric arc can appear in more than a single category.

For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999).
For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ Results are given in Table 1.

Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs. $$$$$ Now define a meta-pattern as any sequence of HFWs and CWs.
Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs. $$$$$ Additional patterns include ‘from x to y’, ‘x and/or y’ (punctuation is treated here as white space), ‘x and a y’, and ‘neither x nor y’.
