For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ Standard context-free parsing algorithms are inefficient in such a case.
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ Among items of the same width, those of type .L should be considered last.

We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ The reader is assumed to be familiar with context-free grammars.

While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ We omit the formal proof that G and H admit isomorphic derivations and hence generate the same languages, observing only that if (x, y) = (bib2 • • • bi,b3+1- • • bk) E L(Ha)— a condition used in defining La above—then A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk], for any A, B1, .
While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ In general, G has p = 0(IVD13) = 0(t3).

However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). $$$$$ For a bilexical grammar, the worst case is IPI = VD I 3 ' I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• We observe that in practical applications we always have n < IVTI• Let us then restrict our analysis to the (infinite) set of input instances of the parsing problem that satisfy relation n < WTI.
However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). $$$$$ The algorithm makes a single pass through the possible items, setting the bit for each if it can be derived using any rule in (b) from items whose bits are already set.

The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). $$$$$ But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.
The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). $$$$$ 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).

While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. $$$$$ With this assumption, the asymptotic time complexity of the CKY algorithm becomes 0(n5 • IVD13).
While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. $$$$$ 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).

Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ We use dynamic programming to assemble such subderivations into a full parse.
Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.

The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. $$$$$ Formally, a flip state is one that allows entry on a —> transition and that either allows exit on a transition or is a final state.
The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. $$$$$ A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.

In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ Standard context-free parsing algorithms are inefficient in such a case.
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2).

Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ We are concerned here with head automaton grammars H such that every Ha is split.
Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.

On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.
On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.

First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. $$$$$ We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.
First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. $$$$$ For a practical speedup, add h\j as an antecedent to the MID rule (and fill in the parse table from right to left).

The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ Efficient Parsing For Bilexical Context-Free Grammars And Head Automaton Grammars
The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ 7 Head automaton grammars in time 0(n4) In this section we show that a length-n string generated by a head automaton grammar (Alshawi, 1996) can be parsed in time 0(n4).

In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997).

It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.
It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ Standard context-free parsing algorithms are inefficient in such a case.

Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.
Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ Our dynamic programming techniques for cheaply attaching head information to derivations can also be exploited in parsing formalisms other than rewriting systems.

In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3).
In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.

The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). $$$$$ We are concerned here with head automaton grammars H such that every Ha is split.
The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). $$$$$ A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.

As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. $$$$$ We use dynamic programming to assemble such subderivations into a full parse.
As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. $$$$$ This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.

The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.
The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.
