A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ Two Statistical Parsing Models Applied To The Chinese Treebank
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.

Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).

The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993).
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ In order to put the new Chinese Treebank results into context with the unmodified (English) parsing models, we present results on two test sets from the Wall Street Journal: WSJ-all, which is the complete Section 23 (the de facto standard test set for English parsing), and WSJ-small, which is the first 400 sentences of Section 23 and which is roughly comparable in size to the Chinese test set.

The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank.
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ *3 of the 400 sentences were not parsed due to timeouts and/or pruning problems. t3 of the 348 sentences did not get parsed due to pruning problems, and 2 other sentences had length mismatches (scoring program errors). tight limit was changed to e-9.

We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form where P, L, Ri and H are all lexicalized nonterminals, and P inherits its lexical head from its distinguished head child, H. In this generative model, first P is generated, then its head-child H, then each of the left- and right-modifying nonterminals are generated from the head outward.
We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ See (Chiang, 2000) for more details.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ For brevity, we omit the smoothing details of BBN's model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997).

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ Two Statistical Parsing Models Applied To The Chinese Treebank
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).

 $$$$$ The BBN model is also of the lexicalized PCFG variety.
 $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.

This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).

Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ The BBN model is also of the lexicalized PCFG variety.

 $$$$$ The BBN model is also of the lexicalized PCFG variety.
 $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.

Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).
Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).

Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ Two Statistical Parsing Models Applied To The Chinese Treebank
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.

 $$$$$ The BBN model is also of the lexicalized PCFG variety.
 $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.

The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ See (Chiang, 2000) for more details.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree.

 $$$$$ The BBN model is also of the lexicalized PCFG variety.
 $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.
