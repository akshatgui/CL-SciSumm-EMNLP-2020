For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ cdec: A Decoder Alignment and Learning Framework for Finite-State and Context-Free Translation Models
For translation experiments, we used cdec (Dyer et al, 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).

Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).
Our implementation is mostly in Python on top of the cdec system (Dyer et al, 2010) via the pycdec interface (Chahuneau et al, 2012). $$$$$ 2010.

The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
The weights of the log-linear translation models were tuned towards the BLEU metric on development data using cdec's (Dyer et al, 2010) implementation of MERT (Och, 2003). $$$$$ In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar.

We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).
We implement Linear CP (LCP) on top of Cdec (Dyer et al, 2010), a widely-used hierarchical MT system that includes implementations of standard CP and FCP algorithms. $$$$$ 2010.

In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.

Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009).
Finally, the cdec decoder (Dyer et al, 2010) includes a grammar extractor that performs well only when all rules can be held in memory. $$$$$ 2010.

We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.
We use the cdec decoder (Dyer et al, 2010) with default settings for this purpose. $$$$$ 2010.

Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al, 2010). $$$$$ A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007).

 $$$$$ The rescoring models need not be explicitly represented as FSTs—the state space can be inferred.
 $$$$$ Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.

We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.
We implemented UD on top of a widely-used HMT open-source system, cdec (Dyer et al, 2010). $$$$$ 2010.

We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ 2010.

 $$$$$ The rescoring models need not be explicitly represented as FSTs—the state space can be inferred.
 $$$$$ Discussions with Philipp Koehn, Chris Callison-Burch, Zhifei Li, Lane Schwarz, and Jimmy Lin were likewise crucial to the successful execution of this project.

In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
In our work, we use hierarchical phrase-based translation (Chiang, 2007), as implemented in the cdec framework (Dyer et al, 2010). $$$$$ 2010.

The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm.
The work reported in this paper was carried out while the author was at the University of Cambridge.It has been noted that line optimisation over a lattice can be implemented as a semi-ring of sets of linear functions (Dyer et al, 2010). $$$$$ 2010.

Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
Grammars were extracted from the resulting parallel text and used in our hierarchical phrase-based system using cdec (Dyer et al, 2010) as the decoder. $$$$$ A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007).

We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hypergraphs, and can be computed using either INSIDEOUTSIDE or the expectation semiring and INSIDE.
We have evaluated the one-translation-per-discourse feature using the cdecMT system (Dyer et al, 2010). $$$$$ 2010.

An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ 2010.
An efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010). $$$$$ We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce).

We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
We used cdec (Dyer et al, 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al, 2002) on the NIST MT06 corpus. $$$$$ Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).

In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models.
In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al, 2006) ,cdec for decoding (Dyer et al, 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. $$$$$ Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006).

To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007).
To date, several open-source SMT systems (based on either phrase based models or syntax-based models) have been developed, such as Moses (Koehn et al, 2007), Joshua (Li et al, 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al, 2010), cdec (Dyer et al, 2010), Jane (Vilar et al, 2010) and SilkRoad, and offer good references for the development of the NiuTrans toolkit. $$$$$ We introduce a software package called cdec that manipulates both classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec.
