 $$$$$ (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.
 $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.

Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Second, their models consider only the direction (i.e., orientation) and not the relative jump.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.

Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ This new model leads to significant improvements in MT quality as measured by BLEU (Papineni et al., 2002).
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ The final translation is a hypothesis that covers all source words and has the minimum cost among all possible 9 hypotheses that cover all source words.

This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ The above distortion costs are used in conjunction with other cost components used in our decoder.

 $$$$$ (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.
 $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.

Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ We will also show how to generalize this word distortion model to a phrase-based model.
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ So, when translating into English, one needs to move the verb after the subject, which is often a long compounded phrase.

As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Hence, reordering is better handled during the search algorithm and as part of the optimization function.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.

Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ Their approach show a statistically-significant improvement over a phrase-based monotone decoder.
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004).

As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ There is a fundamental difference between decoding for machine translation and decoding for speech recognition.
As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ Rewriting the input sentence whether using syntactic rules or heuristics makes hard decisions that can not be undone by the decoder.

As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Hence, reordering is better handled during the search algorithm and as part of the optimization function.
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.

We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ The first parameter s denotes the number of source words that are temporarily skipped (i.e., temporarily left uncovered) during the search to cover a source word to the right of the skipped words.

Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ The language model used is an interpolated trigram model described in (Bahl et al., 1983).

The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004).
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ We presented a new distortion model that can be integrated with existing phrase-based SMT decoders.

This assumption is realistic $$$$$ Distortion Models For Statistical Machine Translation
This assumption is realistic $$$$$ In Section 5, we present some empirical results that show the utility of our distortion model for statistical machine translation systems.

The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ Chinese-English.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ Our metric shows that Chinese-English have a closer word order than Arabic-English.

To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ In Section 4, we present our proposed distortion model.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ Our proposed distortion model addresses this weakness of the n-gram language model.

 $$$$$ (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.
 $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.

The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ The second parameter is the window width w, which is defined as the distance (in number of source words) between the left-most uncovered source word and the right-most covered source word.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ Table 5 shows BLEU scores for our SMT decoder with different parameter settings for skip s, window width w, with and without our distortion model.

 $$$$$ (Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.
 $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.

In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ (Och et al., 2004; Tillman, 2004) propose orientation-based distortion models lexicalized on the phrase level.
In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ Second, their models consider only the direction (i.e., orientation) and not the relative jump.
