Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.
Next, it looks promising to try to estimate the dictionary word frequencies using a search engine instead of text corpus, as proposed by Lapata and Keller (2004). $$$$$ All search terms were submitted to the search engine in lower case.

Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). $$$$$ Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.
Lapata and Keller (2004) uses the number of page hits as the web-count of the queried n gram (which is problematic according to Kilgarriff (2007)). $$$$$ In these cases, we set the web count to a large constant (108).

While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. $$$$$ Following Keller and Lapata (2003), web counts for ngrams were obtained using a simple heuristic based on queries to the search engine Altavista.1 In this approach, the web count for a given n-gram is simply the number of hits (pages) returned by the search engine for the queries generated for this n-gram.
While it is possible to exploit search engine queries for various NLP tasks (Lapata and Keller, 2004), for applications which use corpora as unsupervised training material downloadable base data is essential. $$$$$ All search terms were submitted to the search engine in lower case.

This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.
This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). $$$$$ The reason for this seems to be that the web is much larger than the BNC (about 1000 times); the size seems to compensate for the fact that simple heuristics were used to obtain web counts, and for the noise inherent in web data.

Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ Co-occurrence frequencies were estimated from the web using inflected queries (see Section 2).
Lapata and Keller (2004) first used web-based co-occurrence counts for the bracketing of NCs. $$$$$ This result is consistent with Keller and Lapata’s (2003) findings that the web yields better counts than the BNC.

Aside from counting bigrams, various tasks are attainable using web based models $$$$$ Then we investigate the generality of the web-based approach by applying it to a range of analysis and generations tasks, involving both syntactic and semantic knowledge

Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). $$$$$ We argue that web-based models should therefore be used as a baseline for, rather than an alternative to, standard models.
Therefore, it can be used easily as a baseline, as suggested by (Lapata and Keller, 2004). $$$$$ Rather, in our opinion, web-based models should be used as a new baseline for NLP tasks.

The results are compared against two state of the art approaches $$$$$ Our results were less encouraging when it comes to comparisons with state-of-the-art models.
The results are compared against two state of the art approaches $$$$$ We found that in all but one case, web-based models fail to significantly outperform the state of the art.

More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ The present paper investigates if these results generalize to tasks covering both syntax and semantics, both generation and analysis, and a larger of For the majority of tasks, we find that simple, unsupervised models perform when frequencies are obtained from the web rather than from a large corpus.
More recently, (Lapata and Keller, 2004) showed that simple unsupervised models perform significantly better when the frequencies are obtained from the web, rather than from a large standard corpus. $$$$$ For all but two tasks (candidate selection for MT and noun countability detection) we found that simple, unsupervised models perform significantly better when ngram frequencies are obtained from the web rather than from a standard large corpus.

We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models $$$$$ We compare this model both against a baseline (same model, but parameters estimated on the BNC) and against state-of-the-art models from the literature, which are either supervised (i.e., use annotated training data) or unsupervised but rely on taxonomies to recreate missing counts.
We have experimented with the support vector machines (SVM) model and compared the results against two state-of-the-art models $$$$$ We found that in all but one case, web-based models fail to significantly outperform the state of the art.

(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ Keller and Lapata (2003) investigated the validity of web counts for a range of predicate-argument bigrams (verbobject, adjective-noun, and noun-noun bigrams).
(Lapata and Keller, 2004)'s web-based unsupervised model classifies noun noun instances based on Lauer's list of 8 prepositions and uses the web as training corpus. $$$$$ Lauer uses eight prepositions for the paraphrasing task (of, for, in, at, on, from, with, about).

Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ We also found that there was no significant difference between the best Altavista model and the best model reported by Malouf, a supervised method using positional probability estimates from the BNC and morphological variants.
Although (Lapata and Keller, 2004) used Altavista in their experiments, they showed there is almost no difference between the correlations achieved using Google and Altavista counts. $$$$$ We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.

They then later propose using Web counts as a baseline unsupervised method for many NLP tasks (Lapata and Keller, 2004). $$$$$ The Web As A Baseline

Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ The conceptbased model (see (7)) achieved an accuracy of 28% on this test set, whereas its lexicalized version reached an accuracy of 40% (see Table 11).
Lapata and Keller (2004) achieved their best accuracy (78.68%) with the dependency model and the simple symmetric score #(wi ,wj). $$$$$ On the BNC, the simple unigram model performs best.

This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ He uses a probability ratio to compare the probability of the leftbranching analysis to that of the right-branching (see (4) for the dependency model and (5) for the adjacency model).
This is confirmed by the adjacency model experiments in (Lapata and Keller, 2004) on Lauer's NC set. $$$$$ Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier’s encyclopedia, a corpus of 8 million words.

Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. $$$$$ We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.
Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer's using simple lexical models. $$$$$ We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.

Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ Table 3 compares the web-based models against the BNC models.
Table 3 compares our results to those of Lauer (1995) and of Lapata and Keller (2004). $$$$$ We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.

We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004). $$$$$ Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus.
We have extended and improved upon the state-of-the-art approaches to NC bracketing using an unsupervised method that is more robust than Lauer (1995) and more accurate than Lapata and Keller (2004). $$$$$ We replicated Lauer’s (1995) results for compound noun bracketing using the same test set.

 $$$$$ This choice is typically modeled by confusion sets such as {principal, principle} or {then, than} under the assumption that each word in the set could be mistakenly typed when another word in the set was intended.
 $$$$$ We are grateful to Tim Baldwin, Silviu Cucerzan, Mark Lauer, Rob Malouf, Detelef Prescher, and Adwait Ratnaparkhi for making their data sets available.

The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). $$$$$ He observes that this approach suffers from an acute data sparseness problem and goes on to obtain counts for candidate compounds through web searches, thus achieving a translation accuracy of 86–87%.
The vast size of the Web has been demonstrated to combat the data sparseness problem, for example, in Lapata and Keller (2004). $$$$$ We concentrated solely on countable and uncountable nouns, as they account for the vast majority of the data.
