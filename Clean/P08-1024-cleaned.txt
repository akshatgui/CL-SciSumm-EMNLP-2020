Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ A Discriminative Latent Variable Model for Statistical Machine Translation
Coming up-to-date, (Blunsom et al, 2008) attempt a related estimation problem to (Marcu and Wong, 2002), using the expanded phrase pair set of (Chiang, 2005a), working with an exponential model and concentrating on marginalizing out the latent segmentation variable. $$$$$ For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007).

 $$$$$ The use of richer, more linguistic grammars (e.g., Galley et al. (2004)) may also improve the system.
 $$$$$ The authors acknowledge the support of the EPSRC (Blunsom & Osborne, grant EP/D074959/1; Cohn, grant GR/T04557/01).

Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.
Secondly, as (Blunsom et al, 2008) show, marginalizing out the different segmentations during decoding leads to improved performance. $$$$$ Translation comparison Having demonstrated that accounting for derivational ambiguity leads to improvements for our discriminative model, we now place the performance of our system in the context of the standard approach to hierarchical translation.

The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ This algorithm is similar to the methods for decoding with a SCFG intersected with an n-gram language model, which require language model contexts to be stored in each chart cell.
The version presented in (Blunsom et al., 2008) scales to more than a hundred thousand short training sentences, but does not integrate a language model and thus has performance that improves upon Hiero without a language model only. $$$$$ Additionally we show the scores achieved by MERT training the full set of features for Hiero, with and without a language model.8 We provide these results for reference.

In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler.
In practice, this problem can be circumvented by discarding the training sentence pairs with unreachable reference translations, but this may mean a significant reduction in the amount of training data (24% in (Blunsom et al., 2008)). $$$$$ Instead we discard the unreachable portion of the training sample (24% in our experiments).

Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ Again, we use the inside-outside algorithm to find the ‘reference’ feature expectations from this chart.
Instead of using expected BLEU as a training objective, Blunsom et al (2008) trained their model to directly maximise the log-likelihood of the discriminative model, estimating feature expectations from a packed chart. $$$$$ This is encouraging as our model was trained to optimise likelihood rather than BLEU, yet it is still competitive on that metric.

This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation.
This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al, 2008). $$$$$ Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)).

In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ A Discriminative Latent Variable Model for Statistical Machine Translation
In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al, 2008). $$$$$ Statistical machine translation (SMT) has seen a resurgence in popularity in recent years, with progress being driven by a move to phrase-based and syntax-inspired approaches.

We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007).
We use the forest to train a log-linear model with a latent variable as describe in Blunsom et al (2008). $$$$$ This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007).

Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ A Discriminative Latent Variable Model for Statistical Machine Translation
Of ten, there are many derivations that are distinct yet produce the same translation. Blunsom et al (2008) present a latent variable model that describes the relationship between translation and derivation clearly. $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.

(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.
(6) as max-derivation decoding, which are first termed by Blunsom et al (2008). $$$$$ To do so would require integrating a language model feature into the max-translation decoding algorithm.

Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ Even with a very tight beam of 100, max-translation decoding outperforms maximum-derivation decoding, and performance is increasing even at a width of 10k.
Blunsom et al (2008) first distinguish between max-derivation decoding and max-translation decoding explicitly. $$$$$ To do so would require integrating a language model feature into the max-translation decoding algorithm.

Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ (Koehn et al., 2003).
Both Mi et al (2008) and Blunsom et al (2008) use a translation hyper graph to represent search space. $$$$$ Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.

For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ The model can learn from many of these derivations and thereby learn from all these translation fragments.
For the first model, which includes the sparse parse features, we learn weights in order to optimize penalized conditional log likelihood (Blunsom et al, 2008). $$$$$ The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f, is given by: where Hk(d, e, f) = rEd Here k ranges over the model’s features, and Λ = {Ak} are the model parameters (weights for their corresponding features).

See Blunsom et al (2008) for more information. $$$$$ (Koehn et al., 2003).
See Blunsom et al (2008) for more information. $$$$$ To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006).

We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ In decoding we can search for the maximum probability derivation, which is the standard practice in SMT, or for the maximum probability translation which is what we actually want from our model, i.e. the best translation.
We might also want to calculate the total probability of all possible derivations, which is useful for parameter estimation (Blunsom et al, 2008). $$$$$ As well as both modelling the same distribution, when our model is trained with a single parameter per-rule these systems have the same parameter space, differing only in the manner of estimation.

Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ A Discriminative Latent Variable Model for Statistical Machine Translation
Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al, 2008). $$$$$ We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions.

For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
For the hierarchical phrase-based approach, (Blunsom et al, 2008) present a discriminative rule model and show the difference between using only the viterbi alignment in training and using the full sum over all possible derivations. $$$$$ The distribution is globally normalised by the partition function, ZA(f), which sums out the numerator in (1) for every derivation (and therefore every translation) of f: Given (1), the conditional probability of a target translation given the source is the sum over all of its derivations: where O(e, f) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm.

The first method maximizes data likelihood as is standard in EM, while the second method maximizes conditional likelihood for a log linear model following Blunsom et al (2008). $$$$$ This model maximises the conditional likelihood of the data, p(e

Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)).
Note that the marginalization for a particular y would be tractable; it is used at training time in certain training objective functions, e.g., maximizing the conditional likelihood of a reference translation (Blunsom et al, 2008). $$$$$ The feature functions Hk are predefined real-valued functions over the source and target sentences, and can include overlapping and non-independent features of the data.
