However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ To achieve this effect, we increment the value of N by one after each bootstrapping iteration.
However, (Thelen and Riloff, 2002) did not focus on the issue of convergence, and on leveraging negative categories to achieve or improve convergence. $$$$$ Riloff and Jones acknowledged this issue and used a second level of bootstrapping (the “Meta” bootstrapping level) to alleviate this problem.

Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ Semantic class information has proven to be useful for many natural language processing tasks, including information extraction (Riloff and Schmelzenbach, 1998; Soderland et al., 1995), anaphora resolution (Aone and Bennett, 1996), question answering (Moldovan et al., 1999; Hirschman et al., 1999), and prepositional phrase attachment (Brill and Resnik, 1994).
Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al, 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ (2) If a word is hypothesized for both category A and category B during the same iteration, then it to the “one sense per discourse” observation (Gale et al., 1992)) that a word belongs to a single semantic category within a limited domain.

(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm.
(Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. $$$$$ Finally, we present results showing that learning multiple semantic categories simultaneously improves performance.

Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ Semantic class information has proven to be useful for many natural language processing tasks, including information extraction (Riloff and Schmelzenbach, 1998; Soderland et al., 1995), anaphora resolution (Aone and Bennett, 1996), question answering (Moldovan et al., 1999; Hirschman et al., 1999), and prepositional phrase attachment (Brill and Resnik, 1994).
Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al (2003), and Etzioni et al (2005), among others. $$$$$ The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.

Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ Semantic class information has proven to be useful for many natural language processing tasks, including information extraction (Riloff and Schmelzenbach, 1998; Soderland et al., 1995), anaphora resolution (Aone and Bennett, 1996), question answering (Moldovan et al., 1999; Hirschman et al., 1999), and prepositional phrase attachment (Brill and Resnik, 1994).
Over the years, researchers have successfully shown how to build ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al, 2007), and concept lists (Katz et al, 2003). $$$$$ (2) If a word is hypothesized for both category A and category B during the same iteration, then it to the “one sense per discourse” observation (Gale et al., 1992)) that a word belongs to a single semantic category within a limited domain.

Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ The patterns are then applied to the corpus and all of their extracted noun phrases are recorded.
Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. $$$$$ The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.

Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ Several learning algorithms have also been developed for named entity recognition (e.g., (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)).
Similar approaches are used among others in (Thelen and Riloff, 2002) for learning semantic lexicons, in (Collins and Singer, 1999) for named entity recognition, and in (Fagni and Sebastiani, 2007) for hierarchical text categorization. $$$$$ (Collins and Singer, 1999) used contextual information of a different sort than we do.

Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts
Thelen and Riloff (2002)'s bootstrapping method iteratively performs feature selection and word selection for each class. $$$$$ This hypothesis makes sense only if a word cannot belong to more than one semantic class.

In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts
In Section 5, we report experiments using syntactic features shown to be useful by the above studies, and compare performance with Thelen and Riloff (2002)'s bootstrapping method. $$$$$ So we implemented the meta-bootstrapping algorithm ourselves to directly compare its performance with that of Basilisk.

We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. $$$$$ A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts
We use the following algorithms as baseline: EM, co training, and co-EM, as established techniques for learning from unlabeled data in general; the bootstrapping method proposed by Thelen and Riloff (2002) (hereafter, TRB and TR) as a state-of-the-art bootstrapping method designed for semantic lexicon construction. $$$$$ Figure 2 shows the bootstrapping process that follows, which we explain in the following sections.

The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ The input to Basilisk is a text corpus and a set of seed words.
The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. $$$$$ While meta-bootstrapping trusts individual extraction patterns to make unilateral decisions, Basilisk gathers collective evidence from a large set of extraction patterns.

Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ These seed words form the initial semantic lexicon.
Pattern generalization is disabled, and the ranking of patterns and facts follows strictly the criteria and scoring functions from (Thelen and Riloff, 2002), which are also used in slightly different form in (Lita and Carbonell, 2004) and (Agichtein and Gravano,2000). $$$$$ All extraction patterns are used during this step, not just the patterns in the pattern pool.

As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ The input to Basilisk is a text corpus and a set of seed words.
As a more realistic compromise over overly-cautious acquisition, the baseline run retains as many of the top candidate facts as the size of the current seed, whereas (Thelen and Riloff, 2002) only add the top five candidate facts to the seed set after each iteration. $$$$$ The next step is to score the candidate words.

Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ None of these previous algorithms used extraction patterns or similar contexts to infer semantic class associations.
Multi-category algorithms out perform MLB (Thelen and Riloff, 2002), and we focus on these algorithms in our experiments. $$$$$ For both algorithms, the conflict resolution procedure works as follows.

In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ We modified Basilisk’s scoring function to prefer words that have strong evidence for one category but little or no evidence for competing categories.
In BASILISK (Thelen and Riloff, 2002), candidate terms are ranked highly if they have strong evidence for a category and little or no evidence for other categories. $$$$$ A word is ranked highly only if it has a high score for the targeted category and there is little evidence that it belongs to a different category.

To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ We generated seed words by sorting the words in the corpus by frequency and manually identifying the 10 most frequent nouns that belong to each category.
To improve the seeds, the frequency of the potential seeds in the corpora is often considered, on the assumption that highly frequent seeds are better (Thelen and Riloff, 2002). $$$$$ Only these nouns are considered for addition to the lexicon.

This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ A Bootstrapping Method For Learning Semantic Lexicons Using Extraction Pattern Contexts
This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). $$$$$ The first step in the bootstrapping process is to score the extraction patterns based on their tendency to extract known category members.

Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm.
Seed-based supervision is closely related to the idea of seeding in the bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). $$$$$ The algorithm most closely related to Basilisk is meta-bootstrapping (Riloff and Jones, 1999), which also uses extraction pattern contexts for semantic lexicon induction.

Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ Basilisk (Bootstrapping Approach to SemantIc Lexicon Induction using Semantic Knowledge) is a weakly supervised bootstrapping algorithm that automatically generates semantic lexicons.
Thelen and Riloff (2002) use a bootstrapping algorithm to learn semantic lexicons of nouns for six semantic categories, one of which is EVENTS. $$$$$ We used Basilisk to learn semantic lexicons for six semantic categories: BUILDING, EVENT, HUMAN, LOCATION, TIME, and WEAPON.

Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Third, we explore the idea of learning multiple semantic categories simultaneously by adding this capability to Basilisk as well as another bootstrapping algorithm.
Multi-category bootstrapping algorithms, such as Basilisk (Thelen and Riloff, 2002), NOMEN (Yangarber et al, 2002), and WMEB (McIntosh and Curran, 2008), aim to reduce semantic drift by extracting multiple semantic categories simultaneously. $$$$$ Basilisk’s bootstrapping algorithm exploits two ideas: (1) collective evidence from extraction patterns can be used to infer semantic category associations, and (2) learning multiple semantic categories simultaneously can help constrain the bootstrapping process.
