More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.

Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Multi-Prototype Vector-Space Models of Word Meaning
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Cluster similarity metrics: Besides AvgSim and MaxSim, there are many similarity metrics over mixture models, e.g.

It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ The results demonstrate the superiority of a clustered approach over both traditional prototype and exemplar-based vector-space models.
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ Figure 1 gives an overview of this process.

As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ All results reported in this paper use cosine similarity, 1 We compare across two different feature functions tf-idf weighting and X2 weighting, chosen due to their ubiquity in the literature (Agirre et al., 2009; Curran, 2004).
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ The exemplar approach yields significantly higher correlation than the single prototype approach in all cases except Gigaword with tf-idf features (p < 0.05).

In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ Multi-Prototype Vector-Space Models of Word Meaning
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ Also, once again, the performance of the multi-prototype approach is better for homonyms than polysemes.

Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ The set of vectors for a word is determined by unsupervised word sense discovery (WSD) (Sch¨utze, 1998), which clusters the contexts in which a word appears.
Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.

Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Multi-Prototype Vector-Space Models of Word Meaning
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.

Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ Figure 2 plots Spearman’s p on WordSim-353 against the number of clusters (K) for Wikipedia and Gigaword corpora, using pruned tf-idf and k2 features.2 In general pruned tf-idf features yield higher correlation than k2 features.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ However we have not yet evaluated its performance when using more powerful feature representations such those based on Latent or Explicit Semantic Analysis (Deerwester et al., 1990; Gabrilovich and Markovitch, 2007).

Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ Furthermore, it performs significantly worse 2(Feature pruning) We find that results using tf-idf features are extremely sensitive to feature pruning while x2 features are more robust.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ Squares indicate performance when combining across clusterings. than combined multi-prototype for tf-idf features, and does not differ significantly for x2 features.

We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ Based on preliminary experiments comparing various clustering methods, we found movMF gave the best results.
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ However, given the right number of clusters, it also produces better results for polysemous words.

Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Multi-Prototype Vector-Space Models of Word Meaning
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ This paper shows how they can be combined to create an improved vector-space model of lexical semantics.

The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Such models have been widely studied in the Psychology literature (Griffiths et al., 2007; Love et al., 2004; Rosseel, 2002).
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ The Usage Similarity (USim) data set collected in Erk et al. (2009) provides such similarity scores from human raters.

We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.

A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ Occurrences are clustered and cluster centroids are used as prototype vectors.
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).

The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ This approach is commonly employed in unsupervised word sense discovery; however, we do not assume that clusters correspond to traditional word senses.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ They are grouped into homonyms (words with very distinct senses) and polysemes (words with related senses).

Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ We next evaluated the multi-prototype approach on its ability to determine the most closely related words for a given target word (using the Wikipedia corpus with tf-idf features).
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ This notion is evaluated empirically by computing the correlation between the predicted similarity using the contextual multi-prototype method and human similarity judgements for different usages of the same word.

Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ First, a word’s contexts are clustered to produce groups of similar context vectors.
Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ Occurrences are clustered and cluster centroids are used as prototype vectors.

 $$$$$ However, movMF introduces an additional per-cluster concentration parameter controlling its semantic breadth, allowing it to more accurately model non-uniformities in the distribution of cluster sizes.
 $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.

Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ In previous work, vector-space lexical similarity and word sense discovery have been treated as two separate tasks.
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ Clustering more accurately identifies homonyms’ clearly distinct senses and produces prototypes that better capture the different uses of these words.
