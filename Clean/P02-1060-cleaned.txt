 $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
 $$$$$ In the near feature, we would like to incorporate the following into our system:

Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ Another important question is about the effect of different sub-features.
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.

However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.

Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ This kind of internal sub-feature has been widely used in machine-learning systems, such as BBN's IdendiFinder and New York Univ.

The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ These systems are mainly rule-based.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ 2) 2 f is very useful for NER and increases the performance further by 10% to 87.4%.

Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ Named Entity Recognition Using An HMM-Based Chunk Tagger
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].

For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.
For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.

The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ Named Entity Recognition Using An HMM-Based Chunk Tagger
The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.

In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ Named Entity Recognition Using An HMM-Based Chunk Tagger
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ Ideally, it can be estimated by using the forward-backward algorithm [Rabiner89] recursively for the 1st-order [Rabiner89] or 2nd -order HMMs [Watson+92].

Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Named Entity Recognition Using An HMM-Based Chunk Tagger
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.

This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above.

Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ More generally how does the performance vary as the training data size changes?
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.

In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.

Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.

An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (

Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence ti−1ti is valid / invalid while &quot;valid on&quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .

Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.

The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence.
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ The result is shown in Figure 2 for MUC-7 NE task.

In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ Initially, we also consider part-of-speech (POS) sub-feature.
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ Therefore, POS is discarded.

Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ During decoding, the NEs already recognized from the document are stored in a list.
