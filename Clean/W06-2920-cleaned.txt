The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). $$$$$ One of the reasons is the lack of a de-facto standard for an evaluation metric (labeled or unlabeled, separate root accuracy?
The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). $$$$$ The predicted values were compared to the gold standard HEAD and DEPREL.6 The official evaluation metric is the labeled attachment score (LAS), i.e. the percentage of “scoring” tokens for which the system has predicted the correct HEAD and DEPREL.

For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ This approach assumes projective dependency structures.
For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ (2006).

Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ Nivre’s parser has been tested for Swedish (Nivre et al., 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al. (2005), while McDonald’s parser has been applied to English (McDonald et al., 2005a), Czech (McDonald et al., 2005b) and, very recently, Danish (McDonald and Pereira, 2006).
Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ Even though McDonald et al. (2006) and Nivre et al.

19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ “ROOT”) for tokens with HEAD=0); percentage of scoring tokens with HEAD=0, a head that precedes or a head that follows the token (this nicely shows which languages are predominantly head-initial or head-final); the average number of scoring tokens with HEAD=0 per parse tree unit; the percentage of (scoring and non-scoring) non-projective relations and of parse tree units with at least one non-projective relation.
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ All data sets except the Chinese one contain some non-projective dependency arcs, although their proportion varies from 0.1% to 5.4%.

The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ Some dependency grammars, and also some treebanks, allow tokens to have more than one head, although often there is a distinction between primary and optional secondary relations, e.g. in the Danish Dependency Treebank (Kromann, 2003), the Dutch Alpino Treebank (van der Beek et al., 2002b) and the German TIGER treebank (Brants et al., 2002).
The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ We used the following five treebanks of this type

We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. $$$$$ (2006).
We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. $$$$$ It has the smallest training set.

Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks. $$$$$ We used the following five treebanks of this type

 $$$$$ For the Portuguese treebank, the conversion was complicated by the fact that a detailed specification existed which tokens should be the head of which other tokens, e.g. the finite verb must be the head of the subject and the complementzier but the main verb must be the head of the complements and adjuncts.23 Given that the Floresta sint´a(c)tica does not use traditional VP constituents but rather verbal chunks (consisting mainly of verbs), a simple MagermanCollins-style head table was not sufficient to derive the required dependency structure.
 $$$$$ We hope that the resources created and lessons learned during this shared task will be valuable for many years to come but also that they will be extended and improved by others in the future, and that the shared task website will grow into an informational hub on multilingual dependency parsing.

We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. $$$$$ All the sentences are in one text file and they are separated by a blank line after each sentence.
Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. $$$$$ Each token is represented on one line, consisting of 10 fields.

Datasets and Evaluation Our experiments are run on five different languages $$$$$ (2006).
Datasets and Evaluation Our experiments are run on five different languages $$$$$ Other easy PoS are articles, with accuracies in the nineties for German, Dutch, Swedish, Portuguese and Spanish.

Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ (2006).

We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ In general, there is a high correlation between the best scores and the average scores.

We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ We will refer to this as “bottom-up-trees”.
We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ This approach assumes projective dependency structures.

Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). $$$$$ Some dependency grammars, and also some treebanks, allow tokens to have more than one head, although often there is a distinction between primary and optional secondary relations, e.g. in the Danish Dependency Treebank (Kromann, 2003), the Dutch Alpino Treebank (van der Beek et al., 2002b) and the German TIGER treebank (Brants et al., 2002).
Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). $$$$$ Even though McDonald et al. (2006) and Nivre et al.

The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ CoNLL-X Shared Task On Multilingual Dependency Parsing
Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). $$$$$ Nivre’s parser has been tested for Swedish (Nivre et al., 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al. (2005), while McDonald’s parser has been applied to English (McDonald et al., 2005a), Czech (McDonald et al., 2005b) and, very recently, Danish (McDonald and Pereira, 2006).
The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). $$$$$ Even though McDonald et al. (2006) and Nivre et al.
