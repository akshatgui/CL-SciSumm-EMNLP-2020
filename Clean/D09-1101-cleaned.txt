 $$$$$ These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically.
 $$$$$ This work was supported in part by NSF Grant IIS-0812261.

 $$$$$ These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically.
 $$$$$ This work was supported in part by NSF Grant IIS-0812261.

For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). $$$$$ For illustrative purposes, we will use the text segment shown in Figure 1.
For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). $$$$$ However, this is not the case when system mentions are used.

The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Luo et al. (2004) represent one of the earliest attempts to investigate learning-based entity-mention models.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Our cluster-ranking model.

The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ The feature set used to represent the instance is primarily composed of features that describe the relationship between mj and mk, as well as a few cluster-level features.
The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued.

Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Heuristic-based cluster ranking.
Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Our cluster-ranking model.

We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ The mention-ranking baseline.
We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ Our cluster-ranking model.

Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ The mention-ranking baseline.
Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ Our cluster-ranking model.

In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ Also, our joint-learning approach to discourse-new detection and coreference resolution consistently yields cluster rankers that outperform those adopting the pipeline architecture.
In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ Discourse-new detection.

We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ The discourse-new classifier used in the resolution step is trained with 26 of the 37 features2 described in Ng and Cardie (2002a) that are deemed useful for distinguishing between anaphoric and non-anaphoric mentions.
We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ Also, we retain twinless system mentions that are nonsingletons, as the resolver should be penalized for identifying spurious coreference relations.

We follow the procedure described in (Rahman and Ng, 2009). $$$$$ Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new.
We follow the procedure described in (Rahman and Ng, 2009). $$$$$ (2004), as described below.

In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ To train a ranker, we use the SVM ranker-learning algorithm from the SVMlZght package.
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ We use the ACE 2005 coreference corpus as released by the LDC, which consists of the 599 training documents used in the official ACE evaluation.3 To ensure diversity, the corpus was created by selecting documents from six different sources

 $$$$$ These features have largely been employed by state-of-the-art learning-based coreference systems (e.g., Soon et al. (2001), Ng and Cardie (2002b), Bengtson and Roth (2008)), and are computed automatically.
 $$$$$ This work was supported in part by NSF Grant IIS-0812261.

Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). $$$$$ We propose a simple solution to this problem

The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ We believe that our proposal addresses Stoyanov et al.â€™s (2009) problem of having very low precision when applying the CEAF scorer to score partitions of system mentions.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Our cluster-ranking model.

To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set. $$$$$ Mention extractor.
To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set. $$$$$ To extract system mentions from a test text, we trained a mention extractor on the training texts.

Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ Noun phrase (NP) coreference resolution is the task of identifying which NPs (or mentions) refer to the same real-world entity or concept.
Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ The features for an instance can be divided into two types

Details of the CR model can be found in Rahman and Ng (2009). $$$$$ In this section, we describe three coreference models that will serve as our baselines

Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). $$$$$ In this section, we describe three coreference models that will serve as our baselines

The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster. $$$$$ After training, the resulting cluster ranker processes the mentions in a test text in a left-to-right manner.
The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster. $$$$$ Our cluster-ranking model.
