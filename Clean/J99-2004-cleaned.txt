These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in Tree Adjoining Grammar models (Bangalore and Joshi,1999). $$$$$ We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.
These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in Tree Adjoining Grammar models (Bangalore and Joshi,1999). $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).

Supertags are the elementary structures of Lexicalized Tree Adjoining Grammars (LTAGs) (Bangalore and Joshi, 1999). $$$$$ We first employed these ideas in the context of Lexicalized Tree Adjoining grammars (LTAG) in Joshi and Srinivas (1994).
Supertags are the elementary structures of Lexicalized Tree Adjoining Grammars (LTAGs) (Bangalore and Joshi, 1999). $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).

We used the supertagger (Bangalore and Joshi, 1999) to supertag each word in the corpus. $$$$$ A total of 300 different supertags were used in these experiments. mance of the trigram supertagger on the IBM Manual corpus, a set of 14,000 words correctly supertagged was used as the training corpus and a set of 1,000 words was used as a test corpus.
We used the supertagger (Bangalore and Joshi, 1999) to supertag each word in the corpus. $$$$$ The performance of the supertagger on this corpus is shown in Table 6.

Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. $$$$$ Moreover, since the supertags encode dependency information, we can also use information about the distribution of distances between a given supertag and its dependent supertags.
Insertion of further information such as supertags (Bangalore and Joshi, 1999) or word stems can also be beneficial for further processing. $$$$$ One method of disambiguating the supertags assigned to each word is to order the supertags by the lexical preference that the word has for them.

The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns rich descriptions (supertags) that impose complex constraints in a local context. $$$$$ Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.
The process is quite similar to supertagging (Bangalore and Joshi, 1999), which assigns rich descriptions (supertags) that impose complex constraints in a local context. $$$$$ By associating rich descriptions (supertags) that impose complex constraints in a local context, we have been able to use local computational models for effective supertag disambiguation.

The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). $$$$$ A major drawback of this early work was that it used no lexical information in the supertagging process as the training material consisted of (part-of-speech, supertag) pairs.
The notions of a supertag as a lexical category and the process of supertagging are both crucial here (Bangalore and Joshi, 1999). $$$$$ To establish the dependency links among the words of the sentence, we exploit the dependency requirements Bangalore and Joshi Supertagging encoded in the supertags.

Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing. $$$$$ Supertagging: An Approach To Almost Parsing
Bangalore and Joshi (1999) indicated that, correct disambiguation with supertagging, i.e., assignment of lexical entries before parsing, enabled effective LTAG (Lexicalized Tree-Adjoining Grammar) parsing. $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).

As methodologies deriving well-formedness of a sentence we use super tagging (Bangalore and Joshi, 1999) with lightweight dependency analysis (LDA) (Bangalore, 2000), link grammars (Sleator and Temperley, 1993) and a maximum entropy (ME) based chunk parser (Bender et al, 2003). $$$$$ Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al.
As methodologies deriving well-formedness of a sentence we use super tagging (Bangalore and Joshi, 1999) with lightweight dependency analysis (LDA) (Bangalore, 2000), link grammars (Sleator and Temperley, 1993) and a maximum entropy (ME) based chunk parser (Bender et al, 2003). $$$$$ We have presented a lightweight dependency analyzer (LDA) that takes the output of the supertagger and uses the dependency requirements of the supertags to produce a dependency linkage for a sentence.

Supertagging (Bangalore and Joshi, 1999) uses the Lexicalized Tree Adjoining Grammar formalism (LTAG). $$$$$ We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.
Supertagging (Bangalore and Joshi, 1999) uses the Lexicalized Tree Adjoining Grammar formalism (LTAG). $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).

We used the supertagger of Bangalore and Joshi (1999). $$$$$ As can be seen, from the above experiments, the baseline performance of the supertagger is about 77% and the performance improves to about 92% with the inclusion of contextual information, an Bangalore and joshi Supertagging improvement of 19.5%.
We used the supertagger of Bangalore and Joshi (1999). $$$$$ The output of the supertagger has also been used as a front end to a lexicalized grammar parser.

This is a variant of the approach above, but using super tags (Bangalore and Joshi, 1999) instead of PoS tags. $$$$$ We call the elementary structures associated with each lexical item super parts-of-speech (super POS) or supertags.3 Figure 1 illustrates a few elementary trees associated with each word of the sentence: the purchase price includes two ancillary companies.
This is a variant of the approach above, but using super tags (Bangalore and Joshi, 1999) instead of PoS tags. $$$$$ The relatively low baseline performance for the supertagger is a direct consequence of the fact that there are many more supertags per word than there are POS tags.

It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). $$$$$ The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995).
It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). $$$$$ A broad-coverage grammar system, XTAG, has been implemented in the LTAG formalism.

It was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999). $$$$$ We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.
It was first proposed for lexicalized tree adjoining grammar (LTAG) (Bangalore and Joshi, 1999). $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).

We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). $$$$$ As in standard part-of-speech disambiguation, we can use local statistical information in the form of n-gram models based on the distribution of supertags in an LTAG parsed corpus.
We condition prosody not only on word strings and their parts-of-speech but also on richer syntactic information encapsulated in the form of Supertags (Bangalore and Joshi, 1999). $$$$$ For the words that do not appear in the training corpus we back off to the part of speech of the word and use the most frequent supertag associated with that part of speech as the supertag for the word. the previously discussed two sets of data.

In addition to the POS tags, we also annotate the utterance with Supertags (Bangalore and Joshi, 1999). $$$$$ We will call these elementary structures supertags, in order to distinguish them from the standard partof-speech tags.
In addition to the POS tags, we also annotate the utterance with Supertags (Bangalore and Joshi, 1999). $$$$$ The relatively low baseline performance for the supertagger is a direct consequence of the fact that there are many more supertags per word than there are POS tags.

Supertags have been successfully applied to guide parsing in symbolic frameworks such as Lexicalised Tree-Adjoning grammar (Bangalore and Joshi, 1999). $$$$$ In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).
Supertags have been successfully applied to guide parsing in symbolic frameworks such as Lexicalised Tree-Adjoning grammar (Bangalore and Joshi, 1999). $$$$$ The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995).

The parser uses a two-stage system, first employing a super tagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. $$$$$ We call the elementary structures associated with each lexical item super parts-of-speech (super POS) or supertags.3 Figure 1 illustrates a few elementary trees associated with each word of the sentence: the purchase price includes two ancillary companies.
The parser uses a two-stage system, first employing a super tagger (Bangalore and Joshi, 1999) to propose lexical categories for each word, and then applying the CKY chart parsing algorithm. $$$$$ In the first stage, the parser looks up the lexicon and selects all the supertags associated with each word of the sentence to be parsed.

We automatically annotate a user's utterance with super tags (Bangalore and Joshi, 1999). $$$$$ We call the elementary structures associated with each lexical item super parts-of-speech (super POS) or supertags.3 Figure 1 illustrates a few elementary trees associated with each word of the sentence: the purchase price includes two ancillary companies.
We automatically annotate a user's utterance with super tags (Bangalore and Joshi, 1999). $$$$$ A more detailed discussion about these properties is presented in Joshi (1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and Schabes (1996).

The second direction concerns experiments on supertagging (Bangalore and Joshi, 1999) followed by a parsing stage the tagging stage associates to each word a supertag. $$$$$ It should be clear that by reducing the number of supertags that are selected in the first stage, the search-space for the second stage can be reduced significantly and hence the parser can be made more efficient.
The second direction concerns experiments on supertagging (Bangalore and Joshi, 1999) followed by a parsing stage the tagging stage associates to each word a supertag. $$$$$ Supertagging associates each word with a unique supertag.

Bangalore and Joshi (1999), Clark and Curran (2004) and Matsuzaki et al (2007) show that by using a supertagger before (CCG and HPSG) parsing, the space required for discriminative training is drastically reduced. $$$$$ Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al.
Bangalore and Joshi (1999), Clark and Curran (2004) and Matsuzaki et al (2007) show that by using a supertagger before (CCG and HPSG) parsing, the space required for discriminative training is drastically reduced. $$$$$ The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995).
