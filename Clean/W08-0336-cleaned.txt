Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ The MaxMatch segmenter is a simple and common baseline for the Chinese word segmentation task.
Chinese word segmentation is done by the Stanford Chinese segmenter (Chang et al, 2008). $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.

The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
The most obvious example of this lies in languages that do not separate words with white space such as Chinese, Japanese, or Thai, in which the choice of a segmentation standard has a large effect on translation accuracy (Chang et al., 2008). $$$$$ Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter “words” (Peng et al., 2002), paralleling the IR gains from compound splitting in languages like German (Hollink et al., 2004), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon (Gao et al., 2005).

It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ It has been recognized that different NLP applications have different needs for segmentation.
It has been recognized that varying segmentation granularities are needed for SMT (Chang et al, 2008). $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.

All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
All Chinese data was re-segmented with the CRF-based Stanford Chinese segmenter (Chang et al, 2008) that is trained on the segmentation of the Chinese Treebank for consistency. $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.

The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.
The Chinese text was segmented with a CRF-based Chinesesegmenter optimized for MT (Chang et al, 2008). $$$$$ Since the word segmentation standard under consideration (Chinese Treebank (Xue et al., 2005)) was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT.

Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
Chinese words were automatically segmented with a conditional random field (CRF) classifier (Chang et al, 2008) that conforms to the Chinese Treebank (CTB) standard. $$$$$ Since the word segmentation standard under consideration (Chinese Treebank (Xue et al., 2005)) was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT.

These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ Without a standardized notion of a word, traditionally, the task of Chinese word segmentation starts from designing a segmentation standard based on linguistic and task intuitions, and then aiming to building segmenters that output words that conform to the standard.
These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al, 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the tree bank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. $$$$$ Note that consistency is only one of the competing factors of how good a segmentation is for MT performance.

Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.
Chang et al (2008) enhanced a CRF s segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. $$$$$ Since the word segmentation standard under consideration (Chinese Treebank (Xue et al., 2005)) was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT.

Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ Unless these issues are attended to, simple baseline segmenters can be more effective inside an MT system than more complex machine learning based models, with much lower word segmentation error rate.
Chang et al (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. $$$$$ The linguistic features help capturing words that were unseen to the segmenter; while the lexicon-based features constrain the segmenter with external knowledge of what sequences are likely to be words.

The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ (2004), who treated word segmentation as a binary decision task.
The optimal set of the model parameter values was found on dev MT to be k= 3, t AC= 0.0 and t COOC= 15. The comparison candidates also involve two popular off-the-shelf segmentation models $$$$$ We chose the λ0 = 2 on another dev set (MT02).

Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ Chinese information retrieval (IR) systems benefit from a segmentation that breaks compound words into shorter “words” (Peng et al., 2002), paralleling the IR gains from compound splitting in languages like German (Hollink et al., 2004), whereas automatic speech recognition (ASR) systems prefer having longer words in the speech lexicon (Gao et al., 2005).

The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.
The third and fourth tokenizations come from the CRF-based Stanford Chinese segmenter described by Chang et al (2008). $$$$$ More details of CRF-Lex will be described in Section 5.1.

This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ Optimizing Chinese Word Segmentation for Machine Translation Performance
This affirms our be lief that consistency in tokenization is important for machine translation, which was also mentioned by Chang et al (2008). $$$$$ We find that other factors such as segmentation consistency and granularity of Chinese “words” can be more important for machine translation.

The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ In this paper, we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better MT performance.
The use of monolingual probabilistic models does not necessarily yield a better MT performance (Chang et al, 2008). $$$$$ (ii) a higher F measure segmenter does not necessarily outperforms on the MT task.

 $$$$$ The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.
 $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM.

we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.
we first segmented the sentences using the Stanford Chinese Word Segmenter (Chang et al, 2008). $$$$$ For example, for a word “ABC” in the gold segmentation, we look at how it is segmented with a segmenter.

We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
We tokenized the English with packages from the Stan ford Parser (Klein and Manning, 2003) according to the Penn Treebank standard (Marcus et al, 1993), the Arabic with the Stanford Arabic segmenter (Green and DeNero, 2012) according to the Penn Arabic Treebank standard (Maamouri et al, 2008), and the Chinese with the Stanford Chinese segmenter (Chang et al, 2008) according to the Penn Chinese Treebank standard (Xue et al, 2005). $$$$$ Since the word segmentation standard under consideration (Chinese Treebank (Xue et al., 2005)) was neither specifically designed nor optimized for MT, it seems reasonable to investigate whether any segmentation granularity in continuum between character-level and CTB-style segmentation is more effective for MT.

For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ One widely used standard is the Penn Chinese Treebank (CTB) Segmentation Standard (Xue et al., 2005).
For English corpora, the pre-processing are the same as that in (Qiu et al, 2009), and for Chinese corpora, the Stanford Word Segmenter (Changet al, 2008) is used to perform word segmentation. $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.

The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). $$$$$ The MT system used in this paper is Moses, a stateof-the-art phrase-based system (Koehn et al., 2003).
The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al, 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). $$$$$ The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.

A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ Having words as the basic units helps the reordering model.
A variety of segmentation granularities, or atomic units, exist, including segmentations at the morpheme (e.g., Sirts and Alumae 2012), word (e.g., Chang et al 2008), sentence (e.g., Reynar and Ratnaparkhi 1997), and paragraph (e.g., Hearst 1997) levels. $$$$$ CRF is a statistical sequence modeling framework introduced by Lafferty et al. (2001), and was first used for the Chinese word segmentation task by Peng et al.
