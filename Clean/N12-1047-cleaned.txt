(Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing. $$$$$ The loss `i(~w) is 0 only if w~ separates each e ∈ Ei from ez by a margin proportional to their BLEU differentials.
(Cherry and Foster, 2012), which is closer to the usual loss used for max-margin in machine learing. $$$$$ In terms of (4), PRO defines where (x)+ = max(0, x).

Cherry and Foster (2012) have concurrently performed a similar analysis. $$$$$ To solve the necessary quadratic programming sub-problems, we use a multiclass SVM similar to LIBLINEAR (Hsieh et al., 2008).
Cherry and Foster (2012) have concurrently performed a similar analysis. $$$$$ We tested 20, 15, 10, 8 and 5 shards during development. tings that performed well in general.

The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). $$$$$ These methods can be split into two groups

We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al, 2008) and batch MIRA (Cherry and Foster, 2012). $$$$$ The ability to optimize these models according to an error metric has become a standard assumption in SMT, due to the wide-spread adoption of Minimum Error Rate Training or MERT (Och, 2003).
We are currently in the process of implementing and testing other parameter tuning methods (in addition to manual tuning and PRO), specifically lattice-based minimum error rate training (Macherey et al, 2008) and batch MIRA (Cherry and Foster, 2012). $$$$$ We have reviewed three tuning methods and introduced three tuning methods.

As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al, 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. $$$$$ First employed in SMT by Watanabe et al. (2007), and refined by Chiang et al.
As baselines we use MERT (Och, 2003), PRO, and the Moses (Koehn et al, 2007) implementation of k-best MIRA, which Cherry and Foster (2012) recently showed to work as well as online MIRA (Chiang, 2012) for feature-rich models. $$$$$ This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.

Second, we ran the k-best batchMIRA (kbMIRA) (Cherry and Foster, 2012) implementation in Moses. $$$$$ This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.
Second, we ran the k-best batchMIRA (kbMIRA) (Cherry and Foster, 2012) implementation in Moses. $$$$$ To cope with the resulting large number of configurations, we ran all experiments using an efficient phrase-based decoder similar to Moses (Koehn et al., 2007).

Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. $$$$$ This strategy has been adopted by Moses (Hasler et al., 2011), and it is the one we adopt in our MIRA implementation.
Cherry and Foster (2012) reported the same result, and their implementation is available in Moses. $$$$$ Six-feature lexicalized distortion models were estimated and applied as in Moses.

These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. $$$$$ However, MERT has trouble scaling to more than 30 features, which has led to a surge in research on tuning schemes that can handle high-dimensional feature spaces.
These works use radically different experimental setups, and to our knowledge only (Cherry and Foster, 2012) and this work compare to at least two high dimensional baselines. $$$$$ Our primary contribution is to empirically compare eight tuning algorithms and variants, focusing on methods that work within MERT’s established outer loop.

We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). $$$$$ Batch k-best MIRA inherits all of the MERT architecture.
We tune with the k-best batch MIRA algorithm (Cherry and Foster, 2012). $$$$$ These methods can be split into two groups

See (Cherry and Foster, 2012) for details on objectives. $$$$$ Gimpel and Smith (2012) discuss these issues in greater detail, while also providing an interpretable alternative to MIRA.
See (Cherry and Foster, 2012) for details on objectives. $$$$$ Thanks to Mark Hopkins, Zhifei Li and Jonathan May for their advice while implementing the methods in this review, and to Kevin Gimpel, Roland Kuhn and the anonymous reviewers for their valuable comments on an earlier draft.
