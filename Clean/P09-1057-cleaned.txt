Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008).
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007).

Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ Our approach is related to minimum description length (MDL).
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998).

The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008).
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008).

Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ So we see a benefit to our explicit small-model approach.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ The procedure is simple and proceeds as follows

However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets.
However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model.

This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques.

Ravi and Knight (2009) exploited this to iteratively improve their POS tag model $$$$$ Here we obtain 81.7% accuracy, which is better than the 3-gram model.
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model $$$$$ For the rest of this paper, we will limit ourselves to a 2-gram tag model.

Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8).

(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ The IP solver finds the smallest grammar set that can explain the given word sequence.

The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.).
The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output.

The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one).

Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.).
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead.

A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead.
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar).

To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ We also create variables for every possible tag bigram and word/tag dictionary entry.
To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP.

Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods.
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ Fortunately, we can use EM for that.

This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al. (2008).
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008).

 $$$$$ We obtain this answer by formulating the problem in an integer programming (IP) framework.
 $$$$$ For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.

Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%.
Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters.

Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values.

For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ We also create variables for every possible tag bigram and word/tag dictionary entry.
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1.
