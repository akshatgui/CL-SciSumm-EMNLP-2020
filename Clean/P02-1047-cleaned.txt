A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ An Unsupervised Approach To Recognizing Discourse Relations
A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.

Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ In spite of the difficulty of determining the discourse relations that hold between arbitrary text spans, it is clear that such an ability is important in many applications.

Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations $$$$$ The advantage of operating with coarsely defined discourse relations is that it enables us to automatically construct relatively low-noise datasets that can be used for learning.
Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations $$$$$ That is, we considered that a CONTRAST relation held between two text spans if a human annotator labeled the relation between those spans as ANTITHESIS, CONCESSION, OTHERWISE or CONTRAST.

Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ Table 2 lists some of the cue phrases we used in order to extract CONTRAST, CAUSEEXPLANATION-EVIDENCE, ELABORATION, and CONDITION relations and the number of examples extracted from the Raw corpus for each type of discourse relation.
Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ We did this because when trying to determine whether a CONTRAST relation holds between two spans of texts separated by the cue phrase “but”, for example, we want to take advantage of the cue phrase occurrence as well.

Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). $$$$$ We wrote a simple program that extracted the nouns, verbs, and cue phrases in each sentence/clause.
Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). $$$$$ We call these the most representative words of a sentence/discourse unit.

 $$$$$ Table 1 shows the definitions of the relations we considered.
 $$$$$ This work was supported by the National Science Foundation under grant number IIS-0097846 and by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007.

Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ This ensures that our classifiers do not learn, for example, that the word pair if – then is a good indicator of a CONDITION relation, which would simply amount to learning to distinguish between the extraction patterns used to construct the corpus.
Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ Also, since the learning curve for the BLIPP corpus is steeper than the learning curve for the Raw corpus, this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs (unannotated data).

Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ We automatically extracted from this manually annotated corpus all CONTRAST, CAUSE-EXPLANATION-EVIDENCE, CONDITION and ELABORATION relations that hold between two adjacent elementary discourse units.
Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ For example, the RST-annotated corpus of Carlson et al. (2001) contains 238 CONTRAST relations that hold between two adjacent elementary discourse units.

(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ Table 2 lists some of the cue phrases we used in order to extract CONTRAST, CAUSEEXPLANATION-EVIDENCE, ELABORATION, and CONDITION relations and the number of examples extracted from the Raw corpus for each type of discourse relation.
(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ To achieve this, we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus.

There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). $$$$$ In our work, we decide to focus only on four types of relations, which we call

(Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. $$$$$ The learning curves in Figure 1 are illuminating as they show that if one uses as features only the most representative word pairs, one needs only about 100,000 training examples to achieve the same level of performance one achieves using 1,000,000 training examples and features defined over all word pairs.
(Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. $$$$$ For example, the RST-annotated corpus of Carlson et al. (2001) contains 238 CONTRAST relations that hold between two adjacent elementary discourse units.

(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ However, empirical work of Marcu (2000) and Carlson et al. (2001) suggests that the majority of occurrences of “but”, for example, do signal CONTRAST relations.
(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ The values are computed using maximum likelihood estimators, which are smoothed using the Laplace method (Manning and Sch¨utze, 1999).

To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. $$$$$ We retrained then all classifiers on the Raw corpus, but this time without removing from the corpus the cue phrases that were used to generate the training examples.
To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. $$$$$ The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations.

(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. $$$$$ In our paper, we show that massive amounts of data can have a major impact on discourse processing research as well.
(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. $$$$$ Developing lower-noise methods for automatically collecting training data and discovering features of higher predictive power for discourse relation classification than the features presented in this paper appear to be research avenues that are worthwhile to pursue.

Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ In Lascarides and Asher’s theory (1993), we would label the relation between 2.a and 2.b as EXPLANATION because the event in 2.b explains why the event in 2.a happened (perhaps by CAUSING it).
Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ We label such examples NO-RELATION-SAME-TEXT.

(Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. $$$$$ But a direct comparison between two classifiers trained on different corpora is not fair because with just 100,000 examples per relation, the systems trained on the Raw corpus are much worse than those trained on the BLIPP data.
(Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. $$$$$ A program trained only on Carlson et al.’s corpus, would, therefore, identify at most 79 of the 307 relations correctly.

When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ In general, we hypothesize that lexical item pairs can provide clues about the discourse relations that hold between the text spans in which the lexical items occur.
When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ This corresponds to an increase in accuracy from to .

An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ An Unsupervised Approach To Recognizing Discourse Relations
An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.

We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ The values are computed using maximum likelihood estimators, which are smoothed using the Laplace method (Manning and Sch¨utze, 1999).
We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ For each discourse relation pair , we train a word-pair-based classifier using the automatically derived training examples in the Raw corpus, from which we first removed the cue-phrases used for extracting the examples.

We draw on and extend the work of Marcu and Echihabi (2002). $$$$$ In our work, we decide to focus only on four types of relations, which we call
