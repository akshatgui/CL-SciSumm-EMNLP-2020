Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Training Tree Transducers
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).

Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ Training Tree Transducers
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ We now turn to tree-to-string transducers (xRS).
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).

Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ However, we can simulate lookahead using states, as in these productions

Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ Training Tree Transducers
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ We now turn to tree-to-string transducers (xRS).

The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.

It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ We now turn to tree-to-string transducers (xRS).

While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.

Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ Training Tree Transducers
Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling.

WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ Recently, specific probabilistic tree-based models have been proposed not only for machine translation (Wu, 1997; Alshawi, Bangalore, and Douglas, 2000; Yamada and Knight, 2001; Gildea, 2003; Eisner, 2003), but also for This work was supported by DARPA contract F49620-001-0337 and ARDA contract MDA904-02-C-0450. summarization (Knight and Marcu, 2002), paraphrasing (Pang, Knight, and Marcu, 2003), natural language generation (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Corston-Oliver et al., 2002), and language modeling (Baker, 1979; Lari and Young, 1990; Collins, 1997; Chelba and Jelinek, 2000; Charniak, 2001; Klein and Manning, 2003).
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ (Rounds, 1970) and (Thatcher, 1970) independently introduced tree transducers as a generalization of FSTs.

Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ One advantage of working with tree transducers is the large and useful body of literature about these automata; two excellent surveys are (Gécseg and Steinby, 1984) and (Comon et al., 1997).
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ We give an explicit tree-to-string transducer example in the next section.

The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ We give an explicit tree-to-string transducer example in the next section.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ We now build a trainable xRS tree-to-string transducer that embodies the same P(Japanese string

The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ The inside and outside probabilities of this packed derivation structure are used to compute expected counts of the productions from the original, given transducer (Sections 6-7).
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990).

Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Training Tree Transducers
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ Training Tree Transducers
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

Such an algorithm is presented by Graehl and Knight (2004). $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.

We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ If enumerating rules matching transducer input-patterns and output-subtrees has cost L (constant given a transducer), then DERIV has time complexity O(L ·

The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.
