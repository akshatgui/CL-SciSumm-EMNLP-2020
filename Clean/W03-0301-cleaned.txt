 $$$$$ A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.
 $$$$$ In particular, we would like to thank Dan Melamed for suggesting the two different subtasks (limited and unlimited resources), and Michael Carl and Phil Resnik for initiating interesting discussions regarding phrase-based evaluations.

We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts

The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003) $$$$$ The intersection of the Sure alignments produced by the two annotators led to the final Sure aligned set, while the reunion of the Probable alignments led to the final Probable aligned set.
The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003) $$$$$ We conducted therefore 14 evaluations for each submission file

HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). $$$$$ BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2

HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003). $$$$$ BiBr.EF.1 BiBr.EF.2 BiBr.EF.3 BiBr.EF.4 BiBr.EF.5 BiBr.EF.6 BiBr.EF.7 BiBr.EF.8 Limited Unlimited Unlimited Limited Unlimited Unlimited Limited Unlimited baseline of bilingual bracketing baseline of bilingual bracketing + English POS tagging baseline of bilingual bracketing + English POS tagging and base NP reverse direction of BiBr.EF.1 reverse direction of BiBr.EF.2 reverse direction of BiBr.EF.3 intersection of BiBr.EF.1 & BiBr.EF.3 intersection of BiBr.EF.3 & BiBr.EF.6 ProAlign.EF.1 Unlimited cohesion between source and target language + English parser + distributional similarity for English words Ralign.EF.1 Limited Giza (IBM Model 2) + recursive parallel segmentation UMD.EF.1 Limited IBM Model 2, trained with 1/20 of the corpus, distortion 2, iterations 4 XRCE.Base.EF.1 XRCE.Nolem.EF.2 XRCE.Nolem.EF.3 Limited GIZA++ (IBM Model 4) with English and French lemmatizer GIZA++ only (IBM Model 4), trained with 1/4 of the corpus GIZA++ only (IBM Model 4), trained with 1/2 of the corpus Table 2

hlt-03 best is xrce.nolem (mihalcea and pedersen, 2003). $$$$$ For Romanian-English, limited resources, XRCE systems (XRCE.Nolem-56k.RE.2 and XRCE.Trilex.RE.3) seem to lead to the best results.
hlt-03 best is xrce.nolem (mihalcea and pedersen, 2003). $$$$$ For unlimited resources, ProAlign.RE.1 has the best performance.

We evaluated the alignment performance of the proposed models with two tasks $$$$$ The word alignment result files had to include one line for each word-to-word alignment.
We evaluated the alignment performance of the proposed models with two tasks $$$$$ A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.

As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). $$$$$ A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.
As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). $$$$$ Participants were required to run their word alignment systems on these two sets, and submit word alignments.

(Mihalcea and Pedersen, 2003). $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts

This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). $$$$$ An Evaluation Exercise For Word Alignment
This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). $$$$$ The only evaluation set where Romanian-English data leads to better performance is the Probable alignments set.

The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ Participants were required to run their word alignment systems on these two sets, and submit word alignments.
The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ We conducted therefore 14 evaluations for each submission file

For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ Trial sets consisted of 37 English-French, and 17 Romanian-English aligned sentences.
For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ The only evaluation set where Romanian-English data leads to better performance is the Probable alignments set.

To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). $$$$$ Trial sets consisted of sentence aligned texts, provided together with manually determined word alignments.
To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). $$$$$ The Sure alignment set is guaranteed to be a subset of the Probable alignment set.

This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). $$$$$ Seven teams from around the world participated in the word alignment shared task.
This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). $$$$$ A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.

In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts

We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts

We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts

The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ For English and French, we used a version of the tokenizers provided within the EGYPT Toolkit2.
The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ For Romanian, we used our own tokenizer.

The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ The gold standard for the two language pair alignments were produced using slightly different alignment procedures, which allowed us to study different schemes for producing gold standards for word aligned data.
The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.

The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks. $$$$$ As part of the HLT/NAACL 2003 workshop on ”Building and Using Parallel Texts
