So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). $$$$$ The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003).
So we first segmented Chinese text with a Chinese word segmenter that was based on maximum entropy modeling (Ng and Low, 2004). $$$$$ An English POS tagger based on maximum entropy modeling was built by (Ratnaparkhi, 1996).

If we extend these positional tags to include POS information ,segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). $$$$$ (If W is a single-character word, then the single character is assigned “s”.)
If we extend these positional tags to include POS information ,segmentation and POS tagging can be performed by a single pass under a unify classification framework (Ng and Low, 2004). $$$$$ In this approach, both word segmentation and POS tagging will be performed in a combined, single step simultaneously.

In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. $$$$$ Assuming a one-at-a-time processing architecture, Chinese POS tagging using a character-based approach gives higher accuracy compared to a word-based approach.
In the rest of the paper, we call this operation mode Joint S&T. Experiments of Ng and Low (2004) shown that, compared with performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging. $$$$$ There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.

As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively. $$$$$ Each character can be assigned one of 4 possible boundary tags

The features we use to build the classifier are generated from the templates of Ng and Low (2004). $$$$$ We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003).
The features we use to build the classifier are generated from the templates of Ng and Low (2004). $$$$$ The features we used are similar to his tag features, except that we did not use features with three consecutive characters, since we found that the use of these features did not improve accuracy.

The table's upper column lists the templates that immediately from Ng and Low (2004). $$$$$ The following feature templates were chosen.
The table's upper column lists the templates that immediately from Ng and Low (2004). $$$$$ Table 1 summarizes the methods investigated in this paper.

Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). $$$$$ The default feature, boundary tag feature of the previous character, and boundary tag feature of the character two before the current character used in (Xue and Shen, 2003) were dropped from our word segmenter, as they did not improve word segmentation accuracy in our experiments.
Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004). $$$$$ Each character is assigned both a boundary tag and a POS tag, for example “b_NN” (i.e., the first character in a word with POS tag NN).

Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). $$$$$ Assuming a one-at-a-time processing architecture, Chinese POS tagging using a character-based approach gives higher accuracy compared to a word-based approach.
Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004). $$$$$ There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.

Ac cording to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types $$$$$ Each character can be assigned one of 4 possible boundary tags

In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). $$$$$ Each character is assigned both a boundary tag and a POS tag, for example “b_NN” (i.e., the first character in a word with POS tag NN).
In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004). $$$$$ In the following templates, B refers to the boundary tag assigned.

Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. $$$$$ The following feature templates were chosen.
Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target. $$$$$ The features we used are identical to those employed in the character-based POS tagger described in section 4.1, except that features (g) and (h) are replaced with those listed below.

Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. $$$$$ The following feature templates were chosen.
Note that the templates of Ng and Low (2004) have already contained some lexical-target ones. $$$$$ In the following templates, B refers to the boundary tag assigned.

Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved F measure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation. $$$$$ Our word segmenter achieved higher F-measure than the best reported F-measure in the SIGHAN bakeoff on the ASc, HKc, and PKc corpus.
Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved F measure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation. $$$$$ We found that with this additional AS training data added to the original 3 Last ranked participant of SIGHAN CTB (closed) with F-measure 73.2% is not shown in Figure 2 due to space constraint. official released CTB training data of SIGHAN, our word segmenter achieved an F-measure of 92.2%, higher than the best reported F-measure in the CTB open task.

In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). $$$$$ (Note that the top participant of CTBc (Zhang et al., 2003) used additional named entity knowledge/data in their word segmenter).
In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004). $$$$$ On this CTBO task, we used as additional training data the AS training corpus provided by SIGHAN, after converting the AS training corpus to GB encoding.

For example, Li et al (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it is beneficial to integrate word segmentation and part-of-speech tagging into one model. $$$$$ Chinese Part-Of-Speech Tagging

In this bakeoff, our models built for the tasks are similar to that in the work of Ng and Low (2004). $$$$$ The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003).
In this bakeoff, our models built for the tasks are similar to that in the work of Ng and Low (2004). $$$$$ Relatively less work has been done on Chinese POS tagging.

For this task, because of the time limitation as mentioned in the previous section, we could only port our implemented model by using a part of the feature set which was used in the word-based tagger discussed in the work of Ng and Low (2004). $$$$$ So we next investigate such a one-at-a-time, character-based POS tagger.
For this task, because of the time limitation as mentioned in the previous section, we could only port our implemented model by using a part of the feature set which was used in the word-based tagger discussed in the work of Ng and Low (2004). $$$$$ With a one-at-a-time, character-based POS tagger, the average POS tagging accuracy improved to 91.7%, 7.6% higher than that achieved by the one-at-a-time, word-based POS tagger.

Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). $$$$$ (Note that the top participant of CTBc (Zhang et al., 2003) used additional named entity knowledge/data in their word segmenter).
Our linguistic features are adopted from (Ng and Low, 2004) and (Tseng et al., 2005). $$$$$ Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).

 $$$$$ Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter’s output.
 $$$$$ This research is partially supported by a research grant R252-000-125-112 from National University of Singapore Academic Research Fund.

Similar to (Ng and Low, 2004), we found the overall F measure only goes up a tiny bit, but we do find a significant OOV recall rate improvement. $$$$$ For CTBc, due to the exceptionally high out-of-vocabulary (OOV) rate of the test data (18.1%), our word segmenter’s Fmeasure ranked in the third position.
Similar to (Ng and Low, 2004), we found the overall F measure only goes up a tiny bit, but we do find a significant OOV recall rate improvement. $$$$$ In addition, since the out-of-vocabulary (OOV) rate for Chinese words is much higher than the OOV rate for Chinese characters, in the presence of an unknown word, using the component characters in the word to help predict the correct POS tag is a good heuristic.
