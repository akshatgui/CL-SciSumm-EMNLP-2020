We should keep in mind that (1) a tree bank PCFG is not state-of-the-art $$$$$ Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002).
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art $$$$$ The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
 $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.

For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

previously used for self-training of parsers (McClosky et al, 2006). $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

 $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
 $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.

We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ As Figure 2 makes clear, the relative performance of the self-trained and baseline parsers does not self-trained parser improve the parse with the highest probability vary linearly with sentence length, so we introduced binned sentence length (with each bin of length 10) as a factor.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ Because the self-trained and baseline parsers produced equivalent output on 3,346 (66%) of the sentences, we restricted attention to the 1,693 sentences on which the self-trained and baseline parsers’ fscores differ.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ It is not surprising that self-training is not normally effective

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ Our labeled data comes from the Penn Treebank (Marcus et al., 1993) and consists of about 40,000 sentences from Wall Street Journal (WSJ) articles annotated with syntactic information.

The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ Table 1 shows the difference in parser’s (not reranker’s) performance when trained on parser-best reranker-best sentences from NANC to WSJ training data.
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ “WSJ + NANC” represents the system trained on WSJ training (with a relative weight of 5) and 1,750k sentences from the reranker-best list of NANC.

McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ Effective Self-Training For Parsing
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005).

Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ In these cases, we will use the term reranking parser.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ If this were the case, we would not see an improvement when evaluating a reranking parser on the same models.

We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ Effective Self-Training For Parsing
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.

Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ Clark et al. (2003) applies self-training to POS-tagging and reports the same outcomes.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ While the reranker was used to produce the reranker-best sentences, we performed this evaluation using only the first-stage parser to parse all sentences from section 22.

SELF-CRF $$$$$ When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners.
SELF-CRF $$$$$ A variation suggested by Dasgupta et al. (2001) is to add data to the training set when multiple learners agree on the label.

To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003).
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ It is not surprising that self-training is not normally effective

A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ Effective Self-Training For Parsing
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.

Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ A simple method of incorporating unlabeled data into a new model is self-training.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ One would assume that errors in the original model would be amplified in the new model.
