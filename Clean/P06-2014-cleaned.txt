(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ Soft Syntactic Constraints For Word Alignment Through Discriminative Training
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ Note that this will not search the exact same alignment space as a cohesion-constrained beam search; instead it uses the union of the cohesion constraint and the weaker ITG constraints (Cherry and Lin, 2006).

Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ Soft Syntactic Constraints For Word Alignment Through Discriminative Training
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ An ITG parser is used for the alignment search, exposing two syntactic features

At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has exactly one generator in the source.
At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links.

An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006).
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ Note that this will not search the exact same alignment space as a cohesion-constrained beam search; instead it uses the union of the cohesion constraint and the weaker ITG constraints (Cherry and Lin, 2006).

The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005).
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ We use the same feature representation ψ(l) as (Taskar et al., 2005), with some small exceptions.

(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner.
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006).

The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ We will refer to the parsed language as English, and the unparsed language as Foreign.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006).

Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ This indicates that these cohesion constraints are a strong alignment feature.

Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ Unfortunately, Cherry and Lin’s beam search solution does not lend itself to a soft cohesion constraint.

One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ Soft Syntactic Constraints For Word Alignment Through Discriminative Training
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ We have presented a discriminative, syntactic word alignment method.

The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ Soft Syntactic Constraints For Word Alignment Through Discriminative Training
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings.
