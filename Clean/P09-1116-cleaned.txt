We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ The features are the words (tokens) in the window.
We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ Second, it reduces the variance in the sizes of the feature vectors of the phrases.

Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. $$$$$ However, the supervised learning algorithms can typically identify useful clusters and assign proper weights to them, effectively adapting the clusters to the domain.
Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. $$$$$ In the two-stage cluster-based approaches such as ours, clustering is mostly decoupled from the supervised learning problem.

Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.

Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ Phrases are much less so because the words in a phrase provide contexts for one another.
Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ The idea is that if we have already seen 300K instances of a phrase, we should have already collected enough data for the phrase.

 $$$$$ The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.
 $$$$$ The authors wish to thank the anonymous reviewers for their comments.

Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ To obtain a list of phrases to be clustered, we followed the approach in (Lin et al., 2008) by collecting 20 million unique queries from an anonymized query log that are found in a 700 billion token web corpus with a minimum frequency count of 100.
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ Moreover, we use soft clustering instead of hard clustering.

We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Under this approach, even if a word is not found in the training data, it may still fire cluster-based features as long as it shares cluster assignments with some words in the labeled data.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ For natural language words and phrases, the soft cluster assignments often reveal different senses of a word.

Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ Our named entity recognition system achieves an F1-score of 90.90 on the CoNLL 2003 English data set, which is about 1 point higher than the previous best result.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ The best F-score of 90.90, which is about 1 point higher than the previous best result, is obtained with a combination of clusters.

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a distributed version of a much simpler K-Means clustering that allows us to cluster tens of millions of elements.

Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ The goal of query classification is to determine to which ones of a predefined set of classes a query belongs.
Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ Since the query classes are not mutually exclusive, we treat the query classification task as 67 binary classification problems.

 $$$$$ The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.
 $$$$$ The authors wish to thank the anonymous reviewers for their comments.

We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ In comparison, there are 79 templates in (Suzuki and Isozaki, 2008).
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ Part-of-speech tags were used in the topranked systems in CoNLL 2003, as well as in many follow up studies that used the data set (Ando and Zhang 2005; Suzuki and Isozaki 2008).

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a distributed version of a much simpler K-Means clustering that allows us to cluster tens of millions of elements.

K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ K-Means is an embarrassingly parallelizable algorithm.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ Although K-Means is generally described as a hard clustering algorithm (each element belongs to at most one cluster), it can produce soft clustering simply by assigning an element to all clusters whose similarity to the element is greater than a threshold.

Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ Given a set of elements represented as feature vectors and a number, k, of desired clusters, the K-Means algorithm consists of the following steps: i.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ Following previous approaches to distributional clustering of words, we represent the contexts of a phrase as a feature vector.

We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ Although K-Means is generally described as a hard clustering algorithm (each element belongs to at most one cluster), it can produce soft clustering simply by assigning an element to all clusters whose similarity to the element is greater than a threshold.
We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ Instead, any word above a minimum frequency threshold is included.

In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ In addition to word-clusters, we also use phraseclusters as features.
In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ Note that many of these queries are not phrases in the linguistic sense. language, english spanish language, learn foreign language, free english learning, language study english, spanish immersion course, how to speak french, spanish learning games, .....

Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ The Brown algorithm uses essentially the same information as our 1-word window clusters.

As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Phrase Clustering for Discriminative Learning
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Moreover, we use soft clustering instead of hard clustering.

 $$$$$ The most straightforward MapReduce implementation of K-Means would be to have mappers perform Step ii and reducers perform Step iii.
 $$$$$ The authors wish to thank the anonymous reviewers for their comments.
