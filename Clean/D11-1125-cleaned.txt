 $$$$$ We repeated each setting three times, generating different random data each time.
 $$$$$ Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.

RAMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. $$$$$ Throughout all experiments with PRO we choose Γ = 5000, Ξ = 50, and the following step function α for each αz

We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling. $$$$$ The results of the noisy synthetic experiments, but still The idea of learning from difference vectors also lies at the heart of the MIRA-based approaches (Watanabe et al., 2007; Chiang et al., 2008b) and the approach of Roth et al. (2010), which, similar to our method, uses sampling to select vectors.
We found similar fluctuations for the cdec implementations of PRO (Hopkins and May, 2011) or hyper graph-MERT (Kumar et al, 2009) both of which depend on hyper graph sampling. $$$$$ Lattice- and hypergraphbased variants of MERT (Macherey et al., 2008; Kumar et al., 2009) are more stable than traditional MERT, but also require significant engineering efforts.

Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. $$$$$ Tuning as Ranking
Additionally, we present Joshua's implementation of the pairwise ranking optimization (Hopkins and May, 2011) approach to translation model tuning. $$$$$ We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).

Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation. $$$$$ Tuning as Ranking
Pairwise ranking optimization (PRO) proposed by (Hopkins and May, 2011) is a new method for discriminative parameter tuning in statistical machine translation. $$$$$ We offer a simple, effective, and scalable method for statistical machine translation parameter tuning based on the pairwise approach to ranking (Herbrich et al., 1999).

 $$$$$ We repeated each setting three times, generating different random data each time.
 $$$$$ Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.

It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. $$$$$ In practice, tuning optimizes over a finite subset of source sentences3 and a finite subset of candidate translations as well.
It optimizes a logistic objective identical to that of PRO (Hopkins and May, 2011) with stochastic gradient descent, although other objectives are possible. $$$$$ Tillmann and Zhang (2005) used a customized form of multi-class stochastic gradient descent to learn feature weights for an MT model.

We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT. $$$$$ Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs.
We cast MT tuning as pairwise ranking (Herbrich et al, 1999, inter alia), which Hopkins and May (2011) applied to MT. $$$$$ We follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007).

Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. $$$$$ Tuning as Ranking
Introduced by Hopkins and May (2011), Pairwise Ranking Optimization (PRO) aims to handle large feature sets inside the traditional MERT architecture. $$$$$ Unlike the popular MERT algorithm (Och, 2003), our pairwise ranking optimization (PRO) method is not limited to a handful of parameters and can easily handle systems with thousands of features.

Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. $$$$$ Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations.
Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison. $$$$$ Xiong et al. (2006) also used a maximum entropy classifier, in this case to train the reordering component of their MT model.

We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011). $$$$$ For each language pair and each MT model we used MERT, MIRA, and PRO to tune with a standard set of baseline features, and used the latter two methods to tune with an extended set of features.8 At the end of every experiment we used the final feature weights to decode a held-out test set and evaluated it with case-sensitive BLEU.
We used sparse feature templates that are equivalent to the PBMT set described in (Hopkins and May, 2011). $$$$$ We used the MegaM classifier and sampled as described in Section 4.2.

Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. $$$$$ We trained a 5-gram English language model on the English side of the training data.
Feature weights were re-tuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. $$$$$ We trained a 4-gram English language model on the English side of the training data.

Hopkins and May (2011) presented a method that uses a binary classifier. $$$$$ It uses off-the-shelf linear binary classifier software and can be built on top of an existing MERT framework in a matter of hours.
Hopkins and May (2011) presented a method that uses a binary classifier. $$$$$ Throughout all experiments with PRO we choose Γ = 5000, Ξ = 50, and the following step function α for each αz

Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. $$$$$ Tuning as Ranking
Hopkins and May (2011) introduced the method of pairwise ranking optimization (PRO), which casts the problem of tuning as a ranking problem between pairs of translation candidates. $$$$$ Specifically, we follow the pairwise approach to ranking (Herbrich et al., 1999; Freund et al., 2003; Burges et al., 2005; Cao et al., 2007), in which the ranking problem is reduced to the binary classification task of deciding between candidate translation pairs.

Following Hopkins and May (2011), we used the following parameters for the sampling task $$$$$ For each source sentence i, the sampler generates F candidate translation pairs hj, j'i, and accepts each pair with probability αi(

 $$$$$ We repeated each setting three times, generating different random data each time.
 $$$$$ Thanks also to the anonymous reviewers, especially the reviewer who implemented PRO during the review period and replicated our results.

Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion. $$$$$ Tuning as Ranking
Like Hopkins and May (2011), we optimize ranking in n-best lists, but learn parameters in an online fashion. $$$$$ An advantage of MERT is that it can directly optimize for non-decomposable scoring functions like BLEU.

Unlike Hopkins and May (2011), we do not randomly sample from all the pairs in the n-best translations, but extract pairs by selecting one oracle translation and one other translation in the n-bests other than those in ORACLE. $$$$$ For each source sentence i, the sampler generates F candidate translation pairs hj, j'i, and accepts each pair with probability αi(

Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration. $$$$$ We would like to modify MERT so that it scales well to high-dimensionality candidate spaces.
Hopkins and May (2011) applied a MERT-like procedure in Alg 1 in which Equation 4 was solved to obtain new parameters in each iteration. $$$$$ While MERT and MIRA use each iteration’s final weights as a starting point for hill-climbing the next iteration, the pairwise ranking approach has no explicit tie to previous iterations.

Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. $$$$$ We obtained similar results using P = = 100, and for each a logistic sigmoid function centered at the mean g differential of candidate translation pairs for the ith source sentence.
Hopkins and May (2011) minimized logistic loss sampled from the merged n-bests, and sentence-BLEU was used for determining ranks. $$$$$ We used the MegaM classifier and sampled as described in Section 4.2.
