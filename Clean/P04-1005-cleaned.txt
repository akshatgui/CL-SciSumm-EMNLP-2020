These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004]. $$$$$ Testing was performed using data from the parsed version since this data is cleaner, and it enables a direct comparison with earlier work.
These steps result in an improvement of 43.98% percent relative error reduction in F-score over an earlier best result in edited detection when punctuation is included in both training and testing data [Charniak and Johnson 2001], and 20.44% percent relative error reduction in F-score over the latest best result where punctuation is excluded from the training and testing data [Johnson and Charniak 2004]. $$$$$ For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.

 $$$$$ At the end of each repair, a (possibly null) interregnum is appended to the reparandum.
 $$$$$ Since the two approaches seem to have different strengths, a combined model may outperform both of them.

These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. $$$$$ As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[01]*.mrg files.
These steps result in a significant improvement in F-score over the earlier best result reported in [Charniak and Johnson 2001], where punctuation is included in both the training and testing data of the Switchboard corpus, and a significant error reduction in F-score over the latest best result [Johnson and Charniak 2004], where punctuation is ignored in both the training and testing data of the Switchboard corpus. $$$$$ For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.

When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results. $$$$$ For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.
When compared with the latest results from [Johnson and Charniak 2004], where no punctuations are used for either training or testing data, we also observe the same trend of the improved results. $$$$$ Finally we show the results using the parser language model.

Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. $$$$$ There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system.
Noisy channel models have done well on the disfluency detection task in the past; the work of Johnson and Charniak (2004) first explores such an approach. $$$$$ It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of Charniak and Johnson (2001).

Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses. $$$$$ A TAG-Based Noisy-Channel Model Of Speech Repairs
Following Johnson and Charniak (2004), we use a noisy channel model to propose a 25-best list of possible speech disfluency analyses. $$$$$ We also provide results for our noisy channel model using a bigram language model and a second trigram model where the twenty most likely analyses are rescored.

Further details of the noisy channel model can be found in Johnson and Charniak (2004). $$$$$ A TAG-Based Noisy-Channel Model Of Speech Repairs
Further details of the noisy channel model can be found in Johnson and Charniak (2004). $$$$$ The noisy channel model using a bigram language model does a slightly worse job at identifying reparandum and interregnum words than the classifier proposed in Charniak and Johnson (2001).

To improve performance over the standard noisy channel model we use a re-ranker, as previously suggest by Johnson and Charniak (2004). $$$$$ A TAG-Based Noisy-Channel Model Of Speech Repairs
To improve performance over the standard noisy channel model we use a re-ranker, as previously suggest by Johnson and Charniak (2004). $$$$$ We measure model performance using standard precision p, recall r and f-score f, measures.

As Johnson and Charniak (2004) noted, although this model performs well, a log linear re-ranker can be used to increase performance. $$$$$ We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model.
As Johnson and Charniak (2004) noted, although this model performs well, a log linear re-ranker can be used to increase performance. $$$$$ So to increase processing speed we only compute analyses for strings of length 12 or less.

 $$$$$ At the end of each repair, a (possibly null) interregnum is appended to the reparandum.
 $$$$$ Since the two approaches seem to have different strengths, a combined model may outperform both of them.

In this work, we use a total of 62 variables, which include 16 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004), an additional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). $$$$$ These estimated probability distributions are the linear interpolation of the corresponding empirical distributions from the main sub-corpus using various subsets of conditioning variables (e.g., bigram models are mixed with unigram models, etc.) using Chenâ€™s bucketing scheme Chen and Goodman (1998).
In this work, we use a total of 62 variables, which include 16 variables from Charniak and Johnson (2001) and Johnson and Charniak (2004), an additional 29 variables from Zhang and Weng (2005), 11 hierarchical POS tag variables, and 8 prosody variables (labels and their confidence scores). $$$$$ For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.

Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al (2005). $$$$$ For comparison we include the results of running the word-by-word classifier described in Charniak and Johnson (2001), but where partial words and punctuation have been removed from the training and test data.
Because the edit region identification results on the original Switchboard are not directly comparable with the results on the newly segmented data, the state-of-art results reported by Charniak and Johnson (2001) and Johnson and Charniak (2004) are repeated on this new corpus by Kahn et al (2005). $$$$$ Finally we show the results using the parser language model.

Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004). $$$$$ A TAG-Based Noisy-Channel Model Of Speech Repairs
Speech is often disfluent, and speech repairs are known to repeat large portions of the preceding context (Johnson and Charniak, 2004). $$$$$ This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts.

The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak (2004). $$$$$ The corpus also includes punctuation and partial words, which are ignored in both training and evaluation here since we felt that in realistic applications these would not be available in speech recognizer output.
The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the filessw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and Charniak (2004). $$$$$ As mentioned earlier, following Charniak and Johnson (2001) our test data consisted of all Penn III Switchboard tree-bank sw4[01]*.mrg files.

The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations. $$$$$ In training we ignored all overlapping repairs (i.e., cases where the reparandum of one repair is the repair of another).
The TAG system (Johnson and Charniak, 2004) achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations. $$$$$ Since the weighted grammar just given does not generate the source string X, the score of the parse using the weighted TAG is P(Y

The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004). $$$$$ From our training data we estimate a number of conditional probability distributions.
The prior probability distributions over alignment operations is estimated from data in the Switchboard in a similar manner to Johnson and Charniak (2004). $$$$$ The other distributions are defined over aligned reparandum/repair strings, and are estimated from the aligned repairs extracted from the training data.

Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice. $$$$$ First, we demonstrate that using a syntactic parser-based language model Charniak (2001) instead of bi/trigram language models significantly improves the accuracy of repair detection and correction.
Given that state-of-the-art edit detection performs at about 80% f-measure (Johnson and Charniak, 2004), much of the benefit derived here from oracle repair detection should be realizable in practice. $$$$$ There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system.

The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements. $$$$$ A TAG-Based Noisy-Channel Model Of Speech Repairs
The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements. $$$$$ It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of Charniak and Johnson (2001).

The output of the JC04 model (Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. $$$$$ We use two language models here: a bigram language model, which is used in the search process, and a syntactic parser-based language model Charniak (2001), which is used to rescore a set of the most likely analysis obtained using the bigram model.
The output of the JC04 model (Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments. $$$$$ In our experiments below we extract the 20 most likely parses for each sentence.

The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al, 1992) data as described in (Johnson and Charniak, 2004). $$$$$ The model is trained and tested on the Switchboard disfluency-annotated corpus.
The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al, 1992) data as described in (Johnson and Charniak, 2004). $$$$$ We now describe how the weights on the TAG productions described in subsection 2.2 are estimated from this training data.
