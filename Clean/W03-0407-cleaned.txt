We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ Towards the end of the co-training run, more material is being selected for C&C than TNT.
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ The results are shown in Table 4.

paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ Although bootstrapping from unlabelled data is particularly valuable when only small amounts of training material are available, it is also interesting to see if selftraining or co-training can improve state of the art POS taggers.

Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ Both taggers were initialised with either 500 or 50 seed sentences, and agreement-based co-training was applied, using a cache size of 500 sentences.
Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ These results show no significant improvement using either self-training or co-training with very large seed datasets.

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ Bootstrapping POS-Taggers Using Unlabelled Data
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ However, our experiments are relevant for languages for which there is little or no annotated data.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ We see that naive co-training improves as the cache size increases.

Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ The ME tagger, which we refer to as C&C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003).
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ TNT is very fast for both training and tagging.

Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ Co-training using this much larger amount of unlabelled material did improve our previously mentioned results, but not by a large margin.

Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ Using standard sections of the WSJ Penn Treebank as seed data, we have been unable to improve the performance of the taggers using selftraining or co-training.

Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).

Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ The ME tagger, which we refer to as C&C, uses the same features as MXPOST, but is much faster for training and tagging (Curran and Clark, 2003).

For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.
For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.

Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective.

According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.
According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.

In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).

Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ For a large cache, the performance levels for naive co-training are very similar to those produced by our agreement-based co-training method.
Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.

(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ Figure 3 shows the results for self-training, in which each tagger is simply retrained on its own labelled cache at each round.
(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ We also performed a number of experiments using much more unlabelled training material than before.

However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ The results are shown in Table 4.

Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).

Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ In this paper we take a different approach, based on theoretical work by Dasgupta et al. (2002) and Abney (2002), in which newly labelled training examples are selected using a greedy algorithm which explicitly maximises the POS taggers’ agreement on unlabelled data.
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s).
