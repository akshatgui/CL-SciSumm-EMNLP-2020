The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.

The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ TINA: A Natural Language System For Spoken Language Applications
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ A spoken language system relies on its natural language component to provide the meaning representation of a given sentence.

As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ If the natural language component's computational and memory requirements are not excessive, and if it is organized in such a way that it can easily predict a set of next-word candidates, then it can be incorporated into the active search process of the recognizer, dynamically predicting possible words to follow a hypothesized word sequence, and pruning away hypotheses that cannot be completed in any way.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ This approach resembles the work by Grishman et al. (1986) and Hirschman et al.

Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ The results were essentially the same for the training and the test sentences, as shown in Table 2.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ Their speech was recorded in a simulation mode in which the speech recognition component was excluded.

The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ In addition to continued research on the transcription problem, i.e., the conversion of the speech signal to text, many researchers have begun to address as well the problem of speech understanding.1 This shift is at least partly brought on by the realization that many of the applications involving human/machine interface using speech require an &quot;understanding&quot; of the intended message.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ This is a problem to be aware of in building grammars from example sentences.

Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ Instead, an experimenter in a separate room typed in the utterances as spoken by the subject.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ We were able to collect a total, of nearly 5000 utterances in this fashion.

In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ TINA: A Natural Language System For Spoken Language Applications
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ We have not yet incorporated probabilities from TINA into the search, but they are used effectively to resort the final output sentence candidates.

We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ TINA is based on a context-free grammar augmented with a set of features used to enforce syntactic and semantic constraints.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The process of conversion to a new grammar involves parsing the new sentences one by one, and adding context-free rules whenever a parse fails.

The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ By encoding meaning in the structural entities of the parse tree, it becomes feasible to realize probabilistic semantic restrictions in an efficient manner.
The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ The speech material was then used to train the recognizer component, and the text material was used to train the natural language and back-end components.

The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept.

Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ Furthermore, the same [do-question] grammar node deals with the yes/no question &quot;Did Mike buy the pies?,&quot; except in this case there is no CURRENTFOCUS and hence no gap.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ This approach resembles the work by Grishman et al. (1986) and Hirschman et al.
