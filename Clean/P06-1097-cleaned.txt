Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ This is a small sampling of the kinds of knowledge sources we can use in this framework; many others have been proposed in the literature.
Also, several models are proposed to address the problem of improving generative models with small amount of manual data, including Model 6 (Och and Ney, 2003) and the model proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ See (Fraser and Marcu, 2006) for further details.

Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ See (Fraser and Marcu, 2006) for further details.
Fraser and Marcu (2006) propose an EMD algorithm, where labeled data is used for discriminative reranking. $$$$$ We hope that Minimum Error / Maximum Likelihood training using the EMD algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.

With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ See (Fraser and Marcu, 2006) for further details.
With the exception of Fraser and Marcu (2006), these previous publications do not entirely discard the generative models in that they integrate IBM model predictions as features. $$$$$ All of these also used knowledge from one of the IBM Models in order to obtain competitive results with the baseline (with the exception of (Moore, 2005)).

Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998).
Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). $$$$$ See (Fraser and Marcu, 2006) for further details.

 $$$$$ The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.
 $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.

EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ Table 7 evaluates the EMD semi-supervised training algorithm.
EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. $$$$$ See (Fraser and Marcu, 2006) for further details.

 $$$$$ The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.
 $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.

For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4.
For an alignment model, most of these use the Aachen HMM approach (Vogel et al, 1996), the implementation of IBM Model 4 in GIZA++ (Och and Ney, 2000) or, more recently, the semi-supervised EMD algorithm (Fraser and Marcu, 2006). $$$$$ Table 7 evaluates the EMD semi-supervised training algorithm.

 $$$$$ The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.
 $$$$$ We would like to thank the USC Center for High Performance Computing and Communications.

If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ These steps lead to word alignments of higher accuracy which, in our case, correlate with higher MT accuracy.
If human-aligned data is available, the EMD algorithm provides higher baseline alignments than GIZA++ that have led to better MT performance (Fraser and Marcu, 2006). $$$$$ See (Fraser and Marcu, 2006) for further details.

We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ Table 7 evaluates the EMD semi-supervised training algorithm.
We use the semi-supervised EMD algorithm (Fraser and Marcu, 2006b) to train the model. $$$$$ See (Fraser and Marcu, 2006) for further details.

We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ Semi-Supervised Training For Statistical Word Alignment
We compare semi-supervised LEAF with a previous state of the art semi-supervised system (Fraser and Marcu, 2006b). $$$$$ Table 7 evaluates the EMD semi-supervised training algorithm.

We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ The discriminatively reranked extended model outperforms four iterations of Model 4 in both cases with the best heuristic symmetrization, but some of the gain is lost as we are optimizing the F-measure of the 1-to-many alignments rather than the F-measure of the many-to-many alignments directly.
We ran the baseline semi-supervised system for two iterations (line 2), and in contrast with (Fraser and Marcu, 2006b) we found that the best symmetrization heuristic for this system was  union, which is most likely due to our use of fully linked alignments which was discussed at the end of Section 3. $$$$$ We also show the F-measure after heuristic symmetrization of the alignment test sets.

(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ We reinterpret the five groups of parameters of Model 4 listed in the first five lines of Table 3 as sub-models of a log-linear model (see Equation 1).
(Fraser and Marcu, 2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model. $$$$$ Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion.

Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ This approximation is called Viterbi training.
Examples of this line of research include Model 6 (OchandNey, 2003) and the EMD training approach proposed by Fraser and Marcu (2006) and its extension called LEAF aligner (Fraser and Marcu, 2007). $$$$$ See (Fraser and Marcu, 2006) for further details.

Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ We reinterpret the five groups of parameters of Model 4 listed in the first five lines of Table 3 as sub-models of a log-linear model (see Equation 1).
Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. $$$$$ Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion.

A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ We use the “union”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment.
A super set of the parallel data was word aligned by GIZA union (Och and Ney, 2003) and EMD (Fraser and Marcu, 2006). $$$$$ See (Fraser and Marcu, 2006) for further details.

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ See (Fraser and Marcu, 2006) for further details.

Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.
Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. $$$$$ See (Fraser and Marcu, 2006) for further details.

(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.
(Fraser and Marcu, 2006) have proposed an algorithm for doing word alignment which applies a discriminative step at every iteration of the traditional Expectation-Maximization algorithm used in IBM models. $$$$$ See (Fraser and Marcu, 2006) for further details.
