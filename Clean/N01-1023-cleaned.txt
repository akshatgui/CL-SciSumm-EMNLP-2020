Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ The current crop of statistical parsers share a similar training methodology.
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.

In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures.
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)).

The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. $$$$$ • In our experiments, unlike (Blum and Mitchell, 1998) we do not balance the label priors when picking new labeled examples for addition to the training data.
The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostly unsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. $$$$$ In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data.

Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. $$$$$ Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.
Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. $$$$$ We used EVALB (written by Satoshi Sekine and Michael Collins) which scores based on PARSEVAL (Black et al., 1991); with the standard parameter file (as per standard practice, part of speech brackets were not part of the evaluation).

 $$$$$ Since both of these steps ultimately have to agree with each other, we can utilize an iterative method called CoTraining that attempts to increase agreement between a pair of statistical models by exploiting mutual constraints between their output.
 $$$$$ We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilistic treatment.

Co-traininghas been successfully applied to various applications, such as statistical parsing (Sarkar, 2001). $$$$$ Applying Co-Training Methods To Statistical Parsing
Co-traininghas been successfully applied to various applications, such as statistical parsing (Sarkar, 2001). $$$$$ (Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications.

Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ In this paper, we use one such machine learning technique: Co-Training, which has been used successfully in several classification tasks like web page classification, word sense disambiguation and named-entity recognition.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)).

(Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. $$$$$ Applying Co-Training Methods To Statistical Parsing
(Sarkar, 2001) applied co-training to statistical parsing, where two component models are trained and the most confident parsing outputs of the existing model are incorporated into the next training. $$$$$ We propose a novel Co-Training method for statistical parsing.

Sarkar (2001) and Steedman et al (2003 ) investigated using co-training for parsing. $$$$$ In the representation we use, parsing using a lexicalized grammar is done in two steps: Each of these two steps involves ambiguity which can be resolved using a statistical model.
Sarkar (2001) and Steedman et al (2003 ) investigated using co-training for parsing. $$$$$ These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.

Sarkar (2001) applied co-training to LTAG parsing, in which the super tagger and parser provide the two views. $$$$$ Their definition of Co-Training includes the notion (exploited in this paper) that different models can constrain each other by exploiting different ‘views’ of the data.
Sarkar (2001) applied co-training to LTAG parsing, in which the super tagger and parser provide the two views. $$$$$ These results show that training a statistical parser using our Co-training method to combine labeled and unlabeled data strongly outperforms training only on the labeled data.

Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). $$$$$ We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domain of statistical parsing.
Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). $$$$$ Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and namedentity identification (Collins and Singer, 1999).

The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. $$$$$ Applying Co-Training Methods To Statistical Parsing
The iterative training procedure proposed in this work shares some similarity with the co-training algorithm in parsing (Sarkar, 2001), where the training procedure lets two different models learn from each other during parsing the raw text. $$$$$ The two-step procedure used in our Co-Training method for statistical parsing was incipient in the SuperTagger (Srinivas, 1997) which is a statistical model for tagging sentences with elementary lexicalized structures.
