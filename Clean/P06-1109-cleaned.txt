While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ An All-Subtrees Approach To Unsupervised Parsing
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as Data-Oriented Parsing (DOP).

Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ On Penn Wall Street Journal po-s-strings â‰¤ 10 (WSJ10), Klein and Manning (2002) report 71.1% unlabeled f-score with CCM.
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).

While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ U-DOP extends DOP1 to unsupervised parsing (Bod 2006).
While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).

We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ (Zollmann and Sima'an 2005 propose a different consistent estimator for DOP, which we cannot go into here).

This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ U-DOP therefore randomly samples a large subset from the total number of parse trees from the chart (see Bod 2006) and next converts the subtrees from these parse trees into a PCFG-reduction (Goodman 2003).
This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).

Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ Unfortunately, no compact PCFG-reduction of MLDOP is known.
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ Note that UML-DOP achieves these improved results with fewer subtrees than U-DOP, due to UML-DOP's more drastic pruning of subtrees.

To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ To compare UML-DOP to U-DOP, we started out with the WSJ10 corpus, which contains 7422 sentences <_ 10 words after removing empty elements and punctuation.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ 2002) both containing 2200+ sentences <_ 10 words after removing punctuation.

All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ To this end, we used a random 90%/10% division of WSJ40 into a training set and a test set.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ The ML-PCFG had thus access to the Penn WSJ trees in the training set, while UML-DOP had to bootstrap all structure from the flat strings from the training set to next parse the 10% test set -- clearly a much more challenging task.

Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.

Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ For these splits, UML-DOP achieved an average f-score of 66.9%, while ML-PCFG obtained an average f-score of 64.7%.
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40.

While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ U-DOP extends DOP1 to unsupervised parsing (Bod 2006).
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ After computing the most probable parse trees, UML-DOP achieved an f-score of 82.9% which is a 20.5% error reduction compared to U-DOP's f-score of 78.5% on the same data (Bod 2006).

DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ Of course, if we only had the sentence Investors suffered heavy losses in our corpus, there would be no difference in probability between the five parse trees in figure 4.

Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP.
Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ As initial probabilities we use the subtrees' relative frequencies as described in section 2 (smoothed by Good-Turing -- see Bod 1998), though it would also be interesting to see how the model works with other initial parameters, in particular with the usage frequencies proposed by Zuidema (2006).

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.

Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.

Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approach known as Data-Oriented Parsing (DOP).
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ U-DOP extends DOP1 to unsupervised parsing (Bod 2006).

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.

Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Table 1 shows the results of UML-DOP compared to U-DOP, the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) as well as their combined model DMV+CCM.

For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ Bod (2006) reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.
For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ The extension to word strings is straightforward as there exist highly accurate unsupervised part-of-speech taggers (e.g.

Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ An All-Subtrees Approach To Unsupervised Parsing
Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ U-DOP extends DOP1 to unsupervised parsing (Bod 2006).
