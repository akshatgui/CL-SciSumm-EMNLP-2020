A number of studies are related to the work we presented, most specifically work on parallel-text based "information projection" for parsing (Hwa et al., 2002), but also grammar induction work based on constituent/distituent information (Klein and Manning, 2002) and (language-internal) alignment based learning (van Zaanen, 2000). $$$$$ In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b).
A number of studies are related to the work we presented, most specifically work on parallel-text based "information projection" for parsing (Hwa et al., 2002), but also grammar induction work based on constituent/distituent information (Klein and Manning, 2002) and (language-internal) alignment based learning (van Zaanen, 2000). $$$$$ To the extent that such approaches work, they work because good local heuristics have been engineered (Klein and Manning, 2001a; Clark, 2001).

Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. $$$$$ A Generative Constituent-Context Model For Improved Grammar Induction
Empirically, our algorithm performs favorably compared to the constituent context model of Klein and Manning (2002) without the need for careful initialization. $$$$$ First, random initialization is not always good, or necessary.

We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). $$$$$ A Generative Constituent-Context Model For Improved Grammar Induction
We primarily compare our method to the constituent-context model (CCM) of Klein and Manning (2002). $$$$$ CCM is our system, as described above.

CCM is used with the initializer proposed in Klein and Manning (2002). $$$$$ This distribution was not used in the model itself, however.
CCM is used with the initializer proposed in Klein and Manning (2002). $$$$$ CCM is our system, as described above.

The EM algorithm with the CCM requires very careful initialization, which is described in Klein and Manning (2002). $$$$$ First, random initialization is not always good, or necessary.
The EM algorithm with the CCM requires very careful initialization, which is described in Klein and Manning (2002). $$$$$ CCM is our system, as described above.

Empirically, our algorithm performs favorably to the CCM of Klein and Manning (2002) without the need for careful initialization. $$$$$ First, random initialization is not always good, or necessary.
Empirically, our algorithm performs favorably to the CCM of Klein and Manning (2002) without the need for careful initialization. $$$$$ CCM is our system, as described above.

Finally, there are "unsupervised" strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). $$$$$ In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b).
Finally, there are "unsupervised" strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002). $$$$$ On the right, sequences have been labeled according to whether their occurrences are constituents more or less of the time than a cutoff (of 0.2).

When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ tree bank together with additional WSJ newswire (Klein and Manning,2002). $$$$$ Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input.
When Klein and Manning induce the parts-of-speech, they do so from a much larger corpus containing the full WSJ tree bank together with additional WSJ newswire (Klein and Manning,2002). $$$$$ We will induce trees by inducing tree-equivalent bracketings.

An excellent recent result is by Klein and Manning (2002). $$$$$ Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input.
An excellent recent result is by Klein and Manning (2002). $$$$$ The distituents must necessarily outnumber the constituents, and so such distributional clustering will result in mostly distituent classes.

We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. $$$$$ This grammar, which we refer to as DEP-PCFG will be evaluated in more detail in section 4.
We refer readers to Klein and Manning (2002) or Cover and Thomas (1991, p. 72) for details; computing expected counts for a sentence is a closed form operation. $$$$$ The completions (bracketings) cannot be efficiently enumerated, and so a cubic dynamic program similar to the inside-outside algorithm is used to calculate the expected counts of each yield and context, both as constituents and distituents.

The third line corresponds to the setup reported by Klein and Manning (2002). $$$$$ Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input.
The third line corresponds to the setup reported by Klein and Manning (2002). $$$$$ A bracketing is binary if it corresponds to a binary tree.

We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags. $$$$$ We followed this for most experiments, but in section 4.3, we use distributionally induced tags as input.
We implement the baseline system, which Klein and Manning (2002) use for their grammar induction experiments with induced part-of-speech tags. $$$$$ Figure 8 shows the performance with induced tags compared to correct tags.

We follow Klein and Manning (2002) in using K means to cluster the d dimensional word vectors into parts-of-speech. $$$$$ Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input.
We follow Klein and Manning (2002) in using K means to cluster the d dimensional word vectors into parts-of-speech. $$$$$ The resulting vectors were clustered into 200 word classes by a weighted k-means algorithm, and then grammar induction operated over these classes.

We chose the baseline system primarily to match previous evaluations of grammar induction using induced tags (Klein and Manning, 2002). $$$$$ Nevertheless, using these tags as input still gave induced structure substantially above right-branching.
We chose the baseline system primarily to match previous evaluations of grammar induction using induced tags (Klein and Manning, 2002). $$$$$ Figure 8 shows the performance with induced tags compared to correct tags.

Klein and Manning (2002) present a generative model for inducing constituent boundaries from part-of-speech tagged text. $$$$$ A Generative Constituent-Context Model For Improved Grammar Induction
Klein and Manning (2002) present a generative model for inducing constituent boundaries from part-of-speech tagged text. $$$$$ We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.

We evaluate induced constituency trees against those of the Penn Treebank using the versions of unlabeled precision, recall, and F-score used by Klein and Manning (2002). $$$$$ By the F1 measure used in the experiments in section 4, an induced dependency PCFG scores 48.2, compared to a score of 82.1 for a supervised PCFG read from local trees of the treebank.
We evaluate induced constituency trees against those of the Penn Treebank using the versions of unlabeled precision, recall, and F-score used by Klein and Manning (2002). $$$$$ Evaluation was done by measuring unlabeled precision, recall, and their harmonic mean F1 against the treebank parses.

Evaluation of the algorithm is done according to PARSEVAL, except for a few changes that are also proposed by Klein and Manning (2002). $$$$$ We now essentially have our induction algorithm.
Evaluation of the algorithm is done according to PARSEVAL, except for a few changes that are also proposed by Klein and Manning (2002). $$$$$ Evaluation was done by measuring unlabeled precision, recall, and their harmonic mean F1 against the treebank parses.

Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ Klein and Manning (2001b) and Clark (2001) take treebank part-of-speech sequences as input.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ The induction algorithm combines the benefits of EM-based parameter search and distributional clustering methods.

To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. $$$$$ Here, we improve on that model in several ways.
To improve the quality of the induced trees, we combine our PCFG induction with the CCM model of Klein and Manning (2002), which has complementary stengths: it identifies brackets but does not label them. $$$$$ The top row of figure 8 shows the recall of nontrivial brackets, split according the bracketsâ€™ labels in the treebank.

Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a high quality bracketing model. $$$$$ Especially since the CCM does not model recursive structure explicitly, one might be concerned that the high overall accuracy is due to a high accuracy on short-span constituents.
Finally, we intersect the feature-augmented PCFG with the CCM model of Klein and Manning (2002), a high quality bracketing model. $$$$$ The conditional model of Klein and Manning (2001b) had the drawback that the variance of final F1, and qualitative grammars found, was fairly high, depending on small differences in first-round random parses.
