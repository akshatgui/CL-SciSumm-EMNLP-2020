 $$$$$ The basic keyword search approach retrieves all documents which contain some or all of the query terms.
 $$$$$ Thanks are due to the anonymous reviewers for their invaluable comments; Masao Utiyama and Hitoshi Isahara for providing the U00 algorithm and detailed results; Marti Hearst for guidance on the evaluation problem; Mary McGee Wood for support and HCRC for making this work possible.

PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al, 2001), and offers a more solid statistical foundation. $$$$$ Latent Semantic Analysis For Text Segmentation
PLSA is the probabilistic variant of latent semantic analysis (LSA) (Choi et al, 2001), and offers a more solid statistical foundation. $$$$$ Inter-sentence similarity is estimated by latent semantic analysis (LSA).

Choi at al. used LSA for segmentation (Choi et al., 2001). $$$$$ Segmentation accuracy is measured by the metric proposed in (Beeferman et al., 1999).
Choi at al. used LSA for segmentation (Choi et al., 2001). $$$$$ C99 (Choi, 2000a) was used as the test bench.

(Choi et al, 2001) used all vocabulary words to compute low-dimensional document vectors. $$$$$ In other words, the first k columns of U, or Ak, is the best approximation of BBT in k—dimensional space.
(Choi et al, 2001) used all vocabulary words to compute low-dimensional document vectors. $$$$$ Ak is the k—dimensional LSA space for A.

(Choi et al, 2001) used clustering to predict boundaries whereas we used the average similarity scores. $$$$$ Existing algorithms used a sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998; Utiyama and Isahara, 2001), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994; Choi, 2000a) to determine the optimal segmentation.
(Choi et al, 2001) used clustering to predict boundaries whereas we used the average similarity scores. $$$$$ C99 (Choi, 2000a) was used as the test bench.

It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al, 1995) and text segmentation (Choi et al, 2001) and, more recently, multi and single-document summarization. $$$$$ Features are combined using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995), probabilistic models (Hajime et al., 1998) and maximum entropy models (Beeferman et al., 1997; Reynar, 1998).
It is a very powerful technique already used for NLP applications such as information retrieval (Berry et al, 1995) and text segmentation (Choi et al, 2001) and, more recently, multi and single-document summarization. $$$$$ Segmentation accuracy is measured by the metric proposed in (Beeferman et al., 1999).

In CWM (Choi et al, 2001), a variant of C99, each word of a sentence is replaced by its representation in a Latent Semantic Analysis (LSA) space. $$$$$ Latent Semantic Analysis For Text Segmentation
In CWM (Choi et al, 2001), a variant of C99, each word of a sentence is replaced by its representation in a Latent Semantic Analysis (LSA) space. $$$$$ Inter-sentence similarity is estimated by latent semantic analysis (LSA).

 $$$$$ The basic keyword search approach retrieves all documents which contain some or all of the query terms.
 $$$$$ Thanks are due to the anonymous reviewers for their invaluable comments; Masao Utiyama and Hitoshi Isahara for providing the U00 algorithm and detailed results; Marti Hearst for guidance on the evaluation problem; Mary McGee Wood for support and HCRC for making this work possible.

While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al, 2001)) and synchronous dialogs (e.g., (Galley et al, 2003), (Hsueh et al, 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). $$$$$ Features are combined using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995), probabilistic models (Hajime et al., 1998) and maximum entropy models (Beeferman et al., 1997; Reynar, 1998).
While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al, 2001)) and synchronous dialogs (e.g., (Galley et al, 2003), (Hsueh et al, 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). $$$$$ Segmentation accuracy is measured by the metric proposed in (Beeferman et al., 1999).

The CWM algorithm (Choi et al, 2001) applies the same procedure to a similarity matrix of LSI vectors. $$$$$ The input matrix X can either be the similarity matrix M or the rank matrix R, depending on whether ranking is applied to M. Topic boundaries are identified by the divisive clustering procedure in C99.
The CWM algorithm (Choi et al, 2001) applies the same procedure to a similarity matrix of LSI vectors. $$$$$ Our new algorithm, CWM, was used in this experiment.

As a preliminary test of the error measure, I evaluated two algorithms from Choi et al (2001) on the standard segmentation data set that Choi (2000) compiled. $$$$$ Test results show LSA is a more accurate similarity measure than the
As a preliminary test of the error measure, I evaluated two algorithms from Choi et al (2001) on the standard segmentation data set that Choi (2000) compiled. $$$$$ C99 and C99b are the algorithms described in (Choi, 2000a).

The C99 (Choi, 2000) and CWM (Choi et al, 2001) algorithms were evaluated. $$$$$ C99 and C99b are the algorithms described in (Choi, 2000a).
The C99 (Choi, 2000) and CWM (Choi et al, 2001) algorithms were evaluated. $$$$$ C99 (Choi, 2000a) was used as the test bench.

Because of these differences, the implementation of HCWM reported here differs somewhat from the implementation of CWM reported by Choi et al (2001). $$$$$ For implementation details and optimisations, see (Choi, 2000a).
Because of these differences, the implementation of HCWM reported here differs somewhat from the implementation of CWM reported by Choi et al (2001). $$$$$ This implementation of C99 has three parameters.

(12.5%) matches what Choi et al (2001) reported (12%), while the error for HCWM (12.1%) is higher than that reported for the version with a paragraph-based 500-dimension LSI space (9%) but appears comparable to their sentence-based 400-dimension LSI space. $$$$$ The values {100, 200, 300, 400, 500} represent the dimensionality of the trained space.
(12.5%) matches what Choi et al (2001) reported (12%), while the error for HCWM (12.1%) is higher than that reported for the version with a paragraph-based 500-dimension LSI space (9%) but appears comparable to their sentence-based 400-dimension LSI space. $$$$$ For instance, &quot;p, 400&quot; is a 400-dimensional space that was trained on paragraphs.

And the result for NONE (46.1%) agrees with Choi et al (2001)'s results for their NONE (46%) base line. $$$$$ Features are combined using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995), probabilistic models (Hajime et al., 1998) and maximum entropy models (Beeferman et al., 1997; Reynar, 1998).
And the result for NONE (46.1%) agrees with Choi et al (2001)'s results for their NONE (46%) base line. $$$$$ The significance of our results has been confirmed by both t-test and KS-test (Press et al., 1992).

In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al, 2001). $$$$$ LSA has been shown to match human similarity judgements on a wide range of tasks (Landauer and Dumais, 1997; Wolfe et al., 1998; Wiemer-Hastings et al., 1999, for example).
In addition, LSA has been applied to a number of NLP tasks, such as text segmentation (Choi et al, 2001). $$$$$ B? serves as the baseline for methods that determines the optimal segmentation, i.e. the number of topic segments in a text.
