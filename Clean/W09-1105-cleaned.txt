Our goal was to investigate the performance of a memory-based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al, 2008) or negation scope detection (Morante and Daelemans, 2009). $$$$$ The approach to the treatment of negation in NLP presented in this paper was introduced in Morante et al. (2008).
Our goal was to investigate the performance of a memory-based approach to the event extraction task, using only the information available in the training corpus and modelling the task applying an approach similar to the one that has been applied to tasks like semantic role labeling (Morante et al, 2008) or negation scope detection (Morante and Daelemans, 2009). $$$$$ The system has been developed using the BioScope corpus (Szarvas et al., 2008; Vincze et al., 2008)1, a freely available resource that consists of medical and biological texts.

In keeping with the evaluation presented by Morante and Daelemans (2009), the number of perfectly identified negation scopes is measured separately as the percentage of correct scopes (PCS). $$$$$ This system achieved a 50.05 percentage of correct scopes but had a number of important shortcomings.
In keeping with the evaluation presented by Morante and Daelemans (2009), the number of perfectly identified negation scopes is measured separately as the percentage of correct scopes (PCS). $$$$$ Additionally, we evaluate the percentage of correct scopes (PCS).

The best reported performance to date on the BioScope full papers corpus was presented by Morante and Daelemans (2009), who achieved an F1 score of 70.9 with predicted negation signals, and an F1 score of 84.7 by feeding the manually annotated negation cues to their scope finding system. $$$$$ In the corpus, every sentence is annotated with information about negation and speculation.
The best reported performance to date on the BioScope full papers corpus was presented by Morante and Daelemans (2009), who achieved an F1 score of 70.9 with predicted negation signals, and an F1 score of 84.7 by feeding the manually annotated negation cues to their scope finding system. $$$$$ Table 7 also shows that, except for can not, all negation signals score a lower PCS on the papers subcorpus.

Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). $$$$$ We use IGTREE as implemented in TiMBL (version 6.1.2) (Daelemans et al., 2007).
Morante and Daelemans describe a method for improving resolution of the scope of negation by combining IGTREE, CRF, and Support Vector Machines (SVM) (Morante and Daelemans, 2009). $$$$$ The fourth classifier, a metalearner, is also a CRF as implemented in CRF++.

The effect of negation has been broadly studied in NLP (Morante and Daelemans, 2009) and sentiment analysis (Jia et al, 2009). $$$$$ The approach to the treatment of negation in NLP presented in this paper was introduced in Morante et al. (2008).
The effect of negation has been broadly studied in NLP (Morante and Daelemans, 2009) and sentiment analysis (Jia et al, 2009). $$$$$ We use IGTREE as implemented in TiMBL (version 6.1.2) (Daelemans et al., 2007).

Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. $$$$$ Feature selection experiments were carried out with the memory-based learning classifier.
Still, Morante and Daelemans (2009), for example, used various classifiers (Memory-based Learners, Support Vector Machines, and Conditional Random Fields) to detect negation cues and their scope. $$$$$ Typical example sentences with this negation signal are shown in (4).

Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. $$$$$ Machine learning techniques have been used in some cases.
Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. $$$$$ The two classification tasks (identifying negation signals and finding the scope) are implemented using supervised machine learning methods trained on part of the annotated corpus.

Compared with negation scope finding, negation signal finding is much simpler and has been well resolved in the literature, e.g. with the accuracy of 95.8% -98.7% on the three sub corpora of the Bioscope corpus (Morante and Daelemans, 2009). $$$$$ Finding the scope of a negation signal means determining at sentence level the sequence of words in the sentence that is affected by the negation.
Compared with negation scope finding, negation signal finding is much simpler and has been well resolved in the literature, e.g. with the accuracy of 95.8% -98.7% on the three sub corpora of the Bioscope corpus (Morante and Daelemans, 2009). $$$$$ In the negation finding task, a negation token is correctly classified if it has been classified as being at the beginning or inside the negation signal.

Morante et al (2008) and Morante and Daelemans (2009) pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal. $$$$$ Finding the scope of a negation signal means determining at sentence level the sequence of words in the sentence that is affected by the negation.
Morante et al (2008) and Morante and Daelemans (2009) pioneered the research on negation scope finding by formulating it as a chunking problem, which classifies the words of a sentence as being inside or outside the scope of a negation signal. $$$$$ In the scope finding task, a token is correctly classified if it has been correctly classified as being inside or outside of the scope of all the negation signals that there are in the sentence.

For example, given golden negation signals on the Bioscope corpus, Morante and Daelemans (2009) only got the performance of 50.26% in PCS (percentage of correct scope) measure on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS measure on the clinical reports subcorpus (6.6 words per sentence on average). $$$$$ An alternative to the PCS-2 measure would be to mark in the corpus the relevant negated content words and evaluate if they are under the scope.
For example, given golden negation signals on the Bioscope corpus, Morante and Daelemans (2009) only got the performance of 50.26% in PCS (percentage of correct scope) measure on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS measure on the clinical reports subcorpus (6.6 words per sentence on average). $$$$$ This difference can not be caused by the sentence length, since the average sentence length in the abstracts subcorpus (26.43 tokens) is similar to the average sentence length in the papers subcorpus (26.24).

Morante and Daelemans (2009) further improved the performance by combing several classifiers. $$$$$ However, the classifiers only predict the first and last element of the scope.
Morante and Daelemans (2009) further improved the performance by combing several classifiers. $$$$$ The architecture of the system is new for this problem, with three classifiers and a metalearner that takes as input the output of the first classifiers.

For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). $$$$$ Table 1 shows statistics about the corpora.
For detailed statistics about the three subcorpora, please see Morante and Daelemans (2009). $$$$$ We take the scope to the right for the baseline because it is much more frequent than the scope to the left, as is shown by the statistics contained in Table 1 of Section 3.

Following the experimental setting in Morante and Daelemans (2009), the abstracts sub corpus is randomly divided into 10 folds so as to perform 10-fold cross validation, while the performance on both the papers and clinical reports subcorpora is evaluated using the system trained on the whole abstracts subcorpus. $$$$$ The results provided for the abstracts part of the corpus have been obtained by performing 10-fold cross validation experiments, whereas the results provided for papers and clinical reports have been obtained by training on the full abstracts subcorpus and testing on the papers and clinical reports subcorpus.
Following the experimental setting in Morante and Daelemans (2009), the abstracts sub corpus is randomly divided into 10 folds so as to perform 10-fold cross validation, while the performance on both the papers and clinical reports subcorpora is evaluated using the system trained on the whole abstracts subcorpus. $$$$$ One factor is the length of sentences

Although Morante and Daelemans (2009) reported the performance of 95.8%-98.7% on negation signal finding, it lowers the performance of negation scope finding by about 7.29%-16.52% in PCS measure. $$$$$ Finding the scope of a negation signal means determining at sentence level the sequence of words in the sentence that is affected by the negation.
Although Morante and Daelemans (2009) reported the performance of 95.8%-98.7% on negation signal finding, it lowers the performance of negation scope finding by about 7.29%-16.52% in PCS measure. $$$$$ In the negation finding task, a negation token is correctly classified if it has been classified as being at the beginning or inside the negation signal.

 $$$$$ Golding and Chapman (2003) experiment with Naive Bayes and Decision Trees to distinguish whether a medical observation is negated by the word not in a corpus of hospital reports.
 $$$$$ We are grateful to four anonymous reviewers for their valuable comments and suggestions.

For example, given golden negation cues on the Bioscope corpus, Morante and Daelemans (2009a) only got the performance of 50.26% in PCS on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS on the clinical reports sub corpus (6.6 words per sentence on average). $$$$$ The average length of a sentence for clinical reports is 7.73 tokens, whereas for abstracts it is 26.43 and for papers 26.24.
For example, given golden negation cues on the Bioscope corpus, Morante and Daelemans (2009a) only got the performance of 50.26% in PCS on the full papers sub corpus (22.8 words per sentence on average), compared to 87.27% in PCS on the clinical reports sub corpus (6.6 words per sentence on average). $$$$$ This difference can not be caused by the sentence length, since the average sentence length in the abstracts subcorpus (26.43 tokens) is similar to the average sentence length in the papers subcorpus (26.24).

Speculation Scope Learning Similar to Morante and Daelemans (2009a), Morante and Daelemans (2009b) formulated speculation scope identification as a chunking problem which predicts whether a word in the sentence is inside or outside of the speculation scope, with proper post-processing to ensure consecutiveness of the speculation scope. $$$$$ In the corpus, every sentence is annotated with information about negation and speculation.
Speculation Scope Learning Similar to Morante and Daelemans (2009a), Morante and Daelemans (2009b) formulated speculation scope identification as a chunking problem which predicts whether a word in the sentence is inside or outside of the speculation scope, with proper post-processing to ensure consecutiveness of the speculation scope. $$$$$ In the scope finding task, a token is correctly classified if it has been correctly classified as being inside or outside of the scope of all the negation signals that there are in the sentence.

 $$$$$ Golding and Chapman (2003) experiment with Naive Bayes and Decision Trees to distinguish whether a medical observation is negated by the word not in a corpus of hospital reports.
 $$$$$ We are grateful to four anonymous reviewers for their valuable comments and suggestions.

 $$$$$ Golding and Chapman (2003) experiment with Naive Bayes and Decision Trees to distinguish whether a medical observation is negated by the word not in a corpus of hospital reports.
 $$$$$ We are grateful to four anonymous reviewers for their valuable comments and suggestions.

 $$$$$ Golding and Chapman (2003) experiment with Naive Bayes and Decision Trees to distinguish whether a medical observation is negated by the word not in a corpus of hospital reports.
 $$$$$ We are grateful to four anonymous reviewers for their valuable comments and suggestions.
