Finally, we use a POS tagger trained on tweets (Gimpel et al, 2011) to perform POS tagging on the tweets data and apart from the previously recognised named entities, only words tagged with nouns, verbs or adjectives are kept. $$$$$ Our contributions are as follows

However, POS taggers for Twitter are only available for a limited number of languages such as English (Gimpel et al, 2011). $$$$$ Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993).
However, POS taggers for Twitter are only available for a limited number of languages such as English (Gimpel et al, 2011). $$$$$ In this paper, we produce an English POS tagger that is designed especially for Twitter data.

Table 3 $$$$$ Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.
Table 3 $$$$$ The results are shown in Table 2.

Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text. $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
Our POS results, gathered using a Twitter-specific tagger (Gimpel et al, 2011), echo those of Ashok et al (2013) who looked at predict 14 Of course, simply inserting garbage isn't going to lead to more re-tweets, but adding more information generally involves longer text. $$$$$ Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.

The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011). $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
The training and testing data are run through tweet-specific tokenization, similar to that used in the CMU Twitter NLP tool (Gimpel et al, 2011). $$$$$ Another tag, —, is used for tokens marking specific Twitter discourse functions.

This poses considerable problems for traditional NLP tools, which we redeveloped with other domains in mind, which of ten make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you). One approach to cope with this problem is to annotate in-domain data (Gimpel et al, 2011). $$$$$ We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter.
This poses considerable problems for traditional NLP tools, which we redeveloped with other domains in mind, which of ten make strong assumptions about orthographic uniformity (i.e., there is just one way to spell you). One approach to cope with this problem is to annotate in-domain data (Gimpel et al, 2011). $$$$$ Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”).

More specifically, we had lay annotators on the crowd sourcing platform Crowdflower re-annotate the training section of Gimpel et al (2011). $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
More specifically, we had lay annotators on the crowd sourcing platform Crowdflower re-annotate the training section of Gimpel et al (2011). $$$$$ Finally, we note that, even though 1,000 training examples may seem small, the test set accuracy when training on only 500 tweets drops to 87.66%, a decrease of only 1.7% absolute.

We crowd source the training section of the data from Gimpel et al (2011) 2 with POS tags. $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
We crowd source the training section of the data from Gimpel et al (2011) 2 with POS tags. $$$$$ Most of our tags are refinements of those categories, which in turn are groupings of PTB WSJ tags (see column 2 of Table 1).

Recognizing the limitations of existing systems, Gimpel et al (2011) develop a POS tagger specifically for Twitter, by creating a training corpus as well as devising a tag set that includes parts of speech that are uniquely found in on line language, such as emoticons (smilies). $$$$$ Our contributions are as follows

In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach. $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
In this work, we focus on Twitter because the labeled corpus by Gimpel et al (2011) allows us to quantitatively evaluate our approach. $$$$$ Our contributions are as follows

We then present POS tagging results on the Twitter POS dataset (Gimpel et al, 2011). $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
We then present POS tagging results on the Twitter POS dataset (Gimpel et al, 2011). $$$$$ Our contributions are as follows

Our data is the recent Twitter POS dataset released at ACL 2011 by Gimpel et al (2011) consisting of approximately 26,000 words across 1,827 tweets. $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
Our data is the recent Twitter POS dataset released at ACL 2011 by Gimpel et al (2011) consisting of approximately 26,000 words across 1,827 tweets. $$$$$ A total of 2,217 tweets were distributed to the annotators in this stage; 390 were identified as non-English and removed, leaving 1,827 annotated tweets (26,436 tokens).

The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011). $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
The Twitter dataset uses a domain-dependent tag set of 25 tags that are described in (Gimpel et al, 2011). $$$$$ Our tagger with the full feature set achieves a relative error reduction of 25% compared to the Stanford tagger.

For this experiment, we again make use of the Twitter POS dataset (Gimpel et al, 2011). $$$$$ The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011).
For this experiment, we again make use of the Twitter POS dataset (Gimpel et al, 2011). $$$$$ Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.

We tokenize each tweet with Twitter NLP (Gimpel et al, 2011), remove the @ user and URLs of each tweet, and filter the tweets that are too short (< 7 words). $$$$$ Thus, we sought to design a coarse tagset that would capture standard parts of speech3 (noun, verb, etc.) as well as categories for token varieties seen mainly in social media

A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003). $$$$$ Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O’Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.
A tweet-specific tokenizer (Gimpel et al, 2011) is employed, and the dependency parsing results are computed by Stanford Parser (Klein and Manning, 2003). $$$$$ The results are shown in Table 2.
