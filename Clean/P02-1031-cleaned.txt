In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). $$$$$ In this paper, we quantify the effect of parser accuracy on these systems' performance, and examine the question of whether a flatter &quot;chunked&quot; representation of the input can be as effective for the purposes of semantic role identification.
In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). $$$$$ In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).

Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. $$$$$ We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction.
Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. $$$$$ The system first passed sentences through an automatic parser, extracted syntactic features from the parses, and estimated probabilities for semantic roles from the syntactic and lexical features.

For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ Voice: The distinction between active and passive verbs plays an important role in the connection between semantic role and grammatical function, since direct objects of active verbs correspond to subjects of passive verbs.
For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system.

We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).
We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ To predict argument roles in new data, we wish to estimate the probability of each role given these five features and the predicate p: P(rlpt, path, position, voice, hw, p).

The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ The FrameNet data contained at least ten examples from each predicate, while 17% of the Propbank data had fewer than ten training examples.
The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ It is also possible that this approach may be more robust to error than parsers.

Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ The parse constituent spanning each set of words annotated as an argument was found, and the constituent's nonterminal label was taken as the phrase type.

These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ Results are shown in Tables 1 and 2.
These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ In interpreting these results, it is important to keep in mind the differences between this task and other information extraction datasets.

Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). $$$$$ In fact, this system achieves 27.6% precision and 22.0% recall.
Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). $$$$$ With this scoring regime, the chunk-based system performs at 49.5% precision and 35.1% recall, still significantly lower than the 57.7% precision/50.0% recall for exact matches using automatically generated parses.

For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.
For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).

In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.
In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).

 $$$$$ The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &quot;temporal&quot; or &quot;locative&quot;.
 $$$$$ Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.

The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003). $$$$$ Alshawi (1992)), to simpler finite-state or statistical systems such as Hobbs et al. (1997) and Miller et al.
The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003). $$$$$ A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).

 $$$$$ The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &quot;temporal&quot; or &quot;locative&quot;.
 $$$$$ Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.

Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ The Necessity Of Parsing For Predicate Argument Recognition
Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ This chunker-based result is only slightly lower than the 79.2% obtained using automatic parses in the known boundary condition.

For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2.
For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system.

The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).
The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used.

 $$$$$ The core arguments of each predicate are simply numbered, while remaining arguments are given labels such as &quot;temporal&quot; or &quot;locative&quot;.
 $$$$$ Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.

the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ This chunker-based result is only slightly lower than the 79.2% obtained using automatic parses in the known boundary condition.
the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data.

More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.
More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).

This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ The Necessity Of Parsing For Predicate Argument Recognition
This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.
