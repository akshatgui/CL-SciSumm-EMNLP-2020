 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).

Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ Akiba et al. (2001) extended the idea to accommodate multiple references.
Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ Nießen et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations.

In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.

Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ Automatic Evaluation Of Machine Translation Quality Using Longest Common Subsequence And Skip-Bigram Statistics
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ To apply LCS in machine translation evaluation, we view a translation as a sequence of words.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

Stemming is enabled (Lin and Och, 2004a). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
Stemming is enabled (Lin and Och, 2004a). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.

The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). $$$$$ ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). $$$$$ ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).

Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.

ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ In other words, consecutive matches are awarded more scores than non-consecutive matches.
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Finally we applied ROUGE-L, ROUGE-W with weighting function k1.2, and ROUGE-S without skip distance 4 BLEUN computes BLEU over n-grams up to length N. Only BLEU1, BLEU4, and BLEU12 are shown in Table 1. and fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NIST score, ROUGE-L is LCS-based F-measure (0 = 1), ROUGE-W is weighted LCS-based F-measure (0 = 1).

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

 $$$$$ However, S2 and S3 have very different meanings.
 $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.

Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.
Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams.

Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ Skipbigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams.
