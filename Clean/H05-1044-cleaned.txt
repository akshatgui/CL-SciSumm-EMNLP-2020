Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. $$$$$ has the same prior and contextual polarity.
Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. $$$$$ For the first step, we concentrate on whether clue instances are neutral or polar in context (where polar in context refers to having a contextual polarity that is positive, negative or both).

For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).
For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. $$$$$ Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).

In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues.
In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ 6.9% of the clues in the lexicon are marked as neutral.

Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. $$$$$ We use a two-step process that employs machine learning and a variety of features.
Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. $$$$$ The polarity classification results for this second step in the contextual disambiguation process are given in Table 5.

Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005). $$$$$ In these lexicons, entries are tagged with their a priori prior polarity: out of context, doesthe word seem to evoke something positive or some thing negative.
Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005). $$$$$ Only a small number of clues (0.3%) are marked as having both positive and negative polarity.

 $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
 $$$$$ This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA).

Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).
Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. $$$$$ Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).

The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).
The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.

More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). $$$$$ We define the gold standard used to train and test the system in terms of the manual annotations described in Section 2.
More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). $$$$$ However, they do not use the other types of features in our experiments, and they restrict their tags to positiveand negative (excluding our both and neutral categories).

To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).
To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ Words that are subjective in most contexts were marked strongly subjective (strongsubj), and those that may only have certain subjective usages were marked weakly subjective (weaksubj).

We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). $$$$$ For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues.
We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). $$$$$ Adding a feature for the prior 351 Word Features word token word prior polarity: positive, negative, both, neutral Polarity Features negated: binary negated subject: binary modifies polarity: positive, negative, neutral, both, notmod modified by polarity: positive, negative, neutral, both, notmod conj polarity: positive, negative, neutral, both, notmod general polarity shifter: binary negative polarity shifter: binary positive polarity shifter: binary Table 6: Features for polarity classification polarity improves recall so that it is only 4.4% lower, but this hurts precision, which drops to 4.2% lower than the 28-feature classifier?s precision.

 $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
 $$$$$ This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA).

PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ has the same prior and contextual polarity.
PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ The next step was to tag the clues in the lexicon with their prior polarity.

 $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
 $$$$$ This work was supported in part by the NSF under grant IIS-0208798 and by the Advanced Research and Development Activity (ARDA).

Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ 6.3.1 Neutral-Polar Classification The neutral-polar classifier uses 28 features, listed in Table 3.
Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ Word token and word prior polarity are un changed from the neutral-polar classifier.

We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. $$$$$ A subjective expression is any word or phrase used to express an opinion, emotion, evaluation, stance, speculation, 1The MPQA Corpus is described in (Wiebe et al, 2005) and available at nrrc.mitre.org/NRRC/publications.htm.
We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).

In this work we use MPQA (Wilson et al, 2005). $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).
In this work we use MPQA (Wilson et al, 2005). $$$$$ Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).

The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. $$$$$ A subjective expression is any word or phrase used to express an opinion, emotion, evaluation, stance, speculation, 1The MPQA Corpus is described in (Wiebe et al, 2005) and available at nrrc.mitre.org/NRRC/publications.htm.
The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. $$$$$ For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues.

They use the Opinion Finder lexicon (Wilson et al, 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. $$$$$ For the experiments in this paper, we use a lexicon of over 8,000 subjectivity clues.
They use the Opinion Finder lexicon (Wilson et al, 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. $$$$$ 6.9% of the clues in the lexicon are marked as neutral.

To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005). $$$$$ Please see (Wiebe et al, 2005) for more details on the existing annotations in the MPQA Corpus.etc. A general covering term for such states is private state (Quirk et al, 1985).
To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005). $$$$$ Much work on sentiment analysis classifies documents by their overall sentiment, for example deter mining whether a review is positive or negative (e.g., (Turney, 2002; Dave et al, 2003; Pang and Lee,2004; Beineke et al, 2004)).
