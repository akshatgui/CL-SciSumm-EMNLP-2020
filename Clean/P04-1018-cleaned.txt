Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ These numbers are obtained with a fixed search beam and pruning threshold (widening the search beam or using a smaller pruning threshold did not change results significantly).
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ A beam search algorithm is used to search the best entity result.

Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ In Section 2, we present how the Bell tree can be used to represent the process of creating entities from mentions and the search space.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ Since the number of tree leaves is the number of possible coreference outcomes and it equals the Bell Number (Bell, 1934), the tree is called the Bell tree.

As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ Since MUC does not require entity label and level, the conversion from ACE to MUC is “lossless.” Table 5 tabulates the test results on the true mentions of the MUC6 formal test set.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ The MUC scorer cannot be used since it inherently favors systems that output fewer number of entities (e.g., putting all mentions of the MUC6 formal test set into one entity will yield a recall and precision of links, which gives an F-measure).

To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ The last line returns top results, where denotes the result ranked by

Details of this process can be found in Luo et al (2004). $$$$$ Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well.
Details of this process can be found in Luo et al (2004). $$$$$ Details of the mention detection and coreference system can be found in (Florian et al., 2004).

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ For tests on the MUC data, we report both F-measure using the official MUC score (Vilain et al., 1995) and ECM-F.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ Details of the mention detection and coreference system can be found in (Florian et al., 2004).

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ The Bell tree clearly represents the search space of the coreference resolution problem.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ The Bell tree represents the search space of the coreference resolution problem.

As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ Effective training algorithm exists (Berger et al., 1996) once the set of features is selected.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ A features.

For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ Note that, however, the mention-pair model uses times more features than the entity-pair model.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ Since the mention-pair model is better, subsequent analyses are done with the mention pair model only.

Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ The full entity-mention model has about features, due to less number of conjunction features and training examples.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ A features.

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ We also train a coreference system using the MUC6 data and competitive results are obtained.
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ State-of-the-art performance has been achieved on the ACE coreference data across three languages.

Distances have been used in e.g. Luo et al (2004). $$$$$ Early work of anaphora resolution focuses on finding antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998), while recent advances (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycheriah et al., 2003) employ statistical machine learning methods and try to resolve reference among all kinds of noun phrases (NP), be it a name, nominal, or pronominal phrase – which is the scope of this paper as well.
Distances have been used in e.g. Luo et al (2004). $$$$$ Details of the mention detection and coreference system can be found in (Florian et al., 2004).

 $$$$$ Mention index represents the order it appears in the document.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ The second mention is Figure 1

They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ Details of the mention detection and coreference system can be found in (Florian et al., 2004).
They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ State-of-the-art performance has been achieved on the ACE coreference data across three languages.
