Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Building Deep Dependency Structures Using A Wide-Coverage CCG Parser
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.

The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ Furthermore, since we want to model the dependencies in such structures, the probability model is defined over these structures rather than the derivation.
The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques (Abney, 1997; Johnson et al., 1999), so we have defined a conditional model, similar to the model of Collins (1996) (see also the conditional model in Eisner (1996b)).

Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Most recent wide-coverage statistical parsers have used models based on lexical dependencies (e.g.

This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.

Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ These derived structures can be of any form we like—for example, they could in principle be standard Penn Treebank structures.

This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ Building Deep Dependency Structures Using A Wide-Coverage CCG Parser
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ A CCG parser can directly build derived structures, including longrange dependencies.

The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ The supertagger (described in Clark (2002)) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability category for that word, given the surrounding context.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ If there was no such category, all categories spanning the whole string were considered.

Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ To measure the performance of the parser, we compared the dependencies output by the parser with those in the gold standard, and computed precision and recall figures over the dependencies.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ Labelled precision and recall on Section 00 for the most frequent dependency types are shown in Table 2 (for the model without distance measures).9 The columns # deps give the total number of dependencies, first the number put forward by the parser, and second the number in the gold standard.

 $$$$$ We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the word, pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996).
 $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.

The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ One solution is to consider only the normal-form (Eisner, 1996a) derivation, which is the route taken in Hockenmaier and Steedman (2002b).1 Another problem with the non-standard surface derivations is that the standard PARSEVAL performance measures over such derivations are uninformative (Clark and Hockenmaier, 2002).
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ The results are shown in Table 1, with an additional column giving the category accuracy.

This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ This paper has shown that accurate, efficient widecoverage parsing is possible with CCG.

Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ For example, the following category for the transitive verb bought specifies its first argument as a noun phrase (NP) to its right and its second argument as an NP to its left, and its result as a sentence: For parsing purposes, we extend CCG categories to express category features, and head-word and dependency information directly, as follows: The feature dcl specifies the category’s S result as a declarative sentence, bought identifies its head, and the numbers denote dependency relations.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ Derivations are written as follows, with underlines indicating combinatory reduction and arrows indicating the direction of the application: Formally, a dependency is defined as a 4-tuple: hf f s ha , where hf is the head word of the functor,2 f is the functor category (extended with head and dependency information), s is the argument slot, and ha is the head word of the argument—for example, the following is the object dependency yielded by the first step of derivation (3): The head of the infinitival complement’s subject is identified with the head of the object, using the variable X. Unification then “passes” the head of the object to the subject of the infinitival, as in standard unification-based accounts of control.3 The kinds of lexical items that use the head passing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns.

Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ Derivations are written as follows, with underlines indicating combinatory reduction and arrows indicating the direction of the application: Formally, a dependency is defined as a 4-tuple: hf f s ha , where hf is the head word of the functor,2 f is the functor category (extended with head and dependency information), s is the argument slot, and ha is the head word of the argument—for example, the following is the object dependency yielded by the first step of derivation (3): The head of the infinitival complement’s subject is identified with the head of the object, using the variable X. Unification then “passes” the head of the object to the subject of the infinitival, as in standard unification-based accounts of control.3 The kinds of lexical items that use the head passing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).

We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ Thus we ignore the normalisation factor, thereby simplifying the parsing process.
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ However, the few that exist (Lin, 1995; Carroll et al., 1998; Collins, 1999) have used either different data or different sets of dependencies (or both).

See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ Building Deep Dependency Structures Using A Wide-Coverage CCG Parser
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).

Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ However, the dependency structures with high enough PCS to be among the highest probability structures are likely to have similar category sequences.
Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ The supertagger (described in Clark (2002)) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability category for that word, given the surrounding context.

This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ Furthermore, since we want to model the dependencies in such structures, the probability model is defined over these structures rather than the derivation.
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ We conjecture that it is possible to define a generative model that includes the deep dependencies.

The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques (Abney, 1997; Johnson et al., 1999), so we have defined a conditional model, similar to the model of Collins (1996) (see also the conditional model in Eisner (1996b)).
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ (A similar argument is used by Collins (1996) in the context of his parsing model.)

Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ If the end-result of parsing is interpretable predicate-argument structure or the related dependency structure, then the question arises: why build derivation structure at all?
Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ Recall that a dependency is defined as a 4-tuple: a head of a functor, a functor category, an argument slot, and a head of an argument.

Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ Figures were calculated for labelled dependencies (LP,LR) and unlabelled dependencies (UP,UR).
