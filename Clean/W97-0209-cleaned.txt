WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). $$$$$ In other work, Yarowsky (1993) has shown that local collocational information, including selectional constraints, can be used to great effect in sense disambiguation, though his algorithm requires supervised training.
WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky 1995) (Resnik 1997). $$$$$ More important is information beyond selectional preference, notably the wider context utilized by Yarowsky (1992).

Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al 2011), and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Selectional Preference And Sense Disambiguation
Previous work on selectional preferences has used them primarily for natural language analytic tasks such as word sense disambiguation (Resnik, 1997), dependency parsing (Zhou et al 2011), and semantic role labeling (Gildea and Jurafsky, 2002). $$$$$ Following Miller et al. (1994), disambiguation by random choice was used as a baseline

I followed (Resnik, 1993)/ (Resnik, 1997) who defined selectional preference as the amount of information a verb provides about its semantic argument classes. $$$$$ The model defines the selectional preference strength of a predicate as

The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). $$$$$ It has long been observed that selectional constraints and word sense disambiguation are closely linked.
The annotation of word senses such as used by machine-learning based word sense disambiguation (WSD) tools corresponds to the task of selecting the correct semantic class or concept for a word from an underlying ontology such as WordNet (Resnik, 1997). $$$$$ Thus, despite the absence of class annotation in the training text, it is still possible to arrive at a usable estimate of class-based probabilities.

Resnik (1997) described a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes. $$$$$ The basis of the approach is a probabilistic model capturing the co-occurrence behavior of predicates and conceptual classes in the taxonomy.
Resnik (1997) described a method to acquire a set of conceptual classes for word senses, employing selectional preferences, based on the idea that certain linguistic predicates constraint the semantic interpretation of underlying words into certain classes. $$$$$ But since text corpora contain words, not classes, it is necessary to treat each occurrence of a word in an argument position as if it might represent any of the conceptual classes to which it belongs, and assign frequency counts accordingly.

 $$$$$ For i from 1 to k, compute

These promising results enable a number of future researches $$$$$ The model defines the selectional preference strength of a predicate as

Techniques for automatically detecting selections preferences have been discussed in (McCarthy and Carrol, 2003) and (Resnik, 1997). $$$$$ The treatment of selectional preference used here is that proposed by Resnik (1993a; 1996), combining statistical and knowledge-based methods.
Techniques for automatically detecting selections preferences have been discussed in (McCarthy and Carrol, 2003) and (Resnik, 1997). $$$$$ The results of the experiment show that disambiguation using automatically acquired selectional constraints leads to performance significantly better than random choice.

A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al, 2008), textual inference (Pantel et al, 2007), word-sense disambiguation (Resnik,1997), and many more. $$$$$ In that respect, the most direct point of comparison is the performance of Miller et al. 's (1994) frequency heuristic — always choose the most frequent sense of a word — as evaluated using the full sense-tagged corpus, including nouns, verbs, adjectives, and adverbs.
A large, high-quality database of preferences has the potential to improve the performance of a wide range of NLP tasks including semantic role labeling (Gildea and Jurafsky, 2002), pronoun resolution (Bergsma et al, 2008), textual inference (Pantel et al, 2007), word-sense disambiguation (Resnik,1997), and many more. $$$$$ As in the experiments by Cowie et al., the choice of coarser distinctions presumably accounts in part for the high accuracy.

We adopted the association measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997). $$$$$ The treatment of selectional preference used here is that proposed by Resnik (1993a; 1996), combining statistical and knowledge-based methods.
We adopted the association measure proposed by Resnik (1993) and successfully applied to a number of tasks in NLP including word sense disambiguation (Resnik, 1997). $$$$$ In other work, Yarowsky (1993) has shown that local collocational information, including selectional constraints, can be used to great effect in sense disambiguation, though his algorithm requires supervised training.

To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn Treebank project (Marcus et al., 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al., 1993).
To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ More important is information beyond selectional preference, notably the wider context utilized by Yarowsky (1992).

In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ At present, this is done by distributing the &quot;credit&quot; for an observation uniformly across all the conceptual classes containing an observed argument.
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ Given the frequencies, probabilities are currently estimated using maximum likelihood; the use of word classes is itself a form of smoothing (cf.

We used the training sets, test sets, and evaluation method described in (Resnik, 1997). $$$$$ In marked contrast to annotated training material for partof-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas part-of-speech tag sets tend to differ in the details); (b) sense annotation has a comparatively high error rate (Miller, personal communication, reports an upper bound for human annotators of around 90% for ambiguous cases, using a non-blind evaluation method that may make even this estimate overly optimistic); and (c) no fully automatic method provides high enough quality output to support the &quot;annotate automatically, correct manually&quot; methodology used to provide high volume annotation by data providers like the Penn 'Treebank project (Marcus et al., 1993).
We used the training sets, test sets, and evaluation method described in (Resnik, 1997). $$$$$ The 100 verbs that select most strongly for their objects were identified, excluding verbs appearing only once in the training corpus; test instances of the form (verb, object, correct sense) were then extracted from the merged test corpus, including all triples where verb was one of the 100 test verbs.4 Evaluation materials were obtained in the same manner for several other surface syntactic relationships, including verb-subject (John admires), adjective-noun (tall building), modifier-head (river bank), and head-modifier (river z bank).

Automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). $$$$$ Formally, given a predicate-argument relationship R (for example, the verb-object relationship), a predicate p, and a conceptual class c, where countR(p, w) is the number of times word w was observed as the argument of p with respect to R, and classes(w) is the number of taxonomic classes to which w belongs.
Automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that a word might have, based on the relation it has with other words in context (Resnik, 1997). $$$$$ The results of the experiment show that disambiguation using automatically acquired selectional constraints leads to performance significantly better than random choice.
