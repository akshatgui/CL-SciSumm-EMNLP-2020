The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ This simple approach has the advantage of being very efficient, and we findthat it is accurate enough to enable highly accu rate parsing.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ Here we use the Maximum En tropy models described in Curran and Clark (2003).
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The parser is described in detail in Clark and Curran (2004).

A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ Here we use the Maximum En tropy models described in Curran and Clark (2003).

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ In Clark and Curran (2004) we describe a discrim inative method for estimating the parameters of a log-linear parsing model.

The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ Theper-word accuracy is between 91 and 92% on un seen data in CCGbank; however, Clark (2002) shows this is not high enough for integration into a parser since the large number of incorrect categories results in a significant loss in coverage.
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ For each word inthe sentence, the multi-tagger assigns all those cat ? CATS/ ACC SENT ACC SENT WORD ACC (POS) ACC 0.1 1.4 97.0 62.6 96.4 57.4 0.075 1.5 97.4 65.9 96.8 60.6 0.05 1.7 97.8 70.2 97.3 64.4 0.01 2.9 98.5 78.4 98.2 74.2 0.01k=100 3.5 98.9 83.6 98.6 78.9 0 21.9 99.1 84.8 99.0 83.0 Table 2

However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999).
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.

This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ The parser is described in detail in Clark and Curran (2004).
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.

The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery

The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ Here we use the Maximum En tropy models described in Curran and Clark (2003).
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ The parser is described in detail in Clark and Curran (2004).

Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages $$$$$ Parsing using CCG can be viewed as a two-stage process

The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ Our wide-coverage CCG parser uses a log-linear model to select an analysis.

Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ Clark and Curran (2004) evaluate a number of log-linear parsing models for CCG.
Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence.

We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word.
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm.

Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ 3.1 Model Estimation.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ is the log-likelihood of model ?, and G(?)

Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ For a given sentence the output of the parser is a dependency structure corresponding to the most probable derivation, which can be found using theViterbi algorithm.

The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ The normal-form derivations in CCGbank provide the gold standard training data.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ , S m, to gether with gold standard normal-form derivations, d1, . . .

The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ 2.1 The Lexical Category Set.
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ The parser is described in detail in Clark and Curran (2004).

The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ A value of k = 20was used in this work, and sections 2-21 of CCG bank were used as training data.Table 2 gives the per-word accuracy (acc) on sec tion 00 for various values of ?, together with the average number of categories per word.

Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ 2.1 The Lexical Category Set.
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ Features are defined for each word in the window and for the POS tag of each word.

The CCG parsing consists of two phases $$$$$ To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.
The CCG parsing consists of two phases $$$$$ A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.
