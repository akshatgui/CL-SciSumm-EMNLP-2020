Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.
Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ This alternate decoding path model was developed by Birch et al. (2007).

The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.
The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ All perform better than the three baseline approaches.

Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. $$$$$ Experiments in Domain Adaptation for Statistical Machine Translation
Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. $$$$$ Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.

Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ All perform better than the three baseline approaches.
Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ For the Europarl test sets, we did not use any domain adaptation techniques, but simply used either just the Europarl training data or the combined data — whatever gave the higher score on the development test set, although scores differed by only about 0.1–0.2 %BLEU.

Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ Here, we take advantage of a feature of the Moses decoder’s factored translation model framework.
Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ We decided to use the interpolated language model method described in Section 2.5.

Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ Experiments in Domain Adaptation for Statistical Machine Translation
Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.

Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ Experiments in Domain Adaptation for Statistical Machine Translation
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.

The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.
The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.

See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.
See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data Proceedings of the Second Workshop on Statistical Machine Translation, pages 224–227, Prague, June 2007. c�2007 Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-domain training data (about 1 million words) to optimize translation performance on that particular domain.

Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). $$$$$ Since we want to obtain a language model that gives us the best performance on the target domain, we set this weight so that the perplexity of the development set from that target domain is optimized.
Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). $$$$$ We included them as two separate features, whose weights are set with minimum error rate training.

We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. $$$$$ Language modeling software such as the SRILM toolkit we used (Stolke, 2002) allows the interpolation of these language models.
We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. $$$$$ The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.

In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.
In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ Since training data for statistical machine translation is typically collected opportunistically from wherever it is available, the application domain for a machine translation system may be very different from the domain of the system’s training data.

(Koehn and Schroeder, 2007) used two language models and two translation models $$$$$ In our next setup, we used only in-domain data for training the language model.
(Koehn and Schroeder, 2007) used two language models and two translation models $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.

(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ The log-linear modeling approach of statistical machine translation enables a straight-forward combination of the in-domain and out-of-domain language models.
(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ The relative weight for each model is set directly by optimizing translation performance.

One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. $$$$$ We included them as two separate features, whose weights are set with minimum error rate training.
One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. $$$$$ Again, respective weights are set with minimum error rate training.

Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ The open source Moses (Koehn et al., 2007) MT system was originally developed at the University of Edinburgh and received a major boost through a 2007 Johns Hopkins workshop.
Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ All perform better than the three baseline approaches.

Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.
Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. $$$$$ When interpolating, we give the out-of-domain language model a weight in respect to the in-domain language model.

Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Experiments in Domain Adaptation for Statistical Machine Translation
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ We trained two language models, one for each the out-of-domain and the in-domain training data.

There were three different adaptation measures $$$$$ 4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup.
There were three different adaptation measures $$$$$ The relative weight for each model is set directly by optimizing translation performance.

 $$$$$ The best-scoring translation is searched for by the decoding algorithm and outputted by the system as the best translation.
 $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
