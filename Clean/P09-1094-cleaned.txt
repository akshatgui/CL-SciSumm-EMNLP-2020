Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ Researchers employ the existing SMT models for PG (Quirk et al., 2004).
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ One can refer to (Zhao et al., 2008b) for the details.

Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ The PTs used in this work are constructed using different corpora and different score functions (Section 3.5).
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ In practice, the units of a sentence may be paraphrased using different PTs.

Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ One can refer to (Zhao et al., 2008b) for the details.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.

Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).
Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ One can refer to (Zhao et al., 2008b) for the details.

The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ The results show that the proposed method is promising, which generates useful paraphrases for the given applications.
The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ Paraphrase Model: Paraphrase generation is a decoding process.

Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control the adequacy, fluency, and usability of the paraphrases, respectively1.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Thus it contains no paraphrase planning stage or the usability sub-model.

We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ As far as we know, this is the first statistical model specially designed for paraphrase generation.
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ We applied the approach proposed in (Zhao et al., 2008b).

The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Sentence Simplification: Carroll et al. (1999) has proposed an automatic text simplification method for language-impaired readers.

In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).
In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).

The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ Researchers employ the existing SMT models for PG (Quirk et al., 2004).
The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ One can refer to (Zhao et al., 2008b) for the details.

More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ We consider three paraphrase applications in our experiments, including sentence compression, sentence simplification, and sentence similarity computation.
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).

To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ We applied the approach proposed in (Zhao et al., 2008b).
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences.

Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ One can refer to (Zhao et al., 2008b) for the details.

 $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
 $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.

Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ We applied the approach proposed in (Zhao et al., 2008b).
Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ One can refer to (Zhao et al., 2008b) for the details.

Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.

Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ Application-driven Statistical Paraphrase Generation
Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application.

As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ Application-driven Statistical Paraphrase Generation
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ The application in this example is sentence compression.

Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ We applied the approach proposed in (Zhao et al., 2008b).

More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc.
