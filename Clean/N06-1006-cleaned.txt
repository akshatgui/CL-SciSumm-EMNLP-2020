Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005).
Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.

Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005).
Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.

Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ We show comparable results from recent systems based on lexical similarity (Jijkoun and de Rijke, 2005), graph alignment (Haghighi et al., 2005), weighted abduction (Raina et al., 2005), and a mixed system including theorem proving (Bos and Markert, 2005).

We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence $$$$$ First, the above systems assume a form of upward monotonicity

Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Nearly all current textual inference systems use a single-stage matching/proof process, and differ mainly in the sophistication of the matching stage.
Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Using this multi-stage architecture, we report results on the PASCAL RTE data which surpass previously-reported results for alignment-based systems.

Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ Nearly all current textual inference systems use a single-stage matching/proof process, and differ mainly in the sophistication of the matching stage.
Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment $$$$$ We propose that all three problems can be resolved in a two-stage architecture, where the alignment phase is followed by a separate phase of entailment determination.

Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ Given a good alignment, the determination of entailment reduces to a simple classification decision.
Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing $$$$$ Our system has three stages

We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ First, the above systems assume a form of upward monotonicity

It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ Using this multi-stage architecture, we report results on the PASCAL RTE data which surpass previously-reported results for alignment-based systems.

MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise.
MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ A robust inference guesser will still likely conclude that there is entailment.
