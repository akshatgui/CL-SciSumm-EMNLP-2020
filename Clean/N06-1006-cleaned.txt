Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005).
Pado et al. (2009) uses Textual Entailment features extracted from the Standford Entailment Recognizer (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.

Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ A more structured approach is to formulate the entailment prediction as a graph matching problem (Haghighi et al., 2005; de Salvo Braz et al., 2005).
Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al, 2006) and (Snow et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.

Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Marsi and Krahmer (2005) and MacCartney et al (2006) first advocated pipelined system architectures containing a distinct alignment component, a strategy crucial to the top-performing systems of Hickl et al (2006) and Hickl and Bensley (2007). $$$$$ We show comparable results from recent systems based on lexical similarity (Jijkoun and de Rijke, 2005), graph alignment (Haghighi et al., 2005), weighted abduction (Raina et al., 2005), and a mixed system including theorem proving (Bos and Markert, 2005).

We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. $$$$$ First, the above systems assume a form of upward monotonicity: if a good match is found with a part of the text, other material in the text is assumed not to affect the validity of the match.
We have previously emphasized (MacCartney et al, 2006) that there is more to inferential validity than close lexical or structural correspondence: negations, modals, non-factive and implicative verbs, and other linguistic constructs can affect validity in ways hard to capture in alignment. $$$$$ To capture these phenomena, we compiled small lists of “factive” and non-factive verbs, clustered according to the kinds of entailments they create.

Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Nearly all current textual inference systems use a single-stage matching/proof process, and differ mainly in the sophistication of the matching stage.
Our system is based on the stage architecture of the Stanford RTE system (MacCartney et al, 2006), but adds a stage for event coreference decision. $$$$$ Using this multi-stage architecture, we report results on the PASCAL RTE data which surpass previously-reported results for alignment-based systems.

Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment: an alignment stage followed by an entailment stage. $$$$$ Nearly all current textual inference systems use a single-stage matching/proof process, and differ mainly in the sophistication of the matching stage.
Based on this representation, we apply a two stage entailment process similar to MacCartney et al (2006) developed for textual entailment: an alignment stage followed by an entailment stage. $$$$$ We propose that all three problems can be resolved in a two-stage architecture, where the alignment phase is followed by a separate phase of entailment determination.

Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing: (1) an alignment model aligns terms in the hypothesis to terms in the conversation segment; and (2) an inference model predicts the entailment based on the alignment between the hypothesis and the conversation segment. $$$$$ Given a good alignment, the determination of entailment reduces to a simple classification decision.
Given the clause representation, we follow the idea similar to MacCartney et al (2006), and predict the entailment decision in two stages of processing: (1) an alignment model aligns terms in the hypothesis to terms in the conversation segment; and (2) an inference model predicts the entailment based on the alignment between the hypothesis and the conversation segment. $$$$$ Our system has three stages: linguistic analysis, alignment, and entailment determination.

We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
We base our experiments on the Stanford RTE system which uses a staged architecture (MacCartney et al, 2006). $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Later systems that include more linguistic features extracted from resources such as WordNet have enjoyed more success (MacCartney et al, 2006). $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
Many of these features are inspired by MacCartney et al (2006) and Snow et al (2006), but not as sophisticated. $$$$$ The two models become distinct when there is a good supply of additional linguistic and world knowledge axioms—as in Moldovan et al. (2003) but not Raina et al.

The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
The Stanford Entailment Recognizer (MacCartney et al, 2006) is a stochastic model that computes match and mismatch features for each premise hypothesis pair. $$$$$ First, the above systems assume a form of upward monotonicity: if a good match is found with a part of the text, other material in the text is assumed not to affect the validity of the match.

It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ A third approach, exemplified by Moldovan et al. (2003) and Raina et al.
It has a three-stage architecture similar to the RTE system of MacCartney et al (2006). $$$$$ Using this multi-stage architecture, we report results on the PASCAL RTE data which surpass previously-reported results for alignment-based systems.

MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ During the last five years there has been a surge in work which aims to provide robust textual inference in arbitrary domains about which the system has no expertise.
MacCartney et al (2006) describe a system for doing robust textual inference. $$$$$ A robust inference guesser will still likely conclude that there is entailment.
