Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character. $$$$$ The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.
Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese character. $$$$$ The second translation system, for converting pinyin sequences to character sequences, has a one-toone mapping between symbols and therefore has no words with zero fertility.

Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework. $$$$$ The IBM source-channel model for statistical machine translation (P. Brown et al., 1993) plays a central role in our system.
Virga and Khudanpur (2003) and Kuo et al (2005) adopted the noisy channel modeling framework. $$$$$ In the IBM model described earlier, these are the words which may be “deleted” by the noisy channel when transforming into .

Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). $$$$$ The Hopkins Automated Information Retriever for Combing Unstructured Text (HAIRCUT) is a research retrieval system developed at the Johns Hopkins University Applied Physics Laboratory.
Technologies developed for SMTare borrowed in Virga and Khudanpur (2003) and AbdulJaleel and Larkey (2003). $$$$$ The system was developed to investigate knowledgelight methods for linguistic processing in text retrieval.

The proposed transliteration framework obtained significant improvements over a strong baseline transliteration approach similar to AbdulJaleel and Larkey (2003) and Virga and Khudanpur (2003). $$$$$ A small improvement in mAP is obtained by the Haircut system with name transliteration over the system without name transliteration

This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003). $$$$$ We concede that this performance, while comparable to other systems, is not satisfactory and merits further investigation.
This result was comparable to other state-of-the-art statistical name transliteration systems (Virga and Khudanpur, 2003). $$$$$ In any event, a need for improvement in transliteration is suggested by this result.

 $$$$$ In a more intrinsic and direct evaluation, we have found ways to gainfully filter a large but noisy training corpus to augment the training data for our models and improve transliteration accuracy considerably beyond our starting point, e.g., to reduce Pin-yin error rates from 51.1% to 42.5%.
 $$$$$ We expect to further refine the translation models in the future and apply them in other tasks such as text translation.


Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). $$$$$ Several techniques have been proposed in the recent past for name transliteration.
Past studies on phoneme-based E2C have reported their adverse effects (e.g. Virga and Khudanpur, 2003). $$$$$ For the phoneme-to-GIF translation model, the “words” which need to be inserted in this manner are syllabic nuclei!

Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion. $$$$$ The language model required for translating pinyin sequences to Chinese characters is relatively straightforward.
Virga and Khudanpur (2003) reported 8.3% absolute accuracy drops when converting from Pinyin to Chinese characters, due to homophone confusion. $$$$$ Note that while significantly lower error rates have been reported for converting pin-yin to characters in generic Chinese text, ours is a highly specialized subset of transliterated foreign names, where the choice between several characters sharing the same pin-yin symbol is somewhat arbitrary.

In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits. $$$$$ The results are shown in Table 1, where pin-yin error rate is the edit distance between the “correct” pin-yin representation of the correct transliteration and the pin-yin sequence output by the system.
In CLIR or multilingual corpus alignment (Virga and Khudanpur, 2003), N-best results will be very helpful to increase chances of correct hits. $$$$$ Our results and the corresponding results from Meng et al (2001) are reported in Table 2.

The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003). $$$$$ We then aligned all the (nearly 1M) training “sentence” pairs with this translation model, and extracted roughly a third of the sentences with an alignment score above a certain tunable threshold ().
The reference data are extracted from Table 1 and 3 of (Virga and Khudanpur 2003). $$$$$ The result of this evaluation is reported in Table 3 against the line “Huge MT (Self),” where we also report the transliteration performance of the so-called Big MT system of Table 1 on this new test set.
