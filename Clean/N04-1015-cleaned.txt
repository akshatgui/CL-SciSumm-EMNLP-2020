This methodology is very similar to the way [Barzilay and Lee, 2004] evaluate their probabilistic TS model in comparison to the approach of [Lapata, 2003]. $$$$$ Catching The Drift

Thus, there might exist more than one equally good solution for TS, a view shared by almost all TS researchers, but which has not been accounted for in the evaluation methodologies of [Karamanis et al, 2004] and [Barzilay and Lee, 2004]. $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text

The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ Our technique incorporates a novel adaptation of the standard HMM induction algorithm that is tailored to the task of modeling content.
The simplest formulation we consider is an HMM where each state contains a unigram language model (LM), proposed by Chotimongkol (2008) for task-oriented dialogue and originally developed for discourse analysis by Barzilay and Lee (2004). $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ Document-level analysis of text struc ture is an important instance of such work.
In this work, we also assume a content model, which we fix to be the document-level HMM as used in Barzilay and Lee (2004). $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ rization

Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. $$$$$ Document-level analysis of text struc ture is an important instance of such work.
Barzilay and Lee (2004) showed that it is possible to obtain schema-like knowledge automatically from a corpus for the purposes of extracting sentences and ordering them. However, their work represents patterns at the sentence level, and is thus not directly comparable to our work, given our focus on sentence generation. $$$$$ But rather than manually determine the topics for a given domain, we take a distributional view, learning them directly from un-annotated texts via analysis of word distribution patterns.

Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
Like Barzilay and Lee (2004), this model was used to order extracted sentences in summaries. $$$$$ The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading

Barzilay and Lee (2004) proposed a domain-dependent HMM model to capture topic shift in a text, where topics are represented by hidden states and sentences are observations. $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text

Barzilay and Lee (2004) have proposed content models to deal with topic transition in domain specific text. $$$$$ The focus of ourwork, however, is on an equally fundamental but domain dependent dimension of the structure of text

These methods identify regularities in words (Barzilay and Lee, 2004). $$$$$ Our experiments showthat incorporating content models in these ap plications yields substantial improvement over previously-proposed methods.
These methods identify regularities in words (Barzilay and Lee, 2004). $$$$$ The resulting summaries yield 88%match with human-written output, which compares fa vorably to the 69% achieved by the standard ?leading

To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used $$$$$ corresponds roughly to the notions of topic and topic change.
To make a tool like the HMM work at higher levels, one needs to make stronger assumptions, for instance as signing each sentence a single topic and then topic specific word models can be used $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
Barzilay and Lee (2004)'s knowledge-lean approach attempts to automate the inference of knowledge-rich information using a distributional view of content. $$$$$ We first describe an efficient, knowledge-lean methodfor learning both a set of topics and the relations be tween topics directly from un-annotated documents.

This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ We consider the problem of modeling the content structure of texts within a specific do main, in terms of the topics the texts address and the order in which these topics appear.
This is specific to their approach as both Lapata (2003)'s and Barzilay and Lee (2004)'s approaches are not tailored to summarization and therefore do not experience the topic bias problem. $$$$$ corresponds roughly to the notions of topic and topic change.

The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ Then, we apply techniques based on content models totwo complex text-processing tasks.
The results for Coreference+Syntax+Salience+ and HMM-Based Content Models are reproduced from Barzilay and Lapata (2008). $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ corresponds roughly to the notions of topic and topic change.
When compared to the results obtained by Barzilay and Lapata (2008) and Barzilay and Lee (2004), it would appear that direct sentence to-sentence similarity (as suggested by the Barzilay and Lapata baseline score) or capturing topic sequences are essential for acquiring the correct sequence of sentences in the earthquake dataset. $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as schema items (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988; Marcu, 1997).
This assumption builds on the success of previous research, where comparable and parallel texts have been exploited for a range of related learning tasks, e.g., unsupervised discourse segmentation (Barzilay and Lee, 2004). $$$$$ How ever, research has shown that texts from the same domain tend to exhibit high similarity (Wray, 2002).

However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ In arbi trary document collections, such patterns might be toovariable to be easily detected by statistical means.
However, as Barzilay and Lee (2004) observe, the content of document collections is highly structured, consisting of several topical themes, each with its own vocabulary and ordering preferences. $$$$$ First, we consider in formation ordering, that is, choosing a sequence in whichto present a pre-selected set of items; this is an essen tial step in concept-to-text generation, multi-document summarization, and other text-synthesis problems.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ The development and application of computational models of text structure is a central concern in natural language processing.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee, 2004). $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.

Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ We first present an effective knowledge-leanmethod for learning content models from unannotated documents, utilizing a novel adaptation of algorithms for Hidden Markov Models.
Following Ruch et al (2003) and Barzilay and Lee (2004), we employed Hidden Markov Models to model the discourse structure of MEDLINE abstracts. $$$$$ Content models are Hidden Markov Models (HMMs) wherein states correspond to typesof information characteristic to the domain of interest (e.g., earthquake magnitude or previous earth quake occurrences), and state transitions capture possible information-presentation orderings within that domain.

An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ The development and application of computational models of text structure is a central concern in natural language processing.
An interesting aspect of our generative approach is that we model HMM outputs as Gaussian vectors (log probabilities of observing entire sentences based on our language models), as opposed to sequences of terms, as done in (Barzilay and Lee,2004). $$$$$ For this task, we de velop a new content-model-based learning algorithm for sentence selection.
