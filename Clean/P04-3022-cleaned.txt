They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). $$$$$ Here is an example.
They use two kinds of features: syntactic ones and word based ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004). $$$$$ Dependency (word on which is depedent), (POS of word on which is dependent), (chunk label of word on which is dependent), Parse Tree PERSON-NP-PP-ORGANIZATION, PERSON-NP-PP:of-ORGANIZATION (both derived from the path shown in bold in Figure 1).

Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004). $$$$$ All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework (Ratnaparkhi, 1999).
Supervised learning method using syntactic and word-based features, the path of the pairs of NEs in the parse tree and the word n gram between pairs of NEs (Kambhatla, 2004). $$$$$ Dependency (word on which is depedent), (POS of word on which is dependent), (chunk label of word on which is dependent), Parse Tree PERSON-NP-PP-ORGANIZATION, PERSON-NP-PP:of-ORGANIZATION (both derived from the path shown in bold in Figure 1).

The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. $$$$$ More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees.
The approaches proposed to the ACE RDC task such as kernel methods (Zelenko et al, 2002) and Maximum Entropy methods (Kambhatla, 2004) required the availability of large set of human annotated corpora which are tagged with relation instances. $$$$$ The reader is referred to (Florian et al., 2004; Ittycheriah et al., 2003; Luo et al., 2004) for more details of our mention detection and mention chaining modules.

We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004). $$$$$ We trained Maximum Entropy models using features derived from the feature streams described above.
We compare our results to a state-of-the-art supervised system similar to the system described in (Kambhatla, 2004). $$$$$ We built several models to compare the relative utility of the feature streams described in the previous section.

Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004). $$$$$ Here we present our general approach and describe our ACE results.
Kambhatla (2004) took a similar approach but used multivariate logistic regression (Kambhatla, 2004). $$$$$ Our approach can easily scale to include more features from a multitude of sources–e.g.

 $$$$$ Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.
 $$$$$ We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.

Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features. $$$$$ We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
Kambhatla (2004) developed a method for extracting relations by applying Maximum Entropy models to combine lexical, syntactic and semantic features and report that they obtain improvement in results when they combine variety of features. $$$$$ We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.

 $$$$$ Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.
 $$$$$ We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.

Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc. $$$$$ The feature streams are: Words The words of both the mentions and all the words in between.
Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the smallest parse fragment connecting the two mentions, etc. $$$$$ Words , , , .

For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. $$$$$ We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
For the feature-based methods, Kambhatla (2004) employed Maximum Entropy models to combine diverse lexical, syntactic and semantic features in relation extraction, and achieved the F-measure of 52.8 on the 24 relation subtypes in the ACE RDC 2003 corpus. $$$$$ We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.

Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods. $$$$$ For the Template Relations task of MUC-7, BBN researchers (Miller et al., 2000) augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees.
Another problem is that, although they can explore some structured information in the parse tree (e.g. Kambhatla (2004) used the non-terminal path connecting the given two entities in a parse tree while Zhou et al (2005) introduced additional chunking features to enhance the performance), it is found difficult to well preserve structured information in the parse trees using the feature-based methods. $$$$$ Parse Tree The path of non-terminals (removing duplicates) connecting the two mentions in the parse tree, and the path annotated with head words.

 $$$$$ Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.
 $$$$$ We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.

Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. $$$$$ All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework (Ratnaparkhi, 1999).
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree. $$$$$ Dependency The words and part-of-speech and chunk labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic parse tree.

Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. $$$$$ Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction
Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect. $$$$$ Overlap The number of words (if any) separating the two mentions, the number of other mentions in between, flags indicating whether the two mentions are in the same noun phrase, verb phrase or prepositional phrase.

Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. $$$$$ Automatic Content Extraction (ACE, 2004) is an evaluation conducted by NIST to measure Entity Detection and Tracking (EDT) and relation detection and characterization (RDC).
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes. $$$$$ We report both the F-measure' and the ACE value of relation extraction.

(Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model. $$$$$ Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction
(Culotta and Sorensen, 2004) extended this work to estimate kernel functions between augmented dependency trees, while (Kambhatla, 2004) combined lexical features, syntactic features, and semantic features in a maximum entropy model. $$$$$ More recently, (Zelenko et al., 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees.

However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level. $$$$$ Combining Lexical Syntactic And Semantic Features With Maximum Entropy Models For Information Extraction
However, the semantic features discussed in (Kambhatla, 2004) still focus on the word level instead of the conceptual level. $$$$$ Mention Level The mention level (one of NAME, NOMINAL, PRONOUN) of both the mentions.

Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction. $$$$$ We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text.
Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction. $$$$$ We build Maximum Entropy models for extracting relations that combine diverse lexical, syntactic and semantic features.

 $$$$$ Here is an example: The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.
 $$$$$ We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components, feature suggestions and guidance.

Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features. $$$$$ Parse Tree The path of non-terminals (removing duplicates) connecting the two mentions in the parse tree, and the path annotated with head words.
Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features. $$$$$ Adding the parse tree and dependency tree based features gives us our best result by exploiting the consistent syntactic patterns exhibited between mentions for some relations.
