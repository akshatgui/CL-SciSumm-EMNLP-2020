Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ 2.3.1 Model 6.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ This notation indicates that five iterations of Model 1, five iterations of HMM, three iterations of Model 3, three iterations of Model 4, and three iterations of Model 6 are performed.

The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ The persons conducting the annotation are asked to specify alignments of two different kinds

We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ These definitions of precision, recall and the AER are based on the assumption that a recall error can occur only if an S alignment is not found and a precision error can occur only if the found alignment is not even P. The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ Tables 15 and 16 show precision, recall, and alignment error rate for the last iteration of Model 6 for both translation directions.

It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ Instead, heuristic models are usually used.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ 2.1.2 Heuristic Models.

However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000).
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ 2.3.1 Model 6.

The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000).

The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ Alignment models similar to those studied in this article have been used as a starting point for refined phrase-based statistical machine translation systems (Alshawi, Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al. 2000).

Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.

To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ The entries for each language can be a single word or an entire phrase.

The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001).

All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ 1998; Garc´ıa-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney 2001; Germann et al. 2001).

Then the procedure is quite standard $$$$$ Table 1 offers an overview of the properties of the various alignment models.
Then the procedure is quite standard $$$$$ Table 8 shows the computing time for performing one iteration of the EM algorithm.

Word alignment scores $$$$$ The baseline alignment model does not allow a source word to be aligned with more than one target word.
Word alignment scores $$$$$ To solve this problem, we perform training in both translation directions (source to target, target to source).

The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ 2.1.2 Heuristic Models.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996).

GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ In this paper, we use Models 1 through 5 described in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model described in Vogel, Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, which we call Model 6.
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ This model is also referred to as a homogeneous HMM (Vogel, Ney, and Tillmann 1996).

In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ Statistical alignment models are often the basis of single-word-based statistical machine translation systems (Berger et al. 1994; Wu 1996; Wang and Waibel 1998; Nießen et al.
In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ From this association score matrix, the word alignment is then obtained by applying suitable heuristics.

Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ The method with a varying µ gives worse results, but this method has one fewer parameter to be optimized on held-out data.

The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ So far, refined statistical alignment models have in general been rarely used.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ In such systems, the quality of the machine translation output directly depends on the quality of the initial word alignment (Och and Ney 2000).

Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.

It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ These applications crucially depend on the quality of the word alignment (Och and Ney 2000; Yarowsky and Wicentowski 2000).
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.
