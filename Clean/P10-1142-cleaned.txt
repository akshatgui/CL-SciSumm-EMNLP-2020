As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model $$$$$ From a learning perspective, a two-step approach to coreference — classification and clustering — is undesirable.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model $$$$$ The entity-mention model addresses the expressiveness problem with the mention-pair model.

(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ The entity-mention model addresses the expressiveness problem with the mention-pair model.
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ This model is commonly known as the entity-mention model.

This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ Below we discuss how these weaknesses are addressed by the entity-mention model and ranking models.
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ The entity-mention model addresses the expressiveness problem with the mention-pair model.

A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Note that several leading coreference researchers have published books (e.g., Mitkov (2002)), written survey articles (e.g., Mitkov (1999), Strube (2009)), and delivered tutorials (e.g., Strube (2002), Ponzetto and Poesio (2009)) that provide a broad overview of coreference research.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ It would be interesting to incorporate this idea into a learning-based resolver.

In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ The widespread popularity of machine learning approaches to coreference resolution can be attributed in part to the public availability of annotated coreference corpora.

The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ Some features determine NP type (e.g., are both NPs definite or pronouns?).

In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ One criticism of the closest-first and best-first clustering algorithms is that they are too greedy.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ Note that both scorers have only been defined for the case where the key partition has the same set of NPs as the response partition.

For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ There has been an increasing amount of work on investigating semantic features for coreference resolution.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)).

Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ A Bell tree provides an elegant way of organizing the space of NP partitions.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Below we give an overview of these features.

We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ The mention-pair model is a classifier that determines whether two NPs are coreferent.
We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ In the third method, a mention detector is first trained on the gold-standard NPs in the training texts, and is then applied to automatically extract system mentions in a test text.7 Note that these three extraction methods typically produce different numbers of NPs

The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Below we describe some commonly used coreference clustering algorithms.
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Several algorithms that address one or both of these problems have been used for coreference clustering.

Interested readers can refer to the literature review by Ng (2010). $$$$$ Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s.
Interested readers can refer to the literature review by Ng (2010). $$$$$ While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Converse (2006)), Japanese (e.g., Iida (2007)), Arabic (e.g., Luo and Zitouni (2005)), Dutch (e.g., Hoste (2005)), German (e.g., Wunsch (2010)), Swedish (e.g., Nilsson (2010)), and Czech (e.g., Ngu.y et al. (2009)).

Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ Clinton” and “she” will end up in the same cluster, which is incorrect due to gender mismatch.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)).

For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Note that several leading coreference researchers have published books (e.g., Mitkov (2002)), written survey articles (e.g., Mitkov (1999), Strube (2009)), and delivered tutorials (e.g., Strube (2002), Ponzetto and Poesio (2009)) that provide a broad overview of coreference research.
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ To apply these scorers to automatically extracted NPs, different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al. (2009)).

 $$$$$ In particular, clusters are formed based on a small subset of the pairwise decisions made by the model.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
