Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ A system that can automatically identify the perspective from which a document is written will be a valuable tool for people analyzing huge collections of documents from different perspectives.
Given a pair of document collections A and B, our goal is not to construct classifiers that can predict if a document was written from the perspective of A or B (Lin et al, 2006), but to determine if the document collection pair (A, B) convey opposing perspectives. $$$$$ Given a new document W with a unknown document perspective, the perspective D� is calculated based on the following conditional probability.

In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
In some cases, the author may also introduce their own perspective (Lin et al, 2006) through the use of framing (Greene and Resnik, 2009). $$$$$ We introduce a new binary random variable, S, to model how strongly a perspective is reflected at the sentence level.

Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
Hierarchical Bayesian modelling has recently gained notable popularity in many core areas of natural language processing, from morphological segmentation (Goldwater et al, 2009) to opinion modelling (Lin et al, 2006). $$$$$ Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work.

We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. $$$$$ As an alternative, we evaluate how accurately LSPM predicts the perspective of a document, again using 10-fold cross validation.
We show that using adaptive naive Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al, 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. $$$$$ Using this model, we obtain a classification accuracy of only 0.7529, which is much lower than the accuracy previously achieved at the document level.

The support vector machine (SVM), NB B and LSPM results are taken directly from Lin et al (2006). $$$$$ We evaluate three different models for the task of identifying perspective at the document level

We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ To evaluate the statistical models, we train them on the documents in the bitterlemons corpus and calculate how accurately each model predicts document perspective in ten-fold cross-validation experiments.
We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. Bitter Lemons (BL) $$$$$ As an alternative, we evaluate how accurately LSPM predicts the perspective of a document, again using 10-fold cross validation.

 $$$$$ Given a new document W with a unknown document perspective, the perspective D� is calculated based on the following conditional probability.
 $$$$$ The first half of burn-in samples are discarded.

For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
For these features we replace the opinion words with their positive or negative polarity equivalents (Lin et al, 2006). $$$$$ A positive or negative opinion toward a particular movie or product is fundamentally different from an overall perspective.

They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
They attempt to identify a position of a debate, such as ideological (Somasundaran et al, 2010, Lin et al, 2006) or product comparison debate (Somasundaran et al, 2009). $$$$$ Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work.

These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
These experiments were conducted in political debate corpus (Lin et al 2006). $$$$$ There has been research in discourse analysis that examines how different perspectives are expressed in political discourse (van Dijk, 1988; Pan et al., 1999; Geis, 1987).

(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ As with review classification, we treat perspective identification as a document-level classification task, discriminating, in a sense, between different types of opinions.
(Lin et al, 2006) explores relationships between sentence-level and document-level classification for a stance-like prediction task. Among the literature on ideological subjectivity, perhaps most similar to our work is (Somasundaran and Wiebe, 2010). $$$$$ Identifying perspectives at the sentence level is thus more difficult than identifying perspectives at the document level.

Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ By perspective we mean a point of view, for example, from the perspective of Democrats or Republicans.
Some sentences are written from a certain perspective (Lin et al, 2006) or point of view. $$$$$ While by its very nature we expect much of the language that is used when presenting a perspective or point-of-view to be subjective, labeling a document or a sentence as subjective is not enough to identify the perspective from which it is written.

As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
As it is easy for a human to identify the perspective of an author (Lin et al, 2006), this measure facilitated the annotation task. $$$$$ We include the results for the naive Bayes models from Table 3 for easy comparison.

In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al (2006). $$$$$ We model the process of generating documents from a particular perspective as follows

Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ Denote a training corpus as a set of documents Wn and their perspectives labels Dn, n = 1, ... , N, where N is the total number of documents in the corpus.
Israeli-Palestinian Conflict In order to make a direct comparison here with prior state-of-the-art work on sentiment analysis, we re port on sentiment classification using OPUS features in experiments using a publicly available corpus involving opposing perspectives, the Bitter Lemons corpus introduced by Lin et al (2006) .Corpus. $$$$$ We model the process of generating documents from a particular perspective as follows

Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. $$$$$ Research on the automatic classification of movie or product reviews as positive or negative (e.g., (Pang et al., 2002; Morinaga et al., 2002; Turney and Littman, 2003; Nasukawa and Yi, 2003; Mullen and Collier, 2004; Beineke et al., 2004; Hu and Liu, 2004)) is perhaps the most similar to our work.
Within computational linguistics, what we call implicit sentiment was introduced as a topic of study by Lin et al (2006) under the rubric of identifying perspective, though similar work had begun earlier in the realm of political science. $$$$$ Although their research may have some similar goals, they do not take a computational approach to analyzing large collections of documents.

They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006). $$$$$ So far, research in automatic opinion recognition has primarily addressed learning subjective language (Wiebe et al., 2004; Riloff et al., 2003), identifying opinionated documents (Yu and Hatzivassiloglou, 2003) and sentences (Yu and Hatzivassiloglou, 2003; Riloff et al., 2003), and discriminating between positive and negative language (Pang et al., 2002; Morinaga et al., 2002; Yu and Hatzivassiloglou, 2003; Turney and Littman, 2003; Dave et al., 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Wilson et al., 2005).
They often belong to controversial subjects (e.g., religion, terrorism, etc.) where the same event can beseen from two or more opposing perspectives, like the IsraeliPalestinian conflict (Lin et al, 2006). $$$$$ We compare NB with SVM not only because SVM has been very effective for classifying topical documents (Joachims, 1998), but also to contrast generative models like NB with discriminative models like SVM.
