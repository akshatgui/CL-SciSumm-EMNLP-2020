FERGUS (Bangalore and Rambow,2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. $$$$$ We present initial re- suits showing that a tree-based model derived from a tree-annotated corpus improves on a tree model derived from an unannotated corpus, and that a tree-based stochastic model with a hand- crafted grammar outpertbrms both.
FERGUS (Bangalore and Rambow,2000) took dependency structures as inputs, and produced XTAG derivations by a stochastic tree model automatically acquired from an annotated corpus. $$$$$ There is no stochastic tree model, since, the, re, ~tr(, no trees.

However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). $$$$$ Two methods tbr 1)re- dieting the order of prenonfinal t~djectives in english.
However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). $$$$$ A lexicalized %ee Adjoining Grammar for English.

More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ Gen- eration that exploits corpus-based statistical knowledge.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ Forest-based statistical sentence generation.

The Fergus system (Bangalore and Rambow, 2000) employs a statistical tree model to select probable trees and a word n-gram model to rank the string candidates generated from the best trees. $$$$$ Trees t;hat can adjoin to other trees (and have entries in the adjunct

Fergus (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank's representation. $$$$$ As can be seen, this structure is a dependency tree and resembles a representation of lexical argument structure.
Fergus (Bangalore and Rambow, 2000) used the Penn TreeBank as a corpus, requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank's representation. $$$$$ FERGUS aS presented in this paper is not ready to be used as a module in applications.

Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ To our knowledge, the first to use stochastic techniques in NLG were Langkilde and Knight (1998a) and (1998b).
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ More recent work on aspects of stochastic gen- eration include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000).

We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. $$$$$ More recent work on aspects of stochastic gen- eration include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000).
We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways. $$$$$ Ado, tail(~d dis(:ussion of these experiments and results is t)r(,s(mto, d in (Bangalore (,

WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al, 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). $$$$$ As can be seen, this structure is a dependency tree and resembles a representation of lexical argument structure.
WordNet has been used by many researchers for different purposes ranging from the construction or extension of knowledge bases such as SENSUS (Knight and Luk, 1994) or the Lexical Conceptual Structure Verb Database (LVD) (Green et al, 2001) to the faking of meaning ambiguity as part of system evaluation (Bangalore and Rambow, 2000). $$$$$ Ado, tail(~d dis(:ussion of these experiments and results is t)r(,s(mto, d in (Bangalore (,

These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). $$$$$ This step can be seen as analogous to "supertag- ging" (Bangalore and Joshi, 1999), except that now supertags (i.e., names of trees) must be fbund tbr words in a tree rather than tbr words in a linear sequence.
These concepts are then realized into words resulting in a bag of words with syntactic relations (Bangalore and Rambow, 2000). $$$$$ a The ~IYee Chooser makes the simplifying as- 2In the system that we used in the experiments de- scribed in Section 4, all words (including flmction words) need to be present in tt, e inlmt representation, flflly in- flected.

We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). $$$$$ These metrics, simple accuracy and generation accuracy, allow us to evaluate without human intervention, automatically and objectively.
We have introduced a novel type of supertagger, which we have dubbed a hypertagger, that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of (Bangalore and Rambow, 2000). $$$$$ In all three cases, we will provide both knowledge-based and stochas- tic components, with the aim of comparing their behaviors, and using one type as a back-up tbr the other type.

Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). $$$$$ Explo i t ing a Probabi l ist ic  Hierarchical Mode l  for Generat ion Srinivas Bangalore and Owen Rambow AT&T Labs Research 180 Park Avenue F lorham Park, NJ 07932 {sr in?,  rambow}@research,  a r t .
Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). $$$$$ Trees t;hat can adjoin to other trees (and have entries in the adjunct

 $$$$$ 44 estimate there was no cost for phase the second Figure 4: Inlmt to FEII.GUS Smnl)tions that the (:hoice of n tree.
 $$$$$ upenn, edu/~xtag/ tech- repor t / tech- repor t  .htral, The Insti- tute for Research in Cognitive Science, Uni- versity of Pennsylvania.

In recent years, there has been a steady stream of research in statistical text generation (see Langkilde and Knight (1998), and Bangalore and Rambow (2000)). $$$$$ More recent work on aspects of stochastic gen- eration include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000).
In recent years, there has been a steady stream of research in statistical text generation (see Langkilde and Knight (1998), and Bangalore and Rambow (2000)). $$$$$ Irene Langkilde and Kevin Knight.

 $$$$$ 44 estimate there was no cost for phase the second Figure 4: Inlmt to FEII.GUS Smnl)tions that the (:hoice of n tree.
 $$$$$ upenn, edu/~xtag/ tech- repor t / tech- repor t  .htral, The Insti- tute for Research in Cognitive Science, Uni- versity of Pennsylvania.

In other words, in generating a form f to express an input, one wants to maximize the probability of the form, P (f), with respect to some gold-standard corpus, and thus express the in put in a way that resembles the realizations in the corpus most closely (Bangalore and Rambow, 2000). $$$$$ for our example input.
In other words, in generating a form f to express an input, one wants to maximize the probability of the form, P (f), with respect to some gold-standard corpus, and thus express the in put in a way that resembles the realizations in the corpus most closely (Bangalore and Rambow, 2000). $$$$$ tbr our example input.

FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. $$$$$ The structure of the paper is as tbllows.
FERGUS (Bangalore and Rambow, 2000), on the other hand, employs a model of syntactic structure during sentence realization. $$$$$ As can be seen, this structure is a dependency tree and resembles a representation of lexical argument structure.

Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricutand Marcu (2006). $$$$$ More recent work on aspects of stochastic gen- eration include (Langkilde and Knight, 2000), (Malouf, 1999) and (Ratnaparkhi, 2000).
Both the model of Amalgam and that presented here differ considerably from the n-gram models of Langkilde and Knight (1998), the TAG models of Bangalore and Rambow (2000), and the stochastic generation from semantic representation approach of Soricutand Marcu (2006). $$$$$ Irene Langkilde and Kevin Knight.

Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. $$$$$ FErtGUS follows Langkilde and Knights seminal work in using an n-gram language model, but; we augment it with a tree-based stochastic model and a traditional tree-based syntactic grammar.
Bangalore and Rambow (2000) use n-gram word sequence statistics in a TAG-based generation model to rank output strings and additional statistical and symbolic resources at intermediate generation stages. $$$$$ Forest-based statistical sentence generation.

 $$$$$ 44 estimate there was no cost for phase the second Figure 4: Inlmt to FEII.GUS Smnl)tions that the (:hoice of n tree.
 $$$$$ upenn, edu/~xtag/ tech- repor t / tech- repor t  .htral, The Insti- tute for Research in Cognitive Science, Uni- versity of Pennsylvania.
