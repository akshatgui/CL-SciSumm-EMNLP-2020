Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. $$$$$ Both models have been used to achieve state-of-the-art accuracy for a wide range of languages, as shown in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), but McDonald and Nivre (2007) showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models.
Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. $$$$$ The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task.

 $$$$$ Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.

For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.

Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007).
Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a).

We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). $$$$$ Integrating Graph-Based and Transition-Based Dependency Parsers
We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). $$$$$ Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank.

A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.
A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.

 $$$$$ Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.

Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ Integrating Graph-Based and Transition-Based Dependency Parsers
Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007).

Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a).
Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).

 $$$$$ Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.

Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. $$$$$ As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).
Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. $$$$$ But where MST is good, MSTMalt is often significantly better.

Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ To train a guided version BC of base model B with guide model C and training set T, the guided model is trained, not on the original training set T, but on a version of T that has been parsed with the guide model C under a cross-validation scheme (to avoid overlap with training data for C).
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others.

Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ To learn a scoring function on transitions, these systems rely on discriminative learning methods, such as memory-based learning or support vector machines, using a strictly local learning procedure where only single transitions are scored (not complete transition sequences).
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ Although both Malt and MST use discriminative algorithms, Malt uses a batch learning algorithm (SVM) and MST uses an online learning algorithm (MIRA).

We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ For example, one could start with a Malt model, use it to train a guided MSTMalt model, then use that as the guide to train a MaltMSTM.,t model, etc.
We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.

Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ (2004), who trained classifiers on auxiliary data to guide named entity classifiers.
Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others.

More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). $$$$$ Theoretically, these approaches are very different.
More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). $$$$$ As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007).

 $$$$$ Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.

In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.

 $$$$$ Though MSTParser is capable of defining features over pairs of arcs, we restrict the guide features over single arcs as this resulted in higher accuracies during preliminary experiments.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.

Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ Similarly, MSTMalt improves precision in the range where its base parser is inferior to Malt and for distances up to 4 has an accuracy comparable to or higher than its guide parser Malt.
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ But we also see that the guided models in all cases improve over their base parser and, in most cases, also over their guide parser.
