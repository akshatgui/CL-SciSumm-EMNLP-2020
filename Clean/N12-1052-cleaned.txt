 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ Our first method for inducing cross-lingual clusters has two stages.

Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ Recently, Dhillon et al. (2011) proposed a word embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER.
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ Below, we extend this approach to universal parsing by adding cross-lingual word cluster features.

Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008).
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ The approach proposed by McDonald et al. (2011) relies on these three assumptions.

Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure
Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ Our first method for inducing cross-lingual clusters has two stages.

Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions.
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families.

This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ Although our results are below the best reported results for EN and DE (Lin and Wu, 2009; Faruqui and Pad´o, 2010), the relative improvements of adding word clusters are inline with previous results on NER for EN (Miller et al., 2004; Turian et al., 2010), who report error reductions of approximately 25% from adding word cluster features.
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ The baseline results are comparable to those in McDonald et al. (2011) and thus also significantly outperform the results of recent unsupervised approaches (Berg-Kirkpatrick and Klein, 2010; Naseem et al., 2010).

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine.
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ While the performance of the transfer systems is very poor when no word clusters are used, adding cross-lingual word clusters give substantial improvements across all languages.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011).
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008).

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.

 $$$$$ For all experiments we fixed the number of clusters to 256 as this performed well on held-out data.
 $$$$$ All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.
