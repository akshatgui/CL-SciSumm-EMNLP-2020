Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.
Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ Table 1 lists the number of phrase pairs found in each paraphrase table before and after filtering.

To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ Commonly used metrics such as BLEU and earlier versions of Meteor make no distinction between content and function words.
To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.

For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). $$$$$ The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010).
For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.

Inverted Automatic Scores $$$$$ Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores.
Inverted Automatic Scores $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.

We report two translation measures $$$$$ The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010).
We report two translation measures $$$$$ While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.

System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.
System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ Test set translations are evaluated using BLEU, TER, and Meteor 1.2.

The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). $$$$$ We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6.
The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). $$$$$ Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model.

The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ We use the 2009 NIST Open Machine Translation Evaluation Urdu-English parallel data (Przybocki, 2009) plus 900M words of monolingual data from the English Gigaword corpus (Parker et al., 2009) to build a standard Moses system (Hoang et al., 2007) as follows.

The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. $$$$$ Meteor 1.3

We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ Meteor evaluates translation hypotheses by aligning them to reference translations and calculating sentence-level similarity scores.
We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.

 $$$$$ Stem

We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.
We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ To evaluate the impact of new features on other evaluation tasks, we follow Denkowski and Lavie (2010a), tuning versions of Meteor to maximize length-weighted sentence-level Pearson’s r correlation coefficient with adequacy and H-TER (Snover et al., 2006) scores of translations.
