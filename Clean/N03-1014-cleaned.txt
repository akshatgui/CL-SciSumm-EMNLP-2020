we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) are based on a history-based model of parser actions.
we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ Choosing this representation is a challenge for any history-based statistical parser, because the history is of unbounded size.

Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). $$$$$ We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).
Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). $$$$$ Previous work on applying SSNs to natural language parsing (Henderson, 2000) has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work.

This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process.

The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ In the work presented here, we automatically induce a finite set of real valued features to represent the parse history.
The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ In addition the SSN has a hidden layer, which computes a finite vector of real valued features from a sequence of inputs specifying the derivation history .

In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).
In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).

Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ Our pruning strategy is designed specifically for left-corner parsing.
Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ This allows use to trade off parsing accuracy for parsing speed, which is a much more important issue than training time.

We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. $$$$$ We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. $$$$$ The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.

The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).
The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).

Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ In addition the SSN has a hidden layer, which computes a finite vector of real valued features from a sequence of inputs specifying the derivation history .
Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ This hidden layer vector is the history representation .

Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.
Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ This paper has presented a method for estimating the parameters of a history-based statistical parser which does not require any a priori independence assumptions.

Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.
Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ Training a Simple Synchrony Network (SSN) is similar to training a log-linear model.

We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. $$$$$ To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.
We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. $$$$$ The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process.

SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.
SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ A neural network is trained simultaneously to estimate the probabilities of parser actions and to induce a finite representation of the unbounded parse history.

H03 indicates the model illustrated in (Henderson, 2003). $$$$$ To precisely specify this ordering, it is sufficient to characterize the state of the parser as a stack of nodes which are in the process of being parsed, as illustrated on the right in figure 1.
H03 indicates the model illustrated in (Henderson, 2003). $$$$$ The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).

(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ We also reduced the computational cost of terminal prediction by replacing the very large number of lower frequency tag-word pairs with tag-“unknown-word” pairs, which are also used for tag-word pairs which were not in the training set.
(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ This resulted in a vocabulary size of 512 tag-word pairs.

In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000).
In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).

Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ The neural network is used to estimate the parameters of this probability model.
Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ The method is a form of multi-layered neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000).

We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy.

To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ To achieve this structurally-determined inductive bias, we use Simple Synchrony Networks, which are specifically designed for processing structures.
To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ To investigate this issue, we trained several SSN parsers with an explicit representation of phrasal head.

(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. $$$$$ We can exploit this recency bias in inducing history representations by ensuring that information which is known to be important at a given step in the derivation is input directly to that step’s history representation, and that as information becomes less relevant it has increasing numbers of history representations to pass through before reaching the step’s history representation.
(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. $$$$$ The pre-defined features of the derivation history which are input to for node top at step are chosen to reflect the information which is directly relevant to choosing the next decision .
