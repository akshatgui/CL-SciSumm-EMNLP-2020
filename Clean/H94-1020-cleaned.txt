In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al, 1994). $$$$$ Also included is a skeletally parsed version of the Brown corpus, the classic million word balanced corpus of American English [5, 6].
In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al, 1994). $$$$$ Building a large annotated corpus of English

The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al, 1994) for predictability. $$$$$ ARGUMENT-ADJUNCT STRUCTURE In a well developed predicate-argument scheme, it would seem desirable to label each argument of a predicate with an appropriate semantic label to identify its role with respect o that predicate.
The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al, 1994) for predictability. $$$$$ The computational nalysis of English.

The Switchboard (Marcus et al, 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. $$$$$ Automatically acquiring phrase structure using distributional analysis.
The Switchboard (Marcus et al, 1994) corpus contains transcriptions of spoken, spontaneous conversation annotated with phrase-structure trees. $$$$$ Building a large annotated corpus of English

Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank $$$$$ hand-retagged using the Penn Treebank tagset.
Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank $$$$$ Hand-in-hand with this limi- tation of the existing Penn Treebank for parser testing is a parallel imitation for automatic methods for parser training for parsers based on deeper epresentations.

Figure 1 $$$$$ We have adopted the set of functional tags shown in Figure 2 for use within the current annotation scheme.
Figure 1 $$$$$ Either the adverb is trapped within the VP, so that the complement can occur within the VP, where it belongs, or else the adverb is at- tached to the S, closing off the VP and forcing the comple- ment to attach to the S. This "trapping" problem serves as a limitation for groups that currently use Treebank material to semiautomatically derive lexicons for particular applications.

We will be reporting on results using PropBank (Kingsbury et al, 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs in the Wall Street Journal (WSJ) part of the Penn Tree Bank (Marcus et al, 1994). $$$$$ We have now begun to annotate this level of structure diting the present Penn Treebank; we intend to automatically ex- tract a bank of predicate-argument structures intended at the very least for parser evaluation from the resulting annotated corpus.
We will be reporting on results using PropBank (Kingsbury et al, 2002), a 300k-word corpus in which predicate argument relations are marked for part of the verbs in the Wall Street Journal (WSJ) part of the Penn Tree Bank (Marcus et al, 1994). $$$$$ Structural Ambiguity and Lexical Relations.

We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al, 1994) section 2-21. $$$$$ Hand-in-hand with this limi- tation of the existing Penn Treebank for parser testing is a parallel imitation for automatic methods for parser training for parsers based on deeper epresentations.
We applied our approaches to parsing errors given by the HPSG parser Enju, which was trained on the Penn Treebank (Marcus et al, 1994) section 2-21. $$$$$ We have now begun to annotate this level of structure diting the present Penn Treebank; we intend to automatically ex- tract a bank of predicate-argument structures intended at the very least for parser evaluation from the resulting annotated corpus.

Note that the tag set used by ltpos is the Penn Treebank tag set (Marcus et al, 1994). $$$$$ The -TMP tag here marks time (TeMPoral) phrases.
Note that the tag set used by ltpos is the Penn Treebank tag set (Marcus et al, 1994). $$$$$ We have adopted the set of functional tags shown in Figure 2 for use within the current annotation scheme.

The use of PCFG is tied to the annotation principles of popular tree banks, such as the Penn Treebank (PTB) (Marcus et al, 1994), which are used as a data source for grammar extraction. $$$$$ In the earlier corpus annotation scheme, We originally used only standard syntactic labels (e.g.
The use of PCFG is tied to the annotation principles of popular tree banks, such as the Penn Treebank (PTB) (Marcus et al, 1994), which are used as a data source for grammar extraction. $$$$$ Lexicon and grammar.

A second approach involves querying gold standard treebanks such as the Penn Treebank (Marcus et al, 1994) and Tiger Treebank (Brantset al, 2004) to determine the frequency of certain phenomena. $$$$$ hand-retagged using the Penn Treebank tagset.
A second approach involves querying gold standard treebanks such as the Penn Treebank (Marcus et al, 1994) and Tiger Treebank (Brantset al, 2004) to determine the frequency of certain phenomena. $$$$$ Frequency analysis of English usage.

In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. $$$$$ hand-retagged using the Penn Treebank tagset.
In this study we use PropBanked versions of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1994) and part of the Brown portion of the Penn Treebank. $$$$$ Building a large annotated corpus of English

Bangalore et al (2001) investigate the effect of training size on performance while using grammars automatically extracted from the Penn II Treebank (Marcus et al, 1994) for generation. $$$$$ hand-retagged using the Penn Treebank tagset.
Bangalore et al (2001) investigate the effect of training size on performance while using grammars automatically extracted from the Penn II Treebank (Marcus et al, 1994) for generation. $$$$$ Automatically acquiring phrase structure using distributional analysis.

In the Penn Treebank (PTB) (Marcus et al, 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. $$$$$ hand-retagged using the Penn Treebank tagset.
In the Penn Treebank (PTB) (Marcus et al, 1994), e.g., this mechanism is a combination of special labels and empty nodes, establishing implicit additional edges. $$$$$ In the earlier corpus annotation scheme, We originally used only standard syntactic labels (e.g.

This results in several aspects that distinguish the MH treebank from, e.g., the WSJ Penn tree bank annotation scheme (Marcus et al, 1994). $$$$$ THE PENN TREEBANK

Also, the MH tree bank is much smaller than the ones for, e.g., English (Marcus et al, 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the all subtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). $$$$$ Brill, E., Marcus, M., 1992.
Also, the MH tree bank is much smaller than the ones for, e.g., English (Marcus et al, 1994) and Arabic (Maamouri and Bies, 2004), making it hard to apply data-intensive methods such as the all subtrees approach (Bod, 1992) or full lexicalization (Collins, 2003). $$$$$ The computational nalysis of English.

The idea is to augment the Penn Treebank (Marcus et al, 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al, 2005), and generate a rich training corpus. $$$$$ In the earlier corpus annotation scheme, We originally used only standard syntactic labels (e.g.
The idea is to augment the Penn Treebank (Marcus et al, 1994) constituent labels with the semantic role labels from the PropBank (Palmer et al, 2005), and generate a rich training corpus. $$$$$ Building a large annotated corpus of English

Note that the Berkeley parser is trained on the Penn treebank (Marcus et al, 1994) yet the HPSG parser is trained on the HPSG tree bank (Miyao and Tsujii, 2008). $$$$$ Hand-in-hand with this limi- tation of the existing Penn Treebank for parser testing is a parallel imitation for automatic methods for parser training for parsers based on deeper epresentations.
Note that the Berkeley parser is trained on the Penn treebank (Marcus et al, 1994) yet the HPSG parser is trained on the HPSG tree bank (Miyao and Tsujii, 2008). $$$$$ We have now begun to annotate this level of structure diting the present Penn Treebank; we intend to automatically ex- tract a bank of predicate-argument structures intended at the very least for parser evaluation from the resulting annotated corpus.

We conducted two experiments on Penn Treebank II corpus (Marcus et al, 1994). $$$$$ Many users of the Penn Treebank now want forms of an- notation richer than provided by the projects first phase, as well as an increase in the consistency of the preliminary corpus.
We conducted two experiments on Penn Treebank II corpus (Marcus et al, 1994). $$$$$ Building a large annotated corpus of English

The PennTreebank II (Marcus et al, 1994) marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role. $$$$$ The logical sub- ject by-phrase, if present, is a child of VP, and is tagged -LGS (LoGical Subject).
The PennTreebank II (Marcus et al, 1994) marks subjects (SBJ), logical objects of passives (LGS), some reduced relative clauses (RRC), as well as other grammatical information, but does not mark each constituent with a grammatical role. $$$$$ The example that follows shows two conjoined relative clauses; note that relative clauses are normally adjoined to the antecedent NP.

The same idea was used by Magerman (1995), who developed the first "head table" for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words. $$$$$ Some would also like a less skeletal form of anno- tation, expanding the essentially context-free analysis of the current treebank to indicate non-contiguous structures and dependencies.
The same idea was used by Magerman (1995), who developed the first "head table" for the Penn Treebank (Marcus et al, 1994), and Collins (1996), whose constituent parser is internally based on probabilities of bilexical dependencies, i.e. dependencies between two words. $$$$$ D. Magerman and M. Marcus, 1991.
