Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). $$$$$ The part of speech tags for the development and test data were automatically assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus; to generate part of speech tags for the training data, we used 10-way jackknifing.8 English word clusters were derived from the BLLIP corpus (Charniak et al., 2000), which contains roughly 43 million words of Wall Street Journal text.9 The Czech experiments were performed on the Prague Dependency Treebank 1.0 (Hajiˇc, 1998; Hajiˇc et al., 2001), which is directly annotated with dependency structures.
Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). $$$$$ Our feature mappings are quite high-dimensional, so we eliminated all features which occur only once in the training data.

Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2.
Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).

In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ We chose to work with the Brown algorithm due to its simplicity and prior success in other NLP applications (Miller et al., 2004; Liang, 2005).
In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ As mentioned earlier, our approach was inspired by the success of Miller et al. (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach.

We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. $$$$$ By conducting experiments on datasets of varying sizes, we demonstrate that for fixed levels of performance, the cluster-based approach can reduce the need for supervised data by roughly half, which is a substantial savings in data-annotation costs (see Sections 4.2 and 4.4).
We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. $$$$$ Based on Table 3, we can extrapolate that cluster-based features reduce the need for supervised data by roughly a factor of 2.

Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. $$$$$ Dependency parsing depends critically on predicting head-modifier relationships, which can be difficult due to the statistical sparsity of these word-to-word interactions.
Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. $$$$$ Note that our feature sets were originally tuned for English parsing, and except for the use of Czech clusters, we made no attempt to retune our features for Czech.

The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).
The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ Our research, however, applies this technique to dependency parsing rather than named-entity recognition.

This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. $$$$$ Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).
This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. $$$$$ Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice.

We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ Similar observations regarding the effect of model order have also been made by Carreras (2007).
We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).

A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. $$$$$ Simple Semi-supervised Dependency Parsing
A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.

In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser.
In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ Following Miller et al. (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.

The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. $$$$$ In our experiments, we employed two different feature sets

Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).
Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ We briefly describe the Brown algorithm below.

In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ In order to provide word clusters for our experiments, we used the Brown clustering algorithm (Brown et al., 1992).
In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters.

Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ Previous research in this area includes several models which incorporate hidden variables (Matsuzaki et al., 2005; Koo and Collins, 2005; Petrov et al., 2006; Titov and Henderson, 2007).
Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ Terry Koo was funded by NSF grant DMS-0434222 and a grant from NTT, Agmt.

Moreover, we introduce two extensions related to dependency parsing $$$$$ Simple Semi-supervised Dependency Parsing
Moreover, we introduce two extensions related to dependency parsing $$$$$ In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.

In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections $$$$$ We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.
In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections $$$$$ We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3).

The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ Simple Semi-supervised Dependency Parsing
The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ We present a simple and effective semisupervised method for training dependency parsers.

Our experiments investigate the effectiveness of $$$$$ Simple Semi-supervised Dependency Parsing
Our experiments investigate the effectiveness of $$$$$ In this paper, we introduce lexical intermediaries via a simple two-stage semi-supervised approach.

We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner.
We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ Key to the success of our approach is the use of features which allow word-cluster-based information to assist the parser.

These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section.
These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ The development and test sets were not projectivized, so our secondorder parser is guaranteed to make errors in test sentences containing non-projective dependencies.
