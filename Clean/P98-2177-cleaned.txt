We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy.

(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ While we will be given the candidate attachment sites during testing, the training procedure assumes no a priori information about potential attachment sites.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information.

 $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.
 $$$$$ We thank Dr. Lauri Kartunnen for lending us the Spanish natural language tools, and Mike Collins for helpful discussions on this work.

The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ This extraction heuristic loosely resembles a step in the bootstrapping procedure used to get training data for the classifier of (Hindle and Rooth, 1993).
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.

The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ In the supervised case, both of the potential sites, namely the verb v and the noun n are known before the attachment is resolved.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ This technique uses the bigram counts of the extracted head word tuples, and backs off to the uniform distribution when the denominator is zero. where 7) is the set of possible prepositions, where all the counts c(...) are from the extracted head word tuples.

The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.
The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ Our result shows that the information in imperfect but abundant data from unambiguous attachments, as shown in Tables 2 and 3, is sufficient to resolve ambiguous prepositional phrase attachments at accuracies just under the supervised state-of-the-art accuracy.

As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ Also, the heuristic excludes examples with the verb to be from the training set (but not the test set) since we found them to be unreliable sources of evidence.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ Furthermore, the extraction heuristic was developed and tuned on a &quot;development set&quot;, i.e., a set of annotated examples that did not overlap with either the test set or the training set.

 $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.
 $$$$$ We thank Dr. Lauri Kartunnen for lending us the Spanish natural language tools, and Mike Collins for helpful discussions on this work.

 $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.
 $$$$$ We thank Dr. Lauri Kartunnen for lending us the Spanish natural language tools, and Mike Collins for helpful discussions on this work.

 $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.
 $$$$$ We thank Dr. Lauri Kartunnen for lending us the Spanish natural language tools, and Mike Collins for helpful discussions on this work.

More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ The tagger from (Ratnaparkhi, 1996) first annotates sentences of raw text with a sequence of partof-speech tags.

Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ Furthermore, the extraction heuristic was developed and tuned on a &quot;development set&quot;, i.e., a set of annotated examples that did not overlap with either the test set or the training set.
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ The Spanish test set has fewer ambiguous prepositions than the English test set, as shown by the accuracy of Ciba„.

Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.
Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ The unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser.

A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ We approximate Pr(pla,v,n) as follows: The rationale behind these approximations is that when generating p given a noun (verb) attachment, only the counts involving the noun (verb) are relevant, assuming also that the noun (verb) has an attached prepositional phrase, i.e., 0 = true.

The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary.

equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ Therefore, there is only one possible attachment site for the preposition, and either the verb v or the noun n does not exist, in the case of noun-attached preposition or a verb-attached preposition, respectively.
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ Let the random variable 0 range over {true, false), and let it denote the presence or absence of any preposition that is unambiguously attached to the noun or verb in question.

(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ Most of the previous successful approaches to this problem have been statistical or corpusbased, and they consider only prepositions whose attachment is ambiguous between a preceding noun phrase and verb phrase.
(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ The Spanish test set has fewer ambiguous prepositions than the English test set, as shown by the accuracy of Ciba„.
