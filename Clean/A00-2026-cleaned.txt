More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ Its performance is intended to serve as a baseline result to the more sophisticated models discussed later.

In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ The local and non-local information is integrated with use of features in a maximum entropy probability model, and a highly pruned search procedure attempts to find the best scoring word sequence according to the model.

The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ Trainable Methods For Surface Natural Language Generation
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation.

[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ All systems in this paper use attribute-value pairs as a semantic representation, which suffice as a representation for a limited domain like air travel.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ In the air travel domain, the length of a phrase fragment to describe an attribute is usually only a few words.

Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ NLG2 and NLG3 solve the lexical choice problem by learning the words (via features in the maximum entropy probability model) that correlate with a given attribute and local context, whereas (Elhadad et al., 1997) uses a rule-based approach to decide the word choice.

Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ All the trainable NLG systems in this paper assume the existence of a large corpus of phrases in which the values of interest have been replaced with their corresponding attributes, or in other words, a corpus of generation templates.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ The NLG2 and NLG3 systems automatically attempt to generalize from the knowledge inherent in the training corpus of templates, so that they can generate templates for novel attribute sets.

An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ All systems in this paper use attribute-value pairs as a semantic representation, which suffice as a representation for a limited domain like air travel.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs.

Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ NLG1 serves a baseline system and uses phrase frequencies to generate a whole phrase in one step, while NLG2 and NLG3 use maximum entropy probability models to individually generate each word in the phrase.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ NLG2 and NLG3 are also statistical learning approaches but generate from an actual semantic representation.

A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ We present experiments in which we generate phrases to describe flights in the air travel domain.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998).

More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ The only trainable approaches (known to the author) to surface generation are the purely statistical machine translation (MT) systems such as (Berger et al., 1996) and the corpus-based generation system described in (Langkilde and Knight, 1998).
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ Our approach differs from the corpus-based surface generation approaches of (Langkilde and Knight, 1998) and (Berger et al., 1996).
