In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ Dynamic Programming for Linear-Time Incremental Parsing
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ So we apply the same beam search idea from Sec.

While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991).
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).

Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ Feature templates are functions that draw information from the feature window (see Tab.
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ A new state has the form where [i..j] is the span of the top tree s0, and sd..s1 are merely “left-contexts”.

In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.

Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ Our dynamic programming algorithm also runs on top of beam search in practice.
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.

Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972).
Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context).

The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ So we apply the same beam search idea from Sec.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec.

The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ To solve this problem we borrow the idea of “graph-structured stack” (GSS) from Tomita (1991).
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989).

However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Our dynamic programming algorithm also runs on top of beam search in practice.
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ 5d shows the (almost linear) correlation between dependency accuracy and search quality, confirming that better search yields better parsing.

To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ Dynamic Programming for Linear-Time Incremental Parsing
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice.

The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ Dynamic Programming for Linear-Time Incremental Parsing
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999).

The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ 1(a) for the list of feature templates used in the full model.

W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 10011136), development (secs 886-931 and 11481151), and test (secs 816-885 and 1137-1147) sets, assume gold-standard POS-tags for the input, and use the head rules of Zhang and Clark (2008).

Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). $$$$$ Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.
Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). $$$$$ Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ For Secs.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Following the set-up of Duan et al. (2007) and Zhang and Clark (2008), we split CTB5 into training (secs 001-815 and 10011136), development (secs 886-931 and 11481151), and test (secs 816-885 and 1137-1147) sets, assume gold-standard POS-tags for the input, and use the head rules of Zhang and Clark (2008).

H&S10 refers to the results of Huang and Sagae (2010). $$$$$ The higher frequency of early updates results in faster iterations of perceptron training.
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ Our technique is orthogonal to theirs, and combining these techniques could potentially lead to even better results.

In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ 9,10
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.

Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.
Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ Our dynamic programming algorithm also runs on top of beam search in practice.

We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). $$$$$ This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the queue head position (current word q0 is wj).
We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). $$$$$ 5b shows a similar comparison for dependency accuracy.

This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ 1, which is a subset of those in McDonald et al. (2005b).
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data.
