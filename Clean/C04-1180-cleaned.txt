For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004). $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004). $$$$$ Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.

CCG-based syntactic parsing (Bos et al, 2004). $$$$$ The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).
CCG-based syntactic parsing (Bos et al, 2004). $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). $$$$$ The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).
This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.

It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).

Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.

Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. $$$$$ The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.

Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). $$$$$ The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).
Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). $$$$$ However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.
As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). $$$$$ The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.

At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. $$$$$ The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).

Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ Briscoe and Carroll (2002) gen erate underspecified semantic representations fromtheir robust parser.

In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). $$$$$ The use of the ?-calculusis integral to our method.
In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences. $$$$$ Wide-Coverage Semantic Representations From A CCG Parser
Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences. $$$$$ There are a number of ad vantages to using CCG for this task.
