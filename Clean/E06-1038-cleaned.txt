McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.
McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ In this paper we have described a new system for sentence compression.

However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. $$$$$ The algorithm then either shifts (considers new words and subtrees for x), reduces (combines subtrees from x into possibly new tree constructions) or drops (drops words and subtrees from x) on each step of the algorithm.
However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. $$$$$ We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.

McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.
McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ That is, the algorithm will return the highest scoring compression regardless of length.

For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.
For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ When looking at importance, we see that our system actually does the best – even better than humans.

The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.
The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ We simply augment the dynamic programming table and calculate C[i][r], which is the score of the best compression of length r that ends at word xi.

 $$$$$ We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.
 $$$$$ This work was supported by NSF ITR grants 0205448 and 0428193.

Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006). $$$$$ To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).
Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006). $$$$$ However, we use these syntactic constraints as soft evidence in our model.

 $$$$$ We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.
 $$$$$ This work was supported by NSF ITR grants 0205448 and 0428193.

One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).
One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ This is similar in purpose to the source model from the noisy-channel system.

This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ Discriminative Sentence Compression With Soft Syntactic Evidence
This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ P(x

 $$$$$ We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.
 $$$$$ This work was supported by NSF ITR grants 0205448 and 0428193.

Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ When looking at importance, we see that our system actually does the best – even better than humans.
Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.

This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005).
This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.

McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).

 $$$$$ We also add a feature indicating if yj−1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.
 $$$$$ This work was supported by NSF ITR grants 0205448 and 0428193.

Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora. $$$$$ We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree model is nearer to our system (around 57-58%).
Note that their model is a strong baseline: it performed significantly better than competitive approaches (McDonald, 2006) across a variety of compression corpora. $$$$$ This system has many advantages over previous approaches.

Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ Results are shown in Table 1.
Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ This is quantified in the standard deviation of the two systems.

For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.
For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ Another advantage is distance.

The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ The noisy-channel model defines the problem as finding the compressed sentence with maximum conditional probability P(y) is the source model, which is a PCFG plus bigram language model.
The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.

McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.
McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ This system uses discriminative large-margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.
