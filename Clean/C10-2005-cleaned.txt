Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ For polarity detection, they select the positive examples for the training data from the tweets containing good emoticonsand negative examples from tweets contain ing bad emoticons.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ One exception is TwitterSentiment (Go et al., 2009), for instance, which considers tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and builds a classifier using unigrams and bigrams as features.

 $$$$$ To illustrate which of the proposed features are more effective for this task, the top-5 features in terms of information gain, based on our trainingdata, are

The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ As we show later, this boosts the classification performance, mainlybecause it removes tweets labeled as subjective by the data sources but are in fact objec tive; 3.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ We also investigated the influence of the size oftraining data on classification performance.

The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ A variety of features have been exploited on the problem of sentiment detection (Pang and Lee, 2004; Pang et al, 2002; Wiebe et al, 1999; Wiebeand Riloff, 2005; Riloff et al, 2006) including unigrams, bigrams, part-of-speech tags etc. A natural choice would be to use the raw word represen tation (n-grams) as features, since they obtained good results in previous works (Pang and Lee, 2004; Pang et al, 2002) that deal with large texts.However, as we want to perform sentiment detection on very short messages (tweets), this strategy might not be effective, as shown in our ex periments.
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005).

In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ In this paper, we propose an approach toautomatically detect sentiments on Twit ter messages (tweets) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ 3.1 Features.

We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ Second, wepresent some modifications in the proposed fea tures that are more suitable for this task.

From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ training set, we remove from this set tweets that contain the top-n opinion words in the subjectivity training set, e.g., words as cool, suck, awesome etc. As we show in Section 4, this process is in fact able to remove certain noisy in the training data,leading to a better performing subjectivity classi fier.
From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ However, looking at our training data almost half of the occurrences of this word appears in the positive set and the other half inthe negative set.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ 3.1 Features.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ An other common characteristic of some of them isthe use of n-grams as features to create their mod els.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ 3.1 Features.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ An other common characteristic of some of them isthe use of n-grams as features to create their mod els.

The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ 3.1 Features.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ Meta-features.

Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ 3.2 Subjectivity Classifier.
Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ Finally, since the data qual ity provided by TwitterSentiment is better than the3For this experiment, we used the TwitterSA(single) con figuration.
