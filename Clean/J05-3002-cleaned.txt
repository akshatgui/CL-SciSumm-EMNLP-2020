To evaluate the grammaticality of our generated summaries, following common practice (Barzilay and McKeown, 2005), we randomly selected 50 sentences from original conversations and system generated abstracts, for each dataset. $$$$$ The judge also had access to the original theme from which these sentences were generated.
To evaluate the grammaticality of our generated summaries, following common practice (Barzilay and McKeown, 2005), we randomly selected 50 sentences from original conversations and system generated abstracts, for each dataset. $$$$$ To evaluate our sentence fusion algorithm, we selected 100 themes following the procedure described in the previous section.

Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from "meme" tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). $$$$$ Sentence Fusion For Multidocument News Summarization
Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from "meme" tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). $$$$$ MultiGen takes as input a cluster of news stories on the same event and produces a summary which synthesizes common information across input stories.

The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. $$$$$ This algorithm operates on the dependency trees for pairs of input sentences.
The pioneering work on fusion is Barzilay and McKeown (2005), which introduces the frame work used by subsequent projects: they represent the inputs by dependency trees, align some words to merge the input trees into a lattice, and then extract a single, connected dependency tree as the output. $$$$$ Two dependency trees and their alignment tree.

Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non-overlapping summary sentence. $$$$$ In the subsections that follow, we describe first how this representation is computed, then how dependency subtrees are aligned, and finally how we choose between constituents conveying overlapping information.
Barzilay and McKeown (2005) proposed an idea called sentence fusion that integrates information in overlapping sentences to produce a non-overlapping summary sentence. $$$$$ Instead, we select a combination already present in the input sentences as a basis and transform it into a fusion sentence by removing extraneous information and augmenting the fusion sentence with information from other sentences.

This is the other way around compared to the English dependency such as in Barzilay and McKeown (2005). $$$$$ Two dependency trees and their alignment tree.
This is the other way around compared to the English dependency such as in Barzilay and McKeown (2005). $$$$$ In the previous version of the system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a fusion dependency structure using the language generator FUF/SURGE (Elhadad and Robin 1996).

Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). $$$$$ MultiGen takes as input a cluster of news stories on the same event and produces a summary which synthesizes common information across input stories.
Either a sentences from the cluster is selected (Aliguliyev, 2006) or a new sentence is regenerated from all/some sentences in a cluster (Barzilay and McKeown, 2005). $$$$$ Having multiple sentences in the input poses new challenges—such as a need for sentence comparison—but at the same time it opens up new possibilities for generation.

Recent abstractive approaches, such as sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009) and sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009) have focused on rewriting techniques, without consideration for a complete model which would include a transition to an abstract representation for content selection. $$$$$ While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003).
Recent abstractive approaches, such as sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009) and sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009) have focused on rewriting techniques, without consideration for a complete model which would include a transition to an abstract representation for content selection. $$$$$ Knight and Marcu (2000) treat reduction as a translation process using a noisychannel model (Brown et al. 1993).

The work of (Barzilay and McKeown, 2005) on sentence fusion shows an example of re-using the same syntactical structure of a source sentence to create a new one with a slightly different meaning. $$$$$ (An example of a fusion sentence is shown in Table 1.)
The work of (Barzilay and McKeown, 2005) on sentence fusion shows an example of re-using the same syntactical structure of a source sentence to create a new one with a slightly different meaning. $$$$$ However, removing all such subtrees may result in an ungrammatical or semantically flawed sentence; for example, we might create a sentence without a subject.

The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. $$$$$ This sentence was created by sentence fusion and clearly, there is a problem.
The work of Barzilay and McKeown (2005) on Sentence Fusion introduced the problem of converting multiple sentences into a single summary sentence. $$$$$ Here again, the problem is reference.

In our experiments, dependency parsing is accomplished with Minipar (Lin, 1998) and alignment is done using a bottom-up tree alignment algorithm (Barzilay and McKeown, 2005) modified to account for the shallow semantic role labels produced by the parser. $$$$$ 3.1.2 Alignment.
In our experiments, dependency parsing is accomplished with Minipar (Lin, 1998) and alignment is done using a bottom-up tree alignment algorithm (Barzilay and McKeown, 2005) modified to account for the shallow semantic role labels produced by the parser. $$$$$ Two dependency trees and their alignment tree.

Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009), and a generation based approach that could be called sentence splitting (Genest and Lapalme, 2011). $$$$$ In addition to sentence fusion, compression algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003) and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003) are other instances of such methods.
Abstractive summarization has been explored to some extent in recent years: sentence compression (Knight and Marcu, 2000) (Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005) or revision (Tanaka et al, 2009), and a generation based approach that could be called sentence splitting (Genest and Lapalme, 2011). $$$$$ While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003).

Sentence fusion is a text-to-text generation application, which given two related sentences, outputs a single sentence expressing the information shared by the two input sentences (Barzilay and McKeown 2005). $$$$$ In contrast, for this task we require text-to-text generation, the ability to produce a new text given a set of related texts as input.
Sentence fusion is a text-to-text generation application, which given two related sentences, outputs a single sentence expressing the information shared by the two input sentences (Barzilay and McKeown 2005). $$$$$ In this article, we have presented sentence fusion, a novel method for text-to-text generation which, given a set of similar sentences, produces a new sentence containing the information common to most sentences.

Barzilay and McKeown (2005) argue convincingly that employing such a fusion strategy in a multidocument summarization system can result in more informative and more coherent summaries. $$$$$ Sentence Fusion For Multidocument News Summarization
Barzilay and McKeown (2005) argue convincingly that employing such a fusion strategy in a multidocument summarization system can result in more informative and more coherent summaries. $$$$$ It has been shown that combining information from several sources is a natural strategy for multidocument summarization.

In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). $$$$$ The research challenges in developing such an algorithm lie in two areas: identification of the fragments conveying common information and combination of the fragments into a sentence.
In contrast to these approaches, sentence fusion was introduced to combine fragments of sentences with common information for multi-document summarization (Barzilay and McKeown, 2005). $$$$$ Instead of examining all possible ways to combine these fragments, we select a sentence in the input which contains most of the fragments and transform its parsed tree into the fusion lattice by eliminating nonessential information and augmenting it with information from other input sentences.
