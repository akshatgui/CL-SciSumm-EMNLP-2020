This approach has successfully been applied in numerous Natural Language Processing (NLP) tasks including syntactic parsing (Collins and Koo, 2005), semantic parsing (Ge and Mooney, 2006), machine translation (Shen et al, 2004), spoken language understanding (Dinarelli et al, 2012), etc. $$$$$ Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance.
This approach has successfully been applied in numerous Natural Language Processing (NLP) tasks including syntactic parsing (Collins and Koo, 2005), semantic parsing (Ge and Mooney, 2006), machine translation (Shen et al, 2004), spoken language understanding (Dinarelli et al, 2012), etc. $$$$$ Like machine translation, parsing is another field of natural language processing in which generative models have been widely used.

Shen et al (2004) compared different algorithms for tuning the log-linear weights in a re ranking framework and achieved results comparable to the standard minimum error rate training. $$$$$ Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.
Shen et al (2004) compared different algorithms for tuning the log-linear weights in a re ranking framework and achieved results comparable to the standard minimum error rate training. $$$$$ (Shen and Joshi, 2004).

(Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. $$$$$ Six features from (Och, 2003) were used as baseline features.
(Och et al, 2004) and (Shen et al, 2004) describe the use of syntactic features in reranking the output of a full translation system, but the syntactic features give very small gains. $$$$$ (Shen and Joshi, 2004).

The labels for the SVM are derived as in (Shen et al, 2004), where top 10% of hypotheses by smoothed sentence-BLEU is ranked before the bottom 90%. $$$$$ The splitting algorithm searches a linear function that successfully splits the top -ranked and bottom -ranked translations for each sentence, where .
The labels for the SVM are derived as in (Shen et al, 2004), where top 10% of hypotheses by smoothed sentence-BLEU is ranked before the bottom 90%. $$$$$ (Shen and Joshi, 2004).

A perceptron like algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). $$$$$ In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.
A perceptron like algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004). $$$$$ (Shen and Joshi, 2004).

Moreover, our ranking model is related to reranking (Shen et al, 2004) in SMT as well. $$$$$ The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.
Moreover, our ranking model is related to reranking (Shen et al, 2004) in SMT as well. $$$$$ (Shen and Joshi, 2004).

Work on discriminative reranking has been reported before by Och and Ney (2002), Och et al (2004), and Shen et al (2004). $$$$$ This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).
Work on discriminative reranking has been reported before by Och and Ney (2002), Och et al (2004), and Shen et al (2004). $$$$$ (Shen and Joshi, 2004).

Traditionally, n-best rerankers (Shen et al, 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al, 2007). $$$$$ For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language.
Traditionally, n-best rerankers (Shen et al, 2004) have applied expensive analysis after the translation process, on both the source and target side, though they suffer from being limited to whatever is on the n-best list (Hasan et al, 2007). $$$$$ By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.

It was also used also in (Shen et al, 2004) to re-rank different candidates of the same hypothesis for machine translation. $$$$$ For example, in our data set, the rank of a translation is only the rank among all the translations for the same sentence.
It was also used also in (Shen et al, 2004) to re-rank different candidates of the same hypothesis for machine translation. $$$$$ There are many candidates for .

The same reranking schema has been used also in (Shen et al, 2004) for reranking different candidate hypotheses for machine translation. $$$$$ Discriminative Reranking For Machine Translation
The same reranking schema has been used also in (Shen et al, 2004) for reranking different candidate hypotheses for machine translation. $$$$$ (Shen and Joshi, 2004).

Our modified training procedure is related to the discriminative re-ranking procedure presented in (Shen et al., 2004). $$$$$ We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).
Our modified training procedure is related to the discriminative re-ranking procedure presented in (Shen et al., 2004). $$$$$ (Shen and Joshi, 2004).

Shen et al (2004) and Och et al (2004) presented approaches to re-rank the output of the decoder using syntactic information. $$$$$ Two large margin approaches have been used.
Shen et al (2004) and Och et al (2004) presented approaches to re-rank the output of the decoder using syntactic information. $$$$$ (Shen and Joshi, 2004).
