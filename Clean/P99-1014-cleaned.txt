Two-dimensional EM-based clustering has been applied to tasks in syntax (Rooth et al, 1999), but so far this approach has not been used to derive models of higher dimensionality and, to the best of our knowledge, this is the first time that it is being applied to speech. $$$$$ The basic ideas of our EM-based clustering approach were presented in Rooth (Ms).
Two-dimensional EM-based clustering has been applied to tasks in syntax (Rooth et al, 1999), but so far this approach has not been used to derive models of higher dimensionality and, to the best of our knowledge, this is the first time that it is being applied to speech. $$$$$ There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment.

EM-based clustering has been derived and applied to syntax (Rooth et al, 1999). $$$$$ The basic ideas of our EM-based clustering approach were presented in Rooth (Ms).
EM-based clustering has been derived and applied to syntax (Rooth et al, 1999). $$$$$ For further applications of our clustering model see Rooth et al. (1998).

We evaluated our 3-dimensional clustering models on a pseudo-disambiguation task similar to the one described by Rooth et al (1999), but specied to onset, nucleus, and coda ambiguity. $$$$$ We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail.
We evaluated our 3-dimensional clustering models on a pseudo-disambiguation task similar to the one described by Rooth et al (1999), but specied to onset, nucleus, and coda ambiguity. $$$$$ The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).

e. g. Rooth et al (1999) and Erk (2007). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
e. g. Rooth et al (1999) and Erk (2007). $$$$$ The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).

Similar models have been used to learn sub categorization information (Rooth et al, 1999) or properties of verb argument slots (Yao et al,2011). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
Similar models have been used to learn sub categorization information (Rooth et al, 1999) or properties of verb argument slots (Yao et al,2011). $$$$$ The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).

Rooth et al (1999) referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
Rooth et al (1999) referring to a direct object noun for describing verbs, or used any syntactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003). $$$$$ The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).

Rooth et al (1999) generalize over seen head words using EM-based clustering rather than WordNet. $$$$$ The basic ideas of our EM-based clustering approach were presented in Rooth (Ms).
Rooth et al (1999) generalize over seen head words using EM-based clustering rather than WordNet. $$$$$ For further applications of our clustering model see Rooth et al. (1998).

Experimental design Like Rooth et al (1999) we evaluate selectional preference induction approaches in a pseudo disambiguation task. $$$$$ Both of these considerations suggest the relevance of inductive and experimental approaches to the construction of lexicons with semantic information.
Experimental design Like Rooth et al (1999) we evaluate selectional preference induction approaches in a pseudo disambiguation task. $$$$$ For further applications of our clustering model see Rooth et al. (1998).

Rooth et al (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. $$$$$ However, the elements v, v', and n were required to be part of the training corpus.
Rooth et al (1999) tested 3000 random (v, n) pairs, but required the verbs and nouns to appear between 30 and 3000 times in training. $$$$$ Furthermore, we restricted the verbs and nouns in the evalutation corpus to the ones which occured at least 30 times and at most 3000 times with some verb-functor v in the training corpus.

EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. $$$$$ Inducing A Semantically Annotated Lexicon Via EM-Based Clustering
EM-based clustering, originally introduced for the induction of a semantically annotated lexicon (Rooth et al, 1999), regards classes as hidden variables in the context of maximum likelihood estimation from incomplete data via the expectation maximisation algorithm. $$$$$ Semantic classes corresponding to such pairs are viewed as hidden variables or unobserved data in the context of maximum likelihood estimation from incomplete data via the EM algorithm.

Usually, the classes are from WordNet (Miller et al, 1990), although they can also be inferred from clustering (Rooth et al., 1999). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
Usually, the classes are from WordNet (Miller et al, 1990), although they can also be inferred from clustering (Rooth et al., 1999). $$$$$ The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).

 $$$$$ 5.
 $$$$$ The method is applicable to any natural language where text samples of sufficient size, computational morphology, and a robust parser capable of extracting subcategorization frames with their fillers are available.

Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999). $$$$$ The probability model we use can be found earlier in Pereira et al. (1993).
Probabilistic latent variable frameworks for generalising about contextual behaviour (in the form of verb-noun selectional preferences) were proposed by Pereira et al (1993) and Rooth et al (1999). $$$$$ We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail.

Rooth et al (1999) introduced a model of selectional preference induction that casts the problem in a probabilistic latent-variable framework. $$$$$ For further applications of our clustering model see Rooth et al. (1998).
Rooth et al (1999) introduced a model of selectional preference induction that casts the problem in a probabilistic latent-variable framework. $$$$$ Note that by construction, conditioning of v and n on each other is solely made through the classes c. In the framework of the EM algorithm (Dempster et al., 1977), we can formalize clustering as an estimation problem for a latent class (LC) model as follows.

Semantic classes assigned to predicate arguments in sub categorization frames are either derived automatically through statistical clustering techniques (Rooth et al (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. $$$$$ As shown by Baum et al. (1970), these expectations can be calculated efficiently using dynamic programming techniques.
Semantic classes assigned to predicate arguments in sub categorization frames are either derived automatically through statistical clustering techniques (Rooth et al (1999), Light and Greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the WordNet hierarchy or LDOCE semantic classes. $$$$$ 2 shows an induced semantic class out of a model with 35 classes.

Rooth et al (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. $$$$$ We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.
Rooth et al (1999) present a fundamentally different approach to selectional preference induction which uses soft clustering to form classes for generalisation and does not take recourse to any hand-crafted resource. $$$$$ For further applications of our clustering model see Rooth et al. (1998).

The strategy of our model to derive generalisations directly from corpus data, without recourse to re sources, is similar to another family of corpus-driven selectional preference models, namely EM-based clustering models (Rooth et al, 1999). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
The strategy of our model to derive generalisations directly from corpus data, without recourse to re sources, is similar to another family of corpus-driven selectional preference models, namely EM-based clustering models (Rooth et al, 1999). $$$$$ We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail.

 $$$$$ 5.
 $$$$$ The method is applicable to any natural language where text samples of sufficient size, computational morphology, and a robust parser capable of extracting subcategorization frames with their fillers are available.

We explore a variant of the pseudo-disambiguation task of Rooth et al (1999) which has been applied to SP acquisition by a number of recent papers. $$$$$ For further applications of our clustering model see Rooth et al. (1998).
We explore a variant of the pseudo-disambiguation task of Rooth et al (1999) which has been applied to SP acquisition by a number of recent papers. $$$$$ We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail.

Three and five-dimensional EM-based clustering has been applied to monolingual phonological data (Muller et al, 2000) and two-dimensional clustering to syntax (Rooth et al, 1999). $$$$$ The basic ideas of our EM-based clustering approach were presented in Rooth (Ms).
Three and five-dimensional EM-based clustering has been applied to monolingual phonological data (Muller et al, 2000) and two-dimensional clustering to syntax (Rooth et al, 1999). $$$$$ For further applications of our clustering model see Rooth et al. (1998).
