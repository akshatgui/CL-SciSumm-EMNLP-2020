Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ To avoid ambiguity, we will use the term coreference element (CE) to refer to the set of linguistic expressions that participate in the coreference relation, as defined for each of the MUC and ACE tasks.1 At times, it will be important to distinguish between the CEs that are included in the gold standard — the annotated CEs — from those that are generated by the coreference resolution system — the extracted CEs.
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ A twinless extracted CE signals that the resolver extracted a spurious CE, while an annotated CE is twinless when the resolver fails to extract it.

In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ Soon et al. (2001) and Yang et al.
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ One option, B3all, retains all twinless extracted CEs.

 $$$$$ The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations.
 $$$$$ In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).

A04CU $$$$$ Soon et al. (2001) and Yang et al.
A04CU $$$$$ When available, we use the standard test/train split.

A05ST $$$$$ When available, we use the standard test/train split.
A05ST $$$$$ Otherwise, we randomly split the data into a training and test set following a 70/30 ratio.

 $$$$$ The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations.
 $$$$$ In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).

We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ Soon et al. (2001) and Yang et al.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Coreference or Not

While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ We expect CE detection to be an important subproblem for an end-to-end coreference system.

To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ Soon et al. (2001) and Yang et al.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ Soon et al. (2001) and Yang et al.
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions).
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ Different types of anaphora that have to be handled by coreference resolution systems exhibit different properties.

We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ Soon et al. (2001) and Yang et al.
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ Soon et al. (2001) and Yang et al.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ Table 3, box 1 shows the performance of RECONCILEACL09 using a default (0.5) coreference classifier threshold.
We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al., 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g.

Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ We propose two different ways to deal with twinless CEs for B3.

We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions).
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ Table 3, box 1 shows the performance of RECONCILEACL09 using a default (0.5) coreference classifier threshold.

 $$$$$ The Berkeley parser (Petrov and Klein, 2007) generates phrase structure parse trees, and the de Marneffe et al. (2006) system produces dependency relations.
 $$$$$ In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).

B3 here is the B3All version of Stoyanov et al (2009). $$$$$ Soon et al. (2001) and Yang et al.
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ We briefly summarize the features here and refer the reader to Stoyanov et al. (2009) for more details.

The perceptron baseline in this work (Reconcile settings $$$$$ We will refer to the specific configuration of RECONCILE used for this paper as RECONCILEACL09.
The perceptron baseline in this work (Reconcile settings $$$$$ Comparison to the BASELINE system (box 2) shows that using gold standard NEs leads to improvements on all data sets with the exception of ACE2 and ACE05, on which performance is virtually unchanged.
