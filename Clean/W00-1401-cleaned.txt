Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ This metric has also been used to measure accuracy of MT systems (Alshawi et al., 1998).
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ It is based on string edit distance between the output of the generation system and the reference corpus string.

More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ This metric has also been used to measure accuracy of MT systems (Alshawi et al., 1998).
More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ (For a more detailed comparisons of different tree models, see (Bangalore and Rainbow, 2000).)

Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). $$$$$ Consider the following example.
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). $$$$$ In our example sentence (2), we see that the insertion and deletion of no can be collapsed into one move.

The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ Evaluation Metrics For Generation
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ To our knowledge, the first to use stochastic techniques in an NLG realization module were Langkilde and Knight (1998a) and (1998b) (see also (Langkilde, 2000)).

Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ This metric has also been used to measure accuracy of MT systems (Alshawi et al., 1998).
Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ .We conducted a series of linear regressions with normalized judgments of understanding and quality as the dependent measures and as independent measures different combinations of one of our four metrics with sentence length, and with the &quot;problem&quot; variables that we used to define the string metrics (S, I, D, Al, , D' - see Section 3 for definitions).

In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ In order to determine whether the metrics correlate with independent notions understandability or quality, we have performed evaluation experiments with human subjects.

Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. $$$$$ Evaluation Metrics For Generation
Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. $$$$$ The simple accuracy, generation accuracy, simple tree accuracy and generation tree accuracy for the two experiments are tabulated in Table 2.

Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ In Section 2. we briefly describe the architecture of FERGUS, and some of the modules.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ This metric has also been used to measure accuracy of MT systems (Alshawi et al., 1998).

Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ It is based on string edit distance between the output of the generation system and the reference corpus string.
Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ The simple accuracy, generation accuracy, simple tree accuracy and generation tree accuracy for the two experiments are tabulated in Table 2.

 $$$$$ An alignment algorithm using substitution, insertion and deletion of tokens as operations attempts to match the generated string with the reference string.
 $$$$$ This is indeed the case: the -results are summarized in Table 4.

We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ The first metric, simple accuracy, is the same string distance metric used for measuring speech recognition accuracy.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ It is based on string edit distance between the output of the generation system and the reference corpus string.

 $$$$$ An alignment algorithm using substitution, insertion and deletion of tokens as operations attempts to match the generated string with the reference string.
 $$$$$ This is indeed the case: the -results are summarized in Table 4.

(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ In order to determine whether the metrics correlate with independent notions understandability or quality, we have performed evaluation experiments with human subjects.

It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ Evaluation Metrics For Generation
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ A substitution represents a case in the string metrics in which not only a word is in the wrong place, but the word that should have been in that place is somewhere else.

This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ The first metric, simple accuracy, is the same string distance metric used for measuring speech recognition accuracy.
