We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ Proceedings of the Conference on Empirical Methods in Natural of tags and each tag/word pair have associated parameters.
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). $$$$$ These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.
For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. $$$$$ Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. $$$$$ Although we concentrate on taggingproblems in this paper, the theoretical frame work and algorithm described in section 3 ofthis paper should be applicable to a wide va riety of models where Viterbi-style algorithmscan be used for decoding: examples are Proba bilistic Context-Free Grammars, or ME models for parsing.

(Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks. $$$$$ These algorithms have been shown by (Freund  Schapire 99) to be competitive with modern learning algorithms such as support vector machines; however, theyhave previously been applied mainly to classi cation tasks, and it is not entirely clear how the algorithms can be carried across to NLP tasks such as tagging or parsing.This paper describes variants of the perceptron algorithm for tagging problems.
(Collins 2002b) describes how the voted perceptron can be used to train maximum-entropy style taggers, and also gives a more thorough discussion of the theory behind the perceptron algorithm applied to ranking tasks. $$$$$ 2.1 HMM Taggers.

We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.
We use the averaged perceptron (Collins, 2002) to train a global linear model and score each action. $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002). $$$$$ (Laerty et al 2001) give experimental results suggesting that CRFs can per form signi cantly better than ME models.In this paper we describe parameter estima tion algorithms which are natural alternatives toCRFs.
We split the Penn Treebank corpus (Marcus et al, 1994) into training, development and test sets as in (Collins, 2002). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

 $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
 $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

A perceptron algorithm gives 97.11% (Collins, 2002). $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
A perceptron algorithm gives 97.11% (Collins, 2002). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

 $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
 $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

 $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
 $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). $$$$$ The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).
Theparameters? of each parsing model are estimated from a training set using an averaged perceptron algorithm, following Collins (2002) and Huang (2008). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

We trained this model using the averaged perceptron algorithm (Collins, 2002). $$$$$ The algorithms are based on the percep tron algorithm (Rosenblatt 58), and the voted or averaged versions of the perceptron described in (Freund  Schapire 99).
We trained this model using the averaged perceptron algorithm (Collins, 2002). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker. $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
We adopt the perceptron algorithm (Collins, 2002) to train the re-ranker. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

 $$$$$ We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
 $$$$$ In a trigram tagger the score for a tagged sequence t [1:n]paired with a word se quence w [1:n] is 2 P n i=1  t i

Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y. $$$$$ Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
Collins (2002)'s perceptron training algorithm were adopted again, to learn a discriminative classifier, mapping from inputs x X to outputs y Y. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. $$$$$ Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). $$$$$ Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
We compare those algorithms to generalized iterative scaling (GIS) (Darroch and Ratcliff, 1972), non-preconditioned CG, and voted perceptron training (Collins, 2002). $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. $$$$$ Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.

Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.
Collins (2002) reported and we confirmed that this averaging reduces over fitting considerably. $$$$$ Language Processing (EMNLP), Philadelphia, July 2002, pp.

Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding. $$$$$ Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al 2000) for their use on a FAQ segmentation task.
Minor variants support voted perceptron (Collins, 2002) and MEMMs (McCallum et al, 2000) with the same efficient feature encoding. $$$$$ See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the voted perceptron to NLP problems.
