USR is the weakly supervised system of Naseem et al (2010). $$$$$ Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a).
USR is the weakly supervised system of Naseem et al (2010). $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

 $$$$$ As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).
 $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.

Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ Using Universal Linguistic Knowledge to Guide Grammar Induction
Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ The central hypothesis of this work is that unsupervised dependency grammar induction can be improved using universal linguistic knowledge.

The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a).
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ Our model takes as input (i.e., as observed) a set of sentences where each word is annotated with a coarse part-of-speech tag.

 $$$$$ As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).
 $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.

Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ The DMV results are taken from Berg-Kirkpatrick and Klein (2010).
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We also provide the performance of two baselines — the dependency model with valence (DMV) (Klein and Manning, 2004) and the phylogenetic grammar induction (PGI) model (Berg-Kirkpatrick and Klein, 2010).

In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ Using Universal Linguistic Knowledge to Guide Grammar Induction
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).

 $$$$$ As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).
 $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.

Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).
Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ In this paper we demonstrated that syntactic universals encoded as declarative constraints improve grammar induction.

We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ Instead, following the approach of Grac¸a et al. (2007), we constrain the posterior to satisfy the rules in expectation during inference.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ To compare the two, we run HDP-DEP using the 20 rules given by Druck et al. (2009).

For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ Using Universal Linguistic Knowledge to Guide Grammar Induction
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a).
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ This approach is based on the posterior regularization technique (Grac¸a et al., 2009), which we apply to a variational inference algorithm for our parsing model.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ We are interested in estimating the posterior p(B, z

 $$$$$ As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).
 $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.

Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ In fact, much recent work has demonstrated that learning cross-lingual correspondences from corpus data greatly reduces the ambiguity inherent in syntactic analysis (Kuhn, 2004; Burkett and Klein, 2008; Cohen and Smith, 2009a; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010).
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

 $$$$$ As we explain at the end of this section, without this aspect the generative story closely resembles the classic dependency model with valence (DMV) of Klein and Manning (2004).
 $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.

In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac¸a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a).
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ We test the effectiveness of our grammar induction model on six Indo-European languages from three language groups: English, Danish, Portuguese, Slovene, Spanish, and Swedish.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ Datasets and Evaluation We test the effectiveness of our grammar induction approach on English, Danish, Portuguese, Slovene, Spanish, and Swedish.
