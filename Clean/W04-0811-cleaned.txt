We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ The English All-Words Task
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ Although we did not compute a baseline score, we received several baseline figures from our participants.

State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The English All-Words Task
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ The English All-Words Task
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ The English All-Words Task
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ The English All-Words Task
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ The English All-Words Task
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ The English All-Words Task
Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.

Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).

As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ Two sets of scores were computed for each system.
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ The English All-Words Task
Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.

Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ The English All-Words Task
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ Table 1 shows the system performance under the first interpretation of the results (&quot;With U&quot;).
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).

Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ The English All-Words Task
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
