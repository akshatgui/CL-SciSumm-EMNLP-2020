Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ Parser Combination By Reparsing
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.

In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ Six dependency parsers were used in our combination experiments, as described below.

To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.

Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).
Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ The large-margin parser described in (McDonald et al., 2005) was used with no alterations.

A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).
A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ The deterministic shift-reduce parsing algorithm of (Nivre & Scholz, 2004) was used to create two parsers2, one that processes the input sentence from left-to-right (LR), and one that goes from right-toleft (RL).

Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ This is done in a two stage process of reparsing.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ For constituent parsing, we used the section splits of the Penn Treebank as described above, as has become standard in statistical parsing research.

Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.

An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).

The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ The large-margin parser described in (McDonald et al., 2005) was used with no alterations.

Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ Balancing precision and recall is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ The parsers that were used in the constituent reparsing experiments are

This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data.
This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ Six dependency parsers were used in our combination experiments, as described below.

Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ Six dependency parsers were used in our combination experiments, as described below.

(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees.
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ Setting 1 is the emulation of Henderson and Brill’s voting.

Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ Balancing precision and recall is accomplished by discarding every constituent with weight below a threshold t before the search for the final parse tree starts.
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ In other words, if a constituent appears at least (m + 1)/2 times in the output of the m parsers, the constituent is added to the final tree.

Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score).
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.

Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees.
Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ Setting 1 is the emulation of Henderson and Brill’s voting.

However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ However, the scheme heavily favors precision over recall.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ In setting 2, t is set for balancing precision and recall.

To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ By changing the threshold t we can control the precision/recall tradeoff.
To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.

 $$$$$ In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.
 $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.

We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ Therefore, we achieve additional parser diversity with the same algorithm, simply by varying the direction of parsing.
