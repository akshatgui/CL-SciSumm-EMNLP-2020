Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991), Kozima (1993), and Galley, et al. $$$$$ Text Segmentation Based On Similarity Between Words
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991), Kozima (1993), and Galley, et al. $$$$$ Morris and Hirst (1991) used Roget's thesaurus to determine whether or not two words have lexical cohesion.

As in Kozima's work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text. $$$$$ Kozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it.
As in Kozima's work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text. $$$$$ Experiments on several window shapes (e.g. triangle window, etc.) shows that Harming window is best for clarifying the macroscopic features of LCP.

Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. $$$$$ The similarity of words, which represents their cohesiveness, is computed using a semantic network.
Related words have been located using spreading activation on a semantic network (Kozima, 1993), although only one text was segmented. $$$$$ Lexical cohesiveness is defined as word similarity (Kozima and Furugori, 1993) computed by spreading activation on a semantic network.

But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to get better results than work that is only based on word distribution in texts. $$$$$ Text Segmentation Based On Similarity Between Words
But work such as (Kozima, 1993), (Ferret, 1998) or (Kaufmann, 1999) showed that using a domain independent source of knowledge for text segmentation doesn't necessarily lead to get better results than work that is only based on word distribution in texts. $$$$$ However, VMP does not work well on a high-density text.

This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus. $$$$$ Kozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it.
This significance is defined as in (Kozima, 1993) as its normalized information in a reference corpus. $$$$$ The similarity a- depends on the significance s(w) E [0, 1], i.e. normalized information of the word w in West's corpus (1953).

He identified topic boundaries where the LCP score was low (Kozima, 1993). $$$$$ It is clear that the valleys of the LCP correspond mostly to the dominant segment boundaries.
He identified topic boundaries where the LCP score was low (Kozima, 1993). $$$$$ However, some valleys of the LCP do not exactly correspond to segment boundaries.

For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method. $$$$$ This paper proposes an indicator, called the lexical cohesion profile (LCP), which locates segment boundaries in a narrative text.
For example, the lexical cohesion profile (Kozima, 1993) should be perfectly usable with our fragmentation method. $$$$$ Kozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it.

Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). $$$$$ Morris and Hirst (1991) used Roget's thesaurus to determine whether or not two words have lexical cohesion.
Ever since Morris and Hirst (1991)'s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics, with applications in summarization (Barzilay and Elhadad, 1997), information retrieval (Salton and Allan, 1994), and text understanding (Kozima, 1993). $$$$$ Text segmentation described here provides basic information for text understanding: Segment boundaries provide valuable restriction for determination of the referents.

Therefore, many approaches have concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al, 1999). $$$$$ Text Segmentation Based On Similarity Between Words
Therefore, many approaches have concentrated on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al, 1999). $$$$$ Kozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it.

As in Kozima (1993), the second method exploits lexical cohesion to segment exts, but in a different way. $$$$$ Kozima and Furugori (1993) defined lexical cohesiveness as semantic similarity between words, and proposed a method for measuring it.
As in Kozima (1993), the second method exploits lexical cohesion to segment exts, but in a different way. $$$$$ This paper proposed LCP, all indicator of segment changing, which concentrates on lexical cohesion of a text. segment.

Kozima (1993), for example, used cohesion based on the spreading activation on a semantic network. $$$$$ Lexical cohesiveness is defined as word similarity (Kozima and Furugori, 1993) computed by spreading activation on a semantic network.
Kozima (1993), for example, used cohesion based on the spreading activation on a semantic network. $$$$$ Similarity between words is computed by spreading activation on a semantic network which is systematically constructed from an English dictionary (LDOCE).
