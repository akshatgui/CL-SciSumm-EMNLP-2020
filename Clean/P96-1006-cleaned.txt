We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. $$$$$ The method for extraction of local collocations is similar to that for extraction of keywords.
We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. $$$$$ It is unclear how Yarowsky's method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET.

Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%. $$$$$ LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994).
Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%. $$$$$ The average accuracy of LEXAS over 100 random trials is 87.4%, and the standard deviation is 1.37%.

 $$$$$ Local collocations are common expressions containing the word to be disambiguated.
 $$$$$ We would like to thank: Dr Paul Wu for sharing the Brown Corpus and Wall Street Journal Corpus; Dr Christopher Ting for downloading and installing WORDNET and SEMCOR, and for reformatting the corpora; the 12 undergraduates from the Linguistics Program of the National University of Singapore for preparing the sense-tagged corpus; and Prof K. P. Mohanan for his support of the sense-tagging project.

 $$$$$ Local collocations are common expressions containing the word to be disambiguated.
 $$$$$ We would like to thank: Dr Paul Wu for sharing the Brown Corpus and Wall Street Journal Corpus; Dr Christopher Ting for downloading and installing WORDNET and SEMCOR, and for reformatting the corpora; the 12 undergraduates from the Linguistics Program of the National University of Singapore for preparing the sense-tagged corpus; and Prof K. P. Mohanan for his support of the sense-tagging project.

Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ In order to evaluate the relative contribution of the knowledge sources, including (1) POS and morphological form; (2) unordered set of surrounding words; (3) local collocations; and (4) verb to the left (verb-object syntactic relation), we conducted 4 separate runs of 100 random trials each.
Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source.

The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. $$$$$ These sense tagged word occurrences consist of 191 most frequently occurring and most ambiguous nouns and verbs.
The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. $$$$$ These 192,800 word occurrences consist of 121 nouns and 70 verbs which are the most frequently occurring and most ambiguous words of English.

Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ The task of Word Sense Disambiguation (WSD) is to identify the correct sense of a word in context.
Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ In the output, each word occurrence w is tagged with its correct sense (according to the context) in the form of a sense number i, where i corresponds to the i-th sense definition of w as given in some dictionary.

Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). $$$$$ To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined sense distinctions of WORDNET.
Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). $$$$$ The effectiveness of unsupervised learning on disambiguating words into the refined sense distinction of WORDNET needs to be further investigated.

Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). $$$$$ If the distance between two examples is small, then the two examples are similar.
Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). $$$$$ The distance between two examples is the sum of the distances between the values of all the features of the two examples.

Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. $$$$$ If the distance between two examples is small, then the two examples are similar.
Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. $$$$$ To test the scalability of LEXAS, we have gathered a corpus in which 192,800 word occurrences have been manually tagged with senses from WORD NET 1.5.

Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. $$$$$ In this paper, we present a new approach for WSD using an exemplar-based learning algorithm.
Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. $$$$$ In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm.

The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ All the word tokens other than the word occurrence w in a sentence s are candidates for consideration as keywords.
The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ It is unclear how Yarowsky's method will fare on WSD of a common test data set like the one we used, nor has his method been tested on a large data set with highly ambiguous words tagged with the refined senses of WORDNET.

In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). $$$$$ LEXAS builds one exemplar-based classifier for each content word w. It operates in two phases: training phase and test phase.
In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). $$$$$ We first set aside two subsets for testing.

This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. $$$$$ The task of Word Sense Disambiguation (WSD) is to identify the correct sense of a word in context.
This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. $$$$$ Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.

The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994).
The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ The accuracy of LEXAS on these two test sets is given in Table 5.

The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ For each training sentence with an occurrence of w, LEXAS extracts the parts of speech (POS) of words surrounding w, the morphological form of w, the words that frequently co-occur with w in the same sentence, and the local collocations containing w. For disambiguating a noun w, the verb which takes the current noun w as the object is also identified.

The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). $$$$$ In this paper, we present a new approach for WSD using an exemplar-based learning algorithm.
The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). $$$$$ That is, it uses supervised learning, in particular exemplar-based learning, to achieve WSD.

We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques.
We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.

To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ This default strategy has been advocated as the baseline performance level for comparison with WSD programs (Gale et al., 1992).
To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.

The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000). $$$$$ During the training phase, the appropriate set of features is extracted based on the method described in Section 3.1.
The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000). $$$$$ The work of (Miller et al., 1994; Leacock et al., 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques.
