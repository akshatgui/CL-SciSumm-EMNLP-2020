Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ P(t) is the target language model and P(s

Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ Language Model Adaptation For Statistical Machine Translation Via Structured Query Models
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ We explore unsupervised language model adaptation techniques for Statistical Machine Translation.

individual target hypotheses (Zhao et al, 2004). $$$$$ Statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by a translation model and a target language (Brown et al, 1993).
individual target hypotheses (Zhao et al, 2004). $$$$$ P(t) is the target language model and P(s

Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ Our approach can be characterized as unsupervised data augmentation by retrieval of relevant documents from large monolingual corpora, and interpolation of the specific language model, build from the retrieved data, with a background language model.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ Given a baseline statistical machine translation system, the language model adaptation is done in several steps shown as follows: ? Generate a set of initial translation hypotheses H = {h1 ?hn} for source sentences s, using either the baseline MT system with the background language model or only the translation model ? Use H to build query ? Use query to retrieve relevant sentences from the large corpus ? Build specific language models from retrieved sentences ? Interpolate the specific language model with the background language ? Re-translate sentences s with adapted language model Figure-1: Adaptation Algorithm The specific language model )

(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ For each of the 4 corpora different numbers of similar sentences (1, 10, 100, and 1000) were retrieved to build specific language models.

Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ Then these translations hypotheses are reformulated as queries to retrieve similar sentences from a very large text collection.

Refinements of this approach are described in (Zhao et al., 2004). $$$$$ We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories.
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ After extraction of the information, structured query models are proposed using the structured query language, described in the Section 3.1.

These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ But it can miss some informative translation words, which could lead to better-adapted language models.
These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ The oracle experiment suggests that better initial translations lead to better language models and thereby better 2nd iteration translations.

This adaptation technique was first proposed by Zhao et al (2004). $$$$$ Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998).
This adaptation technique was first proposed by Zhao et al (2004). $$$$$ This result indicates that there is room for further improvements using this language model adaptation technique.

Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ Maximum entropy, minimum discrimination adaptation (Chen, et. al., 1998).
Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories.
