Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ Forest-based Translation Rule Extraction
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).

 $$$$$ Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).
 $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.

This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ Inspired by the parsing literature on pruning (Charniak and Johnson, 2005; Huang, 2008) we penalize a rule r by the posterior probability of its tree fragment frag = lhs(r).
This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ The final BLEU score results are shown in Table 4.

 $$$$$ Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).
 $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.

Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). $$$$$ We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). $$$$$ There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.

The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).
The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.

We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ When we follow one of them to grow a fragment, there again will be multiple choices at each of its tail nodes.
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.

While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ To test the effect of forest-based rule extraction, we parse the training set into parse forests and use three levels of pruning thresholds: pe = 2, 5, 8.
While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.

Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ Our experiments use composed rules.
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.

Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ We review in this section the tree-based approach to machine translation (Liu et al., 2006; Huang et al., 2006), and its rule extraction algorithm (Galley et al., 2004; Galley et al., 2006).
Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.

To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ Forest-based Translation Rule Extraction
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).

Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ Forest-based Translation Rule Extraction
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ Nodes with non-empty, contiguous, and faithful spans form the admissible set (shaded nodes in the figure), which serve as potential cut-points for rule extraction.3 With the admissible set computed, rule extraction is as simple as a depth-first traversal from the root: we “cut” the tree at all admissible nodes to form tree fragments and extract a rule for each fragment, with variables matching the admissible descendant nodes.

Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ We refer readers to Mi et al. (2008) for details of the decoding algorithm.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).

Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ In other words, a forest can be viewed as a virtual weighted k-best list with a huge k. So a rule extracted from a non 1-best parse, i.e., using non 1-best hyperedges, should be penalized accordingly and should have a fractional count instead of a unit one, similar to the E-step in EM algorithms.
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ With pruning threshold pe = 8, forest-based extraction achieves a (case insensitive) BLEU score of 0.2533, which is an absolute improvement of 1.0% points over the 1-best baseline, and is statistically significant using the sign-test of Collins et al. (2005) (p < 0.01).

Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ Forest-based Translation Rule Extraction
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.

As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p < 0.01).

Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ Forest-based Translation Rule Extraction
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ We now extend tree-based extraction algorithm from the previous section to work with a packed forest representing exponentially many parse trees.

Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ Forest-based Translation Rule Extraction
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ Like in tree-based extraction, we extract rules from a packed forest F in two steps: It turns out that the exact formulation developed for admissible set in the tree-based case can be applied to a forest without any change.

Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ Typically, these models extract rules using parse trees from both or either side(s) of the bitext.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.

Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ Forest-based Translation Rule Extraction
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ The conditional probability P(d
