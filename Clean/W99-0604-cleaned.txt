To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al, 1999). $$$$$ This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.

The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al, 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al, 2003). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al, 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al, 2003). $$$$$ For our model, the probability of alignment al for position j depends on the previous alignment position a3_1 (Vogel et al., 1996).

We view our use of part-of-speech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al [1999]. $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
We view our use of part-of-speech patterns as a natural extension to the introduction of structural elements to statistical machine translation by Wang [1998] and Och et al [1999]. $$$$$ Similar aims are pursued by (Alshawi et al., 1998; Wang and Waibel, 1998) but differently approached.

The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al, 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al, 2003). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
The translation models and lexical scores were estimated on the training corpus which was automatically aligned using Giza++ (Och et al, 1999) in both directions between source and target and symmetrised using the growing heuristic (Koehn et al, 2003). $$$$$ The classes used in P and E are automatically trained bilingual classes using the method described in (Och, 1999) and constitute a partition of the vocabulary of source and target language.

Zens et al (2004) introduce a left-to-right decoding algorithm with ITG constraints on the alignment template system (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
Zens et al (2004) introduce a left-to-right decoding algorithm with ITG constraints on the alignment template system (Och et al, 1999). $$$$$ For our model, the probability of alignment al for position j depends on the previous alignment position a3_1 (Vogel et al., 1996).

A significant source of errors in statistical machine translation is the word reordering problem (Och et al, 1999). $$$$$ Improved Alignment Models For Statistical Machine Translation
A significant source of errors in statistical machine translation is the word reordering problem (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.

The blocks are simpler than the alignment templates in (Och et al, 1999) in that they do not have any internal structure. $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
The blocks are simpler than the alignment templates in (Och et al, 1999) in that they do not have any internal structure. $$$$$ 3 shows some of the extracted alignment templates.

We take the intersection of the two alignments as described in (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
We take the intersection of the two alignments as described in (Och et al, 1999). $$$$$ This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.

A similar block selection scheme has been presented in (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
A similar block selection scheme has been presented in (Och et al, 1999). $$$$$ Details of this approach will be presented elsewhere.

We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al, 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). $$$$$ For our DP search, we use a leftto-right beam-search concept having been introduced in speech recognition, where we rely on beam-search as an efficient pruning technique in order to handle potentially huge search spaces.
We use another technique to speed up direct search by storing and re-using search graphs, which consist of lattices in the case of phrase-based decoding (Och et al, 1999) and hypergraphs in the case of hierarchical decoding (Chiang, 2005). $$$$$ For decoding we use the following search criterion

The phrase extraction heuristic then extracts all the bi phrases that are compatible with the word alignment (Och et al, 1999). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
The phrase extraction heuristic then extracts all the bi phrases that are compatible with the word alignment (Och et al, 1999). $$$$$ The basic idea is to model two different alignment levels

Second, using a heuristic proposed in (Och et al, 1999), all the aligned phrase pairs (x?, a?, y?) satisfying the following criteria are extracted $$$$$ The optimal translation is obtained by carrying out the following optimization

 $$$$$ The DP equation is evaluated recursively to find the best partial path to each grid point (j, , e).
 $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).

Only phrases that conform to the so-called consistent alignment restrictions (Och et al, 1999) are extracted. $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
Only phrases that conform to the so-called consistent alignment restrictions (Och et al, 1999) are extracted. $$$$$ 3 shows some of the extracted alignment templates.

Och et al (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
Och et al (1999) proposed a translation template approach that computes phrasal mappings from the viterbi alignments of a training corpus. $$$$$ Thus we obtain a count n(z) of how often an alignment template occurred in the aligned training corpus.

Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al, 1993), template-based (Och et al, 1999) and syntax-based (Yamada, Knight, 2001). $$$$$ Many statistical translation models (Vogel et al., 1996; Tillmann et al., 1997; Niessen et al., 1998; Brown et al., 1993) try to model word-toword correspondences between source and target words.
Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al, 1993), template-based (Och et al, 1999) and syntax-based (Yamada, Knight, 2001). $$$$$ This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.

To implement our phrase extraction technique, the maximum approximation alignments were combined with the union operation as described in (Och et al, 1999), resulting in a dense but inaccurate alignment map as measured against a human aligned gold standard. $$$$$ This alignment representation is a generalization of the baseline alignments described in (Brown et al., 1993) and allows for many-to-many alignments.
To implement our phrase extraction technique, the maximum approximation alignments were combined with the union operation as described in (Och et al, 1999), resulting in a dense but inaccurate alignment map as measured against a human aligned gold standard. $$$$$ However we do not apply maximum approximation in training, thereby obtaining slightly improved alignments.

This method of phrase pair extraction was originally described by Och et al (1999). $$$$$ We determine correlated bilingual classes by using the method described in (Och, 1999).
This method of phrase pair extraction was originally described by Och et al (1999). $$$$$ A phrase-pair is consistent with the alignment if the words within the source phrase are only aligned to words within the target phrase.

Once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted (although earlier this constraint was called consistent alignments (Och et al, 1999)). $$$$$ In this approach, we first assume that the alignments satisfy the monotonicity requirement.
Once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted (although earlier this constraint was called consistent alignments (Och et al, 1999)). $$$$$ There was no constraint on the length of the sentences, and some of the sentences in the test corpus contain more than 50 words.

The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al, 1999). $$$$$ The classes used in P and E are automatically trained bilingual classes using the method described in (Och, 1999) and constitute a partition of the vocabulary of source and target language.
The most common method for obtaining the phrase table is heuristic extraction from automatically word-aligned bilingual training data (Och et al, 1999). $$$$$ We determine correlated bilingual classes by using the method described in (Och, 1999).
