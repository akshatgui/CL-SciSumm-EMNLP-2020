 $$$$$ In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.
 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.

 $$$$$ In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.
 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.

Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009).
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here.

A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009).
A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here.

 $$$$$ In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.
 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.

 $$$$$ In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it.
 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.

Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ Bayesian Learning of a Tree Substitution Grammar
Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.

A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ Bayesian Learning of a Tree Substitution Grammar
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.
