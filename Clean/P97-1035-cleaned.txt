In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al, 1997), and that it addresses these limitations, as well as others. $$$$$ This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents.
In the remainder of this paper, we discuss the PARADISE framework (PARAdigm for Dialogue System Evaluation) (Walker et al, 1997), and that it addresses these limitations, as well as others. $$$$$ This paper describes PARADISE, a general framework for evaluating spoken dialogue agents that addresses these limitations.

Instead, predictions about user satisfaction can be made on the basis of the predictor variables, which is illustrated in the application of PARADISE to sub dialogues in (Walker et al., 1997). $$$$$ PARADISE represents each cost measure as a function ci that can be applied to any (sub)dialogue.
Instead, predictions about user satisfaction can be made on the basis of the predictor variables, which is illustrated in the application of PARADISE to sub dialogues in (Walker et al., 1997). $$$$$ Instead, predictions about user satisfaction can be made on the basis of the predictor variables, as illustrated in the application of PARADISE to subdialogues.

While we discussed the representation of an information-seeking dialogue here, AVM representations for negotiation and diagnostic dialogue tasks are also easily constructed (Walker et al, 1997). $$$$$ We propose that an attribute value matrix (AVM) can represent many dialogue tasks.
While we discussed the representation of an information-seeking dialogue here, AVM representations for negotiation and diagnostic dialogue tasks are also easily constructed (Walker et al, 1997). $$$$$ Thus, even though the dialogue strategies in Figures 2 and 3 are radically different, the AVM task representation for these dialogues is identical and the performance of the system for the same task can thus be assessed on the basis of the AVM representation.

The first approach to predict user judgments on the basis of interaction metrics is the well-known PARADISE model (Walker et al, 1997). $$$$$ One widely used approach to evaluation is based on the notion of a reference answer (Hirschman et al., 1990).
The first approach to predict user judgments on the basis of interaction metrics is the well-known PARADISE model (Walker et al, 1997). $$$$$ In this case, on the basis of the model in Figure 1, US is treated as the predicted factor.

There are also well-known evaluation efforts such as EAGLES (Sparck Jones and Galliers, 1996) and the Paradise evaluation framework (Walker et al, 1997). $$$$$ This paper presents PARADISE (PARAdigm for DIalogue System Evaluation), a general framework for evaluating spoken rlialogue agents.
There are also well-known evaluation efforts such as EAGLES (Sparck Jones and Galliers, 1996) and the Paradise evaluation framework (Walker et al, 1997). $$$$$ Such generalization requires the identification of factors that affect performance (Cohen, 1995; Sparck-Jones and Galliers, 1996).

Walker et al (1997) identified three factors which carry an influence on the performance of SDSs, and which therefore are thought to contribute to its quality perceived by the user $$$$$ In addition to agent factors such as dialogue strategy, task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance.
Walker et al (1997) identified three factors which carry an influence on the performance of SDSs, and which therefore are thought to contribute to its quality perceived by the user $$$$$ Finally, to our knowledge, we are the first to propose using user satisfaction to determine weights on factors related to performance.

The PARADISE framework (Walker et al, 1997) produces such a relationship for a specific scenario, using multivariate linear regression. $$$$$ Given a set of dialogues for which user satisfaction (US), is and the set of ci have been collected experimentally, the weights a and wi can be solved for using multiple linear regression.
The PARADISE framework (Walker et al, 1997) produces such a relationship for a specific scenario, using multivariate linear regression. $$$$$ Multiple linear regression produces a set of coefficients (weights) describing the relative contribution of each predictor factor in accounting for the variance in a predicted factor.

As stated above, the separation of environmental, agent and task factors was motivated by Walker et al (1997). $$$$$ In addition to agent factors such as dialogue strategy, task factors such as database size and environmental factors such as background noise may also be relevant predictors of performance.
As stated above, the separation of environmental, agent and task factors was motivated by Walker et al (1997). $$$$$ PARADISE supports comparisons among dialogue strategies with a task representation that decouples what an agent needs to achieve in terms of the task requirements from how the agent carries out the task via dialogue.

In the PARADISE framework, user satisfaction is composed of maximal task success and minimal dialogue costs (Walker et al, 1997), thus a type of efficiency in the way it was defined here. $$$$$ The model further posits that two types of factors are potential relevant contributors to user satisfaction (namely task success and dialogue costs), and that two types of factors are potential relevant contributors to costs (Walker, 1996).
In the PARADISE framework, user satisfaction is composed of maximal task success and minimal dialogue costs (Walker et al, 1997), thus a type of efficiency in the way it was defined here. $$$$$ The PARADISE performance measure is a function of both task success (K) and dialogue costs (ci), and has a number of advantages.

User satisfaction is a function of task success and the number of user turns based on the PARADISE framework (Walker et al, 1997) and CAS refers to the proportion of repetition and variation in surface forms. $$$$$ Performance is modeled as a weighted function of a task-based success measure and dialogue-based cost measures, where weights are computed by correlating user satisfaction with performance.
User satisfaction is a function of task success and the number of user turns based on the PARADISE framework (Walker et al, 1997) and CAS refers to the proportion of repetition and variation in surface forms. $$$$$ The PARADISE performance measure is a function of both task success (K) and dialogue costs (ci), and has a number of advantages.

Previous studies (E.g., Walker et al, 1997) use a corpus level semantic accuracy measure (semantic Accuracy) to capture the system's understanding ability. $$$$$ In addition, this approach is broadly integrative, incorporating aspects of transaction success, concept accuracy, multiple cost measures, and user satisfaction.
Previous studies (E.g., Walker et al, 1997) use a corpus level semantic accuracy measure (semantic Accuracy) to capture the system's understanding ability. $$$$$ Our performance measure also captures information similar to concept accuracy, where low concept accuracy scores translate into either higher costs for acquiring information from the user, or lower K scores.

Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (Walker et al, 1997), on a scale of 1-5, where 5 indicated highest satisfaction. $$$$$ User satisfaction is typically calculated with surveys that ask users to specify the degree to which they agree with one or more statements about the behavior or the performance of the system.
Once they had completed all tasks in sequence using one system, they filled out a questionnaire to assess user satisfaction by rating 8-9 statements, similar to those in (Walker et al, 1997), on a scale of 1-5, where 5 indicated highest satisfaction. $$$$$ Finally, to our knowledge, we are the first to propose using user satisfaction to determine weights on factors related to performance.

Following the PARADISE evaluation scheme (Walker et al, 1997), we divided performance features into four groups. $$$$$ The PARADISE methodology consists of the following steps

Some studies (e.g., (Walker et al, 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. $$$$$ Section 2.4 describes the use of linear regression and user satisfaction to estimate the relative contribution of the success and cost measures in a single performance function.
Some studies (e.g., (Walker et al, 1997)) build regression models to predict user satisfaction scores from the system log as well as the user survey. $$$$$ Our performance measure also captures information similar to concept accuracy, where low concept accuracy scores translate into either higher costs for acquiring information from the user, or lower K scores.

Future work focuses on usability tests of the prototype system, e.g. using the PARADISE evaluation framework to evaluate the general usability of the system (Walker et al, 1997). $$$$$ The PARADISE model posits that performance can be correlated with a meaningful external criterion such as usability, and thus that the overall goal of a spoken dialogue agent is to maximize an objective related to usability.
Future work focuses on usability tests of the prototype system, e.g. using the PARADISE evaluation framework to evaluate the general usability of the system (Walker et al, 1997). $$$$$ PARADISE is a general framework for evaluating spoken dialogue agents that integrates and enhances previous work.

They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al, 1997) that they improved models of user satisfaction by an absolute 5%, and that the new metrics could be used to understand which system's dialogue strategies were most effective. $$$$$ One widely used approach to evaluation is based on the notion of a reference answer (Hirschman et al., 1990).
They then derived dialogue act metrics from the DATE tags and showed that when these metrics were used in the PARADISE evaluation framework (Walker et al, 1997) that they improved models of user satisfaction by an absolute 5%, and that the new metrics could be used to understand which system's dialogue strategies were most effective. $$$$$ We have presented the PARADISE framework, and have used it to evaluate two hypothetical dialogue agents in a simplified train timetable task domain.

Such metrics have been introduced in other fields, including PARADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation, and ROUGE (Lin, 2004) for summarisation. $$$$$ PARADISE

In doing so, we are essentially exploring system behaviour in a glass box approach $$$$$ One widely used approach to evaluation is based on the notion of a reference answer (Hirschman et al., 1990).
In doing so, we are essentially exploring system behaviour in a glass box approach $$$$$ One limitation of the PARADISE approach is that the task-based success measure does not reflect that some solutions might be better than others.

In particular, unlike the PARADISE framework (Walker et al, 1997), which aims to evaluate dialogue agent strategies by relating overall user satisfaction to various other metrics (task success, efficiency measures, and qualitative measures) our approach takes the agent's dialogue strategy for granted. $$$$$ PARADISE uses a decision-theoretic framework to specify the relative contribution of various factors to an agent's overall performance.
In particular, unlike the PARADISE framework (Walker et al, 1997), which aims to evaluate dialogue agent strategies by relating overall user satisfaction to various other metrics (task success, efficiency measures, and qualitative measures) our approach takes the agent's dialogue strategy for granted. $$$$$ PARADISE uses its AVM representation to link the information goals of the task to any arbitrary dialogue behavior, by tagging the dialogue with the attributes for the task.9 This makes it possible to evaluate any potential dialogue strategies for achieving the task, as well as to evaluate dialogue strategies that operate at the level of dialogue subtasks (subdialogues).

Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al, 1997). $$$$$ PARADISE is a general framework for evaluating spoken dialogue agents that integrates and enhances previous work.
Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al, 1997). $$$$$ We believe that the framework is also applicable to other dialogue modalities, and to human-human task-oriented dialogues.
