Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ Our future work will focus on the assessment of sense induction on a task-oriented basis as well as on clustering evaluation.

Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ The target word dataset consisted of 100 words, i.e.
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ The relations considered were WordNetâ€™s hypernyms, hyponyms, synonyms, meronyms and holonyms.

Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ Neither of these measures were used in the SemEval2007 WSI task.

Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.

The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al., 2009).
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ For that reason, we introduced the second unsupervised evaluation measure (paired F-Score) that penalises systems when they produce

As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ SemEval-2010 Task 14

Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ SemEval-2010 Task 14

In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ This section presents the measures of unsupervised evaluation, i.e V-Measure (Rosenberg and Hirschberg, 2007) and (2) paired F-Score (Artiles et al., 2009).
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ In this evaluation, the clustering problem is transformed into a classification problem.

However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ All systems outperform this baseline, apart from one, whose V-Measure is equal to 0.
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ Despite that, none of the systems outperform the MFS baseline.

We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ SemEval-2010 Task 14
