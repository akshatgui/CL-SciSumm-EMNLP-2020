See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ In (Koehn and Schroeder, 2007) cross-domain adaptation techniques were applied on a phrasebased SMT trained on the Europarl task, in order to translate news commentaries, from French to English.
See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data

In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. $$$$$ Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+indomain language model) and obtain 1-best translation for each source-side sentence. $$$$$ The adaptation of the translation and re-ordering models is performed by generating synthetic bilingual data from monolingual texts, similarly to what proposed in (Schwenk, 2008).

Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains. $$$$$ Both the language and the translation models are retrained on the extracted data.
Bertoldi and Federico (2009) used monolingual data for adapting existing translation models to translation of data from different domains. $$$$$ This paper addresses the issue of adapting an already developed phrase-based translation system in order to work properly on a different domain, for which almost no parallel data are available but only monolingual texts.1 The main components of the SMT system are the translation model, which aims at porting the content from the source to the target language, and the language model, which aims at building fluent sentences in the target language.

In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data

Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). $$$$$ The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system.
Pivoting can also intervene earlier in the process, for instance as a means to automatically generate the missing parallel resource, an idea that has also been considered to adapt an existing translation systems to new domains (Bertoldi and Federico, 2009). $$$$$ Once monolingual adaptation data is automatically translated, we can use the synthetic parallel corpus to estimate new language, translation, and re-ordering models.

 $$$$$ This model is also learnable from parallel data.
 $$$$$ Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.

For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline.
For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data

On the other hand, Bertoldi and Federico (2009) adapted an SMT system with automatic translations and trained the translation and reordering models on the word alignment used by moses. $$$$$ The standard procedure of Moses for the estimation of the translation and re-ordering models from a bilingual corpus consists in three main steps

Further approaches to domain adaptation for SMT include adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Cross-domain adaptation is faced under the assumption that only monolingual texts are available, either in the source language or in the target language.
Further approaches to domain adaptation for SMT include adaptation using in-domain language mod els (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Our work is mostly related to (Koehn and Schroeder, 2007) but explores different assumptions about available adaptation data

 $$$$$ This model is also learnable from parallel data.
 $$$$$ Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.

Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful. $$$$$ In (Eck et al., 2004) adaptation is limited to the target language model (LM).
Three experiments involving the Twitter language model confirm Bertoldi and Federico (2009)'s findings that the language model was most helpful. $$$$$ This paper addresses the issue of adapting an already developed phrase-based translation system in order to work properly on a different domain, for which almost no parallel data are available but only monolingual texts.1 The main components of the SMT system are the translation model, which aims at porting the content from the source to the target language, and the language model, which aims at building fluent sentences in the target language.

As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data. $$$$$ There is another simple option which is to concatenate the synthetic parallel data with the original training data and re-build the system.
As has been observed before by Bertoldi and Federico (2009), it did not matter whether the synthetic data were used on their own or in addition to the original training data. $$$$$ We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus.
