Similar methods to Shen et al (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010). $$$$$ The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008).
Similar methods to Shen et al (2007) have also been used in Shen and Joshi (2008) and Goldberg and Elhadad (2010). $$$$$ Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.

To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). $$$$$ An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
To identify explicit predicate-argument relationships we utilized dependency parsing by the Easy-First parser (Goldberg and Elhadad, 2010). $$$$$ We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007)

Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. $$$$$ Thus, we observe that the feature extraction and score calculation can be performed once for each action/location pair in a given sentence, and reused throughout all the iterations.
Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. $$$$$ In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce (MALT) parsers, and is an order of magnitude more efficient than graph-based (MST) parsers.

BIUTEE provides state-of-the-art pre-processing utilities $$$$$ The unigram and bigram features are adapted from the feature set for left-to-right Arc-Standard dependency parsing described in (Huang et al., 2009).
BIUTEE provides state-of-the-art pre-processing utilities $$$$$ Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.

 $$$$$ As usual, we use parameter averaging to prevent the perceptron from overfitting.
 $$$$$ We hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question

This has some similarity to Goldberg and Elhadad (2010). $$$$$ As usual, we use parameter averaging to prevent the perceptron from overfitting.
This has some similarity to Goldberg and Elhadad (2010). $$$$$ We hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question

We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise "co-reference" vs. "non-coreference" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). $$$$$ We must, therefore, learn how to order the decisions.
We are using a multi-sieve approach (Raghunathan et al., 2010), which splits pairwise "co-reference" vs. "non-coreference" decisions to different types and attempts to make the easy decisions first (Goldberg and Elhadad, 2010). $$$$$ This model is similar to ours, in that it attempts to defer harder decisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point.

 $$$$$ As usual, we use parameter averaging to prevent the perceptron from overfitting.
 $$$$$ We hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question

The Hebrew tagger and parsing models are described in Goldberg and Elhadad (2010). $$$$$ As a result, these models, while accurate, are slow (O(n3) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models).
The Hebrew tagger and parsing models are described in Goldberg and Elhadad (2010). $$$$$ Each section is tagged after training the tagger on all other sections.

We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. $$$$$ In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g.
We plan to examine to model such a complex structure (granduncle) (Goldberg and Elhadad, 2010) or higher-order structure than third-order for reranking which is computationally expensive for a baseline parser. $$$$$ We must, therefore, learn how to order the decisions.

The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. $$$$$ An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) is attractive due to its good accuracy, fast speed and simplicity. $$$$$ Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results.

As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). $$$$$ Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007).
As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). $$$$$ Beam search can be incorporated into our parser as well.

The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions. $$$$$ An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
The easy-first dependency parsing algorithm (Goldberg and Elhadad, 2010) builds a dependency tree by performing two types of actions. $$$$$ Our (projective) parsing algorithm builds the parse tree bottom up, using two kinds of actions

 $$$$$ As usual, we use parameter averaging to prevent the perceptron from overfitting.
 $$$$$ We hope that further work on this non-directional parsing framework will pave the way to better understanding of an interesting cognitive question

Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). $$$$$ The pp-attachment features are similar to the bigram features, but fire only when one of the structures is headed by a preposition (IN).
Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). $$$$$ These features are more lexicalized than the regular bigram features, and include also the word-form of the rightmost child of the PP (rcwp).

Our system not only outperforms the best single system (Bjorkelund et al., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers $$$$$ Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.
Our system not only outperforms the best single system (Bjorkelund et al., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers $$$$$ Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing.

In the context of dependency parsing, the strategy of delaying arc construction when the current configuration is not informative is called the easy-first strategy, and has been first explored by Goldberg and Elhadad (2010). $$$$$ An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
In the context of dependency parsing, the strategy of delaying arc construction when the current configuration is not informative is called the easy-first strategy, and has been first explored by Goldberg and Elhadad (2010). $$$$$ This strategy allows using more context at each decision.

The parsing approach is based upon the non directional easy-first algorithm recently presented by Goldberg and Elhadad (2010). $$$$$ An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing
The parsing approach is based upon the non directional easy-first algorithm recently presented by Goldberg and Elhadad (2010). $$$$$ We presented a non-directional deterministic dependency parsing algorithm, which is not restricted by the left-to-right parsing order of other deterministic parsers.

While theoretically slower, this has a limited impact upon actual parsing times. See Goldberg and Elhadad (2010) for more explanation. $$$$$ Each iteration of the main loop connects two structures and removes one of them, and so the loop repeats for exactly n times.
While theoretically slower, this has a limited impact upon actual parsing times. See Goldberg and Elhadad (2010) for more explanation. $$$$$ However, these changes are limited to a fixed local context around the attachment point of the action.

The feature set is based off the features used by Goldberg and Elhadad (2010) but has a significant number of extensions. $$$$$ The unigram and bigram features are adapted from the feature set for left-to-right Arc-Standard dependency parsing described in (Huang et al., 2009).
The feature set is based off the features used by Goldberg and Elhadad (2010) but has a significant number of extensions. $$$$$ After each iteration we need to update the extracted features and calculated scores for only k locations, where k is a fixed number depending on the window size used in the feature extraction, and usually k « n. Using this technique, we perform only (k + 1)n feature extractions and score calculations for each sentence, that is O(n) feature-extraction operations per sentence.
