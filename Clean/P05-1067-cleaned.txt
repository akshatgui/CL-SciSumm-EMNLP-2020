Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al, 2002) and machine translation (Ding and Palmer, 2005). $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al, 2002) and machine translation (Ding and Palmer, 2005). $$$$$ Our MT system was evaluated using the n-gram based Bleu (Papineni et al., 2002) and NIST machine translation evaluation software.

A syntax-based system might be able to check this sort of agreement if it produced a target-side dependency tree as in Ding and Palmer (2005). $$$$$ In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar.
A syntax-based system might be able to check this sort of agreement if it produced a target-side dependency tree as in Ding and Palmer (2005). $$$$$ In this paper we presented a syntax-based statistical MT system based on a Synchronous Dependency Insertion Grammar and a non-isomorphic stochastic tree-to-tree transducer.

Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. $$$$$ Similar to (Ding and Palmer, 2004a), we also use a heuristic function in Step 1(a) of the algorithm to rank all the word pairs for the tentative tree partitioning operation.
Ding and Palmer (2005) improve over word-based MT baseline with a formalism very similar to STSG. $$$$$ The heuristic function is based on a set of heuristics, most of which are similar to those in (Ding and Palmer, 2004a).

Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process.

Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence. $$$$$ Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence. $$$$$ Ding and Palmer (2004b) described one version of synchronous grammar: Synchronous Dependency Insertion Grammars.

Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ We then used the algorithm in (Xia 2001) to convert the phrasal structure trees to dependency trees to acquire the parallel dependency trees.

Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004). $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
Recently, it is widely adopted by the popular applications of natural language processing techniques, such as machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al, 2002), relation extraction (Culotta and Sorensen, 2004) and lexical resource augmentation (Snow et al, 2004). $$$$$ Our MT system was evaluated using the n-gram based Bleu (Papineni et al., 2002) and NIST machine translation evaluation software.

 $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
 $$$$$ Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.

Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. $$$$$ Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insertion grammar. $$$$$ In this paper, we present a syntax-based statistical matranslation system based on a probabilistic synchronous dependency insertion grammar.

It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). $$$$$ Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
It is not rare to see dependency relations used as features, in tasks such as relation extraction (Bunescu and Mooney, 2005) and machine translation (Ding and Palmer, 2005). $$$$$ This grammar is used as the primary translation knowledge source for our system.

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ The machine translation is done either as a stochastic tree-to-tree transduction or a synchronous parsing process.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ Our MT system was evaluated using the n-gram based Bleu (Papineni et al., 2002) and NIST machine translation evaluation software.

As mentioned in (Ding and Palmer, 2005), most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages. $$$$$ A synchronous derivation process for the two syntactic structures of both languages suggests the level of cross-lingual isomorphism between the two trees (e.g.
As mentioned in (Ding and Palmer, 2005), most of these approaches require some assumptions on the level of isomorphism (lexical and/or structural) between two languages. $$$$$ The synchronous version, SDIG, assumes that the isomorphism of the two syntactic structures is at the ET level, rather than at the word level, hence allowing non-isomorphic tree to tree mapping.

 $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
 $$$$$ Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.

It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). $$$$$ Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars
It is no longer rare to see dependency relations used as features, in tasks such as machine translation (Ding and Palmer, 2005) and relation extraction (Bunescu and Mooney, 2005). $$$$$ This grammar is used as the primary translation knowledge source for our system.

(Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures. $$$$$ Ding and Palmer (2004b) described one version of synchronous grammar: Synchronous Dependency Insertion Grammars.
(Ding and Palmer, 2005) presents a translation model based on Synchronous Dependency Insertion Grammar (SDIG), which handles some of the non-isomorphism but requires both source and target dependency structures. $$$$$ In this paper we presented a syntax-based statistical MT system based on a Synchronous Dependency Insertion Grammar and a non-isomorphic stochastic tree-to-tree transducer.

This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ Let f be the input sentence (foreign language), and e be the output sentence (English).
This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ The recent advances in parsing have achieved parsers with 3 O(n ) time complexity without the grammar constant (McDonald et al., 2005).

Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. $$$$$ Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees.
Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees. $$$$$ Ding and Palmer (2004b) described one version of synchronous grammar: Synchronous Dependency Insertion Grammars.

 $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
 $$$$$ Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.

 $$$$$ Different versions of such training algorithms can be found in (Hajic et al., 2002; Eisner 2003; Gildea 2003; Graehl and Knight 2004).
 $$$$$ Future work includes a full-fledged version of SDIG and a more sophisticated MT pipeline with possibly a tri-gram language model for decoding.
