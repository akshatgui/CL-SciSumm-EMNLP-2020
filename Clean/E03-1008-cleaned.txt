We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.

Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ The results are shown in Figure 5.

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ Second, we do not require the two parsers to have the same training sets.
This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ As the amount of seed data increases, coverage becomes less of a problem, and the co-training advantage is diminished.

(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ Therefore we used two sizes of seed data

The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ Therefore we used two sizes of seed data

The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser and a supertagger for that parser).
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ To apply the theory of co-training to parsing, we need to ensure that each parser is capable of learning the parsing task alone and that the two parsers have different views.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ The domains over which the two models operate are quite distinct.

These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ Self-training experiments were conducted in which each parser was retrained on its own output.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.

Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ Self-training experiments were conducted in which each parser was retrained on its own output.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.

Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small.
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ While the amount of improvement in performance is less than the previous case, co-training provides an additional boost to the parsing performance, to 78.7%.

In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ Since we need to parse all sentences in section 0 at each iteration, in the experiments reported in this paper we only evaluated one of the parsers, the Collins-CFG parser, at each iteration.
In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ During each co-training round, the LTAG parser parsed 30 sentences, and the 20 labelled sentences with the highest scores (according to the LTAG joint probability) were added to the training data of the Collins-CFG parser.

It is not surprising that self-training is not normally effective $$$$$ Self-training was used by Charniak (1997), where a modest gain was reported after re-training his parser on 30 million words.
It is not surprising that self-training is not normally effective $$$$$ During each round of self-training, 30 sentences were parsed by each parser, and each parser was retrained upon the 20 self-labelled sentences which it scored most highly (each parser using its own joint probability (equation 1) as the score).

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Bootstrapping Statistical Parsers From Small Datasets
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.

Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ In contrast, this paper considers co-training two diverse statistical parsers

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.
