 $$$$$ We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.
 $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.

(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).

For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ As δ increases, we penalize long dependencies less.
For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ Grammar induction serves as a tidy example for structural annealing.

We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. $$$$$ A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979).
We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. $$$$$ The central idea of this paper is to gradually change (anneal) the bias δ.

This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). $$$$$ Annealing Structural Bias In Multilingual Weighted Grammar Induction
This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).

These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). $$$$$ first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). $$$$$ In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).

Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ Early in learning, local dependencies are emphasized by setting δ « 0.
Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $.

Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). $$$$$ Note that structural annealing does not always outperform fixed-δ training (English and Portuguese).
Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). $$$$$ Experiment: Annealing β.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).

In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective.
In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ We could describe “brokenness” as a feature in the model whose weight, Q, is chosen extrinsically (and time-dependently), rather than empirically—just as was done with S. Annealing β resembles the popular bootstrapping technique (Yarowsky, 1995), which starts out aiming for high precision, and gradually improves coverage over time.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).

Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).
Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ Experiment: Locality Bias within CE.

Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. $$$$$ We used the EM algorithm to train this model on POS sequences in six languages.
Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. $$$$$ Experiment: Locality Bias within CE.

Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ Performance with unsupervised and supervised model selection across different λ values in add-λ smoothing and three initializers O(0) is reported in Table 1.
Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words—can give guaranteed O(nk2) runtime with little to no loss in accuracy (for simple models).

Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ This allows the learner to accept hypotheses that explain the sentences as independent pieces.
Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ As originally proposed, CE allowed a redefinition of the implicit negative evidence from “all other sentences” (as in MLE) to “sentences like xi, but perturbed.” Allowing segmentation of the training sentences redefines the positive and negative evidence.

Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997).
Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).

These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ Following common practice, we always replace words by part-ofspeech (POS) tags before training or testing.
These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).

Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a). $$$$$ Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model.
Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a). $$$$$ This method was applied with some success to grammar induction models by Smith and Eisner (2004).

 $$$$$ We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.
 $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.

Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. $$$$$ In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias.
Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. $$$$$ Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM.
