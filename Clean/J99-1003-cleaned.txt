Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. $$$$$ When the matching predicate cannot generate enough candidate correspondence points based on cognates, its signal can be strengthened by a seed translation lexicon— a simple list of word pairs that are believed to be mutual translations.
Melamed (1999) aligns texts using correspondence points taken either from orthographic cognates (Michel Simard et al., 1992) or from a seed translation lexicon. $$$$$ The translation lexicon was automatically extracted from an MRBD (Cousin et al. 1991).

The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115-116). $$$$$ The remaining chains are filtered using two threshold parameters: maximum point dispersal and maximum angle deviation.
The latter approach uses other filtering parameters: maximum point ambiguity level, point dispersion and angle deviation (Melamed, 1999, pp. 115-116). $$$$$ SIMR's parameters—the fixed chain size; the LCSR threshold used in the matching predicate; and the thresholds for maximum point dispersal, maximum angle deviation, and maximum point ambiguity—interact in complicated ways.

Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates. $$$$$ Two words are orthographic cognates if they have the same meaning and similar spellings.
Melamed (1999) also filtered candidate correspondence points obtained from orthographic cognates. $$$$$ When the matching predicate cannot generate enough candidate correspondence points based on cognates, its signal can be strengthened by a seed translation lexicon— a simple list of word pairs that are believed to be mutual translations.

This approach is similar to Melamed (1999) but, in contrast, it is statistically supported and uses no heuristics. $$$$$ Another interesting approach is possible when part-of-speech taggers are available for both languages.
This approach is similar to Melamed (1999) but, in contrast, it is statistically supported and uses no heuristics. $$$$$ SIMR uses a fixed chain size k, 6 < k < 11.

We use a similarity function proposed in (Contractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
We use a similarity function proposed in (Contractor et al, 2010) which is based on Longest Common Subsequence Ratio (LCSR) (Melamed, 1999). $$$$$ The LCSR of two tokens is the ratio of the length of their longest (not necessarily contiguous) common subsequence (LCS) and the length of the longer token.

The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996). $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
The former includes string edit distance (Wagner and Fischer, 1974), longest common subsequence ratio (Melamed, 1999), and measures based on shared character n-grams (Brew and McKelvie, 1996). $$$$$ The cognate heuristic of character-based bitext mapping algorithms also works better at the word level, because cognateness can be defined more precisely in terms of words, e.g., using the Longest Common Subsequence Ratio.

Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter's stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST's lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dicecoefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter's stemmer (Porter, 1980), Morpha (Minnen et al,2001), and CST's lemmatiser (Dalianis and Jonge 3LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. $$$$$ The LCSR of two tokens is the ratio of the length of their longest (not necessarily contiguous) common subsequence (LCS) and the length of the longer token.

Other popular measures include Dice's Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005). $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
Other popular measures include Dice's Coefficient (DICE) (Adamson and Boreham, 1974), and the length-normalized measures Longest Common Subsequence Ratio (LCSR) (Melamed, 1999), and Longest Common Prefix Ratio (PREFIX) (Kondrak,2005). $$$$$ The LCSR of two tokens is the ratio of the length of their longest (not necessarily contiguous) common subsequence (LCS) and the length of the longer token.

We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs. $$$$$ Objective evaluation has shown that SIMR's accuracy is consistently high for language pairs as diverse as French/English and Korean/English.
We adopt an improved definition (suggested by Melamed (1999) for the French-English Canadian Hansards) that does not over-propose shorter word pairs. $$$$$ In the nontechnical Canadian Hansards (parliamentary debate transcripts published in English and in French), an LCSR cutoff of .58 finds cognates for roughly one quarter of all text tokens.

Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)). $$$$$ In the nontechnical Canadian Hansards (parliamentary debate transcripts published in English and in French), an LCSR cutoff of .58 finds cognates for roughly one quarter of all text tokens.
Our labelled set is then generated from pairs with LCSR 0.58 (using the cutoff from Melamed (1999)). $$$$$ The TBM consists of a set of TPCs.

A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999). $$$$$ SIMR makes errors of omission and errors of commission.
A few of the errors were genuine, and could be explained by failures of the sentence alignment program that was used to create the corpus (Melamed, 1999). $$$$$ To account for the possibility of modularizing the overall alignment task into paragraph alignment followed by sentence alignment, Simard, Foster, and Isabelle (1992) have reported the accuracy of their sentence alignment algorithm when a perfect alignment at the paragraph level is given.

The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999). $$$$$ As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools.
The Korean-English parallel data was collected from news websites and sentence aligned using two different tools described by Moore (2002) and Melamed (1999). $$$$$ As with other kinds of data, the value of bitexts largely depends on the efficacy of the available data mining tools.

 $$$$$ A rather more complicated algorithm can compute it in 0(n log log n) time on average (Hunt and Szymanski 1977).
 $$$$$ The majority of this work was done at the Department of Computer and Information Science of the University of Pennsylvania, where it was supported by an equipment grant from Sun MicroSystems and partially funded by ARO grant DAAL03-89-00031 PRIME and by ARPA grants N00014-90+1863 and N66001-94C-6043.

Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). $$$$$ The LCSR of two tokens is the ratio of the length of their longest (not necessarily contiguous) common subsequence (LCS) and the length of the longer token.

The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string. $$$$$ The matching predicates in SIMR's current implementation threshold the Longest Common Subsequence Ratio (LCSR).
The Longest Common Subsequence Ratio (LCSR) (Melamed, 1999) of two strings is the ratio of the length of their LCS and the length of the longer string. $$$$$ The LCSR of two tokens is the ratio of the length of their longest (not necessarily contiguous) common subsequence (LCS) and the length of the longer token.

Our work is based on a modification of the SIMR bitext mapping algorithm (Melamed, 1999). $$$$$ The article begins with a geometric interpretation of the bitext mapping problem and a discussion of previous work.
Our work is based on a modification of the SIMR bitext mapping algorithm (Melamed, 1999). $$$$$ SIMR borrows several insights from previous work.

For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). $$$$$ To reduce such errors, GSA asks Gale & Church's length-based alignment algorithm (Gale and Church 1991a; Michel Simard, personal communication) for a second opinion on any aligned block that is not 1 x 1.
For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or - alternatively - Dan Melamed's GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). $$$$$ To account for the possibility of modularizing the overall alignment task into paragraph alignment followed by sentence alignment, Simard, Foster, and Isabelle (1992) have reported the accuracy of their sentence alignment algorithm when a perfect alignment at the paragraph level is given.
