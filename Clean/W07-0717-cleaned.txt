In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. $$$$$ We use two different estimates for the conditional probabilities p(ï¿½t

Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ We adapt both language and translation model features within the overall loglinear combination (1).
Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ The most commonly-used framework for mixture models is a linear one

Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ Our focus in this paper is on adaptation via mixture weights.
Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ This is not the case for dynamic adaptation, where, in the absence of an in-domain development corpus, the only information we can hope to glean are the weights on adapted models compared to other features of the system.

Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ Mixture-Model Adaptation for SMT
Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ However, combined LM and TM adaptation is not better than LM adaptation on its own, indicating that the individual adapted models may be capturing the same information.

Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. $$$$$ Our approach to mixture-model adaptation can be summarized by the following general algorithm

Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. $$$$$ In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.
Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. $$$$$ A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.

For details, refer to (Foster and Kuhn, 2007). $$$$$ We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.
For details, refer to (Foster and Kuhn, 2007). $$$$$ It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990).

In (Foster and Kuhn, 2007), two kinds of linear mixture were described $$$$$ Table 2 shows a comparison between linear and loglinear mixing frameworks, with uniform weights used in the linear mixture.
In (Foster and Kuhn, 2007), two kinds of linear mixture were described $$$$$ Due to this result, all experiments we describe below involve linear mixtures only.

In (Foster and Kuhn, 2007) two basic settings are investigated $$$$$ In cross-domain adaptation, a small sample of parallel in-domain text is available, and it is used to optimize for translating future texts drawn from the same domain.
In (Foster and Kuhn, 2007) two basic settings are investigated $$$$$ In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.

Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.
Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set.

Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. $$$$$ Phrase translation model probabilities are features of the form

In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. $$$$$ The remainder of the paper is structured follows

(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. $$$$$ We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. $$$$$ Set mixture weights as a function of the distances from corpus components to the current source text.

Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). $$$$$ This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006).
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). $$$$$ We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.

Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. $$$$$ The training corpus was divided into seven components according to genre; in all cases these were identical to LDC corpora, with the exception of the Newswire component, which was amalgamated from several smaller corpora.
Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. $$$$$ The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components.

Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). $$$$$ Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text.
Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). $$$$$ Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set.

Mixture-modelling for language models is well established (Foster and Kuhn, 2007). $$$$$ The most commonly-used framework for mixture models is a linear one

Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ Both LM and TM adaptation are effective, with test-set improvements of approximately 1 BLEU point over the baseline for LM adaptation and somewhat less for TM adaptation.
Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ In this setting, TM adaptation is much less effective, not significantly better than the baseline; performance of combined LM and TM adaptation is also lower.

Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ When using a loglinear combining framework as described in section 3.3, mixture weights are set in the same way as the other loglinear parameters when performing cross-domain adaptation.
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation.

They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). $$$$$ The most commonly-used framework for mixture models is a linear one
