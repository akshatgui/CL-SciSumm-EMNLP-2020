It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations. $$$$$ We provided these instructions: Edit Machine Translation Your task is to edit the machine translation making as few changes as possible so that it matches the meaning of the human translation and is good English.
It is possible that the length of stay of an annotator in the pool is not independent of her diligence; for example, Callison-Burch (2009) found in his AMT experiments with tasks related to machine translation that lazy annotators tended to stay longer and do more annotations. $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon's Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. $$$$$ We evaluated the feasibility of using Mechanical Turk to perform HTER.
We will perform a semi-automatic validation of BabelNet, e.g. by exploiting Amazon's Mechanical Turk (Callison-Burch, 2009) or designing a collaborative game (von Ahn, 2006) to validate low-ranking mappings and translations. $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

For example, Callison-Burch (2009) used MTurk to evaluate machine translations. $$$$$ Following Callison-Burch et al. (2008), we assigned a score to each of the 11 MT systems based on how often its translations were judged to be better than or equal to any other system.
For example, Callison-Burch (2009) used MTurk to evaluate machine translations. $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

(Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations. $$$$$ Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.
(Callison-Burch, 2009) uses MTurk workers for manual evaluation of automatic translation quality and experiments with weighed voting to combine multiple annotations. $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).
The use of crowd sourcing to evaluate machine translation and to build development sets was pioneered by Callison-Burch (2009) and Zaidan and Callison-Burch (2009). $$$$$ The cost of using Mechanical Turk is low enough that we might consider attempting quixotic things like human-in-the-loop minimum error rate training (Zaidan and Callison-Burch, 2009), or doubling the amount of training data available for Urdu.

Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators. $$$$$ To avoid letting careless annotators drag down results, we experimented with weighted voting.
Following Callison-Burch (2009), we treat evaluation as a weighted voting problem where each annotator's contribution is weighted by agreement with either a gold standard or with other annotators. $$$$$ The contribution of each component system is weighted by the expectation that it will produce good output.

It has also been used in MT evaluation (Callison-Burch, 2009), though that evaluation used reference translations. $$$$$ The Turkers were shown a source sentence, a reference translation, and translations from five MT systems.
It has also been used in MT evaluation (Callison-Burch, 2009), though that evaluation used reference translations. $$$$$ If none of the five edits was deemed to be acceptable, then we used the edit distance between the MT and the reference.

As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by (Callison-Burch, 2009). $$$$$ We had bilingual graduate students translate the first 50 English sentences of that corpus into French, German and Spanish, so that we could re-use the multiple English reference translations.
As an example, among the collected material several translations in languages other than English revealed a massive and defective use of on-line translation tools by untrusted workers, as also observed by (Callison-Burch, 2009). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009). $$$$$ Turkers are free to select whichever HITs interest them.
We do not select German, French and other language pairs as they have already been explored by Callison-Burch (2009). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008). $$$$$ Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.
Furthermore, previous research shows the effectiveness of crowd sourcing as a method of accomplishing labor intensive natural language processing tasks (Callison-Burch, 2009) and the effectiveness of using MTurk for a variety of natural language automation tasks (Snow, Jurafsy, & O'Connor, 2008). $$$$$ The advantage of this type of evaluation is that the results have a natural interpretation.

The value of this upper bound is quite consistent with the bound computed similarly by Callison-Burch (2009). $$$$$ An upper bound is indicated by the expert-expert bar.
The value of this upper bound is quite consistent with the bound computed similarly by Callison-Burch (2009). $$$$$ This gives an upper bound on the expected quality.

 $$$$$ Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.
 $$$$$ ; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.

 $$$$$ Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.
 $$$$$ ; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.

Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. $$$$$ We report on experiments evaluating translation quality with HTER and with reading comprehension tests.
Callison-Burch (2009) showed similar results for machine translation evaluation, and further showed that Turkers could accomplish complex tasks like translating Urdu or creating reading comprehension tests. $$$$$ We showed how a reading comprehension test could be created, administered, and graded, with only very minimal intervention.

 $$$$$ Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.
 $$$$$ ; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.

Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ These tasks included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.
Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. $$$$$ Therefore, having people evaluate translation output would be preferable, if it were more practical.
Callison-Burch (2009) proposed several ways to evaluate MT output on MTurk. $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

Over the last several of years, Mechanical Turk, introduced by Amazon as "artificial artificial intelligence", has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). $$$$$ Amazon describes its Mechanical Turk web service1 as artificial artificial intelligence.
Over the last several of years, Mechanical Turk, introduced by Amazon as "artificial artificial intelligence", has been used successfully for a number of NLP tasks, including robust evaluation of machine translation systems by reading comprehension (Callison-Burch, 2009), and other tasks explored in the recent NAACL workshop (Callison-Burch and Dredze, 2010b). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).

There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results. $$$$$ Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.
There have been several research papers on using MTurk to help natural language processing tasks, Callison-Burch (2009) used MTurk to evaluate machine translation results. $$$$$ The advantage of this type of evaluation is that the results have a natural interpretation.

On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009). $$$$$ These systems were selected from WMT09 (Callison-Burch et al., 2009).
On another way, an application can combine active learning (Arora et al, 2009) and crowd sourcing, asking non-expertise such as workers of Amazon Mechanical Turk to label crucial alignment links that can improve the system with low cost, which is now a promising methodology in NLP areas (Callison-Burch, 2009). $$$$$ The low cost of the non-expert labor found on Mechanical Turk is cheap enough to collect redundant annotations, which can be utilized to ensure translation quality.
