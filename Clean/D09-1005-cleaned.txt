More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ Under the first-order expectation semiring ER,R-, the inside algorithm of Figure 2 will return (Z, r) where r is a vector of n feature expectations.
More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009). $$$$$ We now show how to compute a few other quantities by choosing re appropriately.

Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ First, at lines 2–3, the inside and outside algorithms are run using only the ke weights, obtaining only k (without x) but also obtaining all inside and outside weights ,Q, α ∈ K as a side effect.
Instead of learning the probabilities on the PCFG, we directly compute the weights on the hyper arcs using a dynamic program similar to the inside-outside algorithm (Li and Eisner, 2009). $$$$$ Second, we can directly apply the inside-outside algorithm there, as we now see.

In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ We are now interested in computing the following quantities on the hypergraph HG: Note that r/Z, s/Z, and t/Z are expectations under p of r(d), s(d), and r(d)s(d), respectively.
In order to learn the weights on the hyper arcs we perform the following procedure iteratively in an Em fashion (Li and Eisner, 2009). $$$$$ During test time, a similar procedure is followed.

We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ So what makes the inside-outside algorithm more efficient?
We converted the grammar into a hypergraph, and learned its probability distributions using a dynamic program similar to the inside-outside algorithm (Liand Eisner, 2009). $$$$$ Second, we can directly apply the inside-outside algorithm there, as we now see.

Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ Then r/Z is the expected hypothesis length.
Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). $$$$$ In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.

A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ Note that, to compute t, one cannot simply construct a first-order expectation semiring by defining t(d) def = r(d)s(d) because t(d), unlike r(d) and s(d), is not additively decomposable over the hyperedges in d.5 Also, when r(d) and s(d) are identical, the second-order expectation semiring allows us to compute variance as t/Z − (r/Z)2, which is why we may call our second-order expectation semiring the variance semiring.
A generic first-order expectation semiring is also provided (Li and Eisner, 2009). $$$$$ In fact, that We now observe that the second-order expectation semiring EP,R,S,T can be obtained indirectly by nesting one first-order expectation semiring inside another!

Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ Then r/Z is the risk (expected loss), which is useful in minimum-risk training.
Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al, 2009). $$$$$ All MR or MR+DA uses an approximated BLEU (Tromble et al., 2008) (for training only), while MERT uses the exact corpus BLEU in training.

As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ But the H and E operators here now denote appropriate operations within P, R, and S respectively (rather than the usual operations within R).
As usual in natural language processing applications, we can exploit appropriate semirings and compute several useful statistical parameters through Mw (T?? T?), as for instance the highest weight of a computation, the inside probability and the rule expectations; see (Li and Eisner, 2009) for further discussion. $$$$$ We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.

The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ Under this paradigm, we use weights from the expectation semiring (Eisner, 2002), to compute first-order statistics (e.g., the expected hypothesis length or feature counts) over packed forests of translations (lattices or hypergraphs).
The sufficient statistics for graph expected BLEU can be computed using expectation semirings (Li and Eisner, 2009). $$$$$ In Section 3, we show how to compute the expected hypothesis length or expected feature counts, using the algorithm of Figure 2 with a first-order expectation semiring ER,R.

For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ So what makes the inside-outside algorithm more efficient?
For inside-outside algorithm, see (Li and Eisner, 2009). $$$$$ Second, we can directly apply the inside-outside algorithm there, as we now see.

The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ Given a hypergraph weighted by a second-order expectation semiring EP,R,S,T.
The risk and its gradient on a hypergraph can be computed by using a second-order expectation semiring (Li and Eisner, 2009). $$$$$ Our objective function could be computed with a first-order expectation semiring, but computing it along with its gradient requires a second-order one.

There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests
There lative weights of these 10 features are tuned via hypergraph-based minimum risk training (Li and Eisner, 2009) on the bilingual data Set. $$$$$ Then r/Z is the risk (expected loss), which is useful in minimum-risk training.

One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ The expectation semiring (Eisner, 2002), originally proposed for finite-state machines, is one such “training” semiring, and can be used to compute feature expectations for the Estep of the EM algorithm, or gradients of the likelihood function for gradient descent.
One is a variant of the F-B on the expectation semiring proposed in Li and Eisner (2009). $$$$$ Next suppose K is the expectation semiring (Eisner, 2002), shown in Table 1.

For the detailed description, see Li and Eisner (2009) and its references. $$$$$ To achieve competitive performance, we adopt a forest reranking approach (Li and Khudanpur, 2009; Huang, 2008).
For the detailed description, see Li and Eisner (2009) and its references. $$$$$ Li and Khudanpur (2008)).

We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ Second, we can directly apply the inside-outside algorithm there, as we now see.
We have implemented the inside algorithm, the outside algorithm, and the inside-outside speedup described by Li and Eisner (2009), plut the first-order expectation semiring (Eisner, 2002) and its second-order version (Liand Eisner, 2009). $$$$$ We presented first-order expectation semirings and inside-outside computation in more detail than (Eisner, 2002), and developed extensions to higher-order expectation semirings.

We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Then r/Z is the risk (expected loss), which is useful in minimum-risk training.
We have implemented the hypergraph based minimum risk training (Li and Eisner, 2009), which minimizes the expected loss of the reference translations. $$$$$ Expected Loss (Risk) Given a reference sentence y*, the expected loss (i.e., Bayes risk) of the hypotheses in the hypergraph is defined as, where Y(d) is the target yield of d and L(y, y*) is the loss of the hypothesis y with respect to the reference y*.

The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ Smith and Eisner (2006) instead propose a differentiable objective that can be optimized by gradient descent: the Bayes risk R(p) of (7).
The work of Smith and Eisner was extended by Li and Eisner (2009) who were able to obtain much better estimates of feature expectations by using a packed chart instead of an n-best list. $$$$$ Solving (14) for a given T requires computing the entropy H(p) and risk R(p) and their gradients with respect to θ and γ. Smith and Eisner (2006) followed MERT in constraining their decoder to only an n-best list, so for them, computing these quantities did not involve dynamic programming.

Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ However, r, s, and t can be positive or negative, and we cannot directly take the log of a negative number.
Instead of n-best approximations, we may directly employ forests for a better conditional log-likelihood estimation (Li and Eisner,2009). $$$$$ Second, we can directly apply the inside-outside algorithm there, as we now see.

Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests
Li and Eisner (2009) present work on performing expected BLEU training with deterministic annealing on translation forests generated by Hiero (Chiang, 2007). $$$$$ We implement the expectation and variance semirings in Joshua (Li et al., 2009a), and demonstrate their practical benefit by using minimumrisk training to improve Hiero (Chiang, 2007).

This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ First- and Second-Order Expectation Semirings with Applications to Minimum-Risk Training on Translation Forests
This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. $$$$$ Then r/Z is the risk (expected loss), which is useful in minimum-risk training.
