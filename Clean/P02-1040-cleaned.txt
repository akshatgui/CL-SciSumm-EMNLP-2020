As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ How does one measure translation performance?
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ In Section 3, we evaluate the performance of BLEU.

For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ In Section 3, we evaluate the performance of BLEU.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ Bleu: A Method For Automatic Evaluation Of Machine Translation
One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric.

We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ In Section 3, we evaluate the performance of BLEU.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ A translation using the same words (1-grams) as in the references tends to satisfy adequacy.

The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Experiments over large collections of translations presented in Section 5 show that this ranking ability is a general phenomenon, and not an artifact of a few toy examples.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ Bleu: A Method For Automatic Evaluation Of Machine Translation
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.

The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ In Section 2, we describe the baseline metric in detail.
The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean.

We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ Bleu: A Method For Automatic Evaluation Of Machine Translation
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ We propose such an evaluation method in this paper.

We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ In Section 3, we evaluate the performance of BLEU.
We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.

We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ This outcome suggests that we may use a big test corpus with a single reference translation, provided that the translations are not all from the same translator.
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

 $$$$$ To some extent, the n-gram precision already accomplishes this.
 $$$$$ We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.

BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ Bleu: A Method For Automatic Evaluation Of Machine Translation
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ Unfortunately, MT systems can overgenerate “reasonable” words, resulting in improbable, but high-precision, translations like that of example 2 below.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ We compute the brevity penalty BP, The ranking behavior is more immediately apparent in the log domain, log BLEU = min(1 − In our baseline, we use N = 4 and uniform weights wn = 1/N.
A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ 5 BLEU vs The Human Evaluation Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems.

The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Our belief is reinforced by a recent statistical analysis of BLEU’s correlation with human judgment for translation into English from four quite different languages (Arabic, Chinese, French, Spanish) representing 3 different language families (Papineni et al., 2002)!

Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ Bleu: A Method For Automatic Evaluation Of Machine Translation
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from 1So we call our method the bilingual evaluation understudy, BLEU. the evaluation bottleneck.

To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ In Section 3, we evaluate the performance of BLEU.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ 2.1.2 Ranking systems using only modified n-gram precision To verify that modified n-gram precision distinguishes between very good translations and bad translations, we computed the modified precision numbers on the output of a (good) human translator and a standard (poor) machine translation system using 4 reference translations for each of 127 source sentences.

To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ These translations may vary in word choice or in word order even when they use the same words.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ This rewards using a word as many times as warranted and penalizes using a word more times than it occurs in any of the references.

For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ We now take the worst system as a reference point and compare the BLEU scores with the human judgment scores of the remaining systems relative to the worst system.

For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ How does one measure translation performance?
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ In Section 3, we evaluate the performance of BLEU.
