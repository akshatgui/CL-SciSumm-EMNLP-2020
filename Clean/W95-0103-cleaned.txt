Brill and Resnik (1994) applied Error-Driven Transformation Based Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ (In this case the VP attachment is correct)

As we have argued in Zavrel and Daelemans (1997), this corresponds exactly to the behavior of the Back-Off algorithm of Collins and Brooks (1995), so that it comes as no surprise that the accuracy of both methods is the same. $$$$$ For this reason the two methods deserve closer comparison.
As we have argued in Zavrel and Daelemans (1997), this corresponds exactly to the behavior of the Back-Off algorithm of Collins and Brooks (1995), so that it comes as no surprise that the accuracy of both methods is the same. $$$$$ The accuracy figure is then the percentage accuracy on the test cases where the (v, nl, n2) counts were used.

The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). $$$$$ 'Personal communication from Brill.
The results of Brill's method on the present benchmark were reconstructed by Collins and Brooks (1995). $$$$$ All results are for the IBM data.

Collins and Brooks (1995) used a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results). $$$$$ Crucially they ignore low-count events in training data by imposing a frequency cut-off somewhere between 3 and 5.
Collins and Brooks (1995) used a Back-Off model, which enables them to take low frequency effects into account on the Ratnaparkhi dataset (with good results). $$$$$ The results were as follows

The perhaps underwhelming human performance is partially due to misclassifications by the Treebank assemblers who made these determinations by hand, and also unclear cases, which we discuss in the next section. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. $$$$$ Prepositional Phrase Attachment Through A Backed-Off Model
The perhaps underwhelming human performance is partially due to misclassifications by the Treebank assemblers who made these determinations by hand, and also unclear cases, which we discuss in the next section. Collins and Brooks (1995) introduced modifications to the Ratnaparkhi et al (1994) dataset meant to combat data sparsity and used the modified version to train their backed-off model. $$$$$ All results in this section are on the IBM training and test data, with the exception of the two 'average human' results.

Abney, Schapire, and Singer (1999) used the dataset from Collins and Brooks (1995) with a boosting algorithm and achieved 85.4% accuracy. Their algorithm also was able to order the specific data points by how much weight they were assigned by the learning algorithm. $$$$$ The accuracy of the algorithm is then the percentage of attachments it gets 'correct' on test data, using the A values taken from the treebank as the reference set.
Abney, Schapire, and Singer (1999) used the dataset from Collins and Brooks (1995) with a boosting algorithm and achieved 85.4% accuracy. Their algorithm also was able to order the specific data points by how much weight they were assigned by the learning algorithm. $$$$$ The attachment decisions for these triples were unknown, so an unsupervised training method was used (section 5.2 describes the algorithm in more detail).

Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). $$$$$ [BR94] use 12,000 training and 500 test examples.
Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). $$$$$ In an effort to reduce sparse data problems the following processing was run over both test and training data

Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ In an effort to reduce sparse data problems the following processing was run over both test and training data

Collins and Brooks (1995) used a supervised back-off model to achieve 84.5% precision on the Ratnaparkhi test set. $$$$$ This set was used during development of the attachment algorithm, ensuring that there was no implicit training of the method on the test set itself.
Collins and Brooks (1995) used a supervised back-off model to achieve 84.5% precision on the Ratnaparkhi test set. $$$$$ The development set with no morphological processing was used for these tests.

However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition $$$$$ 'Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple.
However, the baseline is similarly high for the PP problem if the most likely attachment is chosen per preposition $$$$$ A reasonable lower bound seems to be 72.2% as scored by the 'Most likely for each preposition' method.

(Collins and Brooks, 1995) also present a model with multiple back offs. $$$$$ Prepositional Phrase Attachment Through A Backed-Off Model
(Collins and Brooks, 1995) also present a model with multiple back offs. $$$$$ The results were as follows

Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events. $$$$$ Prepositional Phrase Attachment Through A Backed-Off Model
Later, Collins and Brooks (1995) achieved 84.5% accuracy by employing a backed-off model to smooth for unseen events. $$$$$ Results on Wall Street Journal data of 84.5% accuracy are obtained using this method.

In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995). $$$$$ A key observation in choosing between these tuples is that the preposition is particularly important to the attachment decision.
In our experiments, we only considered features that contained P since the preposition is the most important lexical item (Collins and Brooks, 1995). $$$$$ Section 6.2 describes experiments which show that tuples containing the preposition are much better indicators of attachment.

We describe the different classifiers below $$$$$ The decision was made as follows'

The accuracy is reported in (Collins and Brooks, 1995). $$$$$ Results of 77.7% (words only) and 81.6% (words and classes) are reported.
The accuracy is reported in (Collins and Brooks, 1995). $$$$$ The accuracy figure is then the percentage accuracy on the test cases where the (v, nl, n2) counts were used.

p (Rjright; a; b) =# (R; right; a; b)# (right; a; b) (6) e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun 7) p (pobjjright; verb; prep ;desc $$$$$ For each such VP the head verb, first head noun, preposition and second head noun were extracted, along with the attachment decision (1 for noun attachment, 0 for verb).
p (Rjright; a; b) =# (R; right; a; b)# (right; a; b) (6) e.g. for the Verb-PP attachment relation pobj (following (Collins and Brooks, 1995) including the description noun 7) p (pobjjright; verb; prep ;desc $$$$$ The probability of the attachment variable A being 1 or 0 (signifying noun or verb attachment respectively) is a probability, p, which is conditional on the values of the words in the quadruple.

For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase $$$$$ Prepositional Phrase Attachment Through A Backed-Off Model
For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase $$$$$ A PP-attachment algorithm must take each quadruple (V = v, Ni = nl, P = p, N2 = n2) in test data and decide whether the attachment variable A =.-- 0 or 1.

We used the same training and test data as Collins and Brooks (1995). $$$$$ The training and test data were supplied by IBM, being identical to that used in [RRR94].
We used the same training and test data as Collins and Brooks (1995). $$$$$ A quadruple may appear in test data which has never been seen in training data. ie. f(v, nl,p, n2) = 0.

Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations. $$$$$ 'Most likely for each preposition' means use the attachment seen most often in training data for the preposition seen in the test quadruple.
Our approach can be seen as an extension of (Collins and Brooks, 1995) from PP-attachment to most dependency relations. $$$$$ In particular, quadruples and triples seen in test data will frequently be seen only once or twice in training data.

Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). $$$$$ (In this case the VP attachment is correct)
