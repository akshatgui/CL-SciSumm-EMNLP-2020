Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. $$$$$ However, if we evaluate their fluency based on the syntactic similarity with the reference, we will get our desired results.
Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. $$$$$ The syntactic metrics, except the kernel based ones, all outperform BLEU in sentence-level fluency evaluation.

Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ The results show that in both systems our syntactic metrics all achieve a better performance in the correlation with human judgments of fluency.
Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ For the overall evaluation of sentences for fluency and adequacy, the metric based on headword chain performs better than BLEU in both sentencelevel and corpus-level correlation with human judgments.

Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.

Similarities are captured from different viewpoints $$$$$ For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.
Similarities are captured from different viewpoints $$$$$ For this reason, the two methods described in section 3.1 are used to compute the similarity of dependency trees between the MT hypothesis and its references, and the corresponding metrics are denoted DSTM for dependency subtree metric and DTKM for dependency tree kernel metric.

This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.
This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ Therefore, the final score of STM is (6/7+3/4+1/2)/3=0.702.

Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. $$$$$ Syntactic Features For Evaluation Of Machine Translation
Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.

While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.
While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999).

These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.

Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ To do this, the syntactic metrics (computed with the Collins (1999) parser) as well as BLEU were used to evaluate hypotheses in the test set from ACL05 MT workshop, which provides both fluency and adequacy scores for each sentence, and their Pearson coefficients of correlation with the human fluency scores were computed.
Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ In our experiments, every hypothesis is evaluated by referring to three human translations.

This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ Since PRON only occurs once in the reference, its clipped count should be 1 rather than 2.
This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.

The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ As MT systems improve, the shortcomings of n-gram based evaluation are becoming more apparent.
The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ This shows that corpus-level evaluation, compared with the sentence-level evaluation, is much less sensitive to the sparse data problem and thus leaves more space for making use of comprehensive evaluation metrics.

These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ We derived dependency trees from the constituent trees by applying the deterministic headword extraction rules used by the parser of Collins (1999).
These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ A similar trend can be found in syntax tree and dependency tree based metrics, but the decreasing ratios are much lower than BLEU, which indicates that the syntactic metrics are less affected by the sparse data problem.

We use three different kinds of metrics $$$$$ In order to derive a similarity measure ranging from zero to one, we use the cosine of the vectors H

The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ Evidence that we are reaching the limits of ngram based evaluation was provided by Charniak et al. (2003), who found that a syntax-based language model improved the fluency and semantic accuracy of their system, but lowered their BLEU score.
The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ Our syntax-based measures require the existence of a parser for the language in question, however it is worth noting that a parser is required for the target language only, as all our measures of similarity are defined across hypotheses and references in the same language.

coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. $$$$$ Dependency trees were found to correspond better across translation pairs than constituent trees by Fox (2002), and form the basis of the machine translation systems of Alshawi et al. (2000) and Lin (2004).
coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. $$$$$ Using HWCM to denote the headword chain based metric, it is computed as follows

Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ State-of-the-art MT output often contains roughly the correct words and concepts, but does not form a coherent sentence.
Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ Though our syntactic metrics are proposed for evaluating the sentences’ fluency, we are curious how well they do in the overall evaluation of sentences.

For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. $$$$$ For example, in the BLEU metric, the overlapping fractions of n-grams with more than one word are considered as a kind of metric for the fluency of the hypothesis.
For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. $$$$$ Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.

This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ Since PRON only occurs once in the reference, its clipped count should be 1 rather than 2.
This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.

This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ The dependency tree contains both the lexical and syntactic information, which inspires us to use it for the MT evaluation.

With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement. $$$$$ The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a professional human translation, the better it is” (Papineni et al., 2002).
With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement. $$$$$ BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al., 2002; Doddington, 2002).
