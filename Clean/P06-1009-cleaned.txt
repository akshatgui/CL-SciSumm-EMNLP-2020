These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). $$$$$ Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available GIZA++ (Och and Ney, 2003), an implementation of the IBM alignment models (Brown et al., 1993).
These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al (1993), Vogel et al (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al (2005), Moore (2005), and Blunsom and Cohn (2006). $$$$$ Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005).

Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). $$$$$ We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set.
Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). $$$$$ These models allow for the use of arbitrary and overlapping features over the source and target sentences, making the most of small supervised training sets.

The one most similar to ours is the one presented by Blunsom and Cohn (2006). $$$$$ The results presented in this paper were evaluated in terms of AER.
The one most similar to ours is the one presented by Blunsom and Cohn (2006). $$$$$ We have presented a novel approach for inducing word alignments from sentence aligned data.

Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ We use a CRF to model many-to-one word alignments, where each source word is aligned with zero or one target words, and therefore each target word can be aligned with many source words.
Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ We showed how conditional random fields could be used for word alignment.

The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). $$$$$ Discriminative Word Alignment With Conditional Random Fields
The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). $$$$$ This paper presents an alternative discriminative method for word alignment.

Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). $$$$$ We use a Conditional Random Field (CRF), a discriminative model, which is estimated on a small supervised training set.
Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). $$$$$ We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).

The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. $$$$$ We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003).
The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. $$$$$ Liu et al. (2005) used a conditional log-linear model with similar features to those we have employed.

(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ Discriminative Word Alignment With Conditional Random Fields
(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ We showed how conditional random fields could be used for word alignment.

Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available GIZA++ (Och and Ney, 2003), an implementation of the IBM alignment models (Brown et al., 1993).
Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ Toutanova et al. (2002)).

Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). $$$$$ Section 5 presents related work, and we describe future work in Section 6.
Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). $$$$$ Bilingual dictionary Dictionaries are another source of information for word alignment.
