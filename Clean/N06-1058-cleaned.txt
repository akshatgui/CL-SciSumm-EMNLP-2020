Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ This synthetic reference then replaces the original human reference in automatic evaluation.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ The synthetic reference keeps the meaning of the original reference, but has a higher word overlap with the system output.

Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ These techniques are widely used in NLP applications, including language modeling, information extraction, and dialogue processing (Haghighi et al., 2005; Serafin and Eugenio, 2004; Miller et al., 2004).
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ The Pearson correlation is calculated over these ten pairs (Papineni et al., 2002; Stent et al., 2005).

Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005).
Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ The Pearson correlation is calculated over these ten pairs (Papineni et al., 2002; Stent et al., 2005).

Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ The key findings of our work are as follows: (1) Automatically generated paraphrases improve the accuracy of the automatic evaluation methods.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.

Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ As a solution to this problem, researchers use multiple references to refine automatic evaluation.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ Thus, among many possible paraphrases of the reference, we are interested only in those that use words appearing in the system output.

Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ For the negative examples, a random position in a sentence is selected for extracting the context.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ To evaluate the accuracy of different paraphrasing methods, we randomly extracted 200 paraphrasing examples from each method.

Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004).
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Our method for reference paraphrasing can be combined with any of these metrics.

In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ In practice, this comparison breaks down to n-gram overlap between the reference and the machine output. machine translation from the NIST 2004 MT evaluation.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002).

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Papineni et al. (2002) shows that expanding the number of references reduces the gap between automatic and human evaluation.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ As expected, the more paraphrases identified, the higher the BLEU score for the method.

Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.
Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002).

We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ We consider a pair as a substitution candidate if its members are synonyms in WordNet.

To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ We consider a pair as a substitution candidate if its members are synonyms in WordNet.

For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ Papineni et al. (2002) shows that expanding the number of references reduces the gap between automatic and human evaluation.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002).

Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ In the following section, we provide an overview of existing work on automatic paraphrasing.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ The results of statistical significance testing are summarized in Table 5.

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Papineni et al. (2002) shows that expanding the number of references reduces the gap between automatic and human evaluation.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ As expected, the more paraphrases identified, the higher the BLEU score for the method.

 $$$$$ Applying this step to the two sentences in Table 2, we obtain two candidate pairs (home, place) and (difficult, hard).
 $$$$$ Any opinions, findings and conclusions expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA or NSF.

PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ Automatic Evaluation Measures A variety of automatic evaluation methods have been recently proposed in the machine translation community (NIST, 2002; Melamed et al., 2003; Papineni et al., 2002).
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ The Pearson correlation is calculated over these ten pairs (Papineni et al., 2002; Stent et al., 2005).

This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ Paraphrasing For Automatic Evaluation
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ These results have two important implications: (1) refining standard measures such as BLEU with paraphrase information moves the automatic evaluation closer to human evaluation and (2) applying paraphrases to MT evaluation provides a task-based assessment for paraphrasing accuracy.

Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ Ideally, the similarity would reflect the semantic proximity between the two.
Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ Both techniques are based on distributional similarity.
