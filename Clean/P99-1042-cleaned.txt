The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ Deep Read: A Reading Comprehension System
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ We developed a simple, modular, baseline system that uses pattern matching (bag-of-words) techniques and limited linguistic processing to select the sentence from the text that best answers the query.

The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Deep Read: A Reading Comprehension System
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Crucially, the reading comprehension task is neither too easy nor too hard, as the performance of our pilot system demonstrates.

(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ The correlation coefficient for AnsWdRecall to HumSent in our test set is 98%, and from HumSent to AutSent is also 98%.
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ This gave us a score of 45% on a small multiplechoice test set.

The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. $$$$$ Deep Read: A Reading Comprehension System
The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. $$$$$ We developed a simple, modular, baseline system that uses pattern matching (bag-of-words) techniques and limited linguistic processing to select the sentence from the text that best answers the query.

Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. $$$$$ This paper describes our initial work exploring reading comprehension tests as a research problem and an evaluation method for language understanding systems.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. $$$$$ First, the evaluation should be automatic.

We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ We used our development corpus to explore several alternative evaluation techniques, and then evaluated on the test set, which was kept blind.
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ This occurs 11% of the time in our test corpus.

We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ We used our development corpus to explore several alternative evaluation techniques, and then evaluated on the test set, which was kept blind.
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ This occurs 11% of the time in our test corpus.
