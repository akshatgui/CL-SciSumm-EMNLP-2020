(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ Given sufficiently “local” features, the decoding and parameter estimation problems can be solved using dynamic programming algorithms.
(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ In this paper, we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation.

Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ In reranking methods (Johnson et al., 1999; Collins, 2000; Shen et al., 2003), an initial parser is used to generate a number of candidate parses.
Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ Recently, it has also been extended to graphical models (Taskar et al., 2003; Altun et al., 2003) and shown to outperform the standard maxlikelihood methods.

We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).

We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ We used the Penn English Treebank for all of our experiments.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ We report results here for each model and setting trained and tested on only the sentences of length < 15 words.

 $$$$$ There is a major problem with both the primal and the dual formulations above: since each potential mistake must be ruled out, the number of variables or constraints is proportional to

 $$$$$ There is a major problem with both the primal and the dual formulations above: since each potential mistake must be ruled out, the number of variables or constraints is proportional to

It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ We report results here for each model and setting trained and tested on only the sentences of length < 15 words.

An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ We have presented a maximum-margin approach to parsing, which allows a discriminative SVM-like objective to be applied to the parsing problem.

A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ Recently, it has also been extended to graphical models (Taskar et al., 2003; Altun et al., 2003) and shown to outperform the standard maxlikelihood methods.
A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.

Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ Max-Margin Parsing
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ The primary contribution of this paper is the extension of the max-margin approach of Taskar et al. (2003) to context free grammars.

For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ We used the Penn English Treebank for all of our experiments.

We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ We used the Penn English Treebank for all of our experiments.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ We report results here for each model and setting trained and tested on only the sentences of length < 15 words.

 $$$$$ There is a major problem with both the primal and the dual formulations above: since each potential mistake must be ruled out, the number of variables or constraints is proportional to

Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ The first feature was the prediction of the generative baseline; this feature added little information, but made the learning phase faster.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ On a test set of ≤ 15 word sentences, the featurerich model outperforms both its own natural generative baseline and the Collins parser on Fl.

Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ For example, O could include features which identify the rule used in the production, or features which track the rule identity together with features of the words at positions s, m, e, and neighboring positions in the sentence x.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ For a span (s, e) of a sentence x, the base lexical features were: These base features were conjoined with the span length for spans of length 3 and below, since short spans have highly distinct behaviors (see the examples below).

In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ In this paper, we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation.
In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.

We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ As shown in Taskar et al. (2003), the dual in Eq.
We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ For a span (s, e) of a sentence x, the base lexical features were: These base features were conjoined with the span length for spans of length 3 and below, since short spans have highly distinct behaviors (see the examples below).

This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ Our method extends the maxmargin approach of Taskar et al. (2003) to the case of context-free grammars.
This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ In addition, we assume that the loss function L(x, y, ˆy) also decomposes into a sum of local loss functions l(x, y, r) over parts, as follows: One approach would be to define l(x, y, r) to be 0 only if the non-terminal A spans words s ... e in the derivation y and 1 otherwise.
