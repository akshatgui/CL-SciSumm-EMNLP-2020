In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. $$$$$ This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task.
In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).

 $$$$$ All sieves traverse the candidate list until they find a coreferent antecedent according to their criteria or reach the end of the list.
 $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).

Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). $$$$$ This paper details the coreference resolution system submitted by Stanford at the CoNLL- 2011 shared task.
Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).

Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ Note that the above rules extract both mentions in appositive and copulative relations, e.g., [[Yongkang Zhou], the general manager] or [Mr. Savoca] had been [a consultant... ].

After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ Each sieve uses syntactic parse trees, identified named entity mentions, and a few manually written patterns based on heuristics and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007).

They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). $$$$$ Please see (Raghunathan et al., 2010) for more details.

In this paper, we have chosen two coreference resolution systems $$$$$ Stanfordâ€™s Multi-Pass Sieve Coreference Resolution System at the CoNLL-2011 Shared Task
In this paper, we have chosen two coreference resolution systems $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).

which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. $$$$$ Parsing errors often introduce incorrect mention boundaries, which yield both recall and precision errors.
which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. $$$$$ Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors.

attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ Specifically, mentions in a cluster share their attributes (e.g., number, gender, animacy) between them so coreference decision are better informed.

The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). $$$$$ Each sieve uses syntactic parse trees, identified named entity mentions, and a few manually written patterns based on heuristics and OntoNotes specifications (Hovy et al., 2006; Pradhan et al., 2007).

We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). $$$$$ To increase the precision of the above two sieves, we use additional constraints before two mentions can match

Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ Please see (Raghunathan et al., 2010) for more details.
Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ The core of our coreference resolution system is an incremental extension of the system described in Raghunathan et al. (2010).

Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ Parsing errors often introduce incorrect mention boundaries, which yield both recall and precision errors.
Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors.

For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent. $$$$$ In the first and highest recall sieve, we mark all noun phrase (NP), possessive pronoun, and named entity mentions in each sentence as candidate mentions.
For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent. $$$$$ Pronoun distance - sentence distance between a pronoun and its antecedent cannot be larger than 3.

 $$$$$ All sieves traverse the candidate list until they find a coreferent antecedent according to their criteria or reach the end of the list.
 $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).

For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).

We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). $$$$$ Please see (Raghunathan et al., 2010) for more details.

Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ Please see (Raghunathan et al., 2010) for more details.

Note that this and several other rules rely on coreference information, which we obtain from two sources $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
Note that this and several other rules rely on coreference information, which we obtain from two sources $$$$$ This clearly indicates that pipeline architectures where mentions are identified first are inadequate for this task, and that coreference resolution might benefit from the joint modeling of mentions and coreference chains.

In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011). $$$$$ This paper describes the coreference resolution system used by Stanford at the CoNLL-2011 shared task (Pradhan et al., 2011).
In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011). $$$$$ Our system extends the multi-pass sieve system of Raghunathan et al. (2010), which applies tiers of deterministic coreference models one at a time from highest to lowest precision.
