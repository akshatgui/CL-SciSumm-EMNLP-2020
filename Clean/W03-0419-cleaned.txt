For comparison, we also computed two baselines: one in which each character is labeled with its most frequent label (Baseline1 in Table 2), and one in which each entity that was seen in training data is labeled with its most frequent classification (Baseline2 in Table 2 this baseline is computed using the software provided with the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ Table 4 shows the error reduction of the systems Table 4: Error reduction for the two development data sets when using extra information like gazetteers (G), unannotated data (U) or externally developed named entity recognizers (E).
For comparison, we also computed two baselines: one in which each character is labeled with its most frequent label (Baseline1 in Table 2), and one in which each entity that was seen in training data is labeled with its most frequent classification (Baseline2 in Table 2 this baseline is computed using the software provided with the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ A baseline rate was computed for the English and the German test sets.

 $$$$$ An extra named entity category called MISC was added to denote all names which are not already in the other categories.
 $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.

In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). $$$$$ Mayfield et al. (2003) stacked two learners and obtained better performance.
In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). $$$$$ (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.

The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). $$$$$ Named entity recognition is an important task of information extraction systems.
The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). $$$$$ The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).

Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). $$$$$ Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003).
Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). $$$$$ However, the difference between their performance and that of the Maximum Entropy approach of Chieu and Ng (2003) is not significant.

Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ They have also produced a scheme for entity annotation (Chinchor et al., 1999).
Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ Mostly, MUC conventions were followed (Chinchor et al., 1999).

The annotation is distributed in the standard column based BIO format applied for e.g. CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004) data, among other established datasets. $$$$$ Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al., 2003; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003).
The annotation is distributed in the standard column based BIO format applied for e.g. CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004) data, among other established datasets. $$$$$ Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al., 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.

The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. $$$$$ Two more systems used them in combination with other techniques (Florian et al., 2003; Klein et al., 2003).
The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. $$$$$ Another combination of five systems (Carreras et al., 2003b; Mayfield et al., 2003; McCallum and Li, 2003; Munro et al., 2003; Zhang and Johnson, 2003) obtained the best result for the German development data.

Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. $$$$$ Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003).
Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. $$$$$ However, the difference between their performance and that of the Maximum Entropy approach of Chieu and Ng (2003) is not significant.

We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. $$$$$ The performance in this task is measured with FÎ²=1 rate: Table 3: Main features used by the the sixteen systems that participated in the CoNLL-2003 shared task sorted by performance on the English test data.
We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. $$$$$ All systems that participated in the shared task have outperformed the baseline system.

This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ Tjong Kim Sang is financed by IWT STWW as a researcher in the ATraNoS project.

Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). $$$$$ They employed a wide variety of machine learning techniques as well as system combination.
Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). $$$$$ Five systems used this statistical learning method.

We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). $$$$$ Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition
We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). $$$$$ We have described the CoNLL-2003 shared task: language-independent named entity recognition.

The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ Klein et al. (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models.
The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al., 2003).

Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). $$$$$ Maximum Entropy Models seem to be a good choice for this kind of task: the top three results for English and the top two results for German were obtained by participants who employed them in one way or another.
Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). $$$$$ Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al., 2003; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003).

Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). $$$$$ The most frequently applied technique in the CoNLL-2003 shared task is the Maximum Entropy Model.
Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). $$$$$ Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003).

All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). $$$$$ Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). $$$$$ The shared task of CoNLL-2003 concerns language-independent named entity recognition.

language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ Introduction To The CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition
language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ The shared task of CoNLL-2003 concerns language-independent named entity recognition.

 $$$$$ An extra named entity category called MISC was added to denote all names which are not already in the other categories.
 $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.

The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).
The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ However, in the CoNLL-2002 shared task we found out that choice of features is at least as important.
