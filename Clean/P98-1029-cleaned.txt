Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ Classifier Combination for Improved Lexical Disambiguation
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ If all taggers made the exact same errors, there would obviously be no chance of improving accuracy through classifier combination.

Committee-based approaches to POS tagging have been in focus the last decade $$$$$ We ran experiments to determine whether the outputs of the different taggers could be effectively combined.
Committee-based approaches to POS tagging have been in focus the last decade $$$$$ We tried simple voting, using the Maximum Entropy, Transformation-Based and Trigram taggers.

In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ In transformation-based tagging (Brill (1995)), every word is first assigned an initial tag, This tag is the most likely tag for a word if the word is known and is guessed based upon properties of the word if the word is not known.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ For instance, we may learn that the Trigram tagger is most accurate at tagging the word up or that the Unigrarn tagger does best at tagging the word (Daelemans(1996)). race when the word that follows is and.

The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. $$$$$ # of errors in A only In other words, Comp(A,B) measures the percentage of time when tagger A is wrong that tagger B is correct.
The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy. $$$$$ For instance, when the maximum entropy tagger is wrong, the transformationbased tagger is right 37.7% of the time, and when the transformation-based tagger is wrong, the maximum entropy tagger is right 41.7% of the time.

The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). $$$$$ Next, we show how this complementary behavior can be used to our advantage.
The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998). $$$$$ All experiments presented in this paper were run on the Penn Treebank Wall Street Journal corpus (Marcus (1993)).

The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naive Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). $$$$$ In Figure 2 we show the complementary rates between the different taggers.
The investigated MMVC model proves to be a very effective participant in classifier combination, with substantially different output to Naive Bayes (9.6% averaged complementary rate, as defined in Brill and Wu (1998)). $$$$$ However, note that the high complementary rate between tagger errors in itself does not necessarily imply that there is anything to be gained by classifier combination.

We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ Classifier Combination for Improved Lexical Disambiguation
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ These taggers are described below.

One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2). $$$$$ For example, when the oracle can examine the output of the Maximum Entropy, Transformation-Based and Trigram taggers, it could achieve an error rate of 1.62%.
One opossibility is the example-based combiner in Brill and Wu (1998, Sec. 3.2). $$$$$ We used a version of example-based learning to determine whether these tagger differences could be exploited.5 To determine 5 Example-based learning has also been applied succesfully in building a single part of speech tagger the tag of a word, we use the previous word, current word, next word, and the output of each tagger for the previous, current and next word.

Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers. $$$$$ Whereas the transformation-based tagger enforces multiple constraints by having multiple rules fire, the maximum-entropy tagger can have all of these constraints play a role at setting the probability estimates for the model's parameters.
Related work includes learning ensembles of POS taggers, as in the work of Brill and Wu (1998), where an ensemble consisting of a unigram model, an N-gram model, a transformation-based model, and an MEMM for POS tagging achieves substantial results beyond the individual taggers. $$$$$ The tagger uses essentially the same parameters as the transformation-based tagger, but employs them in a different model.

(Van Halteren et al, 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. $$$$$ If all taggers made the exact same errors, there would obviously be no chance of improving accuracy through classifier combination.
(Van Halteren et al, 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers. $$$$$ We ran experiments to determine whether the outputs of the different taggers could be effectively combined.

This suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them). $$$$$ Classifier Combination for Improved Lexical Disambiguation
This suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them). $$$$$ In case of ties (all taggers disagree), the Maximum Entropy tagger output is chosen, since this tagger had the highest overall accuracy (this was determined by using a subset of the training set, not by using the test set).

Brill and Wu (1998) call this complementary disagreement complementarity. $$$$$ In Figure 2 we show the complementary rates between the different taggers.
Brill and Wu (1998) call this complementary disagreement complementarity. $$$$$ Next, we check whether tagger complementarity is additive.

Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ In the future, we plan to expand our repertoire of base taggers, to determine whether performance continues to improve as we add additional systems.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ We also plan to explore different methods for combining classifier outputs.

Comparison of different taggers on the WSJ corpus TBL and ME (Brill and Wu, 1998). $$$$$ In Figure 2 we show the complementary rates between the different taggers.
Comparison of different taggers on the WSJ corpus TBL and ME (Brill and Wu, 1998). $$$$$ Next, we try to exploit the idiosyncracies of the different taggers.

Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). $$$$$ Another possibility could be that all of the different machine learning techniques are essentially doing the same thing.
Combination techniques have earlier been applied to various applications including machine translation (Jayaraman and Lavie, 2005), part-of-speech tagging (Brill and Wu, 1998) and base noun phrase identification (Sang et al., 2000). $$$$$ The rule Change a tag from NOUN to VERB if the previous tag is a MODAL would be applied to the sentence, resulting in the correct tagging.
