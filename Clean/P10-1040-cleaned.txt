 $$$$$ We also create a corrupted or noise n-gram x˜ = (w1, ... , wn_q, ˜wn), where ˜wn # wn is chosen uniformly from the vocabulary.1 For convenience, we write e(x) to mean e(w1) ® ... ® e(wn).
 $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).

Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). $$$$$ We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.
Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). $$$$$ The Collobert and Weston (2008) (C&W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs.

In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)—would provide better results than cleaning.
In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.)

Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table.
Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ We used a learning rate of 1e-3 for both model parameters and embedding parameters.

Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.
Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ But, if only one word representation is to be used, Brown clusters have the highest accuracy.

256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. $$$$$ Brown clusters have been used successfully in a variety of NLP applications

Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ We can scale the embeddings by a hyperparameter, to control their standard deviation.
Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0.1.

Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ In most of the other approaches discussed, the columns represent word contexts.
Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.

As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.
As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.

Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ Each dimension’s value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature.
Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties.

We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows.
We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows.

This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ The cleaned RCV1 corpus has 269K word types.
This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ RCV1 is a superset of the CoNLL03 corpus.

The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.
The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009).

Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).
Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.

Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).
Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ We also induced 100, 320, and 3200 Brown clusters, for comparison.

We downloaded these embeddings from Turian et al (2010). $$$$$ In Turian et al. (2009), we were not able to prescribe a default value for scaling the embeddings.
We downloaded these embeddings from Turian et al (2010). $$$$$ Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings.

Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). $$$$$ Brown clusters have been used successfully in a variety of NLP applications

For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).
For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ Because Brown clusters are hierarchical, we can use cluster supersets as features.

Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ LSA (Dumais et al., 1988; Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context.
Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.

The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in accuracy.
The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical.
