 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

Collins et al (1999) used transformed constituency tree bank from Prague Dependency Treebank for constituent parsing on Czech. $$$$$ The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception

This is consistent with the findings of Collins et al (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%. $$$$$ Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (Jelinek et al. 94; Magerman 95; Ratnaparkhi 97; Charniak 97; Collins 96; Collins 97)); several (e.g., (Eisner 96; Collins 96; Collins 97; Charniak 97)) emphasize the use of parameters associated with dependencies between pairs of words.
This is consistent with the findings of Collins et al (1999) for Czech, where the bigram model upped dependency accuracy by about 0.9%, as well as for English where Charniak (2000) reports an increase in F-score of approximately 0.3%. $$$$$ We ran three versions of the parser over the final test set

 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). $$$$$ The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception

In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features. $$$$$ Statistical parsers of English typically make use of the roughly 50 POS tags used in the Penn Treebank corpus, but the Czech PDT corpus provides a much richer set of POS tags, with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus.
In particular, we used the method of Collins et al (1999) to simplify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features. $$$$$ (The detailed POS was used for the main POS values D, J, V, and X; the case field was used for the other possible main POS values.)

The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English. $$$$$ A Statistical Parser For Czech
The Czech parser of Collins et al (1999) was run on a different data set and most other dependency parsers are evaluated using English. $$$$$ The parsers of (Collins 96,97) encoded this as a hard constraint.

Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. $$$$$ Figure 4 illustrates how the baseline transformation method can lead to parsing errors in relative clause cases.
Lattice parsing (Chappelier et al, 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart. $$$$$ Table 2 shows a phrase from the corpus, with the alternative possible tags and machine-selected tag for each word.

For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al (1999). $$$$$ The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception

 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

 $$$$$ The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.
 $$$$$ It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.

We are grateful to Yamada and Matsumoto for letting us use their rule set, which is a slight modification of the rules used by Collins (1999). $$$$$ Each rule has the fonnl

This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999). $$$$$ See figure 5.
This F-measure is based on the recall and precision figures reported in Figure 7.15 in Collins (1999). $$$$$ This is a rule-based system which is based on a manually designed set of rules.

As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al, 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). $$$$$ Much of the recent research on statistical parsing has focused on English; languages other than English are likely to pose new problems for statistical methods.
As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al, 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). $$$$$ Thus the techniques and results found for Czech should be relevant to parsing several other languages.

The mechanism we employ for incorporating morphology into the PCFG model (the Model 1 parser in (Collins, 1999)) is the modification of its part-of speech (POS) tag set; in this paper, we explain how this mechanism allows the parser to better capture morphological constraints. $$$$$ The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.
The mechanism we employ for incorporating morphology into the PCFG model (the Model 1 parser in (Collins, 1999)) is the modification of its part-of speech (POS) tag set; in this paper, we explain how this mechanism allows the parser to better capture morphological constraints. $$$$$ We ran three versions of the parser over the final test set

The authors in (Collins et al, 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech. $$$$$ In these cases the baseline approach gives tree structures such as that in figure 5(a).
The authors in (Collins et al, 1999) describe an approach that gives 80% accuracy in recovering unlabeled dependencies in Czech. $$$$$ (Collins 99) describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of (Collins 97).

Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information. $$$$$ The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.
Ourbaseline model, which we used to evaluate the effects of using morphology, was Model 1 (Collins, 1999) with a simple POS tag set containing almost no morphological information. $$$$$ We ran three versions of the parser over the final test set

It is also true of the adaptation of the Collins parser for Czech (Collins et al, 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). $$$$$ A Statistical Parser For Czech
It is also true of the adaptation of the Collins parser for Czech (Collins et al, 1999) and the finite-state dependency parser for Turkish by Oflazer (2003). $$$$$ The parser of (Collins 96) used punctuation as an indication of phrasal boundaries.
