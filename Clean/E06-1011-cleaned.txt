To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005c).
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.

In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b).

The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ This is done through the creation of a sibling item in part (B).
The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ We also include conjunctions between these features and the direction and distance from sibling j to sibling k. We determined the usefulness of these features on the development set, which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy.

This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ This independence between left and right descendants allow us to use a O(n3) second-order projective parsing algorithm, as we will see later.
This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ We write s(xi, −, xj) when xj is the first left or first right dependent of word xi.

Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ Another reasonable approach would be to first find the highest scoring first-order non-projective parse, and then re-arrange edges based on second order scores in a similar manner to the algorithm we described.

A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set.
A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ For our experiments we used the Danish Dependency Treebank v1.0.

Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ We implemented this method and found that the results were slightly worse.
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy.

These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ We should note that f(i, j) can be based on arbitrary features of the edge and the input sequence x.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).

We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ As noted earlier, this representation subsumes the first-order representation of McDonald et al. (2005b), so we can incorporate all of their features as well as the new second-order features we now describe.
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second-order model.

This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.
This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ Furthermore, it is easy to show that each iteration of the loop takes O(n2) time, resulting in a O(n3 + Mn2) runtime algorithm.

 $$$$$ The expression y[i → j] denotes the dependency graph identical to y except that xi’s parent is xi instead shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of h1 was h2.
 $$$$$ This work was supported by NSF ITR grants 0205448.

McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ Indeed, our second-order projective parser analyzes the test set in 16m32s, and the non-projective approximate parser needs 17m03s to parse the entire evaluation set, showing that runtime for the approximation is completely dominated by the initial call to the second-order projective algorithm and that the post-process edge transformation loop typically only iterates a few times per sentence.

This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms.

However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions.
However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.

For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ In this section, we review the work of McDonald et al. (2005b) for online large-margin dependency parsing.
For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ McDonald et al. (2005c) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models.

In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees.
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ Unfortunately, second-order non-projective MST parsing is NP-hard, as shown in appendix A.

McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ The second-order model allows us to condition on the most recent parsing decision, that is, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser (Charniak, 2000).
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ As noted earlier, this representation subsumes the first-order representation of McDonald et al. (2005b), so we can incorporate all of their features as well as the new second-order features we now describe.

McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).

We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).

Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al., 2005b) or even all possible trees by using factored representations (Taskar et al., 2004; McDonald et al., 2005c).
