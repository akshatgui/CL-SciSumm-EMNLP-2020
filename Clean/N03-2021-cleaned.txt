Another example of a loss function in this class is the MTeval metric introduced in Melamed et al (2003). $$$$$ Precision And Recall Of Machine Translation

We used the following n-gram-based metrics: BLEU (Papineni et al, 2002), NIST (Dod ding ton, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al, 2003), METEOR (Banerjee and Lavie, 2005). $$$$$ Precision And Recall Of Machine Translation

GTM General Text Matching (Melamed et al, 2003) calculates word overlap between a reference and a solution, without double counting duplicate words. $$$$$ Precision And Recall Of Machine Translation

Table 2 reports the translation performance as measured by BLEU (Papineni et al,2002), GTM (Melamed et al, 2003) and ME TEOR2 (Banerjee and Lavie, 2005) for Apertium and the three systems presented in the previous section, as well as the size of the phrase table and the amount of unknown words in the test set. $$$$$ Precision And Recall Of Machine Translation

Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU. $$$$$ Precision And Recall Of Machine Translation

We have selected a representative set of 22 metric variants corresponding to six different families: BLEU (Papineni et al, 2001), NIST (Dodding ton, 2002), GTM (Melamed et al, 2003), Per (Leusch et al, 2003) , WER (NieBen et al, 2000) and ROUGE (Lin and Och, 2004a). $$$$$ Precision And Recall Of Machine Translation

Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). $$$$$ Precision And Recall Of Machine Translation

Other well-known metrics are WER (NieBen et al, 2000), NIST (Doddington, 2002), GTM (Melamed et al, 2003), ROUGE (Lin and Och, 2004a), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al, 2006), just to name a few. $$$$$ Precision And Recall Of Machine Translation

We have computed the BLEU score (accumulated up to 4-grams) (Papineni et al, 2001), the NIST score (accumulated up to 5-grams) (Doddington, 2002), the General Text Matching (GTM) F-measure (e= 1, 2) (Melamed et al, 2003), and the METEOR measure (Banerjee and Lavie, 2005). $$$$$ Precision And Recall Of Machine Translation

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ Precision And Recall Of Machine Translation

Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties, and suggested, in order to avoid this effect, to use the non-parametric Spearman correlation coefficients instead. $$$$$ Precision And Recall Of Machine Translation

Melamed et al (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty. $$$$$ Precision And Recall Of Machine Translation

Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al, 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation. $$$$$ Precision And Recall Of Machine Translation

For evaluation we have selected a set of 8 metric variants corresponding to seven different families: BLEU (n= 4) (Papineni et al, 2001), NIST (n= 5) (Lin and Hovy, 2002), GTM F1-measure (e= 1, 2) (Melamed et al, 2003), 1-WER (NieBen et al, 2000), 1-PER (Leusch et al, 2003), ROUGE (ROUGE-S*) (Lin and Och, 2004) and METEOR3 (Banerjee and Lavie, 2005). $$$$$ Precision And Recall Of Machine Translation

Melamed et al (2003) argued, at the time of introducing the GTM metric, that Pearson correlation coefficients can be affected by scale properties. $$$$$ Precision And Recall Of Machine Translation

each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al, 2003). $$$$$ Precision And Recall Of Machine Translation

At this time, the realizations are also scored using the General Text Matcher method (GTM) (Melamed et al, 2003), by comparing them to the original sentence. $$$$$ Precision And Recall Of Machine Translation

Melamed et al (2003) used unigram F-measure to estimate machine translation quality and showed that unigram F measure was as good as BLEU. $$$$$ Precision And Recall Of Machine Translation

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ Precision And Recall Of Machine Translation

Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Precision And Recall Of Machine Translation
