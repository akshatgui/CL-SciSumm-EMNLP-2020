Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). $$$$$ MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).
Following (Blitzer et al, 2006), we present an application of structural correspondence learning to non-projective dependency parsing (McDonald et al, 2005). $$$$$ We use the parser described by McDonald et al. (2005b).

In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al, 2006) in the domain adaptation task given by the CoNLL 2007. $$$$$ Domain Adaptation With Structural Correspondence Learning
In this paper, we investigate the effectiveness of structural correspondence learning (SCL) (Blitzer et al, 2006) in the domain adaptation task given by the CoNLL 2007. $$$$$ Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation.

Prettenhofer and Stein (2010) investigate cross lingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al, 2006). $$$$$ Domain Adaptation With Structural Correspondence Learning
Prettenhofer and Stein (2010) investigate cross lingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning (Blitzer et al, 2006). $$$$$ Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation.

There has been a lot of work in domain adaption for NLP (Dai et al, 2007) (Jiang and Zhai, 2007) and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in (Blitzer et al, 2006) and (Blitzer et al, 2007b). $$$$$ There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).
There has been a lot of work in domain adaption for NLP (Dai et al, 2007) (Jiang and Zhai, 2007) and one suitable choice for our problem is the approach based on structural correspondence learning (SCL) as in (Blitzer et al, 2006) and (Blitzer et al, 2007b). $$$$$ MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).

These features can either be identified with heuristics (Blitzer et al, 2006) or by automatic selection (Blitzer et al, 2007b). $$$$$ There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).
These features can either be identified with heuristics (Blitzer et al, 2006) or by automatic selection (Blitzer et al, 2007b). $$$$$ MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).

However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al, 2006). $$$$$ We use the parser described by McDonald et al. (2005b).
However, another approach is to train a separate out-of-domain parser, and use this to generate additional features on the supervised and unsupervised in-domain data (Blitzer et al, 2006). $$$$$ We train the parser and PoS tagger on the same size of WSJ data.

Recent work by McClosky et al (2006) and Blitzer et al (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. $$$$$ There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).
Recent work by McClosky et al (2006) and Blitzer et al (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. $$$$$ For our experiments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006).

SCL is the structural correspondence learning technique of Blitzer et al (2006). $$$$$ Domain Adaptation With Structural Correspondence Learning
SCL is the structural correspondence learning technique of Blitzer et al (2006). $$$$$ We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task.

Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al, 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). $$$$$ Structural learning models the correlations which are most useful for semi-supervised learning.
Note that there are some similarities between our two-stage semi-supervised learning approach and the semi-supervised learning method introduced by (Blitzer et al, 2006), which is an extension of the method described by (Ando and Zhang, 558 2005). $$$$$ SCL consistently outperformed both supervised and semi-supervised learning with no labeled target domain training data.

SCL (Blitzer et al, 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. $$$$$ In our experiments, the supervised task is part of speech tagging.
SCL (Blitzer et al, 2006) is one feature representation approach that has been effective on certain high-dimensional NLP problems, including part-of-speech tagging and sentiment classification. $$$$$ That is, 0x is the desired mapping to the (low dimensional) shared feature representation.

The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006). $$$$$ Pivot features are features which behave in the same way for discriminative learning in both domains.
The other dimensions of both domains were difficult to interpret. We experimented with using the SCL features together with the raw features (n-grams and length), as suggested by (Blitzer et al, 2006). $$$$$ Using SCL features still does, however.

As in (Blitzer et al, 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. $$$$$ There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).
As in (Blitzer et al, 2006), we found it necessary to scale up the SCL features to increase their utilization in the presence of the raw features; however, it was difficult to guess the optimal scaling factor without having access to labeled target data. $$$$$ We also showed how to combine an SCL tagger with target domain labeled data using the classifier combination techniques from Florian et al. (2004).

In this way our problem resembles the part-of-speech tagging task (Blitzer et al, 2006), where the category of each word is predicted using values of the left, right, and current word token. $$$$$ That is, we do not use any feature derived from the right word when solving a right token pivot predictor.
In this way our problem resembles the part-of-speech tagging task (Blitzer et al, 2006), where the category of each word is predicted using values of the left, right, and current word token. $$$$$ Ticks to the left or right indicate relative positive or negative values for a word under this projection.

(Blitzer et al, 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. $$$$$ Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a).
(Blitzer et al, 2006) applies the multitask algorithm of (Ando and Zhang, 2005) to domain adaptation problems in NLP. $$$$$ We found it necessary to make a change to the ASO algorithm as described in Ando and Zhang (2005a).

Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)). $$$$$ We introduce learning automatically induce correspondences among features from different domains.
Of course, it is a known fact that machine learning techniques do not transfer well across different domains (e.g., Blitzer et al (2006)). $$$$$ Aside from Florian et al. (2004), several authors have also given techniques for adapting classification to new domains.

Finally, we compare two domain adaptation approaches to utilize unlabeled speech data $$$$$ Domain Adaptation With Structural Correspondence Learning
Finally, we compare two domain adaptation approaches to utilize unlabeled speech data $$$$$ There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003).

In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features. $$$$$ Our ASO baseline uses unlabeled data from the target domain.
In contrast with bootstrapping, SCL (Blitzer et al,2006) uses the unlabeled target data to learn domain independent features. $$$$$ It uses unlabeled data and frequently-occurring pivot features from both source and target domains to find correspondences among features from these domains.

Blitzer et al (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of Speech tagger. $$$$$ Domain Adaptation With Structural Correspondence Learning
Blitzer et al (2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of Speech tagger. $$$$$ We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task.

For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al, 2006). $$$$$ Figure 1 shows two PoS-tagged sentences, one each from the Wall Street Journal (hereafter WSJ) and MEDLINE.
For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al, 2006). $$$$$ The word “signal” in this sentence is a noun, but a tagger trained on the WSJ incorrectly classifies it as an adjective.

Blitzer et al (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. $$$$$ However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data.
Blitzer et al (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. $$$$$ In this case, we make use of the out-of-domain data by using features of the source domain tagger’s predictions in training and testing the target domain tagger (Florian et al., 2004).
