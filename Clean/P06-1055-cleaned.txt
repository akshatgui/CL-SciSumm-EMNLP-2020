The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Matsuzaki et al. (2005) discuss two approximations.

 $$$$$ In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.
 $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ For instance, Matsuzaki et al. (2005) start by annotating their grammar with the identity of the parent and sibling, which are observed (i.e. not latent), before adding latent annotations.4 If these manual annotations are good, they reduce the search space for EM by constraining it to a smaller region.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).

We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees.
We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ For example, the whdeterminers (WDT) split into one class for that and another for which, while the wh-adverbs align by reference type: event-based how and why vs. entity-based when and where.

For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005).
For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ Their exciting result was that, while grammars quickly grew too large to be managed, a 16-subsymbol induced grammar reached the parsing performance of Klein and Manning (2003)â€™s manual grammar.

In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ Recently, Matsuzaki et al. (2005) and also Prescher (2005) exhibited an automatic approach in which each symbol is split into a fixed number of subsymbols.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.

We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ Therefore the inside score of A is: Since A can be produced as A1 or A2 by its parents, its outside score is: Replacing these quantities in (2) gives us the likelihood Pn(w, T) where these two annotations and their corresponding rules have been merged, around only node n. We approximate the overall loss in data likelihood due to merging A1 and A2 everywhere in all sentences wi by the product of this loss for each local change: This expression is an approximation because it neglects interactions between instances of a symbol at multiple places in the same tree.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ We refer to the operation of splitting annotations and re-merging some them based on likelihood loss as a split-merge (SM) cycle.

 $$$$$ In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.
 $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al. (2005).
However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).

 $$$$$ In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.
 $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ However, we use a more sophisticated split-and-merge approach that allocates subsymbols adaptively where they are most effective, like a linguist would.
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Parameter smoothing leads to even better accuracy for grammars with high complexity. producing a packed forest representation of the posterior symbol probabilities for each span.

 $$$$$ In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.
 $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ Matsuzaki et al. (2005) discuss two approximations.

In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ Matsuzaki et al. (2005) discuss two approximations.
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ Since this method is not a contribution of this paper, we refer the reader to the fuller presentations in Goodman (1996) and Matsuzaki et al. (2005).

Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ Our experiments are based on a completely unannotated X-bar style grammar, obtained directly from the Penn Treebank by the binarization procedure shown in Figure 1.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).

To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars.
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).

This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ The results are shown in Figure 3.
This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ For example, after 4 SM cycles, the Fl scores of the 4 trained grammars have a variance of only 0.024, which is tiny compared to the deviation of 0.43 obtained by Matsuzaki et al. (2005)).

We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ Beginning with this baseline grammar, we repeatedly split and re-train the grammar.
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ As a result, the percentage of complete matches with the max-rule parser is typically higher than with the Viterbi parser.

 $$$$$ In our case, the variance is higher for models with few subcategories; because not all dependencies can be expressed with the limited number of subcategories, the results vary depending on which one EM selects first.
 $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Therefore, it would be to our advantage to split the latent annotations only where needed, rather than splitting them all as in Matsuzaki et al. (2005).
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Matsuzaki et al. (2005) discuss two approximations.
