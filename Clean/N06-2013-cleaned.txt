Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ Since we wish to study the effect of word-level preprocessing, we do not utilize any syntactic information.

This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ REGEX is the baseline technique.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ We found that the effect of the choice of the preprocessing technique+scheme was amplified.

Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ We use the phrase-based SMT system, Portage (Sadat et al., 2005).
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ We use an Arabic-English parallel corpus of about 5 million words for translation model training data.4 We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).

Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS tagged English and affix-stem segmented Arabic to determine appropriate tokenizations.
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ We use an Arabic-English parallel corpus of about 5 million words for translation model training data.4 We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).

Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ We use the phrase-based SMT system, Portage (Sadat et al., 2005).
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ All of the training data we use is available from the Linguistic Data Consortium (LDC).

 $$$$$ +hm their/them).
 $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.

Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ It is limited to splitting off punctuations and numbers from words and removing any diacritics that appear in the input.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ English preprocessing comprised down-casing, separating punctuation from words and splitting off “’s”.

The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ BAMA, Buckwalter Arabic Morphological Analyzer (Buckwalter, 2002), is used to obtain possible word analyses.
The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ MADA, The Morphological Analysis and Disambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambow, 2005).

We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ Arabic Preprocessing Schemes For Statistical Machine Translation
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ We performed similar experiments as reported above on this subset of MT04.

More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ The anecdotal intuition in the field is that reduction of word sparsity often improves translation quality.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ Her results show that morphological preprocessing helps, but only for the smaller corpora.

Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ ST: Simple Tokenization is the baseline preprocessing scheme.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ This scheme is intended to minimize differences between Arabic and English.

Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ The second is more expensive, requiring the use of a morphological analyzer.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ MADA, The Morphological Analysis and Disambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambow, 2005).

 $$$$$ +hm their/them).
 $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.

For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ MADA, The Morphological Analysis and Disambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambow, 2005).
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ ST: Simple Tokenization is the baseline preprocessing scheme.

Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ Approaches to statistical machine translation (SMT) are robust when it comes to the choice of their input representation: the only requirement is consistency between training and evaluation.1 This leaves a wide range of possible preprocessing choices, even more so for morphologically rich languages such as Arabic.
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ Recent publications on the effect of morphology on SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005).

A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ MADA, The Morphological Analysis and Disambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambow, 2005).

On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ We use an Arabic-English parallel corpus of about 5 million words for translation model training data.4 We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ English preprocessing comprised down-casing, separating punctuation from words and splitting off “’s”.

Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ Arabic Preprocessing Schemes For Statistical Machine Translation
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ Arabic preprocessing was varied using the proposed schemes and techniques.

The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ Her results show that morphological preprocessing helps, but only for the smaller corpora.
The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ A scheme is a specification of the form of preprocessed output; whereas a technique is the method used to create such output.

The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ Our results are comparable to hers in terms of BLEU score and consistent in terms of conclusions.
The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ For example, a 19% improvement in BLEU score (for MT04 under MADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-level BLEU score.
