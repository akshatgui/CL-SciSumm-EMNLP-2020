Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ One of the main motivations for research on parsing is that syntactic structure provides important information for semantic interpretation; hence syntactic parsing is an important first step in a variety of useful tasks.
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ This paper describes a simple patternmatching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees.

As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ The standard Parseval precision and recall measures for evaluating parse accuracy do not measure the accuracy of empty node and antecedent recovery, but there is a fairly straightforward extension of them that can evaluate empty node and antecedent recovery, as described in section 3.
As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ The first, which measures the accuracy of empty node recovery but not co-indexation, is just the standard Parseval evaluation applied to empty nodes only, viz., precision and recall and scores derived from these.

 $$$$$ The accuracy of transitivity labelling was not systematically evaluated here.
 $$$$$ Possibly there is a way to use both skeletal and the original kind of patterns in a single system.

In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.
In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ The standard Parseval precision and recall measures for evaluating parse accuracy do not measure the accuracy of empty node and antecedent recovery, but there is a fairly straightforward extension of them that can evaluate empty node and antecedent recovery, as described in section 3.

 $$$$$ The accuracy of transitivity labelling was not systematically evaluated here.
 $$$$$ Possibly there is a way to use both skeletal and the original kind of patterns in a single system.

Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). $$$$$ Table 4 provides these measures for the same two corpora described earlier.
Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). $$$$$ This is not unlikely since the statistical model used by the parser does not model these larger tree fragments.

Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ This is not unlikely since the statistical model used by the parser does not model these larger tree fragments.

Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ Note that this is a particularly stringent evaluation measure for a system including a parser, since it is necessary for the parser to produce a non-empty node of the correct category in the correct location to serve as an antecedent for the empty node.
Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ As a comparison of tables 3 and 4 shows, the pattern-matching algorithm’s biggest weakness is its inability to correctly distinguish co-indexed NP * (i.e., NP PRO) from free (i.e., unindexed) NP *.

Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. $$$$$ We used sections 2–21 of the Penn Treebank as the training corpus; section 24 was used as the development corpus for experimentation and tuning, while the test corpus (section 23) was used exactly once (to obtain the results in section 3).
Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. $$$$$ The first phase of the algorithm extracts the patterns from the trees in the training corpus.

We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). $$$$$ There are modications and variations on this algorithm that are worth exploring in future work.

The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. $$$$$ Before the trees are used in the training and insertion phases they are passed through a common preproccessing step, which relabels preterminal nodes dominating auxiliary verbs and transitive verbs.
The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. $$$$$ We used pattern depth as the ranking criterion to produce the results reported below because it ensures that “deep” patterns receive a chance to apply.

Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). $$$$$ Many current linguistic theories of non-local dependencies are extremely complex, and would be difficult to apply with the kind of broad coverage described here.
Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). $$$$$ Note that this is a particularly stringent evaluation measure for a system including a parser, since it is necessary for the parser to produce a non-empty node of the correct category in the correct location to serve as an antecedent for the empty node.

While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ The pattern-matching approach is not tied to any particular linguistic theory, but it does require a treebank training corpus from which the algorithm extracts its patterns.

Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents
Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ This paper describes a simple patternmatching algorithm for post-processing the output of such parsers to add a wide variety of empty nodes to its parse trees.

This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. $$$$$ A Simple Pattern-Matching Algorithm For Recovering Empty Nodes And Their Antecedents
This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. $$$$$ It can also be regarded as a kind of tree transformation, so the overall system architecture (including the parser) is an instance of the “transform-detransform” approach advocated by Johnson (1998).

in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ Broad coverage syntactic parsers with good performance have recently become available (Charniak, 2000; Collins, 2000), but these typically produce as output a parse tree that only encodes local syntactic information, i.e., a tree that does not include any “empty nodes”.
in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ If a pattern p matches a tree t, then it is possible to substitute p for the fragment of t that it matches.

Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ There are modications and variations on this algorithm that are worth exploring in future work.
Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ Inspired by results suggesting that the patternmatching algorithm suffers from over-learning (e.g., testing on the training corpus), we experimented with more abstract “skeletal” patterns, which improved performance on some types of empty nodes but hurt performance on others, leaving overall performance approximately unchanged.

For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. $$$$$ Broad coverage syntactic parsers with good performance have recently become available (Charniak, 2000; Collins, 2000), but these typically produce as output a parse tree that only encodes local syntactic information, i.e., a tree that does not include any “empty nodes”.

P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ (Note that because empty nodes dominate the empty string, their left and right string positions of empty nodes are always identical).
P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ This is not unlikely since the statistical model used by the parser does not model these larger tree fragments.

To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. $$$$$ Perhaps surprisingly, all produced similiar results on the development corpus.
To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. $$$$$ Performance drops considerably when using trees produced by the parser, even though this parser’s precision and recall is around 0.9.
