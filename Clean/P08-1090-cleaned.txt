Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ Unsupervised Learning of Narrative Event Chains
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ Narrative chains are partially ordered sets of events centered around a common protagonist.

These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). $$$$$ They are related to structured sequences of participants and events that have been called scripts (Schank and Abelson, 1977) or Fillmorean frames.
These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). $$$$$ However, it is worthwhile to construct discrete narrative chains, if only to see whether the combination of event learning and ordering produce scriptlike structures.

(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. $$$$$ Unsupervised Learning of Narrative Event Chains
(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. $$$$$ This paper induces a new representation of structured knowledge called narrative event chains (or narrative chains).

This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. $$$$$ We make the following assumption of narrative coherence: verbs sharing coreferring arguments are semantically connected by virtue of narrative discourse structure.
This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. $$$$$ There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.

We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). $$$$$ A narrative chain, by definition, includes a partial ordering of events.
We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). $$$$$ The PMI is then defined as between all occurrences of two verbs in the same document.

The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ For this we need a metric for the relation between an event and a narrative chain.
The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ A distributional score based on how often two events share grammatical arguments (using pointwise mutual information) is used to create this pairwise relation.

Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ Unsupervised Learning of Narrative Event Chains
Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ Other features include event-event syntactic properties such as the syntactic dominance relations between the two events, as well as new bigram features of tense, aspect and class (e.g.

We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. $$$$$ The second applies a temporal classifier to partially order the connected events.
We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. $$$$$ This paper presents work toward a partial ordering and leaves logical constraints as future work.

Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. $$$$$ A distributional score based on how often two events share grammatical arguments (using pointwise mutual information) is used to create this pairwise relation.
Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. $$$$$ Given a list of observed verb/dependency counts, we approximate the pointwise mutual information (PMI) by: where e(w, d) is the verb/dependency pair w and d (e.g. e(push,subject)).

We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ We show, using a new evaluation task called narrative cloze, that our protagonist-based method leads to better induction than a verb-only approach.
We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ We present a new cloze task that requires narrative knowledge to solve, the narrative cloze.

In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ The Timebank corpus does not include obituaries, thus we suffer from sparsity in training data.
In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ In addition, we applied state of the art temporal classification to show that sets of events can be partially ordered.

Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. $$$$$ We used 10 news stories from the 1994 section of the corpus for development.
Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. $$$$$ We used 69 news stories from the 2001 (year selected randomly) section of the corpus for testing (also removed from training).

Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ A narrative event chain is a partially ordered set of events related by a common protagonist.
Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ Narrative chains are partially ordered sets of events centered around a common protagonist.

Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ For each document, the verb pairs that share coreferring entities are recorded with their dependency types.
Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ A narrative chain can be viewed as defining the semantic roles of an event, constraining it against roles of the other events in the chain.

Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent. $$$$$ A single document may contain more than one narrative (or topic), but the narrative assumption states that a series of argument-sharing verbs is more likely to participate in a narrative chain than those not sharing.
Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain: focus on the situational descriptions explicitly pertaining to a single protagonist, a series of references within a document that are automatically labeled as co referent. $$$$$ This approach is called Protagonist.

Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ We parse the text into typed dependency graphs with the Stanford Parser (de Marneffe et al., 2006)3, recording all verbs with subject, object, or prepositional typed dependencies.
Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.

Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). $$$$$ These verb/dependency events make up a narrative cloze model.
Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). $$$$$ Our evaluation data is the same 69 documents used in the test set for learning narrative relations.

The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). $$$$$ There are a number of algorithms for determining the temporal relationship between two events (Mani et al., 2006; Lapata and Lascarides, 2006; Chambers et al., 2007), many of them trained on the TimeBank Corpus (Pustejovsky et al., 2003) which codes events and their temporal relationships.
The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). $$$$$ Taking a cue from Mani et al. (2006), we also increased Timebank’s size by applying transitivity rules to the hand labeled data.

Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ The PMI is then defined as between all occurrences of two verbs in the same document.
Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ Each pair of events in a gigaword document that share a coreferring argument is treated as a separate ordering classification task.

Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved. $$$$$ Learning these prototypical schematic sequences of events is important for rich understanding of text.
Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved. $$$$$ From each document, the entity involved in the most events was selected as the protagonist.
