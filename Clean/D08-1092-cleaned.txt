In bitext parsing, we can use the information based on "bilingual constraints" (Burkett and Klein, 2008), which do not exist in monolingual sentences. $$$$$ On the other hand, the presence of translation pairs offers a new source of information

Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.
Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. $$$$$ These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.

Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005).
Following the studies of Burkett and Klein (2008), Huang et al. (2009) and Chen et al. (2010), we used the exact same data split $$$$$ For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al.

 $$$$$ However, lexical heads are generally more representative than other spanned words.
 $$$$$ We would like to thank the anonymous reviewers for helpful comments on an earlier draft of this paper and Adam Pauls and Jing Zheng for help in running our MT experiments.

Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ While other Chinese treebank parsers do not have access to English side translations, this Chinese figure does outperform all published monolingual Chinese treebank results on an equivalent split of the data.
Burkett and Klein (2008) use the additional knowledge from Chinese-English parallel Treebank to improve Chinese parsing accuracy. $$$$$ On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.

It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ Joint parse selection improves the English trees by 2.5 F1 and the Chinese trees by 1.8 F1.
It also improves over the discriminative, bilingual parsing model of Burkett and Klein (2008), yielding the highest joint parsing F1 numbers on this data set. $$$$$ All the data used to train the joint parsing model and to evaluate parsing performance were taken from articles 1-325 of the Chinese treebank, which all have English translations with gold-standard parse trees.

We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ While other Chinese treebank parsers do not have access to English side translations, this Chinese figure does outperform all published monolingual Chinese treebank results on an equivalent split of the data.
We compare our parsing results to the monolingual parsing models and to the English-Chinese bilingual reranker of Burkett and Klein (2008), trained on the same dataset. $$$$$ We also note that since all parsing evaluations were performed on Chinese treebank data, the Chinese test sentences were in-domain, whereas the English sentences were very far out-of-domain for the Penn Treebank-trained baseline English parser.

In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ Two Languages are Better than One (for Syntactic Parsing)
In addition, our English parsing results are better than those of the Burkett and Klein (2008) bilingual reranker, the current top-performing English-Chinesebilingual parser, despite ours using a much simpler set of synchronization features. $$$$$ Alignment features

For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ We show that jointly parsing a bitext can substantially improve parse quality on both sides.
For example, Burkett and Klein (2008) show that parsing with joint models on bitexts improves performance on either or both sides. $$$$$ These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.

We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ The results are in Table 4.
We did not compare our system with the joint model of Burkett and Klein (2008) because they reported the results on phrase structures. $$$$$ The results are in Table 5.

In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them.
In bitext parsing, Burkett and Klein (2008) and Fraser et al (2009) used feature functions defined on triples of (parse tree in language 1, parse tree in language 2, word alignment), combined in a log-linear model trained to maximize parse accuracy, requiring translated tree banks. $$$$$ Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s')

 $$$$$ However, lexical heads are generally more representative than other spanned words.
 $$$$$ We would like to thank the anonymous reviewers for helpful comments on an earlier draft of this paper and Adam Pauls and Jing Zheng for help in running our MT experiments.

However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ Two Languages are Better than One (for Syntactic Parsing)
However, although it is not essentially different, we only focus on dependency parsing itself, while the parsing scheme in (Burkett and Klein, 2008) based on a constituent representation. $$$$$ All the data used to train the joint parsing model and to evaluate parsing performance were taken from articles 1-325 of the Chinese treebank, which all have English translations with gold-standard parse trees.

 $$$$$ However, lexical heads are generally more representative than other spanned words.
 $$$$$ We would like to thank the anonymous reviewers for helpful comments on an earlier draft of this paper and Adam Pauls and Jing Zheng for help in running our MT experiments.

For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ We used the English and Chinese parsers in Petrov and Klein (2007)5 to generate all k-best lists and as our evaluation baseline.
For this task, we follow the setup of Burkett and Klein (2008), who improved Chinese and English monolingual parsers using parallel, hand-parsed text. $$$$$ For the latter evaluation, sentences that were not in the bilingual corpus were simply parsed with the baseline parsers.

Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ In this work, such features are defined over aligned node pairs for efficiency, but generalizations are certainly possible.
Procedurally, our work is most closely related to that of Burkett and Klein (2008). $$$$$ We also report scores on the full test set to allow easier comparison with past work on Chinese parsing.

We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ Smith and Smith (2004) previously showed that such bilingual constraints can be leveraged to transfer parse quality from a resource-rich language to a resourceimpoverished one.
We parameterize the bilingual view using at most one-to-one matchings between nodes of structured labels in each language (Burkett and Klein, 2008). $$$$$ In principle, we want to find weights which maximize the marginal log likelihood of what we do observe given our sentence pairs

We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ When training our model, we approximate the sets of all trees with k-best lists, T and T0, produced by monolingual parsers.
We approximate this sum using the maximum-scoring matching (Burkett and Klein, 2008). $$$$$ When training, we replace the sum over all tree pairs in (T, T0) in the denominator of (6) with a sum over all tree pairs in (Tpruned, T0pruned).

First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ One indicator of a good nodeto-node alignment between n and n' is that a good word alignment model thinks that there are many word-to-word alignments in their bispan.
First, we use the word alignment density features from Burkett and Klein (2008), which measure how well the aligned entity pair matches up with alignments from an independent word aligner. $$$$$ To compute such features, we define a(v, v') to be the posterior probability assigned to the word alignment between v and v' by an independent word aligner.2 Before defining alignment features, we need to define some additional variables.

For the bilingual model, we use the same bilingual feature set as Burkett and Klein (2008). $$$$$ Bias
