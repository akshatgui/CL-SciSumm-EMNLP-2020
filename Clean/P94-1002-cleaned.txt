Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ Many researchers have studied the patterns of occurrence of characters, setting, time, and the other thematic factors that Chafe mentions, usually in the context of narrative.
Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ This could very well be due to problems with the algorithm used.

TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. $$$$$ Skorochod'ko (1972) suggests discovering a text's structure by dividing it up into sentences and seeing how much word overlap appears among the sentences.
TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. $$$$$ Influenced by Halliday & Hasan's (1976) theory of lexical coherence, Morris developed an algorithm that finds chains of related terms via a comprehensive thesaurus (Roget's Fourth Edition).3 For example, the 2Interestingly, Chafe arrived at the Flow Model after working extensively with, and then becoming dissatisfied with, a hierarchical model of paragraph structure like that of Longacre (1979). words residential and apartment both index the same thesaural category and can thus be considered to be in a coherence relation with one another.

Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ A cutoff based on a particular valley depth is similarly problematic.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ This paper has described algorithms for the segmentation of expository texts into discourse units that reflect the subtopic structure of expository text.

Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). $$$$$ Multi-Paragraph Segmentation Of Expository Text
Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). $$$$$ When the block similarity algorithm is allowed to be off by one paragraph, there is dramatic improvement in the scores for the texts that lower part of Table 2, yielding an overall precision of 83% and recall of 78%.

This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ Most discourse segmentation work is done at a finer granularity than that suggested here.
This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ Closed-class and other very frequent words are eliminated from the analysis.

As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ The structure of expository texts can be characterized as a sequence of subtopical discussions that occur in the context of a few main topic discussions.
As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ From a computational viewpoint, deducing textual topic structure from lexical connectivity alone is appealing, both because it is easy to compute, and also because discourse cues are sometimes misleading with respect to the topic structure (Brown & Yule 1983)(ยง3).

Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).

The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ One way to evaluate these segmentation algorithms is to compare against judgments made by human readers, another is to compare the algorithms against texts premarked by authors, and a third way is to see how well the results improve a computational task.
The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ This paper has described algorithms for the segmentation of expository texts into discourse units that reflect the subtopic structure of expository text.

This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ Multi-Paragraph Segmentation Of Expository Text
This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ The core algorithm has three main parts: Tokenization refers to the division of the input text into individual lexical units.

For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ ... At points where all of these change in a maximal way, an episode boundary is strongly present.
For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ Thus it displays low similarity both to itself and to its neighbors.

Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ A cutoff based on a particular valley depth is similarly problematic.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ This paper has described algorithms for the segmentation of expository texts into discourse units that reflect the subtopic structure of expository text.

Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ Most discourse segmentation work is done at a finer granularity than that suggested here.
Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).

Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).
Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ Many discourse models assume a hierarchical segmentation model, e.g., attentional/intentional structure (Grosz & Sidner 1986) and Rhetorical Structure Theory (Mann & Thompson 1987).

In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).
In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ Since readers often disagree about where to draw a boundary marking for a topic shift, one can only use the general trends as a basis from which to compare different algorithms.

Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ Following the advice of Gale et al. (1992a), I compare the algorithm against both upper and lower bounds.
Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ Another possibility would be to use semantic similarity information as computed in Schiitze (1993), Resnik (1993), or Dagan et al. (1993).

However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. $$$$$ Figure 4 shows a plot of the results of applying the block comparison algorithm to the Stargazer text.
However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. $$$$$ These scores appear in Table 1 (results at 33% are also shown for comparison purposes).

The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. $$$$$ Salton et at.
The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. $$$$$ This section compares the algorithm against reader judgments, since author markups are fallible and are usually applied to text types that this algorithm is not designed for, and Hearst (1994) shows how to use TextTiles in a task (although it does not show whether or not the results of the algorithms used here are better than some other algorithm with similar goals).

Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ In practice, setting w to 20 tokens per token-sequence works best for many texts.
Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ This value, labeled k, varies slightly from text to text; as a heuristic it is the average paragraph length (in token-sequences).

Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. $$$$$ The relative height of the peak to the right of i is added to the relative height of the peak to the left.
Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. $$$$$ These new scores, called depth scores, corresponding to how sharp a change occurs on both sides of the tokensequence gap, are then sorted.

Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).
Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ This section describes two algorithms for discovering subtopic structure using term repetition as a lexical cohesion indicator.
