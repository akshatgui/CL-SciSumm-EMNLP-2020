In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ This parameter is also used by (Ng and Lee, 1996).
In this paper, we used the WSD program reported in (Lee and Ng, 2002). $$$$$ Our reported results in this paper used the linear kernel.

More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.
More precisely, we follow (Lee and Ng, 2002), a reference work for WSD, by adopting a Support Vector Machines (SVM) classifier with a linear kernel and three kinds of features for characterizing each considered occurrence in a text of the reference word. $$$$$ Our reported results in this paper used the linear kernel.

Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ Another approach involves the use of dictionary or thesaurus to perform WSD.
Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. $$$$$ In this paper, we focus on a corpus-based, supervised learning approach.

We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
We adopt the feature design used by Lee and Ng (2002), which consists of the following four types $$$$$ We also investigated the effect of feature selection on syntactic-relation features that are words (i.e., POS, voice, and relative position are excluded).

Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ This parameter is also used by (Ng and Lee, 1996).
Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. $$$$$ Also, using the combination of four knowledge sources gives better performance than using any single individual knowledge source for most algorithms.

Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.
Our approach to building a preposition WSD classifier follows that of Lee and Ng (2002), who evaluated a set of different knowledge sources and learning algorithms for WSD. $$$$$ We ran the different learning algorithms using various knowledge sources.

For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).
For every preposition, a baseline maxent model is trained using a set of features reported in the state-of-the-art WSD system of LeeandNg (2002). $$$$$ Note that we are able to obtain state-of-the-art results using a single learning algorithm (SVM), without resorting to combining multiple learning algorithms.

Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.
Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. $$$$$ Our evaluation is based on all the official training and test data of SENSEVAL-2.

 $$$$$ Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.
 $$$$$ The performance drop from 61.8% may be due to the different collocations used in the two systems.

For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information $$$$$ In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data.

Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). $$$$$ There is a large body of prior research on WSD.

Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). $$$$$ For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB.

These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ This parameter is also used by (Ng and Lee, 1996).
These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness). $$$$$ For SENSEVAL-1, we used the 36 trainable words for our evaluation.

These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.
These knowledge sources were effectively used to build a state-of-the-art WSD pro gram in one of our prior work (Lee and Ng, 2002). $$$$$ This parameter is also used by (Ng and Lee, 1996).

Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ Otherwise, the classifier for is used.
Similar to our previous work (Chan and Ng, 2005b), we used the supervised WSD approach described in (Lee and Ng, 2002) for our experiments, using the naive Bayes algorithm as our classifier. $$$$$ In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data.

For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local col locations, parts-of-speech (POS), and surrounding words. $$$$$ Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.

The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.
The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). $$$$$ This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).

 $$$$$ Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.
 $$$$$ The performance drop from 61.8% may be due to the different collocations used in the two systems.

We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ This parameter is also used by (Ng and Lee, 1996).
We adopt the same syntactic relations as (Lee and Ng, 2002). $$$$$ We use different types of syntactic relations, depending on the POS of .

In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes.
In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). $$$$$ During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.
