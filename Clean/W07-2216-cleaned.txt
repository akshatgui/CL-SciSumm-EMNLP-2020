Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing.
Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ Following the work of Koo et al. (2007) and Smith and Smith (2007), it is possible to compute all expectations in O(n3 +

The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.
The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). $$$$$ However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems.


Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.
Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ Consider a first-order vertical Markovization in which the score for a dependency graph factors over pairs of vertically adjacent edges2, where k hiwk0 ij is the weight of including both edges (h, i)k and (i, j)k0 in the dependency graph.

Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models.
Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b).

A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ On the Complexity of Non-Projective Data-Driven Dependency Parsing
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006).

In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case.

Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.
Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ 2McDonald and Pereira (2006) define this as a second-order Markov assumption.

A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ On the Complexity of Non-Projective Data-Driven Dependency Parsing
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006).

While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ The projective case has been investigated in Paskin (2001).
While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case.

While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. $$$$$ Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages.
While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. $$$$$ Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006).

the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). $$$$$ The primary problem in treating each dependency as independent is that it is not a realistic assumption.
the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). $$$$$ For systems that model arity constraints we give a reduction from the Hamiltonian graph problem suggesting that the parsing problem is intractable in this case.

Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006).
Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.

Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ Many learning paradigms can be defined as inference-based learning.
Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.

Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ On the Complexity of Non-Projective Data-Driven Dependency Parsing
Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.

McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ On the Complexity of Non-Projective Data-Driven Dependency Parsing
McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard.

Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. $$$$$ Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing.
Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. $$$$$ Following the work of Koo et al. (2007) and Smith and Smith (2007), it is possible to compute all expectations in O(n3 +

For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing.
For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ Following the work of Koo et al. (2007) and Smith and Smith (2007), it is possible to compute all expectations in O(n3 +
