These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
These parsers are trained and evaluated using CCGbank (Hockenmaier and Steedman, 2002a), an automatic conversion of the Penn Treebank into the CCG formalism. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).

Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
Several broad coverage parsers have been trained using this resource (Hockenmaier and Steedman, 2002b; Hockenmaier, 2003b). $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).

Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ Generative Models For Statistical Parsing With Combinatory Categorial Grammar
Hockenmaier and Steedman (2002) describe a generative model for CCG, which only requires a non-iterative counting process for training, but it is generally acknowledged that discriminative models provide greater flexibility and typically higher performance. $$$$$ The performance of the baseline model is shown in the top row of table 3.

The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).
The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with punctuation and type-changing. $$$$$ in the sense that analyses involving the combinatory rules of type-raising and composition are only used when syntactically necessary.

Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
Few hand crafted, deep linguistic grammars achieve the coverage and robustness needed to parse large corpora (see (Riezler et al, 2002), (Burke et al, 2004) and (Hockenmaier and Steedman, 2002) for exceptions), and speed remains a serious challenge. $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.

We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ The potential benefit of wide-coverage parsing with CCG lies in its more constrained grammar and itssimple and semantically transparent capture of ex traction and coordination.We present a number of models over syntac tic derivations of Combinatory Categorial Grammar (CCG, see Steedman (2000) and Clark et al (2002), this conference, for introduction), estimated from and tested on a translation of the Penn Treebank to a corpus of CCG normal-form derivations.
We used Clark & Curran's wide coverage statistical parser (Clark and Curran, 2004) trained on CCG-bank, which in turn is derived from the Penn-Treebank (Hockenmaier and Steedman, 2002). $$$$$ For further discussion we refer the reader to Clark and Hockenmaier (2002) .

 $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
 $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i

 $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
 $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i

 $$$$$ Like the models of Goodman (1997), the additional features in our model are generated probabilistically, whereas in the parser of Collins (1997) distance measures are assumed to be a function of the already generated structure and are not generated explicitly.
 $$$$$ In order to estimate the conditional probabilitiesof our model, we recursively smooth empirical es timates e?i of specific conditional distributions with(possible smoothed) estimates of less specific distri butions e?i

In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
In order to obtain CCG derivations for all sentences in the ACE corpus, we used the CCG parser introduced in (Hockenmaier and Steedman,2002). $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).

Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).
Hockenmaier and Steedman (2002) describe a generative model of normal-form derivations. $$$$$ The derivations in CCGbank are ?normal-form?

The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a tree bank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).

The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).
The feature set for the normal-form model is the same except that, following Hockenmaier and Steedman (2002), the dependency features are defined in terms of the local rule instantiations, by adding the heads of the combining categories to the rule instantiation features. $$$$$ The derivations in CCGbank are ?normal-form?

The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002). $$$$$ Therefore, we alsoevaluate performance using a dependency evaluation reported by Collins (1999), which counts word word dependencies as determined by local trees and their labels.

For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). $$$$$ For all experiments reported here and in section 5, the frequency threshold was set to 5.

We focus on two parsing models $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
We focus on two parsing models $$$$$ The performance of the baseline model is shown in the top row of table 3.

Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ CCGbank is a corpus of CCG normal-form derivations obtained by translating the Penn Tree bank trees using an algorithm described by Hockenmaier and Steedman (2002).
Hockenmaier and Steedman (2002) saw a similar effect. $$$$$ The grammar contains a set of type-changing rules similar to the lexical rules described in Carpenter (1992).

The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
The model used by the CCG parser of Hockenmaier and Steedman (2002b) would fail to capture the correct bilexical dependencies in a language with freer word order, such as Dutch. $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.

State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ Generative Models For Statistical Parsing With Combinatory Categorial Grammar
State-of-the-art statistical parsers for Penn Treebank-style phrase-structure grammars (Collins, 1999), (Charniak, 2000), but also for Categorial Grammar (Hockenmaier and Steedman, 2002b), include models of bilexical dependencies defined in terms of local trees. $$$$$ The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.

First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ In order to compare our performance with the parser of Clark et al (2002), we also evaluate our best model according to the dependency evaluation introduced for that parser.
First, we review the dependency model proposed by Hockenmaier and Steedman (2002b). $$$$$ This model was originally described in Hockenmaier (2001), where it was applied to a preliminary version of CCGbank, and its definition is repeated here in the top row of Table 1.
