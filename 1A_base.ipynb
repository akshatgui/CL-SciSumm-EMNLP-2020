{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZQr_2AoWc4x"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6KUID3e8a9O",
        "outputId": "a4c8500f-1e99-4475-ea93-1db614badac6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "472N6GvK8b-2"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import sys\n",
        "import random\n",
        "import itertools\n",
        "from collections import Counter\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stop_words\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Bidirectional, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Dropout, Activation, concatenate, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers, layers, optimizers, losses, metrics\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcIToB_H8gDX"
      },
      "source": [
        "from IPython.display import SVG \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcCUrrC58q1x"
      },
      "source": [
        "pd.options.display.max_colwidth = 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbPzx63D8yxC"
      },
      "source": [
        "\n",
        "def tokenize(sentences, VOCAB_SIZE, filters='!\"#%&()*,+-/:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
        "             lower=True, split=' ', char_level=False, oov_token=\"<unk>\", verbose=True):\n",
        "    \"\"\"Convert the sentences (strings) into sequences of integers and toks.\n",
        "    When using deafult filter; words maybe include the `'` character \n",
        "    \n",
        "    '0' is a reserved index that won't be assigned to any word\n",
        "    If oov_token is passed: it will have index 1 and will be added to word_index, index_word dict\n",
        "    \n",
        "    Even though converted sequences has only VOCAB_SIZE words but all dicts of tokenizer (word_index, word_counts, word_docs) contains all the words.\n",
        "    All the words whose idx > VOCAB_SIZE will be treated as <unk> words\n",
        "    \"\"\"\n",
        "    \n",
        "    # no pruning of words\n",
        "    if VOCAB_SIZE == 0:\n",
        "        VOCAB_SIZE = None\n",
        "    \n",
        "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, filters = filters,\n",
        "                                                   lower=lower, split=split, char_level=char_level,\n",
        "                                                   oov_token=oov_token)\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    idx = tokenizer.texts_to_sequences(sentences)\n",
        "    tok = tokenizer.sequences_to_texts(idx)\n",
        "\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"tokenizer details:\")\n",
        "        \n",
        "        keys1 = ['char_level', 'filters', 'lower',  'split', 'oov_token', \n",
        "                 'document_count', 'num_words']\n",
        "        keys2 = ['index_docs', 'word_docs', 'index_word',  'word_index', 'word_counts']\n",
        "\n",
        "        for key in keys1:\n",
        "            print(f\"\\t{key:<20}: {tokenizer.__getattribute__(key)}\")\n",
        "        print(\"\\n\")\n",
        "        \n",
        "        for key in keys2:\n",
        "            print(f\"\\tlen {key:<20}: {len(tokenizer.__getattribute__(key))}\")\n",
        "        print(\"\\n\")\n",
        "        \n",
        "\n",
        "    return tokenizer, idx, tok\n",
        "\n",
        "\n",
        "def build_word_index(tokenizer, VOCAB_SIZE, pad_idx=0, verbose=True):\n",
        "    \"\"\"Adds pad token to toknizer and returns word2idx, idx2word till VOCAB_SIZE\"\"\"\n",
        "    \n",
        "    # we need to add <pad> token to tokenizer.word_index if plan to use inbuilt sequences_to_texts\n",
        "    # else sequences_to_texts on padded data will replace pad_idx (by default 0) by <unk> rather than <pad>\n",
        "    tokenizer.word_index.update({'<pad>':pad_idx})\n",
        "    tokenizer.index_word.update({pad_idx:'<pad>'})\n",
        "    tokenizer.word_index.update({'<marker>':1})\n",
        "    tokenizer.index_word.update({1:'<marker>'})\n",
        "\n",
        "\n",
        "    # since tokenizer.word_index has all words > we get only what's under VOCAB_SIZE\n",
        "    word2idx = {w:idx for w,idx in tokenizer.word_index.items() if idx < VOCAB_SIZE}\n",
        "    idx2word = {idx:w for w,idx in word2idx.items()}\n",
        "\n",
        "    # if our vocab was less than VOCAB_SIZE > we should adjust it\n",
        "    VOCAB_SIZE = len(word2idx)\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"len word2idx: \", len(word2idx))\n",
        "        print(\"len idx2word: \", len(idx2word))\n",
        "        print(\"vocab size: \", VOCAB_SIZE)\n",
        "        print(\"\")\n",
        "    \n",
        "    return tokenizer, word2idx, idx2word\n",
        "    \n",
        "\n",
        "def built_embed_matrix(embed, word2idx, VOCAB_SIZE, EMBED_DIM, oov_vec = \"mean\"):\n",
        "    \"\"\"Will build embedding matrix using pre-trained word vector\n",
        "    Returns embedding_matrix, unk_words (words that aren't found in pre-trained word embeddings)\n",
        "    \n",
        "    oov_vec: param decides how we want to handle OOV word that aren't present in pre-trained word vectors\n",
        "      \"mean\": average embedding of all words for replacing OOV words\n",
        "      \"norm\": randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "      \n",
        "      todo: optimize the oov_vec part.\n",
        "    \"\"\"\n",
        "    \n",
        "    # stack all pre-trained word embeddings\n",
        "    emb_all = np.stack(list(embed.values()))\n",
        "    \n",
        "    if oov_vec == \"mean\":\n",
        "        # average embedding of all words \n",
        "        emb_oov = np.mean(emb_all, axis=0)\n",
        "        print(f\"embed_all shape: {emb_all.shape} | embed_mean_vec shape: {emb_oov.shape}\")\n",
        "    \n",
        "    \n",
        "    if oov_vec == \"rand\":\n",
        "        # mean and std of all values in embed \n",
        "        # this can be used to randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "        emb_mean, emb_std = emb_all.mean(), emb_all.std()\n",
        "        emb_oov = np.random.normal(emb_mean, emb_std, (EMBED_DIM))\n",
        "        \n",
        "        print(f\"embed_all shape: {emb_all.shape}\")\n",
        "        print(f\"embed mean: {emb_mean} | embed std: {emb_std} | embed rand_vec shape: {emb_oov.shape}\")\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    print('\\nprepare embedding matrix...')\n",
        "\n",
        "    embedding_matrix = np.zeros((VOCAB_SIZE, EMBED_DIM))\n",
        "    embed_cnt = 0\n",
        "    unk_words = []\n",
        "\n",
        "    for word, idx in word2idx.items():\n",
        "        embedding_vector = embed.get(word)\n",
        "\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[idx] = embedding_vector\n",
        "            embed_cnt += 1\n",
        "\n",
        "        else:\n",
        "            # embedding for <pad> token will be 0's as initialized\n",
        "            if word != '<pad>':\n",
        "                if oov_vec == \"mean\":\n",
        "                    # average embedding of all words \n",
        "                    embedding_matrix[idx] = emb_oov\n",
        "                \n",
        "                if oov_vec == \"rand\":\n",
        "                    # randomly initialize each oov token vector with mean, std dev. of all values in pre-trained word embeddings\n",
        "                    emb_oov = np.random.normal(emb_mean, emb_std, (EMBED_DIM))\n",
        "                    embedding_matrix[idx] = emb_oov\n",
        "                    \n",
        "                unk_words.append((word, idx))     \n",
        "\n",
        "    print(f\"\\tembedding matrix shape: {embedding_matrix.shape}\")\n",
        "    print(f\"\\tword embedding found: {embed_cnt}\")\n",
        "    print(f\"\\tword embedding not found: {len(unk_words)}\")\n",
        "    print(\"\")\n",
        "    \n",
        "    return embedding_matrix, unk_words\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-pmOfwa81DZ",
        "outputId": "8b2b0a92-cbb8-4f73-df2f-3b5d20292f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvWB6UXA8_o7"
      },
      "source": [
        "data = open(\"/content/gdrive/My Drive/NLP_Proj/merged.txt\")\n",
        "fin = data.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx72dLkVf8bu"
      },
      "source": [
        "from scipy.spatial import distance\n",
        "df = pd.read_csv(\"/content/gdrive/My Drive/NLP_Proj/test.csv\")\n",
        "sentences = {}\n",
        "list1 = list(df['col1'])\n",
        "list2 = list(df['col2'])\n",
        "flist = list1+list2\n",
        "flist = set(flist)\n",
        "flist = list(flist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZ193D0f_LG"
      },
      "source": [
        "fin = fin + flist[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbmVKtFP9VvP"
      },
      "source": [
        "out= []\n",
        "for i in range(24661):\n",
        "  out.append(1)\n",
        "for i in range(63436-24661):\n",
        "  out.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQtDknbX9put"
      },
      "source": [
        "lens = []\n",
        "for sen in fin:\n",
        "  lens.append(len(sen.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA5kJTQeBi3s",
        "outputId": "6ceaab7f-a283-4de5-dfdd-4492ab51bb9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for q in [.5, .6, .8, .9, .95, 1.0]:\n",
        "    print(f\"\\tquantile: {q:<5} | {np.quantile(lens, q)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tquantile: 0.5   | 44.0\n",
            "\tquantile: 0.6   | 49.0\n",
            "\tquantile: 0.8   | 61.0\n",
            "\tquantile: 0.9   | 72.0\n",
            "\tquantile: 0.95  | 82.0\n",
            "\tquantile: 1.0   | 1413.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtsALaSzIiGF",
        "outputId": "df3d9fe2-be75-492a-adce-1389f70d852f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "c=set()\n",
        "for sent in fin:\n",
        "  words = set(sent.split())\n",
        "  c.update(words)\n",
        "len(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101483"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6MjHrr8B04c",
        "outputId": "7c5fbdc0-119a-4851-9e14-41c755d0f6c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "VOCAB_SIZE = 1000000\n",
        "tokenizer, idx, tok = tokenize(fin, VOCAB_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizer details:\n",
            "\tchar_level          : False\n",
            "\tfilters             : !\"#%&()*,+-/:;<=>?@[\\]^_`{|}~\t\n",
            "\n",
            "\tlower               : True\n",
            "\tsplit               :  \n",
            "\toov_token           : <unk>\n",
            "\tdocument_count      : 83489\n",
            "\tnum_words           : 1000000\n",
            "\n",
            "\n",
            "\tlen index_docs          : 52003\n",
            "\tlen word_docs           : 52003\n",
            "\tlen index_word          : 52004\n",
            "\tlen word_index          : 52004\n",
            "\tlen word_counts         : 52003\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYbMU5-hJt8Y",
        "outputId": "d07233cb-2329-4179-d244-82cc2c92480d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenizer, word2idx, idx2word = build_word_index(tokenizer, VOCAB_SIZE, pad_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len word2idx:  52006\n",
            "len idx2word:  52005\n",
            "vocab size:  52006\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHRx9zJrgkD5"
      },
      "source": [
        "tok1 = tok[63436:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpIdssbMgyg0"
      },
      "source": [
        "tok = tok[:63436]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NdDCi81OyBQ"
      },
      "source": [
        "def convert2seq():\n",
        "  data_idx = []\n",
        "  for sent in tok:\n",
        "    row=[]\n",
        "    for word in sent.split():\n",
        "      row.append(word2idx[word])\n",
        "    data_idx.append(row)\n",
        "  return data_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXLNyCmvPVJJ"
      },
      "source": [
        "data_idx = convert2seq()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMNYm7zxPhXe",
        "outputId": "b333bdca-7ae7-4a56-90b9-7d4b3b928276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MAX_SEQ_LEN = 80\n",
        "data_padded = keras.preprocessing.sequence.pad_sequences(data_idx, maxlen=MAX_SEQ_LEN, \n",
        "                                                         padding='post', truncating='post', value=0)\n",
        "data_padded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63436, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_fxTQj7QCXY"
      },
      "source": [
        "EMBED_DIM = 300\n",
        "embed_path = \"/content/gdrive/My Drive/glove.6B/glove.6B.%sd.txt\" % EMBED_DIM \n",
        "glove_embed = {}\n",
        "f = open(embed_path)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    if len(coefs) == EMBED_DIM:\n",
        "      glove_embed[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN_evn_nQGie",
        "outputId": "678ed20f-7b9f-4635-9deb-220a95e200cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "embedding_matrix, unk_words = built_embed_matrix(glove_embed, word2idx, VOCAB_SIZE, \n",
        "                                                 EMBED_DIM, oov_vec='mean')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embed_all shape: (400000, 300) | embed_mean_vec shape: (300,)\n",
            "\n",
            "prepare embedding matrix...\n",
            "\tembedding matrix shape: (1000000, 300)\n",
            "\tword embedding found: 24626\n",
            "\tword embedding not found: 27379\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y263CAfbUvre",
        "outputId": "4893f871-65e3-484d-d445-3d073031ba71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(np.array(data_padded), np.array(out), random_state=10, test_size=0.2, shuffle=True)\n",
        "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((50748, 80), (50748,), (12688, 80), (12688,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKQ_rlSiU9sp",
        "outputId": "3de65747-c897-41d1-abdb-6c561f8547df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "NUM_CLASSES = 1\n",
        "input_ = Input(shape=(MAX_SEQ_LEN,), name=\"input\")\n",
        "\n",
        "embed_layer = Embedding(input_dim= VOCAB_SIZE, weights=[embedding_matrix], \n",
        "                        output_dim= EMBED_DIM, input_length = MAX_SEQ_LEN, \n",
        "                        trainable= False, mask_zero=True, name=\"embed\")(input_)\n",
        "\n",
        "lstm_layer = Bidirectional(LSTM(units= 128, return_sequences=True), name=\"lstm1\")(embed_layer)\n",
        "lstm_layer = Bidirectional(LSTM(units= 128), name=\"lstm2\")(lstm_layer)\n",
        "\n",
        "dense_layer = Dense(128, activation=\"relu\", name=\"dense\")(lstm_layer)\n",
        "out = Dense(NUM_CLASSES, activation= \"sigmoid\", name=\"output\")(dense_layer) \n",
        "\n",
        "model_lstm = Model(input_, out)\n",
        "\n",
        "model_lstm.compile(optimizer= \"adam\", \n",
        "                   loss= \"binary_crossentropy\", \n",
        "                   metrics= [\"accuracy\"]) \n",
        "\n",
        "model_lstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "embed (Embedding)            (None, 80, 300)           300000000 \n",
            "_________________________________________________________________\n",
            "lstm1 (Bidirectional)        (None, 80, 256)           439296    \n",
            "_________________________________________________________________\n",
            "lstm2 (Bidirectional)        (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "output (Dense)               (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 300,866,561\n",
            "Trainable params: 866,561\n",
            "Non-trainable params: 300,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "305baZKpWAHt"
      },
      "source": [
        "class_weights = compute_class_weight('balanced', np.unique(train_y), train_y)\n",
        "class_weights = {i : class_weights[i] for i in range(2)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qceUf9pLWPyM",
        "outputId": "aef8eef3-2944-4813-9acf-1661d86459af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model_lstm.fit(train_x, train_y,\n",
        "                         batch_size = BATCH_SIZE,\n",
        "                         epochs = EPOCHS,\n",
        "                         shuffle = True,\n",
        "                         class_weight = class_weights,\n",
        "                         validation_data = (test_x, test_y)\n",
        "                        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "793/793 [==============================] - 864s 1s/step - loss: 0.6324 - accuracy: 0.6374 - val_loss: 0.5841 - val_accuracy: 0.6928\n",
            "Epoch 2/20\n",
            "793/793 [==============================] - 861s 1s/step - loss: 0.5896 - accuracy: 0.6714 - val_loss: 0.5853 - val_accuracy: 0.6773\n",
            "Epoch 3/20\n",
            "793/793 [==============================] - 856s 1s/step - loss: 0.5534 - accuracy: 0.7015 - val_loss: 0.5620 - val_accuracy: 0.7011\n",
            "Epoch 4/20\n",
            "793/793 [==============================] - 856s 1s/step - loss: 0.5103 - accuracy: 0.7348 - val_loss: 0.5648 - val_accuracy: 0.7019\n",
            "Epoch 5/20\n",
            "793/793 [==============================] - 857s 1s/step - loss: 0.4599 - accuracy: 0.7675 - val_loss: 0.5805 - val_accuracy: 0.6942\n",
            "Epoch 6/20\n",
            "793/793 [==============================] - 855s 1s/step - loss: 0.3963 - accuracy: 0.8072 - val_loss: 0.6093 - val_accuracy: 0.6928\n",
            "Epoch 7/20\n",
            "793/793 [==============================] - 851s 1s/step - loss: 0.3274 - accuracy: 0.8472 - val_loss: 0.7068 - val_accuracy: 0.6851\n",
            "Epoch 8/20\n",
            "793/793 [==============================] - 849s 1s/step - loss: 0.2582 - accuracy: 0.8847 - val_loss: 0.8381 - val_accuracy: 0.6984\n",
            "Epoch 9/20\n",
            "793/793 [==============================] - 832s 1s/step - loss: 0.1940 - accuracy: 0.9172 - val_loss: 0.9933 - val_accuracy: 0.6962\n",
            "Epoch 10/20\n",
            "793/793 [==============================] - 840s 1s/step - loss: 0.1462 - accuracy: 0.9418 - val_loss: 1.0674 - val_accuracy: 0.6919\n",
            "Epoch 11/20\n",
            "793/793 [==============================] - 844s 1s/step - loss: 0.1113 - accuracy: 0.9562 - val_loss: 1.3100 - val_accuracy: 0.7058\n",
            "Epoch 12/20\n",
            "793/793 [==============================] - 837s 1s/step - loss: 0.0833 - accuracy: 0.9684 - val_loss: 1.5114 - val_accuracy: 0.7088\n",
            "Epoch 13/20\n",
            "793/793 [==============================] - 837s 1s/step - loss: 0.0727 - accuracy: 0.9742 - val_loss: 1.6033 - val_accuracy: 0.6930\n",
            "Epoch 14/20\n",
            "685/793 [========================>.....] - ETA: 1:46 - loss: 0.0597 - accuracy: 0.9779"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTCpYivOWivn",
        "outputId": "6a8727ee-2c85-4022-b266-04eb61d8e852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred = model_lstm.predict(test_x)\n",
        "pred = (pred>=.45).astype(float)\n",
        "\n",
        "count = 0\n",
        "for i in pred:\n",
        "  if i == 1:\n",
        "    count+=1\n",
        "\n",
        "print(count)\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(test_y, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3077\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.84      0.75      7862\n",
            "           1       0.58      0.37      0.45      4826\n",
            "\n",
            "    accuracy                           0.66     12688\n",
            "   macro avg       0.63      0.60      0.60     12688\n",
            "weighted avg       0.64      0.66      0.64     12688\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI0433tKL5TA"
      },
      "source": [
        "model_lstm.save(\"model_lstm.h5\")\n",
        "model=model_lstm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1cSF83mWn89",
        "outputId": "a73d24d9-0ecc-41aa-a397-6be346b1318b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "model = load_model('lstm_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-a6ed564a3d6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lstm_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn36axInYytF"
      },
      "source": [
        "input_ = Input(shape=(MAX_SEQ_LEN,), name=\"input\")\n",
        "\n",
        "embed_layer = Embedding(input_dim= VOCAB_SIZE, weights=[embedding_matrix], \n",
        "                        output_dim= EMBED_DIM, input_length = MAX_SEQ_LEN, \n",
        "                        trainable= False, mask_zero=True, name=\"embed\")(input_)\n",
        "\n",
        "out = Bidirectional(LSTM(units= 64, dropout=.25, \n",
        "                                recurrent_dropout=.25, kernel_regularizer = regularizers.l2(0.01)\n",
        "                               ), name=\"lstm\")(embed_layer)\n",
        "'''out = Bidirectional(LSTM(units= 128, dropout=.25, \n",
        "                                recurrent_dropout=.25, kernel_regularizer = regularizers.l2(0.01)\n",
        "                               ), name=\"lstm1\")(lstm_layer)\n",
        "'''\n",
        "model1 = Model(input_, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bkxoWQOaLQR",
        "outputId": "949bcb9f-8d18-4df2-ff50-1c5532f71e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "embed (Embedding)            (None, 80, 50)            50000000  \n",
            "_________________________________________________________________\n",
            "lstm (Bidirectional)         (None, 128)               58880     \n",
            "=================================================================\n",
            "Total params: 50,058,880\n",
            "Trainable params: 58,880\n",
            "Non-trainable params: 50,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRuMV87-bDKc"
      },
      "source": [
        "for layer in model1.layers:\n",
        "  for layer1 in model_lstm.layers: \n",
        "    if layer.name == layer1.name:\n",
        "      layer.set_weights(layer1.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AErzqNeUdRR8",
        "outputId": "50c27a6b-582f-4713-a6bd-ae757c9b3761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 80)]              0         \n",
            "_________________________________________________________________\n",
            "embed (Embedding)            (None, 80, 50)            50000000  \n",
            "_________________________________________________________________\n",
            "lstm (Bidirectional)         (None, 128)               58880     \n",
            "=================================================================\n",
            "Total params: 50,058,880\n",
            "Trainable params: 58,880\n",
            "Non-trainable params: 50,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oER0WOlPewqL"
      },
      "source": [
        "data_idx = []\n",
        "for sent in tok1:\n",
        "  row=[]\n",
        "  for word in sent.split():\n",
        "    word=word.lower()\n",
        "    if word2idx[word]: \n",
        "      row.append(word2idx[word])\n",
        "    else:\n",
        "      row.append(word2idx['<unk>'])\n",
        "  data_idx.append(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Nznqe0iHRh",
        "outputId": "3f419404-195d-4cbb-9441-f84b2d7ecabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "MAX_SEQ_LEN = 80\n",
        "data_padded = keras.preprocessing.sequence.pad_sequences(data_idx, maxlen=MAX_SEQ_LEN, \n",
        "                                                         padding='post', truncating='post', value=0)\n",
        "data_padded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20053, 80)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmTG6dda4y3Q",
        "outputId": "fe0996ba-f138-4c18-f28a-3f0a5de2a45f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_padded[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   13,  1956,    14,     5,   824,   815,    10,  3849,     6,\n",
              "           5,  3466,   815,   140,   135,    10,     5,   604,   824,\n",
              "       12495,     9,   757,   444,     7,     2,  3466,  4778,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyOlVXcu41kv",
        "outputId": "331963eb-71eb-4a2c-92a7-e33a67735425",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "flist[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'We say that a PCFG derivation is isomorphic to a STSG derivation if there is a corresponding PCFG subderivation for every step in the STSG derivation.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQuaeCDY45GB",
        "outputId": "1c340d51-1282-4f3e-f618-d9996f03a2c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word2idx['we']\n",
        "model1.predict(data_padded[ind])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01414258,  0.03027619,  0.05460392, ...,  0.0155709 ,\n",
              "         0.01759867, -0.03623072],\n",
              "       [-0.01773036,  0.02369586,  0.04547004, ...,  0.01808211,\n",
              "         0.01671089, -0.03453201],\n",
              "       [-0.01850735,  0.02252924,  0.04583006, ...,  0.01602512,\n",
              "         0.01719457, -0.03711054],\n",
              "       ...,\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldCO5mINbTeR",
        "outputId": "0c4f4a9b-1f66-4605-851d-74b0f8926e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences={}\n",
        "for ind in range(len(data_padded)):\n",
        "  h_t_keras = model1.predict(data_padded[ind])\n",
        "  sentences[flist[1+ind]]=h_t_keras\n",
        "print('Predicted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 80) for input Tensor(\"input_10:0\", shape=(None, 80), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
            "Predicted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JGhR9_W1Z0c",
        "outputId": "09e44707-719d-4b7e-f85e-65574d666df3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences['Identifying similar pieces of text has many applications (e.g., summarization, information retrieval, text clustering).'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00405581,  0.00867388,  0.0266684 , -0.00584137, -0.01624213,\n",
              "       -0.00910209,  0.01397985, -0.00964349, -0.01093441, -0.01488451,\n",
              "        0.03511903,  0.0286076 , -0.01481849,  0.01690504, -0.00541163,\n",
              "       -0.03090755,  0.02775238, -0.01677078,  0.00384965,  0.01111945,\n",
              "       -0.01154881, -0.0112345 , -0.00628583,  0.01939121, -0.03387162,\n",
              "       -0.03386471,  0.00667189, -0.00263849,  0.00429086, -0.00268894,\n",
              "        0.00079686,  0.00136597,  0.02131243,  0.01572224, -0.00833323,\n",
              "        0.01279465, -0.01036651, -0.02315241,  0.00676249, -0.00045686,\n",
              "       -0.00098436,  0.01811864, -0.01338414, -0.00345931, -0.02099242,\n",
              "       -0.02590306,  0.02940363,  0.00798768, -0.05584986, -0.05106822,\n",
              "       -0.01502245, -0.00098734,  0.01301523,  0.00718705, -0.01810588,\n",
              "       -0.0210581 ,  0.01286295,  0.00777822, -0.0036751 , -0.0382782 ,\n",
              "       -0.00277214,  0.03511423, -0.03962625,  0.0017584 ,  0.00495988,\n",
              "        0.00233423, -0.00306133,  0.0057816 , -0.01654095, -0.01061283,\n",
              "        0.01602335, -0.00917217, -0.00347738,  0.01294268, -0.01411738,\n",
              "        0.00783806, -0.00511499, -0.00999837,  0.00179454,  0.00097898,\n",
              "       -0.01870835, -0.0031927 ,  0.02284   ,  0.00564613,  0.000895  ,\n",
              "       -0.0084434 , -0.0228099 , -0.01120352, -0.00707316,  0.03567713,\n",
              "        0.00978525, -0.00014375,  0.00248316,  0.00293084,  0.00822927,\n",
              "       -0.0088297 ,  0.00073388, -0.00737177,  0.01531721, -0.00527448,\n",
              "        0.02056546, -0.00531892,  0.0070834 ,  0.02403304,  0.01461851,\n",
              "       -0.00663386,  0.00176598, -0.00659582,  0.00428529, -0.01327164,\n",
              "        0.00636772, -0.00694363, -0.01071837,  0.0122988 ,  0.00648624,\n",
              "       -0.02455568, -0.00675782, -0.01886696, -0.00848881, -0.00905374,\n",
              "       -0.00686208,  0.00340982, -0.01080064, -0.02067157, -0.02148788,\n",
              "        0.00898351,  0.01269232, -0.01120381], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRFsVNI3vEsV",
        "outputId": "e928d488-fc70-42b4-dc5c-ab6dbd8139a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for sent in list(set(list(df['col1'])))[1:5]:\n",
        "  print(sent)\n",
        "  inp = sentences[sent][0]\n",
        "  opts = list(df[df['col1']==sent].col2)\n",
        "  act = list(df[(df['col1']==sent) & (df['col3']==1)].col2)\n",
        "  print(act)\n",
        "  dict1={}\n",
        "  for opt in opts:\n",
        "    dict1[opt] = distance.cosine(inp, sentences[opt][0])\n",
        "\n",
        "  dict1 = {k: v for k, v in sorted(dict1.items(), key=lambda item: item[1])}\n",
        "  print(list(dict1.keys())[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates.\n",
            "['We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.']\n",
            "['However, if we assume that most speakers do not change their positions in the course of a discussion, we can conclude that all comments made by the same speaker must receive the same label.', 'However, this comes at the cost of greatly reducing agreement accuracy (development: 64.38%; test: 66.18%) due to lowered recall levels.', 'However, it is interesting to consider whether we really need to consider relationships specifically between speech segments themselves, or whether it suffices to simply consider relationships between the speakers of the speech segments.']\n",
            "We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets.\n",
            "['3.1 Data.']\n",
            "['We tab ulate and analyze the results of participating systems.', 'We used the standard Senseval scorer ? scorer21 to score the systems.', 'We selecteda total of 100 lemmas (65 verbs and 35 nouns) con sidering the degree of polysemy and total instances that were annotated.']\n",
            "For example, in our experiments, the Buchholz et al (1999) system uses the annotation np-sbj to indicate a subject, while the Ferro et al (1999) system uses the annotation subj.\n",
            "['Argamon et at.', 'For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b).']\n",
            "['For many tasks detecting only shallow structures in a sentence in a fast and reliable way is to be preferred over full parsing.', 'For example, in information retrieval it can be enough to find only simple NPs and VPs in a sentence, for information extraction we might also want to find relations between constituents as for example the subject and object of a verb.', 'For more references and information about these algorithms we refer to (Daelemans et al., 1998; Daelemans et al., 1999b).']\n",
            "The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008).\n",
            "['And this is, in essence, the WePS (Web People Search) task we have proposed to SemEval-2007 participants: systems receive a set of web pages(which are the result of a web search for a per son name), and they have to cluster them in as many sets as entities sharing the name.']\n",
            "['The user is then forced ei ther to add terms to the query (probably losing recall and focusing on one single aspect of the person), orto browse every document in order to filter the infor mation about the person he is actually looking for.', 'The first difference is that boundaries be tween word senses in a dictionary are often subtle or even conflicting, making binary decisions harderand sometimes even useless depending on the ap plication.', 'The second differenceis that WSD usually operates with a dictionary con taining a relatively small number of senses that can be assigned to each word.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVkx5bMSWxgj",
        "outputId": "b8142ad5-3230-4d3e-8df6-d7496f03fd41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentences['They were also free to use the gold-standard data to train their own models for the various layers of annotation, if they judged that those would either provide more accurate predictions or alternative predictions for use as multiple views, or wished to use a lattice of predictions.'][0][0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.21708214e-02, -1.12526584e-02,  8.03518761e-03, -6.32692734e-03,\n",
              "       -1.36157200e-02,  4.07527424e-02, -3.00485883e-02, -2.64439266e-02,\n",
              "        5.73045686e-02, -2.71533784e-02, -6.89985137e-03,  3.14300023e-02,\n",
              "        4.35669050e-02, -1.49543840e-03,  5.70946792e-03, -1.06573701e-02,\n",
              "       -1.20032942e-02, -1.19712240e-04,  2.24856101e-02, -1.09190755e-02,\n",
              "       -3.85896070e-03, -4.99813780e-02,  5.94684109e-03, -8.38134717e-03,\n",
              "        8.38036388e-02,  2.39668265e-02,  1.27936285e-02, -6.05915859e-03,\n",
              "       -5.16921356e-02, -1.16998178e-03, -3.45869293e-03,  3.24087627e-02,\n",
              "       -5.59058264e-02, -8.99258070e-03, -8.56186450e-03,  8.20480958e-02,\n",
              "        1.76715124e-02, -1.39234494e-02,  1.06857279e-02,  1.11873597e-02,\n",
              "       -5.69639196e-05,  4.35248762e-03,  5.97330183e-03,  1.32748643e-02,\n",
              "        3.27650481e-03, -2.38264985e-02, -6.73057605e-03,  2.00109687e-02,\n",
              "        6.97190547e-03, -6.09967485e-02,  2.41252165e-02,  1.37784611e-02,\n",
              "       -4.70085815e-03, -4.91412077e-03,  8.76349490e-03,  8.83775856e-03,\n",
              "        2.37407931e-03, -2.80468613e-02, -3.91146168e-02, -4.19638716e-02,\n",
              "        1.90995983e-03, -5.12390584e-02,  1.95529927e-02,  7.60313403e-03,\n",
              "       -9.98160057e-03, -1.39162438e-02, -2.00507995e-02,  1.56396013e-02,\n",
              "        1.96071039e-03, -2.83976011e-02,  3.12604755e-02, -1.47956097e-03,\n",
              "       -1.46278748e-02,  4.27879952e-03,  9.95268859e-03, -7.97097478e-03,\n",
              "        1.95279755e-02, -3.93314660e-03, -4.88579785e-03, -3.15289572e-02,\n",
              "       -1.80001166e-02,  6.12699240e-03, -9.72937141e-03, -3.31909885e-03,\n",
              "        5.34606026e-03, -1.17468666e-02, -2.45005563e-02, -1.17438091e-02,\n",
              "       -3.32483575e-02, -1.25379702e-02,  4.35907766e-03, -8.80819000e-03,\n",
              "        1.27270473e-02,  3.28722550e-03, -5.68445865e-03,  2.46644337e-02,\n",
              "       -2.13621724e-02,  2.57230178e-02,  1.86767559e-02, -2.68672258e-02,\n",
              "        1.84713937e-02, -3.03127822e-02, -2.39861570e-02, -2.25424394e-02,\n",
              "        6.76110713e-03, -3.90185975e-03,  2.82332823e-02,  1.46050705e-02,\n",
              "        2.49932073e-02, -2.57954560e-03, -1.19544072e-02, -1.51734604e-02,\n",
              "        1.14964023e-02, -6.27207151e-03, -1.51039921e-02,  1.35371378e-02,\n",
              "        4.34968900e-03,  1.33432774e-03, -2.06348277e-03,  1.52352285e-02,\n",
              "       -3.11378157e-03,  1.67279609e-03, -5.76050545e-04,  1.00633260e-02,\n",
              "        1.11817354e-02, -1.12748118e-02, -5.49078034e-03,  8.25327821e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    }
  ]
}