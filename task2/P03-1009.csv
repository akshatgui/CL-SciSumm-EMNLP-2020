col1,col2
Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.,{}
We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.,{}
"In contrast to previous work, we particularly focus on clustering polysemic verbs.",{}
"A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying",{}
Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.,"{'title': '1 Introduction', 'number': '1'}"
"(Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).","{'title': '1 Introduction', 'number': '1'}"
"While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.","{'title': '1 Introduction', 'number': '1'}"
∗This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’.,"{'title': '1 Introduction', 'number': '1'}"
"Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002).","{'title': '1 Introduction', 'number': '1'}"
"One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour.","{'title': '1 Introduction', 'number': '1'}"
"In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002).","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner.","{'title': '1 Introduction', 'number': '1'}"
"Previous research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and German verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002).","{'title': '1 Introduction', 'number': '1'}"
"We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information.","{'title': '1 Introduction', 'number': '1'}"
"We use clustering methods that process raw distributional data directly, avoiding complex preprocessing steps required by many advanced methods (e.g.","{'title': '1 Introduction', 'number': '1'}"
Brew and Schulte im Walde (2002)).,"{'title': '1 Introduction', 'number': '1'}"
"In contrast to earlier work, we give special emphasis to polysemy.","{'title': '1 Introduction', 'number': '1'}"
Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).,"{'title': '1 Introduction', 'number': '1'}"
The relatively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data.,"{'title': '1 Introduction', 'number': '1'}"
"However, this sense can vary across corpora (Roland et al., 2000), and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribution of senses in balanced corpus data is flat rather than zipfian (Preiss and Korhonen, 2002).","{'title': '1 Introduction', 'number': '1'}"
"To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold standard.","{'title': '1 Introduction', 'number': '1'}"
This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically.,"{'title': '1 Introduction', 'number': '1'}"
We discuss our gold standards and the choice of test verbs in section 2.,"{'title': '1 Introduction', 'number': '1'}"
Section 3 describes the method for subcategorization acquisition and section 4 presents the approach to clustering.,"{'title': '1 Introduction', 'number': '1'}"
Details of the experimental evaluation are supplied in section 5.,"{'title': '1 Introduction', 'number': '1'}"
Section 6 concludes with directions for future work.,"{'title': '1 Introduction', 'number': '1'}"
"Levin’s taxonomy of verbs and their classes (Levin, 1993) is the largest syntactic-semantic verb classification in English, employed widely in evaluation of automatic classifications.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"It provides a classification of 3,024 verbs (4,186 senses) into 48 broad / 192 fine grained classes.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"Although it is quite extensive, it is not exhaustive.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"As it primarily concentrates on verbs taking NP and PP complements and does not provide a comprehensive set of senses for verbs, it is not suitable for evaluation of polysemic classifications.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
We employed as a gold standard a substantially extended version of Levin’s classification constructed by Korhonen (2003).,"{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"This incorporates Levin’s classes, 26 additional classes by Dorr (1997)1, and 57 new classes for verb types not covered comprehensively by Levin or Dorr.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"110 test verbs were chosen from this gold standard, 78 polysemic and 32 monosemous ones.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
Some low frequency verbs were included to investigate the effect of sparse data on clustering performance.,"{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"To ensure that our gold standard covers all (or most) senses of these verbs, we looked into WordNet (Miller, 1990) and assigned all the WordNet senses of the verbs to gold standard classes.2 Two versions of the gold standard were created: monosemous and polysemic.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"The monosemous one lists only a single sense for each test verb, that corresponding to its predominant (most frequent) sense in WordNet.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
The polysemic one provides a comprehensive list of senses for each verb.,"{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
The test verbs and their classes are shown in table 1.,"{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
"The classes are indicated by number codes from the classifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font.","{'title': '2 Semantic Verb Classes and Test Verbs', 'number': '2'}"
We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997).,"{'title': '3 Subcategorization Information', 'number': '3'}"
"We expect the use of this system to be beneficial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive SCF classifier, which incorporates 163 SCF distinctions, a superset of those found in the ANLT (Boguraev et al., 1987) and COMLEX (Grishman et al., 1994) dictionaries.","{'title': '3 Subcategorization Information', 'number': '3'}"
"The SCFs abstract over specific lexicallygoverned particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement.","{'title': '3 Subcategorization Information', 'number': '3'}"
78 of these ‘coarse-grained’ SCFs appeared in our data.,"{'title': '3 Subcategorization Information', 'number': '3'}"
"In addition, a set of 160 fine grained frames were employed.","{'title': '3 Subcategorization Information', 'number': '3'}"
These were obtained by parameterizing two high frequency SCFs for prepositions: the simple PP and NP + PP frames.,"{'title': '3 Subcategorization Information', 'number': '3'}"
The scope was restricted to these two frames to prevent sparse data problems in clustering.,"{'title': '3 Subcategorization Information', 'number': '3'}"
"A SCF lexicon was acquired using this system from the British National Corpus (Leech, 1992, BNC) so that the maximum of 7000 citations were used per test verb.","{'title': '3 Subcategorization Information', 'number': '3'}"
The lexicon was evaluated against manually analysed corpus data after an empirically defined threshold of 0.025 was set on relative frequencies of SCFs to remove noisy SCFs.,"{'title': '3 Subcategorization Information', 'number': '3'}"
The method yielded 71.8% precision and 34.5% recall.,"{'title': '3 Subcategorization Information', 'number': '3'}"
"When we removed the filtering threshold, and evaluated the noisy distribution, F-measure4 dropped from 44.9 to 38.51.5","{'title': '3 Subcategorization Information', 'number': '3'}"
"Data clustering is a process which aims to partition a given set into subsets (clusters) of elements that are similar to one another, while ensuring that elements that are not similar are assigned to different clusters.","{'title': '4 Clustering Method', 'number': '4'}"
We use clustering for partitioning a set of verbs.,"{'title': '4 Clustering Method', 'number': '4'}"
Our hypothesis is that information about SCFs and their associated frequencies is relevant for identifying semantically related verbs.,"{'title': '4 Clustering Method', 'number': '4'}"
"Hence, we use SCFs as relevance features to guide the clustering process.6 comparing the probability of a randomly chosen pair of verbs verbi and verbj to share the same predominant sense (4.5%) with the probability obtained when verbj is the JS-divergence We chose two clustering methods which do not involve task-oriented tuning (such as pre-fixed thresholds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neighbours (NN) of each verb (figure 1), and (ii) the Information Bottleneck (IB), an iterative soft method (Tishby et al., 1999) based on information-theoretic grounds.","{'title': '4 Clustering Method', 'number': '4'}"
"The NN method is very simple, but it has some disadvantages.","{'title': '4 Clustering Method', 'number': '4'}"
"It outputs only one clustering configuration, and therefore does not allow examination of different cluster granularities.","{'title': '4 Clustering Method', 'number': '4'}"
It is also highly sensitive to noise.,"{'title': '4 Clustering Method', 'number': '4'}"
Few exceptional neighbourhood relations contradicting the typical trends in the data are enough to cause the formation of a single cluster which encompasses all elements.,"{'title': '4 Clustering Method', 'number': '4'}"
Therefore we employed the more sophisticated IB method as well.,"{'title': '4 Clustering Method', 'number': '4'}"
"The IB quantifies the relevance information of a SCF distribution with respect to output clusters, through their mutual information I(Clusters; SCFs).","{'title': '4 Clustering Method', 'number': '4'}"
"The relevance information is maximized, while the compression information I(Clusters; Verbs) is minimized.","{'title': '4 Clustering Method', 'number': '4'}"
This ensures optimal compression of data through clusters.,"{'title': '4 Clustering Method', 'number': '4'}"
The tradeoff between the two constraints is realized nearest neighbour of verbi (36%). through minimizing the cost term: where Q is a parameter that balances the constraints.,"{'title': '4 Clustering Method', 'number': '4'}"
The IB iterative algorithm finds a local minimum of the above cost term.,"{'title': '4 Clustering Method', 'number': '4'}"
"It takes three inputs: (i) SCFverb distributions, (ii) the desired number of clusters K, and (iii) the value of Q.","{'title': '4 Clustering Method', 'number': '4'}"
"Starting from a random configuration, the algorithm repeatedly calculates, for each cluster K, verb V and SCF S, the following probabilities: (i) the marginal proportion of the cluster p(K); (ii) the probability p(S|K) for a SCF to occur with members of the cluster; and (iii) the probability p(K|V ) for a verb to be assigned to the cluster.","{'title': '4 Clustering Method', 'number': '4'}"
"These probabilities are used, each in its turn, for calculating the other probabilities (figure 2).","{'title': '4 Clustering Method', 'number': '4'}"
The collection of all p(S|K)’s for a fixed cluster K can be regarded as a probabilistic center (centroid) of that cluster in the SCF space.,"{'title': '4 Clustering Method', 'number': '4'}"
The IB method gives an indication of the most informative values of K.7 Intensifying the weight Q attached to the relevance information I(Clusters; SCFs) allows us to increase the number K of distinct clusters being produced (while too small Q would cause some of the output clusters to be identical to one another).,"{'title': '4 Clustering Method', 'number': '4'}"
"Hence, the relevance information grows with K. Accordingly, we consider as the most informative output configurations those for which the relevance information increases more sharply between K − 1 and K clusters than between","{'title': '4 Clustering Method', 'number': '4'}"
"When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.","{'title': 'K and K + 1.', 'number': '5'}"
Let denote the most probable cluster of a verb V .,"{'title': 'K and K + 1.', 'number': '5'}"
"For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output clustering approximately hard.","{'title': 'K and K + 1.', 'number': '5'}"
"For this reason, we decided to use only K(V ) as output and defer a further exploration of the soft output to future work.","{'title': 'K and K + 1.', 'number': '5'}"
The input data to clustering was obtained from the automatically acquired SCF lexicon for our 110 test verbs (section 2).,"{'title': '5 Experimental Evaluation', 'number': '6'}"
The counts were extracted from unfiltered (noisy) SCF distributions in this lexicon.8 The NN algorithm produced 24 clusters on this input.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"From the IB algorithm, we requested K = 2 to 60 clusters.","{'title': '5 Experimental Evaluation', 'number': '6'}"
The upper limit was chosen so as to slightly exceed the case when the average cluster size 110/K = 2.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"We chose for evaluation the IB results for K = 25, 35 and 42.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"For these values, the SCF relevance satisfies our criterion for a notable improvement in cluster quality (section 4).","{'title': '5 Experimental Evaluation', 'number': '6'}"
The value K =35 is very close to the actual number (34) of predominant senses in the gold standard.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"In this way, the IB yields structural information beyond clustering.","{'title': '5 Experimental Evaluation', 'number': '6'}"
A number of different strategies have been proposed for evaluation of clustering.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"We concentrate here on those which deliver a numerical value which is easy to interpret, and do not introduce biases towards specific numbers of classes or class sizes.","{'title': '5 Experimental Evaluation', 'number': '6'}"
As we currently assign a single sense to each polysemic verb (sec.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
5.4) the measures we use are also applicable for evaluation against a polysemous gold standard.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Our first measure, the adjusted pairwise precision (APP), evaluates clusters in terms of verb pairs (Schulte im Walde and Brew, 2002) 9: num. of correct pairs in ki |ki|−1 · num. of pairs in ki |ki|+1 .","{'title': '5 Experimental Evaluation', 'number': '6'}"
APP is the average proportion of all within-cluster pairs that are correctly co-assigned.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
It is multiplied by a factor that increases with cluster size.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
This factor compensates for a bias towards small clusters.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).","{'title': '5 Experimental Evaluation', 'number': '6'}"
"We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by nprevalent(K).","{'title': '5 Experimental Evaluation', 'number': '6'}"
Verbs that do not take this class are considered as errors.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Given our task, we are only interested in classes which contain two or more verbs.","{'title': '5 Experimental Evaluation', 'number': '6'}"
We therefore disregard those clusters where nprevalent(K) = 1.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
This leads us to define modified purity: number of verbs .,"{'title': '5 Experimental Evaluation', 'number': '6'}"
The modification we introduce to purity removes the bias towards the trivial configuration comprised of only singletons.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"We first evaluated the clusters against the predominant sense, i.e. using the monosemous gold standard.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"The results, shown in Table 2, demonstrate that both clustering methods perform significantly better on the task than our random clustering baseline.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"Both methods show clearly better performance with fine-grained SCFs (with prepositions, +PP) than with coarse-grained ones (-PP).","{'title': '5 Experimental Evaluation', 'number': '6'}"
"Surprisingly, the simple NN method performs very similarly to the more sophisticated IB.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"Being based on pairwise similarities, it shows better performance than IB on the pairwise measure.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"The IB is, however, slightly better according to the global measure (2% with K = 42).","{'title': '5 Experimental Evaluation', 'number': '6'}"
The fact that the NN method performs better than the IB with similar K values (NN K = 24 vs. IB K = 25) seems to suggest that the JS divergence provides a better model for the predominant class than the compression model of the IB.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"However, it is likely that the IB performance suffered due to our choice of test data.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"As the method is global, it performs better when the target classes are represented by a high number of verbs.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"In our experiment, many semantic classes were represented by two verbs only (section 2).","{'title': '5 Experimental Evaluation', 'number': '6'}"
"Nevertheless, the IB method has the clear advantage that it allows for more clusters to be produced.","{'title': '5 Experimental Evaluation', 'number': '6'}"
At best it classified half of the verbs correctly according to their predominant sense (mPUR = 50%).,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Although this leaves room for improvement, the result compares favourably to previously published results10.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"We argue, however, that evaluation against a monosemous gold standard reveals only part of the picture.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"10Due to differences in task definition and experimental setup, a direct comparison with earlier results is impossible.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mPUR ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"In evaluation against the polysemic gold standard, we assume that a verb which is polysemous in our corpus data may appear in a cluster with verbs that share any of its senses.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"In order to evaluate the clusters against polysemous data, we assigned each polysemic verb V a single sense: the one it shares with the highest number of verbs in the cluster K(V ).","{'title': '5 Experimental Evaluation', 'number': '6'}"
Table 3 shows the results against polysemic and monosemous gold standards.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
The former are noticeably better than the latter (e.g.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
IB with K = 42 is 9% better).,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Clearly, allowing for multiple gold standard classes makes it easier to obtain better results with evaluation.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"In order to show that polysemy makes a nontrivial contribution in shaping the clusters, we measured the improvement that can be due to pure chance by creating randomly polysemous gold standards.","{'title': '5 Experimental Evaluation', 'number': '6'}"
We constructed 100 sets of random gold standards.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"In each iteration, the verbs kept their original predominant senses, but the set of additional senses was taken entirely from another verb - chosen at random.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"By doing so, we preserved the dominant sense of each verb, the total frequency of all senses and the correlations between the additional senses.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"The results included in table 3 indicate, with 99.5% confidence (3Q and above), that the improvement obtained with the polysemous gold standard is not artificial (except in two cases with 95% confidence).","{'title': '5 Experimental Evaluation', 'number': '6'}"
We performed qualitative analysis to further investigate the effect of polysemy on clustering performance.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"The results in table 4 demonstrate that the more two verbs differ in their senses, the lower their chance of ending up in the same cluster.","{'title': '5 Experimental Evaluation', 'number': '6'}"
From the figures in table 5 we see that the probability of two verbs to appear in the same cluster increases with the number of senses they share.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Interestingly, it is not only the degree of polysemy which influences the results, but also the type.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"For verb pairs where at least one of the members displays ‘irregular’ polysemy (i.e. it does not share its full set of senses with any other verb), the probability of co-occurrence in the same cluster is far lower than for verbs which are polysemic in a ‘regular’ manner (Table 5).","{'title': '5 Experimental Evaluation', 'number': '6'}"
Manual cluster analysis against the polysemic gold standard revealed a yet more comprehensive picture.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Consider the following clusters (the IB output with K = 42): A1: talk (37), speak (37) A2: look (30, 35), stare (30) A3: focus (31, 45), concentrate (31, 45) A4: add (22, 37, A56) We identified a close relation between the clustering performance and the following patterns of semantic behaviour: 1) Monosemy: We had 32 monosemous test verbs.","{'title': '5 Experimental Evaluation', 'number': '6'}"
10 gold standard classes included 2 or more or these.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
7 classes were correctly acquired using clustering (e.g.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"A1), indicating that clustering monosemous verbs is fairly ‘easy’.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"2) Predominant sense: 10 clusters were examined by hand whose members got correctly classified together, despite one of them being polysemous (e.g.","{'title': '5 Experimental Evaluation', 'number': '6'}"
A2).,"{'title': '5 Experimental Evaluation', 'number': '6'}"
In 8 cases there was a clear indication in the data (when examining SCFs and the selectional preferences on argument heads) that the polysemous verb indeed had its predominant sense in the relevant class and that the co-occurrence was not due to noise. ysemy11 were frequently assigned to singleton clusters.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"For example, add (A4) has a ‘combining and attaching’ sense in class 22 which involves NP and PP SCFs and another ‘communication’ sense in 37 which takes sentential SCFs.","{'title': '5 Experimental Evaluation', 'number': '6'}"
Irregular polysemy was not a marginal phenomenon: it explains 5 of the 10 singletons in our data.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
These observations confirm that evaluation against a polysemic gold standard is necessary in order to fully explain the results from clustering.,"{'title': '5 Experimental Evaluation', 'number': '6'}"
"Finally, to provide feedback for further development of our verb classification approach, we performed a qualitative analysis of errors not resulting from polysemy.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"Consider the following clusters (the IB output for K = 42): B1: place (9), build (26, 45), publish (26, 25), carve (21, 25, 26) B2: sin (003), rain (57), snow (57, 002) B3: agree (36, 22, A42), appear (020, 48, 29), begin (55), continue (55, 47, 51) B4: beg (015, 32) Three main error types were identified: 11Recall our definition of irregular polysemy, section 5.4. poorly.","{'title': '5 Experimental Evaluation', 'number': '6'}"
"In B2, sin (which had 53 occurrences) is classified with rain and snow because it does not occur in our data with the preposition against the ‘hallmark’ of its gold standard class (’Conspire Verbs’).","{'title': '5 Experimental Evaluation', 'number': '6'}"
3) Problems in SCF acquisition: These were not numerous but occurred e.g. when the system could not distinguish between different control (e.g. subject/object equi/raising) constructions (B3).,"{'title': '5 Experimental Evaluation', 'number': '6'}"
This paper has presented a novel approach to automatic semantic classification of verbs.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
This involved applying the NN and IB methods to cluster polysemic SCF distributions extracted from corpus data using Briscoe and Carroll’s (1997) system.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classification.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
Our investigation revealed that polysemy has a considerable impact on the clusters formed: polysemic verbs with a clear predominant sense and those with similar regular polysemy are frequently classified together.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
Homonymic verbs or verbs with strong irregular polysemy tend to resist any classification.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
"While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the potential and limitations of clustering in inducing semantic information from polysemic SCF data.","{'title': '6 Discussion and Conclusions', 'number': '7'}"
Our results show that it is unrealistic to expect that the ‘important’ (high frequency) verbs in language fall into classes corresponding to single senses.,"{'title': '6 Discussion and Conclusions', 'number': '7'}"
"However, they also suggest that clustering can be used for novel, previously unexplored purposes: to detect from corpus data general patterns of semantic behaviour (monosemy, predominant sense, regular/irregular polysemy).","{'title': '6 Discussion and Conclusions', 'number': '7'}"
"In the future, we plan to investigate the use of soft clustering (without hardening the output) and develop methods for evaluating the soft output against polysemous gold standards.","{'title': '6 Discussion and Conclusions', 'number': '7'}"
"We also plan to work on improving the accuracy of subcategorization acquisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different approaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes.","{'title': '6 Discussion and Conclusions', 'number': '7'}"
