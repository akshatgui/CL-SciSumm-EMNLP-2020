col1,col2
We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.,{}
"This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.",{}
"Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.","{'title': '1 Introduction', 'number': '1'}"
"Even for a single predicate, semantic arguments can have multiple syntactic realizations, as shown by the following paraphrases: Recently, attention has turned to creating corpora annotated with argument structures.","{'title': '1 Introduction', 'number': '1'}"
"The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text.","{'title': '1 Introduction', 'number': '1'}"
Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999).,"{'title': '1 Introduction', 'number': '1'}"
"In this paper, we examine how the syntactic representations used by different statistical parsers affect the performance of such a system.","{'title': '1 Introduction', 'number': '1'}"
"We compare a parser based on Combinatory Categorial Grammar (CCG) (Hockenmaier and Steedman, 2002b) with the Collins parser.","{'title': '1 Introduction', 'number': '1'}"
"As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation.","{'title': '1 Introduction', 'number': '1'}"
"The Treebankparser returns skeletal phrase-structure trees without the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns wordword dependencies that correspond to the underlying predicate-argument structure, including longrange dependencies arising through control, raising, extraction and coordination.","{'title': '1 Introduction', 'number': '1'}"
"The Proposition Bank (Kingsbury and Palmer, 2002) provides a human-annotated corpus of semantic verb-argument relations.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"For each verb appearing in the corpus, a set of semantic roles is defined.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"Roles for each verb are simply numbered Arg0, Arg1, Arg2, etc.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"As an example, the entryspecific roles for the verb offer are given below: These roles are then annotated for every instance of the verb appearing in the corpus, including the following examples: A variety of additional roles are assumed to apply across all verbs.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"These secondary roles can be thought of as being adjuncts, rather than arguments, although no claims are made as to optionality or other traditional argument/adjunct tests.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"The secondary roles include: Location in Tokyo, outside Time last week, on Tuesday, never Manner easily, dramatically Direction south, into the wind Cause due to pressure from Washington Discourse however, also, on the other hand Extent 15%, 289 points Purpose to satisfy requirements Negation not, n’t Modal can, might, should, will Adverbial (none of the above) and are represented in PropBank as “ArgM” with an additional function tag, for example ArgM-TMP for temporal.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
We refer to PropBank’s numbered arguments as “core” arguments.,"{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
Core arguments represent 75% of the total labeled roles in the PropBank data.,"{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"Our system predicts all the roles, including core arguments as well as the ArgM labels and their function tags.","{'title': '2 Predicate-argument relations in PropBank', 'number': '2'}"
"Combinatory Categorial Grammar (CCG) (Steedman, 2000), is a grammatical theory which provides a completely transparent interface between surface syntax and underlying semantics, such that each syntactic derivation corresponds directly to an interpretable semantic representation which includes long-range dependencies that arise through control, raising, coordination and extraction.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"In CCG, words are assigned atomic categories such as NP, or functor categories like (S[dcl]\NP)/NP (transitive declarative verb) or S/S (sentential modifier).","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
Adjuncts are represented as functor categories such as S/S which expect and return the same type.,"{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"We use indices to number the arguments of functor categories, eg.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"(S[dcl]\NP1)/NP2, or S/S1, and indicate the wordword dependencies in the predicate-argument structure as tuples (wh, ch, i, wa), where ch is the lexical category of the head word wh, and wa is the head word of the constituent that fills the ith argument of ch.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
Long-range dependencies can be projected through certain types of lexical categories or through rules such as coordination of functor categories.,"{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"For example, in the lexical category of a relative pronoun, (NP\NP;)/(S[dcl]/NP;), the head of the NP that is missing from the relative clause is unified with (as indicated by the indices i) the head of the NP that is modified by the entire relative clause.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"Figure 1 shows the derivations of an ordinary sentence, a relative clause and a right-node-raising construction.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"In all three sentences, the predicateargument relations between London and denied and plans and denied are the same, which in CCG is expressed by the fact that London fills the first (ie. subject) argument slot of the lexical category of denied, (S[dcl]\NP1)/NP2, and plans fills the second (object) slot.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
The relations extracted from the CCG derivation for the sentence “London denied plans on Monday” are shown in Table 1.,"{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
The CCG parser returns the local and long-range word-word dependencies that express the predicateargument structure corresponding to the derivation.,"{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"These relations are recovered with an accuracy of around 83% (labeled recovery) or 91% (unlabeled recovery) (Hockenmaier, 2003).","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"By contrast, standard Treebank parsers such as (Collins, 1999) only return phrase-structure trees, from which non-local dependencies are difficult to recover.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
"The CCG parser has been trained and tested on CCGbank (Hockenmaier and Steedman, 2002a), a treebank of CCG derivations obtained from the Penn Treebank, from which we also obtain our training data.","{'title': '3 Predicate-argument relations in CCG', 'number': '3'}"
Our aim is to use CCG derivations as input to a system for automatically producing the argument labels of PropBank.,"{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"In order to do this, we wish to correlate the CCG relations above with PropBank arguments.","{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
PropBank argument labels are assigned to nodes in the syntactic trees from the Penn Treebank.,"{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"While the CCGbank is derived from the Penn Treebank, in many cases the constituent structures do not correspond.","{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"That is, there may be no constituent in the CCG derivation corresponding to the same sequence of words as a particular constituent in the Treebank tree.","{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.","{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"For each role label for a verb’s argument in PropBank, we first find the head word for its constituent according to the the head rules of (Collins, 1999).","{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
We then look for the label of the CCG relation between this head word and the verb itself.,"{'title': '4 Mapping between PropBank and CCGbank', 'number': '4'}"
"In previous work using the PropBank corpus, Gildea and Palmer (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1999).","{'title': '5 The Experiments', 'number': '5'}"
We will briefly review their probability model before adapting the system to incorporate features from the CCG derivations.,"{'title': '5 The Experiments', 'number': '5'}"
"For the Treebank-based system, we use the probability model of Gildea and Palmer (2002).","{'title': '5 The Experiments', 'number': '5'}"
"Probabilities of a parse constituent belonging to a given semantic role are calculated from the following features: The phrase type feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP) , and clause (S).","{'title': '5 The Experiments', 'number': '5'}"
The parse tree path feature is designed to capture the syntactic relation of a constituent to the predicate.,"{'title': '5 The Experiments', 'number': '5'}"
"It is defined as the path from the predicate through the parse tree to the constituent in question, represented as a string of parse tree nonterminals linked by symbols indicating upward or downward movement through the tree, as shown in Figure 2.","{'title': '5 The Experiments', 'number': '5'}"
"Although the path is composed as a string of symbols, our systems will treat the string as an atomic value.","{'title': '5 The Experiments', 'number': '5'}"
"The path includes, as the first element of the string, the part of speech of the predicate, and, as the last element, the phrase type or syntactic category of the sentence constituent marked as an argument.","{'title': '5 The Experiments', 'number': '5'}"
"He ate some pancakes Figure 2: In this example, the path from the predicate ate to the argument NP He can be represented as VBTVPTS1NP, with T indicating upward movement in the parse tree and 1 downward movement.","{'title': '5 The Experiments', 'number': '5'}"
The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate.,"{'title': '5 The Experiments', 'number': '5'}"
"This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after.","{'title': '5 The Experiments', 'number': '5'}"
"This feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.","{'title': '5 The Experiments', 'number': '5'}"
"The voice feature distinguishes between active and passive verbs, and is important in predicting semantic roles because direct objects of active verbs correspond to subjects of passive verbs.","{'title': '5 The Experiments', 'number': '5'}"
"An instance of a verb was considered passive if it is tagged as a past participle (e.g. taken), unless it occurs as a descendent verb phrase headed by any form of have (e.g. has taken) without an intervening verb phrase headed by any form of be (e.g. has been taken).","{'title': '5 The Experiments', 'number': '5'}"
"The head word is a lexical feature, and provides information about the semantic type of the role filler.","{'title': '5 The Experiments', 'number': '5'}"
Head words of nodes in the parse tree are determined using the same deterministic set of head word rules used by Collins (1999).,"{'title': '5 The Experiments', 'number': '5'}"
"The system attempts to predict argument roles in new data, looking for the highest probability assignment of roles ri to all constituents i in the sentence, given the set of features Fi = {pti, pathi, posi, vi, hi} at each constituent in the parse tree, and the predicate p: We break the probability estimation into two parts, the first being the probability P(riIFi, p) of a constituent’s role given our five features for the consituent, and the predicate p. Due to the sparsity of the data, it is not possible to estimate this probability from the counts in the training data.","{'title': '5 The Experiments', 'number': '5'}"
"Instead, probabilities are estimated from various subsets of the features, and interpolated as a linear combination of the resulting distributions.","{'title': '5 The Experiments', 'number': '5'}"
"The interpolation is performed over the most specific distributions for which data are available, which can be thought of as choosing the topmost distributions available from a backoff lattice, shown in Figure 3.","{'title': '5 The Experiments', 'number': '5'}"
"The probabilities P(ri|Fi, p) are combined with the probabilities P({r1..n}|p) for a set of roles appearing in a sentence given a predicate, using the following formula: This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation.","{'title': '5 The Experiments', 'number': '5'}"
"In particular, we assume that sets of roles appear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role.","{'title': '5 The Experiments', 'number': '5'}"
"In the CCG version, we replace the features above with corresponding features based on both the sentence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations extracted from it (shown in Table 1).","{'title': '5 The Experiments', 'number': '5'}"
"The parse tree path feature, designed to capture grammatical relations between constituents, is replaced with a feature defined as follows: If there is a dependency in the predicate-argument structure of the CCG derivation between two words w and w', the path feature from w to w' is defined as the lexical category of the functor, the argument slot i occupied by the argument, plus an arrow (← or →) to indicate whether w or w' is the categorial functor.","{'title': '5 The Experiments', 'number': '5'}"
"For example, in our sentence “London denied plans on Monday”, the relation connecting the verb denied with plans is (S[dcl]\NP)/NP.2.←, with the left arrow indicating the lexical category included in the relation is that of the verb, while the relation connecting denied with on is ((S\NP)\(S\NP))/NP.2.→, with the right arrow indicating the the lexical category included in the relation is that of the modifier.","{'title': '5 The Experiments', 'number': '5'}"
"If the CCG derivation does not define a predicateargument relation between the two words, we use the parse tree path feature described above, defined over the CCG derivation tree.","{'title': '5 The Experiments', 'number': '5'}"
"In our training data, 77% of PropBank arguments corresponded directly to a relation in the CCG predicate-argument representation, and the path feature was used for the remaining 23%.","{'title': '5 The Experiments', 'number': '5'}"
Most of these mismatches arise because the CCG parser and PropBank differ in their definition of head words.,"{'title': '5 The Experiments', 'number': '5'}"
"For instance, the CCG parser always assumes that the head of a PP is the preposition, whereas PropBank roles can be assigned to the entire PP (7), or only to the NP argument of the preposition (8), in which case the head word comes from the NP: In embedded clauses, CCG assumes that the head is the complementizer, whereas in PropBank, the head comes from the embedded sentence itself.","{'title': '5 The Experiments', 'number': '5'}"
In complex verb phrases (eg.,"{'title': '5 The Experiments', 'number': '5'}"
"“might not have gone”), the CCG parser assumes that the first auxiliary (might) is head, whereas PropBank assumes it is the main verb (gone).","{'title': '5 The Experiments', 'number': '5'}"
"Therefore, CCG assumes that not modifies might, whereas PropBank assumes it modifies gone.","{'title': '5 The Experiments', 'number': '5'}"
"Although the head rules of the parser could in principle be changed to reflect more directly the dependencies in PropBank, we have not attempted to do so yet.","{'title': '5 The Experiments', 'number': '5'}"
"Further mismatches occur because the predicate-argument structure returned by the CCG parser only contains syntactic dependencies, whereas the PropBank data also contain some anaphoric dependencies, eg.","{'title': '5 The Experiments', 'number': '5'}"
": Such dependencies also do not correspond to a relation in the predicate-argument structure of the CCG derivation, and cause the path feature to be used.","{'title': '5 The Experiments', 'number': '5'}"
The phrase type feature is replaced with the lexical category of the maximal projection of the PropBank argument’s head word in the CCG derivation tree.,"{'title': '5 The Experiments', 'number': '5'}"
"For example, the category of plans is N, and the category of denied is (S[dcl]\NP)/NP.","{'title': '5 The Experiments', 'number': '5'}"
"The voice feature can be read off the CCG categories, since the CCG categories of past participles carry different features in active and passive voice (eg. sold can be (S[pt]\NP)/NP or S[pss]\NP).","{'title': '5 The Experiments', 'number': '5'}"
The head word of a constituent is indicated in the derivations returned by the CCG parser.,"{'title': '5 The Experiments', 'number': '5'}"
We use data from the November 2002 release of PropBank.,"{'title': '5 The Experiments', 'number': '5'}"
"The dataset contains annotations for 72,109 predicate-argument structures with 190,815 individual arguments (of which 75% are core, or numbered, arguments) and has includes examples from 2462 lexical predicates (types).","{'title': '5 The Experiments', 'number': '5'}"
Annotations from Sections 2 through 21 of the Treebank were used for training; Section 23 was the test set.,"{'title': '5 The Experiments', 'number': '5'}"
Both parsers were trained on Sections 2 through 21.,"{'title': '5 The Experiments', 'number': '5'}"
"Because of the mismatch between the constituent structures of CCG and the Treebank, we score both systems according to how well they identify the head words of PropBank’s arguments.","{'title': '6 Results', 'number': '6'}"
"Table 2 gives the performance of the system on both PropBank’s core, or numbered, arguments, and on all PropBank roles including the adjunct-like ArgM roles.","{'title': '6 Results', 'number': '6'}"
"In order to analyze the impact of errors in the syntactic parses, we present results using features extracted from both automatic parser output and the gold standard parses in the Penn Treebank (without functional tags) and in CCGbank.","{'title': '6 Results', 'number': '6'}"
Using the gold standard parses provides an upper bound on the performance of the system based on automatic parses.,"{'title': '6 Results', 'number': '6'}"
"Since the Collins parser does not provide trace information, its upper bound is given by the system tested on the gold-standard Treebank representation with traces removed.","{'title': '6 Results', 'number': '6'}"
"In Table 2, “core” indicates results on PropBank’s numbered arguments (ARG0...ARG5) only, and “all” includes numbered arguments as well as the ArgM roles.","{'title': '6 Results', 'number': '6'}"
Most of the numbered arguments (in particular ARG0 and ARG1) correspond to arguments that the CCG category of the verb directly subcategorizes for.,"{'title': '6 Results', 'number': '6'}"
"The CCG-based system outperforms the system based on the Collins parser on these core arguments, and has comparable performance when all PropBank labels are considered.","{'title': '6 Results', 'number': '6'}"
"We believe that the superior performance of the CCG system on this core arguments is due to its ability to recover long-distance dependencies, whereas we attribute its lower performance on non-core arguments mainly to the mismatches between PropBank and CCGbank.","{'title': '6 Results', 'number': '6'}"
The importance of long-range dependencies for our task is indicated by the fact that the performance on the Penn Treebank gold standard without traces is significantly lower than that on the Penn Treebank with trace information.,"{'title': '6 Results', 'number': '6'}"
"Long-range dependencies are especially important for core arguments, shown by the fact that removing trace information from the Treebank parses results in a bigger drop for core arguments (83.5 to 76.3 F-score) than for all roles (74.1 to 70.2).","{'title': '6 Results', 'number': '6'}"
"The ability of the CCG parser to recover these long-range dependencies accounts for its higher performance, and in particular its higher recall, on core arguments.","{'title': '6 Results', 'number': '6'}"
The CCG gold standard performance is below that of the Penn Treebank gold standard with traces.,"{'title': '6 Results', 'number': '6'}"
We believe this performance gap to be caused by the mismatches between the CCG analyses and the PropBank annotations described in Section 5.2.,"{'title': '6 Results', 'number': '6'}"
"For the reasons described, the head words of the constituents that have PropBank roles are not necessarily the head words that stand in a predicate-argument relation in CCGbank.","{'title': '6 Results', 'number': '6'}"
"If two words do not stand in a predicate-argument relation, the CCG system takes recourse to the path feature.","{'title': '6 Results', 'number': '6'}"
"This feature is much sparser in CCG: since CCG categories encode subcategorization information, the number of categories in CCGbank is much larger than that of Penn Treebank labels.","{'title': '6 Results', 'number': '6'}"
"Analysis of our system’s output shows that the system trained on the Penn Treebank gold standard obtains 55.5% recall on those relations that require the CCG path feature, whereas the system using CCGbank only achieves 36.9% recall on these.","{'title': '6 Results', 'number': '6'}"
"Also, in CCG, the complement-adjunct distinction is represented in the categories for the complement (eg.","{'title': '6 Results', 'number': '6'}"
PP) or adjunct (eg.,"{'title': '6 Results', 'number': '6'}"
(S\NP)\(S\NP) and in the categories for the head (eg.,"{'title': '6 Results', 'number': '6'}"
(S[dcl]\NP)/PP or S[dcl]\NP).,"{'title': '6 Results', 'number': '6'}"
"In generating the CCGbank, various heuristics were used to make this distinction.","{'title': '6 Results', 'number': '6'}"
"In particular, for PPs, it depends on the “closely-related” (CLR) function tag, which is known to be unreliable.","{'title': '6 Results', 'number': '6'}"
"The decisions made in deriving the CCGbank often do not match the hand-annotated complementadjunct distinctions in PropBank, and this inconsistency is likely to make our CCGbank-based features less predictive.","{'title': '6 Results', 'number': '6'}"
A possible solution is to regenerate the CCGbank using the Propbank annotations.,"{'title': '6 Results', 'number': '6'}"
"The impact of our head-word based scoring is analyzed in Table 3, which compares results when only the head word must be correctly identified (as in Table 2) and to results when both the beginning and end of the argument must be correctly identified in the sentence (as in Gildea and Palmer (2002)).","{'title': '6 Results', 'number': '6'}"
"Even if the head word is given the correct label, the boundaries of the entire argument may be different from those given in the PropBank annotation.","{'title': '6 Results', 'number': '6'}"
"Since constituents in CCGbank do not always match those in PropBank, even the CCG gold standard parses obtain comparatively low scores according to this metric.","{'title': '6 Results', 'number': '6'}"
This is exacerbated when automatic parses are considered.,"{'title': '6 Results', 'number': '6'}"
"Our CCG-based system for automatically labeling verb arguments with PropBank-style semantic roles outperforms a system using a traditional Treebankbased parser for core arguments, which comprise 75% of the role labels, but scores lower on adjunctlike roles such as temporals and locatives.","{'title': '7 Conclusion', 'number': '7'}"
"The CCG parser returns predicate-argument structures that include long-range dependencies; therefore, it seems inherently better suited for this task.","{'title': '7 Conclusion', 'number': '7'}"
"However, the performance of our CCG system is lowered by the fact that the syntactic analyses in its training corpus differ from those that underlie PropBank in important ways (in particular in the notion of heads and the complement-adjunct distinction).","{'title': '7 Conclusion', 'number': '7'}"
We would expect a higher performance for the CCG-based system if the analyses in CCGbank resembled more closely those in PropBank.,"{'title': '7 Conclusion', 'number': '7'}"
"Our results also indicate the importance of recovering long-range dependencies, either through the trace information in the Penn Treebank, or directly, as in the predicate-argument structures returned by the CCG parser.","{'title': '7 Conclusion', 'number': '7'}"
"We speculate that much of the performance improvement we show could be obtained with traditional (ie. non-CCG-based) parsers if they were designed to recover more of the information present in the Penn Treebank, in particular the trace co-indexation.","{'title': '7 Conclusion', 'number': '7'}"
An interesting experiment would be the application of our role-labeling system to the output of the trace recovery system of Johnson (2002).,"{'title': '7 Conclusion', 'number': '7'}"
"Our results also have implications for parser evaluation, as the most frequently used constituent-based precision and recall measures do not evaluate how well long-range dependencies can be recovered from the output of a parser.","{'title': '7 Conclusion', 'number': '7'}"
"Measures based on dependencies, such as those of Lin (1995) and Carroll et al. (1998), are likely to be more relevant to real-world applications of parsing.","{'title': '7 Conclusion', 'number': '7'}"
"Acknowledgments This work was supported by the Institute for Research in Cognitive Science at the University of Pennsylvania, the Propbank project (DoD Grant MDA904-00C2136), an EPSRC studentship and grant GR/M96889, and NSF ITR grant 0205 456.","{'title': '7 Conclusion', 'number': '7'}"
"We thank Mark Steedman, Martha Palmer and Alexandra Kinyon for their comments on this work.","{'title': '7 Conclusion', 'number': '7'}"
