col1,col2
"In this paper, we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition.",{}
Active learning targets to minimize the human annotation efforts by selecting examples for labeling.,{}
"To maximize the contribution of the selected examples, we the multiple criteria: informativepropose measures to quantify them.",{}
"More comprehensively, we incorporate all the criteria using two selection strategies, both of which result in less labeling cost than single-criterion-based method.",{}
The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80% without degrading the performance.,{}
"In the machine learning approaches of natural language processing (NLP), models are generally trained on large annotated corpus.","{'title': '1 Introduction', 'number': '1'}"
"However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain.","{'title': '1 Introduction', 'number': '1'}"
"In order to overcome this difficulty, active learning (sample selection) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classification (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al.","{'title': '1 Introduction', 'number': '1'}"
1999; Tang et al. 2002; Steedman et al.,"{'title': '1 Introduction', 'number': '1'}"
"2003), noun phrase chunking (Ngai and Yarowsky 2000), etc.","{'title': '1 Introduction', 'number': '1'}"
Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available.,"{'title': '1 Introduction', 'number': '1'}"
This assumption is valid in most NLP tasks.,"{'title': '1 Introduction', 'number': '1'}"
"Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model.","{'title': '1 Introduction', 'number': '1'}"
This procedure is repeated until the model achieves a certain level of performance.,"{'title': '1 Introduction', 'number': '1'}"
"Practically, a batch of examples are selected at a time, called batchedbased sample selection (Lewis and Catlett 1994) since it is time consuming to retrain the model if only one new example is added to the training set.","{'title': '1 Introduction', 'number': '1'}"
Many existing work in the area focus on two approaches: certainty-based methods (Thompson et al. 1999; Tang et al.,"{'title': '1 Introduction', 'number': '1'}"
2002; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003) and committee-based methods (McCallum and Nigam 1998; Engelson and Dagan 1999; Ngai and Yarowsky 2000) to select the most informative examples for which the current model are most uncertain.,"{'title': '1 Introduction', 'number': '1'}"
"Being the first piece of work on active learning for name entity recognition (NER) task, we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach.","{'title': '1 Introduction', 'number': '1'}"
"For this purpose, we make a more comprehensive consideration on the contribution of individual examples, and more importantly maximizing the contribution of a batch based on three criteria: informativeness, representativeness and diversity.","{'title': '1 Introduction', 'number': '1'}"
"First, we propose three scoring functions to quantify the informativeness of an example, which can be used to select the most uncertain examples.","{'title': '1 Introduction', 'number': '1'}"
"Second, the representativeness measure is further proposed to choose the examples representing the majority.","{'title': '1 Introduction', 'number': '1'}"
"Third, we propose two diversity considerations (global and local) to avoid repetition among the examples of a batch.","{'title': '1 Introduction', 'number': '1'}"
"Finally, two combination strategies with the above three criteria are proposed to reach the maximum effectiveness on active learning for NER.","{'title': '1 Introduction', 'number': '1'}"
We build our NER model using Support Vector Machines (SVM).,"{'title': '1 Introduction', 'number': '1'}"
The experiment shows that our active learning methods achieve a promising result in this NER task.,"{'title': '1 Introduction', 'number': '1'}"
The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80% without degrading the quality of the named entity recognizer.,"{'title': '1 Introduction', 'number': '1'}"
"The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria.","{'title': '1 Introduction', 'number': '1'}"
"To our knowledge, it is the first work on considering the three criteria all together for active learning.","{'title': '1 Introduction', 'number': '1'}"
"Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well.","{'title': '1 Introduction', 'number': '1'}"
"Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
2003).,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In NER, SVM is to classify a word into positive class “1” indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The semantic trigger features consist of some special head nouns for an entity class which is supplied by users.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Furthermore, a window (size = 7), which represents the local context of the target word w, is also used to classify w. However, for active learning in NER, it is not reasonable to select a single word without context for human to label.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Even if we require human to label a single word, he has to make an addition effort to refer to the context of the word.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Therefore, all of the measures we propose for active learning should be applied to the machineannotated named entities and we have to further study how to extend the measures for words to named entities.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Thus, the active learning in SVMbased NER will be more complex than that in simple classification tasks, such as text classification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
We prefer the examples with high informative degree for which the current model are most uncertain.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In the simplest linear form, training SVM is to find a hyperplane that can separate the positive and negative examples in training set with maximum margin.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The margin is defined by the distance of the hyperplane to the nearest of the positive and negative examples.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The training examples which are closest to the hyperplane are called support vectors.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In SVM, only the support vectors are useful for the classification, which is different from statistical models.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The support vectors can later be used to classify the test data.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Intuitively, we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyperplane (equal to 1).,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
This intuition is also justified by (Schohn and Cohn 2000; Tong and Koller 2000) based on a version space analysis.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In our task, we use the distance to measure the informativeness of an example.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The distance of a word’s feature vector to the hyperplane is computed as follows: where w is the feature vector of the word, ai, yi, si corresponds to the weight, the class and the feature vector of the ith support vector respectively.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
N is the number of the support vectors in current model.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"We select the example with minimal Dist, which indicates that it comes closest to the hyperplane in feature space.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
This example is considered most informative for current model.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Based on the above informativeness measure for a word, we compute the overall informativeness degree of a named entity NE.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In this paper, we propose three scoring functions as follows.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Let NE = w1...wN in which wi is the feature vector of the ith word of NE. where, wi is the feature vector of the ith word in NE.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In Section 4.3, we will evaluate the effectiveness of these scoring functions.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In addition to the most informative example, we also prefer the most representative example.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The representativeness of an example can be evaluated based on how many examples there are similar or near to it.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"So, the examples with high representative degree are less likely to be an outlier.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
Adding them to the training set will have effect on a large number of unlabeled examples.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"There are only a few works considering this selection criterion (McCallum and Nigam 1998; Tang et al. 2002) and both of them are specific to their tasks, viz. text classification and statistical parsing.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In this section, we compute the similarity between words using a general vector-based measure, extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In general vector space model, the similarity between two vectors may be measured by computing the cosine value of the angle between them.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The smaller the angle is, the more similar between the vectors are.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"This measure, called cosine-similarity measure, has been widely used in information retrieval tasks (Baeza-Yates and Ribeiro-Neto 1999).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In our task, we also use it to quantify the similarity between two words.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Particularly, the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) .","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Therefore, we adapt the cosine-similarity measure to SVM as follows: (wi,wj)=k (wi , wi)k(wj , wj ) where, wi and wj are the feature vectors of the words i and j.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
This calculation is also supported by (Brinker 2003)’s work.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Furthermore, if we use the linear kernel k(wi, wj) = wi ⋅ w j , the measure is the same as the traditional cosine similarity measgeneral vector-based similarity measure. tities In this part, we compute the similarity between two machine-annotated named entities given the similarities between words.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Regarding an entity as a word sequence, this work is analogous to the alignment of two sequences.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
We employ the dynamic time warping (DTW) algorithm (Rabiner et al. 1978) to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Here, we adapt it to our task.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
A sketch of the modified algorithm is as follows.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Let NE1 = w11w12...w1n...w1N, (n = 1,..., N) and NE2 = w21w22...w2m...w2M, (m = 1,..., M) denote two word sequences to be matched.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
NE1 and NE2 consist of M and N words respectively.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
NE1(n) = w1n and NE2(m) = w2m.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"A similarity value Sim(w1n ,w2m) has been known for every pair of words (w1n,w2m) within NE1 and NE2.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The goal of DTW is to find a path, m = map(n), which map n onto the corresponding m such that the accumulated similarity Sim* along the path is maximized. map n n= A dynamic programming method is used to determine the optimum path map(n).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The accumulated similarity SimA to any grid point (n, m) can be recursively calculated as Certainly, the overall similarity measure Sim* has to be normalized as longer sequences normally give higher similarity value.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"So, the similarity between two sequences NE1 and NE2 is calculated as and may be regarded as a Entity Given a set of machine-annotated named entities NESet = {NE1, .","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
".., NEN}, the representativeness of a named entity NEi in NESet is quantified by its density.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The density of NEi is defined as the average similarity between NEi and all the other entities NEj in NESet as follows.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"If NEi has the largest density among all the entities in NESet, it can be regarded as the centroid of NESet and also the most representative examples in NESet.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
Diversity criterion is to maximize the training utility of a batch.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
We prefer the batch in which the examples have high variance to each other.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"For example, given the batch size 5, we try not to select five repetitious examples at a time.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"To our knowledge, there is only one work (Brinker 2003) exploring this criterion.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In our task, we propose two methods: local and global, to make the examples diverse enough in a batch.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"For a global consideration, we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The named entities in the same cluster may be considered similar to each other, so we will select the named entities from different clusters at one time.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"We employ a Kmeans clustering algorithm (Jelinek 1997), which is shown in Figure 1.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The number of clusters is K Initialization: Randomly equally partition {NE1, ..., NEN} into K initial clusters Cj (j = 1, ... , K).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Loop until the number of changes for the centroids of all clusters is less than a threshold In each round, we need to compute the pairwise similarities within each cluster to get the centroid of the cluster.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"And then, we need to compute the similarities between each example and all centroids to repartition the examples.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"So, the algorithm is time-consuming.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"Based on the assumption that N examples are uniformly distributed between the K clusters, the time complexity of the algorithm is about O(N2/K+NK) (Tang et al. 2002).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In one of our experiments, the size of the NESet (N) is around 17000 and K is equal to 50, so the time complexity is about O(106).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"For efficiency, we may filter the entities in NESet before clustering them, which will be further discussed in Section 3.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"When selecting a machine-annotated named entity, we compare it with all previously selected named entities in the current batch.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"If the similarity between them is above a threshold 8, this example cannot be allowed to add into the batch.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"The order of selecting examples is based on some measure, such as informativeness measure, representativeness measure or their combination.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
This local selection method is shown in Figure 2.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In this way, we avoid selecting too similar examples (similarity value ≥ 8) in a batch.","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
The threshold 8 may be the average similarity between the examples in NESet.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
This consideration only requires O(NK+K2) computational time.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In one of our experiments (N ˜ 17000 and K = 50), the time complexity is about O(105).","{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
It is more efficient than clustering algorithm described in Section 2.3.1.,"{'title': '2 Multi-criteria for NER Active Learning', 'number': '2'}"
"In this section, we will study how to combine and strike a proper balance between these criteria, viz. informativeness, representativeness and diversity, to reach the maximum effectiveness on NER active learning.","{'title': '3 Sample Selection strategies', 'number': '3'}"
We build two strategies to combine the measures proposed above.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
These strategies are based on the varying priorities of the criteria and the varying degrees to satisfy the criteria. most informativeness score from NESet to an intermediate set called INTERSet.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
"By this preselecting, we make the selection process faster in the later steps since the size of INTERSet is much smaller than that of NESet.","{'title': '3 Sample Selection strategies', 'number': '3'}"
Then we cluster the examples in INTERSet and choose the centroid of each cluster into a batch called BatchSet.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
The centroid of a cluster is the most representative example in that cluster since it has the largest density.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
"Furthermore, the examples in different clusters may be considered diverse to each other.","{'title': '3 Sample Selection strategies', 'number': '3'}"
"By this means, we consider representativeness and diversity criteria at the same time.","{'title': '3 Sample Selection strategies', 'number': '3'}"
This strategy is shown in Figure 3.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
One limitation of this strategy is that clustering result may not reflect the distribution of whole sample space since we only cluster on INTERSet for efficiency.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
The other is that since the representativeness of an example is only evaluated on a cluster.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
"If the cluster size is too small, the most representative example in this cluster may not be representative in the whole sample space.","{'title': '3 Sample Selection strategies', 'number': '3'}"
"Given: NESet = {NE1, .","{'title': '3 Sample Selection strategies', 'number': '3'}"
".., NEN} BatchSet with the maximal size K. INTERSet with the maximal size M Steps: which the Info and Density value of NEi are normalized first.","{'title': '3 Sample Selection strategies', 'number': '3'}"
The individual importance of each criterion in this function is adjusted by the tradeoff parameter l ( 0 ≤ l ≤1 ) (set to 0.6 in our experiment).,"{'title': '3 Sample Selection strategies', 'number': '3'}"
"First, we select a candidate example NEi with the maximum value of this function from NESet.","{'title': '3 Sample Selection strategies', 'number': '3'}"
"Second, we consider diversity criterion using the local method in Section 3.3.2.","{'title': '3 Sample Selection strategies', 'number': '3'}"
We add the candidate example NEi to a batch only if NEi is different enough from any previously selected example in the batch.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
The threshold ß is set to the average pair-wise similarity of the entities in NESet.,"{'title': '3 Sample Selection strategies', 'number': '3'}"
"In order to evaluate the effectiveness of our selection strategies, we apply them to recognize protein (PRT) names in biomedical domain using GENIA corpus V1.1 (Ohta et al. 2002) and person (PER), location (LOC), organization (ORG) names in newswire domain using MUC-6 corpus.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"First, we randomly split the whole corpus into three parts: an initial training set to build an initial model, a test set to evaluate the performance of the model and an unlabeled set to select examples.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
The size of each data set is shown in Table 1.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Then, iteratively, we select a batch of examples following the selection strategies proposed, require human experts to label them and add them into the training set.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
The batch size K = 50 in GENIA and 10 in MUC-6.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
Each example is defined as a machine-recognized named entity and its context words (previous 3 words and next 3 words).,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
Domain Class Corpus Initial Training Set Test Set Unlabeled Set Biomedical PRT GENIA1.1 10 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(277 words) 900 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(26K words) 8004 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(223K words) Newswire PER MUC-6 5 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(131 words) 602 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(14K words) 7809 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(157K words) LOC 5 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(130 words) 7809 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(157K words) ORG 5 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(113 words) 7809 sent.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
(157K words) The goal of our work is to minimize the human annotation effort to learn a named entity recognizer with the same performance level as supervised learning.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
The performance of our model is evaluated using “precision/recall/F-measure”.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In this section, we evaluate our selection strategies by comparing them with a random selection method, in which a batch of examples is randomly selected iteratively, on GENIA and MUC-6 corpus.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Table 2 shows the amount of training data needed to achieve the performance of supervised learning using various selection methods, viz.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Random, Strategy1 and Strategy2.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In GENIA, we find: Furthermore, when we apply our model to newswire domain (MUC-6) to recognize person, location and organization names, Strategy1 and Strategy2 show a more promising result by comparing with the supervised learning and Random, as shown in Table 2.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"On average, about 95% of the data can be reduced to achieve the same performance with the supervised learning in MUC-6.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
It is probably because NER in the newswire domain is much simpler than that in the biomedical domain (Shen et al. 2003) and named entities are less and distributed much sparser in the newswire texts than in the biomedical texts.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In this section, we investigate the effectiveness of informativeness criterion in NER task.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Figure 5 shows a plot of training data size versus F-measure achieved by the informativeness-based measures in Section 3.1.2: Info_Avg, Info_Min and Info_S/N as well as Random.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
We make the comparisons in GENIA corpus.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In Figure 5, the horizontal line is the performance level (63.3 F-measure) achieved by supervised learning (223K words).","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
We find that the three informativeness-based measures perform similarly and each of them outperforms Random.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
Table 3 highlights the various data sizes to achieve the peak performance using these selection methods.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
We find that Random (83K words) on average requires over 1.5 times as much as data to achieve the same performance as the informativeness-based selection methods (52K words).,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In addition to the informativeness criterion, we further incorporate representativeness and diversity criteria into active learning using two strategies described in Section 3.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Comparing the two strategies with the best result of the single-criterionbased selection methods Info_Min, we are to justify that representativeness and diversity are also important factors for active learning.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Figure 6 shows the learning curves for the various methods: Strategy1, Strategy2 and Info_Min.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In the beginning iterations (F-measure < 60), the three methods performed similarly.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"But with the larger training set, the efficiencies of Stratety1 and Strategy2 begin to be evident.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
Table 4 highlights the final result of the three methods.,"{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"In order to reach the performance of supervised learning, Strategy1 (40K words) and Strategyy2 (31K words) require about 80% and 60% of the data that Info_Min (51.9K) does.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"So we believe the effective combinations of informativeness, representativeness and diversity will help to learn the NER model more quickly and cost less in annotation.","{'title': '4 Experimental Results and Analysis', 'number': '4'}"
"Since there is no study on active learning for NER task previously, we only introduce general active learning methods here.","{'title': '5 Related Work', 'number': '5'}"
Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000).,"{'title': '5 Related Work', 'number': '5'}"
Our informativeness-based measure is similar to these works.,"{'title': '5 Related Work', 'number': '5'}"
However these works just follow a single criterion.,"{'title': '5 Related Work', 'number': '5'}"
(McCallum and Nigam 1998; Tang et al. 2002) are the only two works considering the representativeness criterion in active learning.,"{'title': '5 Related Work', 'number': '5'}"
(Tang et al. 2002) use the density information to weight the selected examples while we use it to select examples.,"{'title': '5 Related Work', 'number': '5'}"
"Moreover, the representativeness measure we use is relatively general and easy to adapt to other tasks, in which the example selected is a sequence of words, such as text chunking, POS tagging, etc.","{'title': '5 Related Work', 'number': '5'}"
"On the other hand, (Brinker 2003) first incorporate diversity in active learning for text classification.","{'title': '5 Related Work', 'number': '5'}"
Their work is similar to our local consideration in Section 2.3.2.,"{'title': '5 Related Work', 'number': '5'}"
"However, he didn’t further explore how to avoid selecting outliers to a batch.","{'title': '5 Related Work', 'number': '5'}"
"So far, we haven’t found any previous work integrating the informativeness, representativeness and diversity all together.","{'title': '5 Related Work', 'number': '5'}"
"In this paper, we study the active learning in a more complex NLP task, named entity recognition.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"We propose a multi-criteria-based approach to select examples based on their informativeness, representativeness and diversity, which are incorporated all together by two strategies (local and global).","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Experiments show that, in both MUC6 and GENIA, both of the two strategies combining the three criteria outperform the single criterion (informativeness).","{'title': '6 Conclusion and Future Work', 'number': '6'}"
The labeling cost can be significantly reduced by at least 80% comparing with the supervised learning.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
"To our best knowledge, this is not only the first work to report the empirical results of active learning for NER, but also the first work to incorporate the three criteria all together for selecting examples.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Although the current experiment results are very promising, some parameters in our experiment, such as the batch size K and the X in the function of strategy 2, are decided by our experience in the domain.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"In practical application, the optimal value of these parameters should be decided automatically based on the training process.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Furthermore, we will study how to overcome the limitation of the strategy 1 discussed in Section 3 by using more effective clustering algorithm.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
Another interesting work is to study when to stop active learning.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
