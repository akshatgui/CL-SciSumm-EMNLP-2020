col1,col2
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.,Introduction
"These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.",Introduction
"For example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.",Introduction
"Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.",Introduction
We describe and evaluate a method for performing backwards transliterations by machine.,Introduction
"This method uses a generative model, incorporating several distinct stages in the transliteration process.",Introduction
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories.,Experiment/Discussion
"These items are commonly transliterated, i.e., replaced with approximate phonetic equivalents.",Experiment/Discussion
"For example, &quot;computer&quot; in English comes out as &quot;konpyuutaa&quot; in Japanese.",Experiment/Discussion
"Translating such items from Japanese back to English is even more challenging, and of practical interest, as transliterated items make up the bulk of text phrases not found in bilingual dictionaries.",Experiment/Discussion
We describe and evaluate a method for performing backwards transliterations by machine.,Experiment/Discussion
"This method uses a generative model, incorporating several distinct stages in the transliteration process.",Experiment/Discussion
One of the most frequent problems translators must deal with is translating proper names and technical terms.,Experiment/Discussion
"For language pairs like Spanish/English, this presents no great challenge: a phrase like Antonio Gil usually gets translated as Antonio Gil.",Experiment/Discussion
"However, the situation is more complicated for language pairs that employ very different alphabets and sound systems, such as Japanese/English and Arabic/English.",Experiment/Discussion
Phonetic translation across these pairs is called transliteration.,Experiment/Discussion
We will look at Japanese/English transliteration in this article.,Experiment/Discussion
"Japanese frequently imports vocabulary from other languages, primarily (but not exclusively) from English.",Experiment/Discussion
"It has a special phonetic alphabet called katakana, which is used primarily (but not exclusively) to write down foreign names and loanwords.",Experiment/Discussion
"The katakana symbols are shown in Figure 1, with their Japanese pronunciations.",Experiment/Discussion
"The two symbols shown in the lower right corner ( —, ) are used to lengthen any Japanese vowel or consonant.",Experiment/Discussion
"To write a word like golfbag in katakana, some compromises must be made.",Experiment/Discussion
"For example, Japanese has no distinct L and R. sounds: the two English sounds collapse onto the same Japanese sound.",Experiment/Discussion
"A similar compromise must be struck for English H and F. Also, Japanese generally uses an alternating consonant-vowel structure, making it impossible to pronounce LFB without intervening vowels.",Experiment/Discussion
"Katakana writing is a syllabary rather than an alphabet—there is one symbol for ga (If), another for gi eV ), another for gu ( 7 ), etc.",Experiment/Discussion
"So the way to write golfbag in katakana is ''''-' 7 'Z Y , roughly pronounced go-ru-hu-ba-ggu.",Experiment/Discussion
Here are a few more examples: Katakana symbols and their Japanese pronunciations.,Experiment/Discussion
Angela Johnson New York Times ice cream Notice how the transliteration is more phonetic than orthographic; the letter h in Johnson does not produce any katakana.,Experiment/Discussion
"Also, a dot-separator (o) is used to separate words, but not consistently.",Experiment/Discussion
"And transliteration is clearly an information-losing operation: ranpu could come from either lamp or ramp, while aisukuriimu loses the distinction between ice cream and I scream.",Experiment/Discussion
"Transliteration is not trivial to automate, but we will be concerned with an even more challenging problem—going from katakana back to English, i.e., back-transliteration.",Experiment/Discussion
Human translators can often &quot;sound out&quot; a katakana phrase to guess an appropriate translation.,Experiment/Discussion
Automating this process has great practical importance in Japanese/English machine translation.,Experiment/Discussion
Katakana phrases are the largest source of text phrases that do not appear in bilingual dictionaries or training corpora (a.k.a.,Experiment/Discussion
"&quot;notfound words&quot;), but very little computational work has been done in this area.",Experiment/Discussion
"Yamron et al. (1994) briefly mention a pattern-matching approach, while Arbabi et al.",Experiment/Discussion
(1994) discuss a hybrid neural-net/expert-system approach to (forward) transliteration.,Experiment/Discussion
The information-losing aspect of transliteration makes it hard to invert.,Experiment/Discussion
"Here are some problem instances, taken from actual newspaper articles: English translations appear later in this article.",Experiment/Discussion
Here are a few observations about back-transliteration that give an idea of the difficulty of the task: The most desirable feature of an automatic back-transliterator is accuracy.,Experiment/Discussion
"If possible, our techniques should also be: Like most problems in computational linguistics, this one requires full world knowledge for a 100% solution.",Experiment/Discussion
Choosing between Katarina and Catalina (both good guesses for 9 ) might even require detailed knowledge of geography and figure skating.,Experiment/Discussion
"At that level, human translators find the problem quite difficult as well, so we only aim to match or possibly exceed their performance.",Experiment/Discussion
"Bilingual glossaries contain many entries mapping katakana phrases onto English phrases, e.g., (aircraft carrier 2 7 is t &quot;I' 9 7 ).",Experiment/Discussion
"It is possible to automatically analyze such pairs to gain enough knowledge to accurately map new katakana phrases that come along, and this learning approach travels well to other language pairs.",Experiment/Discussion
"A naive approach to finding direct correspondences between English letters and katakana symbols, however, suffers from a number of problems.",Experiment/Discussion
One can easily wind up with a system that proposes iskrym as a back-transliteration of aisukuriimu.,Experiment/Discussion
Taking letter frequencies into account improves this to a more plausible-looking isclim.,Experiment/Discussion
"Moving to real words may give is crime: the i corresponds to ai, the s corresponds to su, etc.",Experiment/Discussion
"Unfortunately, the correct answer here is ice cream.",Experiment/Discussion
"After initial experiments along these lines, we stepped back and built a generative model of the transliteration process, which goes like this: This divides our problem into five subproblems.",Experiment/Discussion
"Fortunately, there are techniques for coordinating solutions to such subproblems, and for using generative models in the reverse direction.",Experiment/Discussion
These techniques rely on probabilities and Bayes' theorem.,Experiment/Discussion
Suppose we build an English phrase generator that produces word sequences according to some probability distribution P(w).,Experiment/Discussion
"And suppose we build an English pronouncer that takes a word sequence and assigns it a set of pronunciations, again probabilistically, according to some P(pi w).",Experiment/Discussion
"Given a pronunciation p, we may want to search for the word sequence w that maximizes P(w Ip).",Experiment/Discussion
"Bayes' theorem lets us equivalently maximize P(w) • P(plw), exactly the two distributions we have modeled.",Experiment/Discussion
"Extending this notion, we settled down to build five probability distributions: Given a katakana string o observed by OCR, we want to find the English word sequence w that maximizes the sum, over all e, j, and k, of of the models in turn.",Experiment/Discussion
The result is a large WFSA containing all possible English translations.,Experiment/Discussion
We have implemented two algorithms for extracting the best translations.,Experiment/Discussion
The first is Dijkstra's shortest-path graph algorithm (Dijkstra 1959).,Experiment/Discussion
"The second is a recently discovered k-shortest-paths algorithm (Eppstein 1994) that makes it possible for us to identify the top k translations in efficient 0(m + n log n + kn) time, where the WFSA contains n states and m arcs.",Experiment/Discussion
The approach is modular.,Experiment/Discussion
We can test each engine independently and be confident that their results are combined correctly.,Experiment/Discussion
"We do no pruning, so the final WFSA contains every solution, however unlikely.",Experiment/Discussion
"The only approximation is the Viterbi one, which searches for the best path through a WFSA instead of the best sequence (i.e., the same sequence does not receive bonus points for appearing more than once).",Experiment/Discussion
This section describes how we designed and built each of our five models.,Experiment/Discussion
"For consistency, we continue to print written English word sequences in italics (golf ball), English sound sequences in all capitals (G AA L F B AO L), Japanese sound sequences in lower case (goruhubooru) and katakana sequences naturally The first model generates scored word sequences, the idea being that ice cream should score higher than ice creme, which should score higher than aice kreem.",Experiment/Discussion
We adopted a simple unigram scoring method that multiplies the scores of the known words and phrases in a sequence.,Experiment/Discussion
"Our 262,000-entry frequency list draws its words and phrases from the Wall Street Journal corpus, an on-line English name list, and an on-line gazetteer of place names.'",Experiment/Discussion
A portion of the WFSA looks like this: los / 0.000087 month I 0.000992 An ideal word sequence model would look a bit different.,Experiment/Discussion
It would prefer exactly those strings which are actually grist for Japanese transliterators.,Experiment/Discussion
"For example, people rarely transliterate auxiliary verbs, but surnames are often transliterated.",Experiment/Discussion
"We have approximated such a model by removing high-frequency words like has, an, are, am, were, their, and does, plus unlikely words corresponding to Japanese sound bites, like coup and oh.",Experiment/Discussion
We also built a separate word sequence model containing only English first and last names.,Experiment/Discussion
"If we know (from context) that the transliterated phrase is a personal name, this model is more precise.",Experiment/Discussion
The next WFST converts English word sequences into English sound sequences.,Experiment/Discussion
"We use the English phoneme inventory from the on-line CMU Pronunciation Dictiofederal I 0.0013 nary, minus the stress marks.2 This gives a total of 40 sounds, including 14 vowel sounds (e.g., AA, AE, UW), 25 consonant sounds (e.g., K, HH, R), plus one special symbol (PAUSE).",Experiment/Discussion
"The dictionary has pronunciations for 110,000 words, and we organized a tree-based WFST from it: Note that we insert an optional PAUSE between word pronunciations.",Experiment/Discussion
"We originally thought to build a general letter-to-sound WFST (Divay and Vitale 1997), on the theory that while wrong (overgeneralized) pronunciations might occasionally be generated, Japanese transliterators also mispronounce words.",Experiment/Discussion
"However, our letter-to-sound WFST did not match the performance of Japanese transliterators, and it turns out that mispronunciations are modeled adequately in the next stage of the cascade.",Experiment/Discussion
"Next, we map English sound sequences onto Japanese sound sequences.",Experiment/Discussion
"This is an inherently information-losing process, as English R and L sounds collapse onto Japanese r, the 14 English vowel sounds collapse onto the 5 Japanese vowel sounds, etc.",Experiment/Discussion
"We face two immediate problems: An obvious target inventory is the Japanese syllabary itself, written down in katakana (e.g., -= ) or a roman equivalent (e.g., ni).",Experiment/Discussion
"With this approach, the English sound K corresponds to one of t (ka), (ki), (ku), (ke), or (ko), depending on its context.",Experiment/Discussion
"Unfortunately, because katakana is a syllabary, we would be unable to express an obvious and useful generalization, namely that English K usually corresponds to Japanese k, independent of context.",Experiment/Discussion
"Moreover, the correspondence of Japanese katakana writing to Japanese sound sequences is not perfectly one-to-one (see Section 3.4), so an independent sound inventory is well-motivated in any case.",Experiment/Discussion
"Our Japanese sound inventory includes 39 symbols: 5 vowel sounds, 33 consonant sounds (including doubled consonants like kk), and one special symbol (pause).",Experiment/Discussion
An English sound sequence like (P R OW PAUSE S AA K ER) might map onto a Japanese sound sequence like (p u r o pause s a kk a a).,Experiment/Discussion
Note that long Japanese vowel sounds Knight and Graehl Machine Transliteration are written with two symbols (a a) instead of just one (aa).,Experiment/Discussion
This scheme is attractive because Japanese sequences are almost always longer than English sequences.,Experiment/Discussion
"Our WFST is learned automatically from 8,000 pairs of English/Japanese sound sequences, e.g., ((S AA K ER) (s a kk a a)).",Experiment/Discussion
We were able to produce these pairs by manipulating a small English-katakana glossary.,Experiment/Discussion
"For each glossary entry, we converted English words into English sounds using the model described in the previous section, and we converted katakana words into Japanese sounds using the model we describe in the next section.",Experiment/Discussion
"We then applied the estimation-maximization (EM) algorithm (Baum 1972; Dempster, Laird, and Rubin 1977) to generate symbol-mapping probabilities, shown in Figure 2.",Experiment/Discussion
Our EM training goes like this: alignments between their elements.,Experiment/Discussion
"In our case, an alignment is a drawing that connects each English sound with one or more Japanese sounds, such that all Japanese sounds are covered and no lines cross.",Experiment/Discussion
"For example, there are two ways to align the pair ( (L OW) <-> (r o 0)): In this case, the alignment on the left is intuitively preferable.",Experiment/Discussion
The algorithm learns such preferences.,Experiment/Discussion
2.,Experiment/Discussion
"For each pair, assign an equal weight to each of its alignments, such that those weights sum to 1.",Experiment/Discussion
"In the case above, each alignment gets a weight of 0.5.",Experiment/Discussion
PAUSE:pause Our WFST has 99 states and 283 arcs.,Experiment/Discussion
"English sounds (in capitals) with probabilistic mappings to Japanese sound sequences (in lower case), as learned by estimation-maximization.",Experiment/Discussion
"Only mappings with conditional probabilities greater than 1% are shown, so the figures may not sum to 1.",Experiment/Discussion
"We have also built models that allow individual English sounds to be &quot;swallowed&quot; (i.e., produce zero Japanese sounds).",Experiment/Discussion
"However, these models are expensive to compute (many more alignments) and lead to a vast number of hypotheses during WFST composition.",Experiment/Discussion
"Furthermore, in disallowing &quot;swallowing,&quot; we were able to automatically remove hundreds of potentially harmful pairs from our training set, e.g., ( (B AA R B ER SH AA P) 4-* (b aab a a) ).",Experiment/Discussion
"Because no alignments are possible, such pairs are skipped by the learning algorithm; cases like these must be solved by dictionary Alignments between English and Japanese sound sequences, as determined by EM training.",Experiment/Discussion
"Best alignments are shown for the English words biscuit, divider, and filter. lookup anyway.",Experiment/Discussion
"Only two pairs failed to align when we wished they had—both involved turning English Y UW into Japanese u, as in ((Y UW K AH L EY L 1Y) +-4 (u k urere)).",Experiment/Discussion
Note also that our model translates each English sound without regard to context.,Experiment/Discussion
"We have also built context-based models, using decision trees recoded as WFSTs.",Experiment/Discussion
"For example, at the end of a word, English T is likely to come out as (t o) rather than (t).",Experiment/Discussion
"However, context-based models proved unnecessary for back-transliteration.",Experiment/Discussion
They are more useful for English-to-Japanese forward transliteration.,Experiment/Discussion
"To map Japanese sound sequences like (m o ot a a) onto katakana sequences like ), we manually constructed two WFSTs.",Experiment/Discussion
"Composed together, they yield an integrated WFST with 53 states and 303 arcs, producing a katakana inventory containing 81 symbols, including the dot-separator (.).",Experiment/Discussion
"The first WFST simply merges long Japanese vowel sounds into new symbols aa, ii, uu, ee, and oo.",Experiment/Discussion
The second WFST maps Japanese sounds onto katakana symbols.,Experiment/Discussion
The basic idea is to consume a whole syllable worth of sounds before producing any katakana.,Experiment/Discussion
For example: This fragment shows one kind of spelling variation in Japanese: long vowel sounds (00) are usually written with a long vowel mark ( 21&quot; ) but are sometimes written with repeated katakana ( 71- 71.).,Experiment/Discussion
We combined corpus analysis with guidelines from a Japanese textbook (Jorden and Chaplin 1976) to turn up many spelling variations and unusual katakana symbols: and so on.,Experiment/Discussion
"Spelling variation is clearest in cases where an English word like switch shows up transliterated variously ( 4 ‘2 4 'Y , 7 9 4 'Y ) in different dictionaries.",Experiment/Discussion
Treating these variations as an equivalence class enables us to learn general sound mappings even if our bilingual glossary adheres to a single narrow spelling convention.,Experiment/Discussion
"We do not, however, generate all katakana sequences with this model; for example, we do not output strings that begin with a subscripted vowel katakana.",Experiment/Discussion
"So this model also serves to filter out some ill-formed katakana sequences, possibly proposed by optical character recognition.",Experiment/Discussion
"Perhaps uncharitably, we can view optical character recognition (OCR) as a device that garbles perfectly good katakana sequences.",Experiment/Discussion
"Typical confusions made by our commercial OCR system include t: for 71. for , 7 for 7, and 7 for I.",Experiment/Discussion
"To generate pre-OCR text, we collected 19,500 characters worth of katakana words, stored them in a file, and printed them out.",Experiment/Discussion
"To generate post-OCR text, we OCR'd the printouts.",Experiment/Discussion
We then ran the EM algorithm to determine symbol-mapping (&quot;garbling&quot;) probabilities.,Experiment/Discussion
"Here is part of that table: This model outputs a superset of the 81 katakana symbols, including spurious quote marks, alphabetic symbols, and the numeral 7.3 We can now use the models to do a sample back-transliteration.",Experiment/Discussion
We start with a katakana phrase as observed by OCR.,Experiment/Discussion
"We then serially compose it with the models, in reverse order.",Experiment/Discussion
Each intermediate stage is a WFSA that encodes many possibilities.,Experiment/Discussion
"The final stage contains all back-transliterations suggested by the models, and we finally extract the best one.",Experiment/Discussion
We start with the masutaazutoonamento problem from Section 1.,Experiment/Discussion
"Our OCR observes: This string has two recognition errors: (ku) for (t a), and (chi) for (ha).",Experiment/Discussion
We turn the string into a chained 12-state/11-arc WFSA and compose it with the P(Iclo) model.,Experiment/Discussion
"This yields a fatter 12-state/15-arc WFSA, which accepts the correct spelling at a lower probability.",Experiment/Discussion
"Next comes the POO model, which produces a 28-state/31-arc WFSA whose highest-scoring sequence is: masutaazutoochimento Next comes P(elj), yielding a 62-state/241-arc WFSA whose best sequence is: Next to last comes P(w le), which results in a 2982-state/4601-arc WFSA whose best sequence (out of roughly three hundred million) is: masters tone am ent awe This English string is closest phonetically to the Japanese, but we are willing to trade phonetic proximity for more sensical English; we rescore this WFSA by composing it with P(w) and extract the best translation: Other Section 1 examples (aasudee and robaato shyoon renaado) are translated correctly as earth day and robert sean leonard.",Experiment/Discussion
We may also be interested in the k best translations.,Experiment/Discussion
"In fact, after any composition, we can inspect several high-scoring sequences using the algorithm of Eppstein (1994).",Experiment/Discussion
Given the following katakana input phrase: Inspecting the k-best list is useful for diagnosing problems with the models.,Experiment/Discussion
"If the right answer appears low in the list, then some numbers are probably off somewhere.",Experiment/Discussion
"If the right answer does not appear at all, then one of the models may be missing a word or suffer from some kind of brittleness.",Experiment/Discussion
"A k-best list can also be used as input to a later context-based disambiguator, or as an aid to a human translator.",Experiment/Discussion
"We have performed two large-scale experiments, one using a full-language P(w) model, and one using a personal name language model.",Experiment/Discussion
"In the first experiment, we extracted 1,449 unique katakana phrases from a corpus of 100 short news articles.",Experiment/Discussion
"Of these, 222 were missing from an on-line 100,000-entry bilingual dictionary.",Experiment/Discussion
We back-transliterated these 222 phrases.,Experiment/Discussion
"Many of the translations are perfect: technical program, sex scandal, omaha beach, new york times, ramon diaz.",Experiment/Discussion
"Others are close: tanya harding, nickel simpson, danger washington, world cap.",Experiment/Discussion
"Some miss the mark: nancy care again, plus occur, patriot miss rea1.4 While it is difficult to judge overall accuracy—some of the phrases are onomatopoetic, and others are simply too hard even for good human translators—it is easier to identify system weaknesses, and most of these lie in the P(w) model.",Experiment/Discussion
"For example, nancy kerrigan should be preferred over nancy care again.",Experiment/Discussion
"In a second experiment, we took (non-OCR) katakana versions of the names of 100 U.S. politicians, e.g.",Experiment/Discussion
": ' 1 (jyon.buroo), 7)1&quot;1&quot; 7 (aruhonsu. damatto), and -Q.",Experiment/Discussion
4 ' 7 .s/ (maiku.dewain).,Experiment/Discussion
We back-transliterated these by machine and asked four human subjects to do the same.,Experiment/Discussion
These subjects were native English speakers and news-aware; we gave them brief instructions.,Experiment/Discussion
The results were as in Table 1.,Experiment/Discussion
There is room for improvement on both sides.,Experiment/Discussion
"Being English speakers, the human subjects were good at English name spelling and U.S. politics, but not at Japanese phonetics.",Experiment/Discussion
A native Japanese speaker might be expert at the latter but not the former.,Experiment/Discussion
"People who are expert in all of these areas, however, are rare.",Experiment/Discussion
"On the automatic side, many errors can be corrected.",Experiment/Discussion
A first-name/last-name model would rank richard bryan more highly than richard brian.,Experiment/Discussion
A bigram model would prefer orren hatch over olin hatch.,Experiment/Discussion
"Other errors are due to unigram training problems, or more rarely, incorrect or brittle phonetic models.",Experiment/Discussion
"For example, Long occurs much more often than Ron in newspaper text, and our word selection does not exclude phrases like Long Island.",Experiment/Discussion
So we get long wyden instead of ron wyden.,Experiment/Discussion
One way to fix these problems is by manually changing unigram probabilities.,Experiment/Discussion
Reducing P(long) by a factor of ten solves the problem while maintaining a high score for P(long rongu).,Experiment/Discussion
"Despite these problems, the machine's performance is impressive.",Experiment/Discussion
"When word separators (p) are removed from the katakana phrases, rendering the task exceedingly difficult for people, the machine's performance is unchanged.",Experiment/Discussion
"In other words, it offers the same top-scoring translations whether or not the separators are present; however, their presence significantly cuts down on the number of alternatives considered, improving efficiency.",Experiment/Discussion
"When we use OCR, 7% of katakana tokens are misrecognized, affecting 50% of test strings, but translation accuracy only drops from 64% to 52%.",Experiment/Discussion
"In a 1947 memorandum, Weaver (1955) wrote: One naturally wonders if the problem of translation could conceivably be treated as a problem of cryptography.",Results/Conclusion
"When I look at an article in Russian, I say: &quot;This is really written in English, but it has been coded in some strange symbols.",Results/Conclusion
"I will now proceed to decode.&quot; (p. 18) Whether this is a useful perspective for machine translation is debatable (Brown et al. 1993; Knoblock 1996)—however, it is a dead-on description of transliteration.",Results/Conclusion
"Most katakana phrases really are English, ready to be decoded.",Results/Conclusion
"We have presented a method for automatic back-transliteration which, while far from perfect, is highly competitive.",Results/Conclusion
It also achieves the objectives outlined in Section 1.,Results/Conclusion
"It ports easily to new language pairs; the P(w) and P(ejw) models are entirely reusable, while other models are learned automatically.",Results/Conclusion
"It is robust against OCR noise, in a rare example of high-level language processing being useful (necessary, even) in improving low-level OCR.",Results/Conclusion
There are several directions for improving accuracy.,Results/Conclusion
The biggest problem is that raw English frequency counts are not the best indication of whether a word is a possible source for transliteration.,Results/Conclusion
Alternative data collection methods must be considered.,Results/Conclusion
We may also consider changes to the model sequence itself.,Results/Conclusion
"As we have presented it, our hypothetical human transliterator produces Japanese sounds from English sounds only, without regard for the original English spelling.",Results/Conclusion
This means that English homonyms will produce exactly the same katakana strings.,Results/Conclusion
"In reality, though, transliterators will sometimes key off spelling, so that tonya and tanya produce toonya and taanya.",Results/Conclusion
It might pay to carry along some spelling information in the English pronunciation lattices.,Results/Conclusion
Sentential context should be useful for determining correct translations.,Results/Conclusion
"It is often clear from a Japanese sentence whether a katakana phrase is a person, an institution, or a place.",Results/Conclusion
"In many cases it is possible to narrow things further—given the phrase &quot;such-and-such, Arizona,&quot; we can restrict our P(w) model to include only those cities and towns in Arizona.",Results/Conclusion
It is also interesting to consider transliteration for other languages.,Results/Conclusion
"In Arabic, for example, it is more difficult to identify candidates for transliteration because there is no distinct, explicit alphabet that marks them.",Results/Conclusion
"Furthermore, Arabic is usually written without vowels, so we must generate vowel sounds from scratch in order to produce correct English.",Results/Conclusion
"Finally, it may be possible to embed phonetic-shift models inside speech recognizers, to explicitly adjust for heavy foreign accents.",Results/Conclusion
"We would like to thank Alton Earl Ingram, Yolanda Gil, Bonnie Glover Stalls, Richard Whitney, Kenji Yamada, and the anonymous reviewers for their helpful comments.",Results/Conclusion
We would also like to thank our sponsors at the Department of Defense.,Results/Conclusion
