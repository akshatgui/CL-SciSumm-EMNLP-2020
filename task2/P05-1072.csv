col1,col2
Semantic role labeling is the process of annotating the predicate-argument structure in text with semantic labels.,{}
In this paper we present a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.,{}
"We show improvements on this system by: i) adding new features including features extracted from dependency parses, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.",{}
Error analysis of the baseline system showed that approximately half of the argument identification errors resulted from parse errors in which there was no syntactic constituent that aligned with the correct argument.,{}
"In order to address this problem, we combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.",{}
All of the reported techniques resulted in performance improvements.,{}
"Semantic Role Labeling is the process of annotating the predicate-argument structure in text with se* This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2004; Hacioglu, 2004).","{'title': '1 Introduction', 'number': '1'}"
"The architecture underlying all of these systems introduces two distinct sub-problems: the identification of syntactic constituents that are semantic roles for a given predicate, and the labeling of the those constituents with the correct semantic role.","{'title': '1 Introduction', 'number': '1'}"
A detailed error analysis of our baseline system indicates that the identification problem poses a significant bottleneck to improving overall system performance.,"{'title': '1 Introduction', 'number': '1'}"
The baseline system’s accuracy on the task of labeling nodes known to represent semantic arguments is 90%.,"{'title': '1 Introduction', 'number': '1'}"
"On the other hand, the system’s performance on the identification task is quite a bit lower, achieving only 80% recall with 86% precision.","{'title': '1 Introduction', 'number': '1'}"
"There are two sources of these identification errors: i) failures by the system to identify all and only those constituents that correspond to semantic roles, when those constituents are present in the syntactic analysis, and ii) failures by the syntactic analyzer to provide the constituents that align with correct arguments.","{'title': '1 Introduction', 'number': '1'}"
The work we present here is tailored to address these two sources of error in the identification problem.,"{'title': '1 Introduction', 'number': '1'}"
The remainder of this paper is organized as follows.,"{'title': '1 Introduction', 'number': '1'}"
We first describe a baseline system based on the best published techniques.,"{'title': '1 Introduction', 'number': '1'}"
We then report on two sets of experiments using techniques that improve performance on the problem of finding arguments when they are present in the syntactic analysis.,"{'title': '1 Introduction', 'number': '1'}"
"In the first set of experiments we explore new features, including features extracted from a parser that provides a different syntactic view – a Combinatory Categorial Grammar (CCG) parser (Hockenmaier and Steedman, 2002).","{'title': '1 Introduction', 'number': '1'}"
"In the second set of experiments, we explore approaches to identify optimal subsets of features for each argument class, and to calibrate the classifier probabilities.","{'title': '1 Introduction', 'number': '1'}"
We then report on experiments that address the problem of arguments missing from a given syntactic analysis.,"{'title': '1 Introduction', 'number': '1'}"
"We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views – one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser – Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a).","{'title': '1 Introduction', 'number': '1'}"
We show that these three views complement each other to improve performance.,"{'title': '1 Introduction', 'number': '1'}"
"For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al., 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al., 1994).","{'title': '2 Baseline System', 'number': '2'}"
PropBank was constructed by assigning semantic arguments to constituents of handcorrected TreeBank parses.,"{'title': '2 Baseline System', 'number': '2'}"
"Arguments of a verb are labeled ARG0 to ARG5, where ARG0 is the PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.","{'title': '2 Baseline System', 'number': '2'}"
"In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to as ARGMs are also marked.","{'title': '2 Baseline System', 'number': '2'}"
"Some examples are ARGM-LOC, for locatives; ARGM-TMP, for temporals; ARGMMNR, for manner, etc.","{'title': '2 Baseline System', 'number': '2'}"
Figure 1 shows a syntax tree along with the argument labels for an example extracted from PropBank.,"{'title': '2 Baseline System', 'number': '2'}"
"We use Sections 02-21 for training, Section 00 for development and Section 23 for testing.","{'title': '2 Baseline System', 'number': '2'}"
"We formulate the semantic labeling problem as a multi-class classification problem using Support Vector Machine (SVM) classifier (Hacioglu et al., 2003; Pradhan et al., 2003; Pradhan et al., 2004) TinySVM2 along with YamCha3 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used to implement the system.","{'title': '2 Baseline System', 'number': '2'}"
"Using what is known as the ONE VS ALL classification strategy, n binary classifiers are trained, where n is number of semantic classes including a NULL class.","{'title': '2 Baseline System', 'number': '2'}"
"The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al., (2004), Surdeanu et al., (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004).","{'title': '2 Baseline System', 'number': '2'}"
Table 1 lists the features used.,"{'title': '2 Baseline System', 'number': '2'}"
"As described in (Pradhan et al., 2004), we postprocess the n-best hypotheses using a trigram language model of the argument sequence.","{'title': '2 Baseline System', 'number': '2'}"
"We analyze the performance on three tasks: Table 2 shows the performance of the system using the hand corrected, TreeBank parses (HAND) and using parses produced by a Charniak parser (AUTOMATIC).","{'title': '2 Baseline System', 'number': '2'}"
"Precision (P), Recall (R) and F1 scores are given for the identification and combined tasks, and Classification Accuracy (A) for the classification task.","{'title': '2 Baseline System', 'number': '2'}"
Classification performance using Charniak parses is about 3% absolute worse than when using TreeBank parses.,"{'title': '2 Baseline System', 'number': '2'}"
"On the other hand, argument identification performance using Charniak parses is about 12.7% absolute worse.","{'title': '2 Baseline System', 'number': '2'}"
"Half of these errors – about 7% are due to missing constituents, and the other half – about 6% are due to mis-classifications.","{'title': '2 Baseline System', 'number': '2'}"
"Motivated by this severe degradation in argument identification performance for automatic parses, we examined a number of techniques for improving argument identification.","{'title': '2 Baseline System', 'number': '2'}"
We made a number of changes to the system which resulted in improved performance.,"{'title': '2 Baseline System', 'number': '2'}"
"The changes fell into three categories: i) new features, ii) feature selection and calibration, and iii) combining parses from different syntactic representations.","{'title': '2 Baseline System', 'number': '2'}"
"While the Path feature has been identified to be very important for the argument identification task, it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al., 2004; Xue and Palmer, 2004).","{'title': '3 Additional Features', 'number': '3'}"
"A dependency grammar should generate shorter paths from the predicate to dependent words in the sentence, and could be a more robust complement to the phrase structure grammar paths extracted from the Charniak parse tree.","{'title': '3 Additional Features', 'number': '3'}"
Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments.,"{'title': '3 Additional Features', 'number': '3'}"
We evaluated features from a CCG parser combined with our baseline feature set.,"{'title': '3 Additional Features', 'number': '3'}"
"We used three features that were introduced by Gildea and Hockenmaier (2003): Parallel to the hand-corrected TreeBank parses, we also had access to correct CCG parses derived from the TreeBank (Hockenmaier and Steedman, 2002a).","{'title': '3 Additional Features', 'number': '3'}"
We performed two sets of experiments.,"{'title': '3 Additional Features', 'number': '3'}"
"One using the correct CCG parses, and the other using parses obtained using StatCCG4 parser (Hockenmaier and Steedman, 2002).","{'title': '3 Additional Features', 'number': '3'}"
We incorporated these features in the systems based on hand-corrected TreeBank parses and Charniak parses respectively.,"{'title': '3 Additional Features', 'number': '3'}"
"For each constituent in the Charniak parse tree, if there was a dependency between the head word of the constituent and the predicate, then the corresponding CCG features for those words were added to the features for that constituent.","{'title': '3 Additional Features', 'number': '3'}"
Table 3 shows the performance of the system when these features were added.,"{'title': '3 Additional Features', 'number': '3'}"
The corresponding baseline performances are mentioned in parentheses.,"{'title': '3 Additional Features', 'number': '3'}"
We added several other features to the system.,"{'title': '3 Additional Features', 'number': '3'}"
"Position of the clause node (S, SBAR) seems to be an important feature in argument identification (Hacioglu et al., 2004) therefore we experimented with four clause-based path feature variations.","{'title': '3 Additional Features', 'number': '3'}"
We added the predicate context to capture predicate sense variations.,"{'title': '3 Additional Features', 'number': '3'}"
"For some adjunctive arguments, punctuation plays an important role, so we added some punctuation features.","{'title': '3 Additional Features', 'number': '3'}"
All the new features are shown in,"{'title': '3 Additional Features', 'number': '3'}"
"In the baseline system, we used the same set of features for all the n binary ONE VS ALL classifiers.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Error analysis showed that some features specifically suited for one argument class, for example, core arguments, tend to hurt performance on some adjunctive arguments.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Therefore, we thought that selecting subsets of features for each argument class might improve performance.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"To achieve this, we performed a simple feature selection procedure.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"For each argument, we started with the set of features introduced by (Gildea and Jurafsky, 2002).","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
We pruned this set by training classifiers after leaving out one feature at a time and checking its performance on a development set.,"{'title': '4 Feature Selection and Calibration', 'number': '4'}"
We used the x2 significance while making pruning decisions.,"{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Following that, we added each of the other features one at a time to the pruned baseline set of features and selected ones that showed significantly improved performance.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Since the feature selection experiments were computationally intensive, we performed them using 10k training examples.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
SVMs output distances not probabilities.,"{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"These distances may not be comparable across classifiers, especially if different features are used to train each binary classifier.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"In the baseline system, we used the algorithm described by Platt (Platt, 2000) to convert the SVM scores into probabilities by fitting to a sigmoid.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"When all classifiers used the same set of features, fitting all scores to a single sigmoid was found to give the best performance.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Since different feature sets are now used by the classifiers, we trained a separate sigmoid for each classifier.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Foster and Stine (2004) show that the pooladjacent-violators (PAV) algorithm (Barlow et al., 1972) provides a better method for converting raw classifier scores to probabilities when Platt’s algorithm fails.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
The probabilities resulting from either conversions may not be properly calibrated.,"{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"So, we binned the probabilities and trained a warping function to calibrate them.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"For each argument classifier, we used both the methods for converting raw SVM scores into probabilities and calibrated them using a development set.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Then, we visually inspected the calibrated plots for each classifier and chose the method that showed better calibration as the calibration procedure for that classifier.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Plots of the predicted probabilities versus true probabilities for the ARCM-TmP VS ALL classifier, before and after calibration are shown in Figure 2.","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
The performance improvement over a classifier that is trained using all the features for all the classes is shown in Table 5.,"{'title': '4 Feature Selection and Calibration', 'number': '4'}"
"Table 6 shows the performance of the system after adding the CCG features, additional features ex","{'title': '4 Feature Selection and Calibration', 'number': '4'}"
Adding new features can improve performance when the syntactic representation being used for classification contains the correct constituents.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Additional features can’t recover from the situation where the parse tree being used for classification doesn’t contain the correct constituent representing an argument.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Such parse errors account for about 7% absolute of the errors (or, about half of 12.7%) for the Charniak parse based system.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"To address these errors, we added two additional parse representations: i) Minipar dependency parser, and ii) chunking parser (Hacioglu et al., 2004).","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The hope is that these parsers will produce different errors than the Charniak parser since they represent different syntactic views.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The Charniak parser is trained on the Penn TreeBank corpus.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Minipar is a rule based dependency parser.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The chunking parser is trained on PropBank and produces a flat syntactic representation that is very different from the full parse tree produced by Charniak.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
A combination of the three different parses could produce better results than any single one.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Minipar (Lin, 1998; Lin and Pantel, 2001) is a rulebased dependency parser.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
It outputs dependencies between a word called head and another called modifier.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Each word can modify at most one word.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The dependency relationships form a dependency tree.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The set of words under each node in Minipar’s dependency tree form a contiguous segment in the original sentence and correspond to the constituent in a constituent tree.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"We formulate the semantic labeling problem in the same way as in a constituent structure parse, except we classify the nodes that represent head words of constituents.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"A similar formulation using dependency trees derived from TreeBank was reported in Hacioglu (Hacioglu, 2004).","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"In that experiment, the dependency trees were derived from hand-corrected TreeBank trees using head word rules.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Here, an SVM is trained to assign PropBank argument labels to nodes in Minipar dependency trees using the following features: Table 8 shows the performance of the Miniparbased semantic parser.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Minipar performance on the PropBank corpus is substantially worse than the Charniak based system.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
This is understandable from the fact that Minipar is not designed to produce constituents that would exactly match the constituent segmentation used in TreeBank.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"In the test set, about 37% of the arguments do not have corresponding constituents that match its boundaries.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"In experiments reported by Hacioglu (Hacioglu, 2004), a mismatch of about 8% was introduced in the transformation from handcorrected constituent trees to dependency trees.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Using an errorful automatically generated tree, a still higher mismatch would be expected.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"In case of the CCG parses, as reported by Gildea and Hockenmaier (2003), the mismatch was about 23%.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"A more realistic way to score the performance is to score tags assigned to head words of constituents, rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003).","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The results for this system are shown in Table 9. on the PropBank training data.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Table 10 lists the features used by this classifier.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"For each token (base phrase) to be tagged, a set of features is created from a fixed size context that surrounds each token.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"In addition to the above features, it also uses previous semantic tags that have already been assigned to the tokens contained in the linguistic context.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
A 5-token sliding window is used for the context.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al., 2004).","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"This system uses SVM classifiers to first chunk input text into flat chunks or base phrases, each labeled with a syntactic tag.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
A second SVM is trained to assign semantic labels to the chunks.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
The system is trained SVMs were trained for begin (B) and inside (I) classes of all arguments and outside (O) class for a total of 78 one-vs-all classifiers.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"Again, TinySVM5 along with YamCha6 (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2001) are used as the SVM training and test software.","{'title': '5 Alternative Syntactic Views', 'number': '5'}"
Table 11 presents the system performances on the PropBank test set for the chunk-based system.,"{'title': '5 Alternative Syntactic Views', 'number': '5'}"
"We combined the semantic parses as follows: i) scores for arguments were converted to calibrated probabilities, and arguments with scores below a threshold value were deleted.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"Separate thresholds were used for each parser. ii) For the remaining arguments, the more probable ones among overlapping ones were selected.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"In the chunked system, an argument could consist of a sequence of chunks.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
The probability assigned to the begin tag of an argument was used as the probability of the sequence of chunks forming an argument.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
Table 12 shows the performance improvement after the combination.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"Again, numbers in parentheses are respective baseline performances. mance on argument identification and argument identification and classification tasks after combining all three semantic parses.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
The main contribution of combining both the Minipar based and the Charniak-based parsers was significantly improved performance on ARG1 in addition to slight improvements to some other arguments.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
Table 13 shows the effect on selected arguments on sentences that were altered during the the combination of Charniak-based and Chunk-based parses. changed during pair-wise Charniak and Chunk combination.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
A marked increase in number of propositions for which all the arguments were identified correctly from 0% to about 46% can be seen.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"Relatively few predicates, 107 out of 4500, were affected by this combination.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"To give an idea of what the potential improvements of the combinations could be, we performed an oracle experiment for a combined system that tags head words instead of exact constituents as we did in case of Minipar-based and Charniak-based semantic parser earlier.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"In case of chunks, first word in prepositional base phrases was selected as the head word, and for all other chunks, the last word was selected to be the head word.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"If the correct argument was found present in either the Charniak, Minipar or Chunk hypotheses then that was selected.","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
The results for this are shown in Table 14.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
It can be seen that the head word based performance almost approaches the constituent based performance reported on the hand-corrected parses in Table 3 and there seems to be considerable scope for improvement. based scoring after oracle combination.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
"Charniak (C), Minipar (M) and Chunker (CH).","{'title': '6 Combining Semantic Labelers', 'number': '6'}"
Table 15 shows the performance improvement in the actual system for pairwise combination of the parsers and one using all three.,"{'title': '6 Combining Semantic Labelers', 'number': '6'}"
We described a state-of-the-art baseline semantic role labeling system based on Support Vector Machine classifiers.,"{'title': '7 Conclusions', 'number': '7'}"
"Experiments were conducted to evaluate three types of improvements to the system: i) adding new features including features extracted from a Combinatory Categorial Grammar parse, ii) performing feature selection and calibration and iii) combining parses obtained from semantic parsers trained using different syntactic views.","{'title': '7 Conclusions', 'number': '7'}"
We combined semantic parses from a Minipar syntactic parse and from a chunked syntactic representation with our original baseline system which was based on Charniak parses.,"{'title': '7 Conclusions', 'number': '7'}"
The belief was that semantic parses based on different syntactic views would make different errors and that the combination would be complimentary.,"{'title': '7 Conclusions', 'number': '7'}"
A simple combination of these representations did lead to improved performance.,"{'title': '7 Conclusions', 'number': '7'}"
This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132.,"{'title': '8 Acknowledgements', 'number': '8'}"
"Computer time was provided by NSF ARI Grant #CDA-9601817, NSF MRI Grant #CNS0420873, NASA AIST grant #NAG2-1646, DOE SciDAC grant #DE-FG02-04ER63870, NSF sponsorship of the National Center for Atmospheric Research, and a grant from the IBM Shared University Research (SUR) program.","{'title': '8 Acknowledgements', 'number': '8'}"
"We would like to thank Ralph Weischedel and Scott Miller of BBN Inc. for letting us use their named entity tagger – IdentiFinder; Martha Palmer for providing us with the PropBank data; Dan Gildea and Julia Hockenmaier for providing the gold standard CCG parser information, and all the anonymous reviewers for their helpful comments.","{'title': '8 Acknowledgements', 'number': '8'}"
