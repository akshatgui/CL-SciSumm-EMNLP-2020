col1,col2
In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.,{}
"We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.",{}
We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.,{}
"Dependency representations of sentences (Hudson, 1984; Me´lˇcuk, 1988) model head-dependent syntactic relations as edges in a directed graph.","{'title': '1 Introduction', 'number': '1'}"
Figure 1 displays a dependency representation for the sentence John hit the ball with the bat.,"{'title': '1 Introduction', 'number': '1'}"
"This sentence is an example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing.","{'title': '1 Introduction', 'number': '1'}"
"Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid.","{'title': '1 Introduction', 'number': '1'}"
"The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent.","{'title': '1 Introduction', 'number': '1'}"
"Though trees are more common, some formalisms allow for words to modify multiple parents (Hudson, 1984).","{'title': '1 Introduction', 'number': '1'}"
"Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees.","{'title': '1 Introduction', 'number': '1'}"
"When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages.","{'title': '1 Introduction', 'number': '1'}"
"However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges.","{'title': '1 Introduction', 'number': '1'}"
This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions.,"{'title': '1 Introduction', 'number': '1'}"
"Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000).","{'title': '1 Introduction', 'number': '1'}"
In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs.,"{'title': '1 Introduction', 'number': '1'}"
"We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads.","{'title': '1 Introduction', 'number': '1'}"
In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable.,"{'title': '1 Introduction', 'number': '1'}"
"We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to state-of-the-art accuracy for English and the best accuracy we know of for Czech and Danish.","{'title': '1 Introduction', 'number': '1'}"
Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was proposed by McDonald et al. (2005c).,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) respectively.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The formulation works by defining the score of a dependency tree to be the sum of edge scores, where x = x1 · · · xn is an input sentence and y a dependency tree for x.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"We can view y as a set of tree edges and write (i, j) E y to indicate an edge in y from word xi to word xj.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Consider the example from Figure 1, where the subscripts index the nodes of the tree.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The score of this tree would then be, We call this first-order dependency parsing since scores are restricted to a single edge in the dependency tree.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The score of an edge is in turn computed as the inner product of a high-dimensional feature representation of the edge with a corresponding weight vector, This is a standard linear classifier in which the weight vector w are the parameters to be learned during training.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"We should note that f(i, j) can be based on arbitrary features of the edge and the input sequence x.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Given a directed graph G = (V, E), the maximum spanning tree (MST) problem is to find the highest scoring subgraph of G that satisfies the tree constraint over the vertices V .","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"By defining a graph in which the words in a sentence are the vertices and there is a directed edge between all words with a score as calculated above, McDonald et al. (2005c) showed that dependency parsing is equivalent to finding the MST in this graph.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Although the MST formulation applies to any directed graph, our feature representations and one of the parsing algorithms (Eisner’s) rely on a linear ordering of the vertices, namely the order of the words in the sentence.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Restricting scores to a single edge in a dependency tree gives a very impoverished view of dependency parsing.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Yamada and Matsumoto (2003) showed that keeping a small amount of parsing history was crucial to improving parsing performance for their locally-trained shift-reduce SVM parser.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
It is reasonable to assume that other parsing models might benefit from features over previous decisions.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Here we will focus on methods for parsing second-order spanning trees.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
These models factor the score of the tree into the sum of adjacent edge pair scores.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"To quantify this, consider again the example from Figure 1.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In the second-order spanning tree model, the score would be, Here we use the second-order score function s(i, k, j), which is the score of creating a pair of adjacent edges, from word xi to words xk and xj.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"For instance, s(2, 4, 5) is the score of creating the edges from hit to with and from hit to ball.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The score functions are relative to the left or right of the parent and we never score adjacent edges that are on different sides of the parent (for instance, there is no s(2,1,4) for the adjacent edges from hit to John and ball).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"This independence between left and right descendants allow us to use a O(n3) second-order projective parsing algorithm, as we will see later.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"We write s(xi, −, xj) when xj is the first left or first right dependent of word xi.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"For example, s(2, −, 4) is the score of creating a dependency from hit to ball, since ball is the first child to the right of hit.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"More formally, if the word xi0 has the children shown in this picture, This second-order factorization subsumes the first-order factorization, since the score function could just ignore the middle argument to simulate first-order scoring.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The score of a tree for secondorder parsing is now where k and j are adjacent, same-side children of i in the tree y.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The second-order model allows us to condition on the most recent parsing decision, that is, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser (Charniak, 2000).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"For projective MST parsing, the first-order algorithm can be extended to the second-order case, as was noted by Eisner (1996).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The intuition behind the algorithm is shown graphically in Figure 3, which displays both the first-order and secondorder algorithms.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In the first-order algorithm, a word will gather its left and right dependents independently by gathering each half of the subtree rooted by its dependent in separate stages.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"By splitting up chart items into left and right components, the Eisner algorithm only requires 3 indices to be maintained at each step, as discussed in detail elsewhere (Eisner, 1996; McDonald et al., 2005b).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"For the second-order algorithm, the key insight is to delay the scoring of edges until pairs 2-order-non-proj-approx(x,s) Sentence x = x0 ... xn, x0 = root Weight function s : (i, k, j) → R of dependents have been gathered.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"This allows for the collection of pairs of adjacent dependents in a single stage, which allows for the incorporation of second-order scores, while maintaining cubictime parsing.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"The Eisner algorithm can be extended to an arbitrary mth-order model with a complexity of O(nm+1), for m > 1.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"An mth-order parsing algorithm will work similarly to the second-order algorithm, except that we collect m pairs of adjacent dependents in succession before attaching them to their parent.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Unfortunately, second-order non-projective MST parsing is NP-hard, as shown in appendix A.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
The approximation works by first finding the highest scoring projective parse.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"It then rearranges edges in the tree, one at a time, as long as such rearrangements increase the overall score and do not violate the tree constraint.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"We can easily motivate this approximation by observing that even in non-projective languages like Czech and Danish, most trees are primarily projective with just a few non-projective edges (Nivre and Nilsson, 2005).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Thus, by starting with the highest scoring projective tree, we are typically only a small number of transformations away from the highest scoring non-projective tree.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
The algorithm is shown in Figure 4.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
The expression y[i → j] denotes the dependency graph identical to y except that xi’s parent is xi instead shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of h1 was h2.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
This is done through the creation of a sibling item in part (B).,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In the first-order model, the dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent. of what it was in y.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
The test tree(y) is true iff the dependency graph y satisfies the tree constraint.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In more detail, line 1 of the algorithm sets y to the highest scoring second-order projective tree.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
The loop of lines 2–16 exits only when no further score improvement is possible.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Each iteration seeks the single highest-scoring parent change to y that does not break the tree constraint.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"To that effect, the nested loops starting in lines 4 and 5 enumerate all (i, j) pairs.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Line 6 sets y' to the dependency graph obtained from y by changing xj’s parent to xi.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Line 7 checks that the move from y to y' is valid by testing that xj’s parent was not already xi and that y' is a tree.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Line 8 computes the score change from y to y'.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"If this change is larger than the previous best change, we record how this new tree was created (lines 9-10).","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"After considering all possible valid edge changes to the tree, the algorithm checks to see that the best new tree does have a higher score.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"If that is the case, we change the tree permanently and re-enter the loop.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
Otherwise we exit since there are no single edge switches that can improve the score.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
This algorithm allows for the introduction of non-projective edges because we do not restrict any of the edge changes except to maintain the tree property.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In fact, if any edge change is ever made, the resulting tree is guaranteed to be nonprojective, otherwise there would have been a higher scoring projective tree that would have already been found by the exact projective parsing algorithm.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
It is not difficult to find examples for which this approximation will terminate without returning the highest-scoring non-projective parse.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
It is clear that this approximation will always terminate — there are only a finite number of dependency trees for any given sentence and each iteration of the loop requires an increase in score to continue.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"However, the loop could potentially take exponential time, so we will bound the number of edge transformations to a fixed value M. It is easy to argue that this will not hurt performance.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Even in freer-word order languages such as Czech, almost all non-projective dependency trees are primarily projective, modulo a few nonprojective edges.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Thus, if our inference algorithm starts with the highest scoring projective parse, the best non-projective parse only differs by a small number of edge transformations.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Furthermore, it is easy to show that each iteration of the loop takes O(n2) time, resulting in a O(n3 + Mn2) runtime algorithm.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"In practice, the approximation terminates after a small number of transformations and we do not need to bound the number of iterations in our experiments.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
We should note that this is one of many possible approximations we could have made.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Another reasonable approach would be to first find the highest scoring first-order non-projective parse, and then re-arrange edges based on second order scores in a similar manner to the algorithm we described.","{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
We implemented this method and found that the results were slightly worse.,"{'title': '2 Maximum Spanning Tree Parsing', 'number': '2'}"
"Kromann (2001) argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank (Kromann, 2003).","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
The formalism allows for a word to have multiple parents.,"{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
An example is shown in Figure 5 for the sentence He looks for and sees elephants.,"{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Here, the pronoun He is the subject for both verbs in the sentence, and the noun elephants the corresponding object.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"In the Danish Dependency Treebank, roughly 5% of words have more than one parent, which breaks the single parent (or tree) constraint we have previously required on dependency structures.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Kromann also allows for cyclic dependencies, though we deal only with acyclic dependency graphs here.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Though less common than trees, dependency graphs involving multiple parents are well established in the literature (Hudson, 1984).","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
We use the non-projective algorithm since the Danish Dependency Treebank contains a small number of non-projective arcs.,"{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
We then modify lines 7-10 of this algorithm so that it looks for the change in parent or the addition of a new parent that causes the highest change in overall score and does not create a cycle2.,"{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Like before, we make one change per iteration and that change will depend on the resulting score of the new tree.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.","{'title': '3 Danish: Parsing Secondary Parents', 'number': '3'}"
"In this section, we review the work of McDonald et al. (2005b) for online large-margin dependency parsing.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"As usual for supervised learning, we assume a training set T = {(xt, yt)}Tt=1, consisting of pairs of a sentence xt and its correct dependency representation yt.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
Figure 6 gives pseudo-code for the algorithm.,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"An online learning algorithm considers a single training instance for each update to the weight vector w. We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfitting.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"On each iteration, the algorithm considers a single training instance.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b).","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting.,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al., 2005b) or even all possible trees by using factored representations (Taskar et al., 2004; McDonald et al., 2005c).","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002).,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004).","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
This robustness to approximations comes from the fact that the online framework sets weights with respect to inference.,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"In other words, the learning method sees common errors due to where y' = arg maxy, s(xt, y'; w(i)) approximate inference and adjusts weights to correct for them.","{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
The work of Daum´e and Marcu (2005) formalizes this intuition by presenting an online learning framework in which parameter updates are made directly with respect to errors in the inference algorithm.,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
We show in the next section that this robustness extends to approximate dependency parsing.,"{'title': '4 Online Learning and Approximate Inference', 'number': '4'}"
"The score of adjacent edges relies on the definition of a feature representation f(i, k, j).","{'title': '5 Experiments', 'number': '5'}"
"As noted earlier, this representation subsumes the first-order representation of McDonald et al. (2005b), so we can incorporate all of their features as well as the new second-order features we now describe.","{'title': '5 Experiments', 'number': '5'}"
"The old first-order features are built from the parent and child words, their POS tags, and the POS tags of surrounding words and those of words between the child and the parent, as well as the direction and distance from the parent to the child.","{'title': '5 Experiments', 'number': '5'}"
"The second-order features are built from the following conjunctions of word and POS identity predicates xi-pos, xk-pos, xj-pos xk-pos, xj-pos xk-word, xj-word xk-word, xj-pos xk-pos, xj-word where xi-pos is the part-of-speech of the ith word in the sentence.","{'title': '5 Experiments', 'number': '5'}"
"We also include conjunctions between these features and the direction and distance from sibling j to sibling k. We determined the usefulness of these features on the development set, which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy.","{'title': '5 Experiments', 'number': '5'}"
We also ignored features over triples of words since this would explode the size of the feature space.,"{'title': '5 Experiments', 'number': '5'}"
"We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis.","{'title': '5 Experiments', 'number': '5'}"
"In our evaluation we exclude punctuation for English and include it for Czech and Danish, which is the standard.","{'title': '5 Experiments', 'number': '5'}"
"To create data sets for English, we used the Yamada and Matsumoto (2003) head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation.","{'title': '5 Experiments', 'number': '5'}"
The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set.,"{'title': '5 Experiments', 'number': '5'}"
These data sets are exclusively projective so we only compare the projective parsers using the exact projective parsing algorithms.,"{'title': '5 Experiments', 'number': '5'}"
"The purpose of these experiments is to gauge the overall benefit from including second-order features with exact parsing algorithms, which can be attained in the projective setting.","{'title': '5 Experiments', 'number': '5'}"
Results are shown in Table 1.,"{'title': '5 Experiments', 'number': '5'}"
We can see that there is clearly an advantage in introducing second-order features.,"{'title': '5 Experiments', 'number': '5'}"
"In particular, the complete tree metric is improved considerably.","{'title': '5 Experiments', 'number': '5'}"
"For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al. (1999).","{'title': '5 Experiments', 'number': '5'}"
"On average, 23% of the sentences in the training, development and test sets have at least one non-projective dependency, though, less than 2% of total edges are actually non-projective.","{'title': '5 Experiments', 'number': '5'}"
Results are shown in Table 2.,"{'title': '5 Experiments', 'number': '5'}"
"McDonald et al. (2005c) showed a substantial improvement in accuracy by modeling nonprojective edges in Czech, shown by the difference between two first-order models.","{'title': '5 Experiments', 'number': '5'}"
"Table 2 shows that a second-order model provides a comparable accuracy boost, even using an approximate non-projective algorithm.","{'title': '5 Experiments', 'number': '5'}"
The second-order nonprojective model accuracy of 85.2% is the highest reported accuracy for a single parser for these data.,"{'title': '5 Experiments', 'number': '5'}"
Similar results were obtained by Hall and N´ov´ak (2005) (85.1% accuracy) who take the best output of the Charniak parser extended to Czech and rerank slight variations on this output that introduce non-projective edges.,"{'title': '5 Experiments', 'number': '5'}"
"However, this system relies on a much slower phrase-structure parser as its base model as well as an auxiliary reranking module.","{'title': '5 Experiments', 'number': '5'}"
"Indeed, our second-order projective parser analyzes the test set in 16m32s, and the non-projective approximate parser needs 17m03s to parse the entire evaluation set, showing that runtime for the approximation is completely dominated by the initial call to the second-order projective algorithm and that the post-process edge transformation loop typically only iterates a few times per sentence.","{'title': '5 Experiments', 'number': '5'}"
For our experiments we used the Danish Dependency Treebank v1.0.,"{'title': '5 Experiments', 'number': '5'}"
The treebank contains a small number of inter-sentence and cyclic dependencies and we removed all sentences that contained such structures.,"{'title': '5 Experiments', 'number': '5'}"
The resulting data set contained 5384 sentences.,"{'title': '5 Experiments', 'number': '5'}"
We partitioned the data into contiguous 80/20 training/testing splits.,"{'title': '5 Experiments', 'number': '5'}"
We held out a subset of the training data for development purposes.,"{'title': '5 Experiments', 'number': '5'}"
"We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).","{'title': '5 Experiments', 'number': '5'}"
All systems use gold-standard part-of-speech since no trained tagger is readily available for Danish.,"{'title': '5 Experiments', 'number': '5'}"
Results are shown in Figure 3.,"{'title': '5 Experiments', 'number': '5'}"
"As might be expected, the nonprojective parser does slightly better than the projective parser because around 1% of the edges are non-projective.","{'title': '5 Experiments', 'number': '5'}"
"Since each word may have an arbitrary number of parents, we must use precision and recall rather than accuracy to measure performance.","{'title': '5 Experiments', 'number': '5'}"
This also means that the correct training loss is no longer the Hamming loss.,"{'title': '5 Experiments', 'number': '5'}"
"Instead, we use false positives plus false negatives over edge decisions, which balances precision and recall as our ultimate performance metric.","{'title': '5 Experiments', 'number': '5'}"
"As expected, for the basic projective and nonprojective parsers, recall is roughly 5% lower than precision since these models can only pick up at most one parent per word.","{'title': '5 Experiments', 'number': '5'}"
"For the parser that can introduce multiple parents, we see an increase in recall of nearly 3% absolute with a slight drop in precision.","{'title': '5 Experiments', 'number': '5'}"
These results are very promising and further show the robustness of discriminative online learning with approximate parsing algorithms.,"{'title': '5 Experiments', 'number': '5'}"
We described approximate dependency parsing algorithms that support higher-order features and multiple parents.,"{'title': '6 Discussion', 'number': '6'}"
We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy.,"{'title': '6 Discussion', 'number': '6'}"
These results show that the gain from allowing richer representations outweighs the loss from approximate parsing and further shows the robustness of online learning algorithms with approximate inference.,"{'title': '6 Discussion', 'number': '6'}"
The approximations we have presented are very simple.,"{'title': '6 Discussion', 'number': '6'}"
They start with a reasonably good baseline and make small transformations until the score of the structure converges.,"{'title': '6 Discussion', 'number': '6'}"
"These approximations work because freer-word order languages we studied are still primarily projective, making the approximate starting point close to the goal parse.","{'title': '6 Discussion', 'number': '6'}"
"However, we would like to investigate the benefits for parsing of more principled approaches to approximate learning and inference techniques such as the learning as search optimization framework of (Daum´e and Marcu, 2005).","{'title': '6 Discussion', 'number': '6'}"
This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second-order model.,"{'title': '6 Discussion', 'number': '6'}"
This work was supported by NSF ITR grants 0205448.,"{'title': 'Acknowledgments', 'number': '7'}"
