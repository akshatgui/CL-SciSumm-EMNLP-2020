col1,col2
We address the problem of analyzing multiple related opinions in a text.,Introduction
"For instance, in a restaurant review such opinions may include food, ambience and service.",Introduction
"We formulate this task as a multiple aspect ranking problem, where the goal is to produce a set of numerical scores, one for each aspect.",Introduction
We present an algorithm that jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.,Introduction
"This algorithm guides the prediction of individual rankers by analyzing meta-relations between opinions, such as agreement and contrast.",Introduction
We prove that our agreementbased joint model is more expressive than individual ranking models.,Introduction
Our empirical results further confirm the strength of the model: the algorithm provides significant improvement over both individual rankers and a state-of-the-art joint ranking model.,Introduction
"Previous work on sentiment categorization makes an implicit assumption that a single score can express the polarity of an opinion text (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003).",Experiment/Discussion
"However, multiple opinions on related matters are often intertwined throughout a text.",Experiment/Discussion
"For example, a restaurant review may express judgment on food quality as well as the service and ambience of the restaurant.",Experiment/Discussion
"Rather than lumping these aspects into a single score, we would like to capture each aspect of the writer’s opinion separately, thereby providing a more fine-grained view of opinions in the review.",Experiment/Discussion
"To this end, we aim to predict a set of numeric ranks that reflects the user’s satisfaction for each aspect.",Experiment/Discussion
"In the example above, we would assign a numeric rank from 1-5 for each of: food quality, service, and ambience.",Experiment/Discussion
"A straightforward approach to this task would be to rank' the text independently for each aspect, using standard ranking techniques such as regression or classification.",Experiment/Discussion
"However, this approach fails to exploit meaningful dependencies between users’ judgments across different aspects.",Experiment/Discussion
"Knowledge of these dependencies can be crucial in predicting accurate ranks, as a user’s opinions on one aspect can influence his or her opinions on others.",Experiment/Discussion
The algorithm presented in this paper models the dependencies between different labels via the agreement relation.,Experiment/Discussion
The agreement relation captures whether the user equally likes all aspects of the item or whether he or she expresses different degrees of satisfaction.,Experiment/Discussion
"Since this relation can often be determined automatically for a given text (Marcu and Echihabi, 2002), we can readily use it to improve rank prediction.",Experiment/Discussion
"The Good Grief model consists of a ranking model for each aspect as well as an agreement model which predicts whether or not all rank aspects are 'In this paper, ranking refers to the task of assigning an integer from 1 to k to each instance.",Experiment/Discussion
"This task is sometimes referred to as “ordinal regression” (Crammer and Singer, 2001) and “rating prediction” (Pang and Lee, 2005). equal.",Experiment/Discussion
The Good Grief decoding algorithm predicts a set of ranks – one for each aspect – which maximally satisfy the preferences of the individual rankers and the agreement model.,Experiment/Discussion
"For example, if the agreement model predicts consensus but the individual rankers select ranks (5, 5, 4), then the decoder decides whether to trust the the third ranker, or alter its prediction and output (5, 5, 5) to be consistent with the agreement prediction.",Experiment/Discussion
"To obtain a model well-suited for this decoding, we also develop a joint training method that conjoins the training of multiple aspect models.",Experiment/Discussion
We demonstrate that the agreement-based joint model is more expressive than individual ranking models.,Experiment/Discussion
"That is, every training set that can be perfectly ranked by individual ranking models for each aspect can also be perfectly ranked with our joint model.",Experiment/Discussion
"In addition, we give a simple example of a training set which cannot be perfectly ranked without agreement-based joint inference.",Experiment/Discussion
Our experimental results further confirm the strength of the Good Grief model.,Experiment/Discussion
Our model significantly outperforms individual ranking models as well as a stateof-the-art joint ranking model.,Experiment/Discussion
"Sentiment Classification Traditionally, categorization of opinion texts has been cast as a binary classification task (Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Dave et al., 2003).",Experiment/Discussion
"More recent work (Pang and Lee, 2005; Goldberg and Zhu, 2006) has expanded this analysis to the ranking framework where the goal is to assess review polarity on a multi-point scale.",Experiment/Discussion
"While this approach provides a richer representation of a single opinion, it still operates on the assumption of one opinion per text.",Experiment/Discussion
Our work generalizes this setting to the problem of analyzing multiple opinions – or multiple aspects of an opinion.,Experiment/Discussion
"Since multiple opinions in a single text are related, it is insufficient to treat them as separate single-aspect ranking tasks.",Experiment/Discussion
This motivates our exploration of a new method for joint multiple aspect ranking.,Experiment/Discussion
"Ranking The ranking, or ordinal regression, problem has been extensivly studied in the Machine Learning and Information Retrieval communities.",Experiment/Discussion
In this section we focus on two online ranking methods which form the basis of our approach.,Experiment/Discussion
The first is a model proposed by Crammer and Singer (2001).,Experiment/Discussion
"The task is to predict a rank y E I1, ..., k} for every input x E R'.",Experiment/Discussion
"Their model stores a weight vector w E R' and a vector of increasing boundaries b0 = −00 < b1 < ... < bk−1 < bk = 00 which divide the real line into k segments, one for each possible rank.",Experiment/Discussion
The model first scores each input with the weight vector: score(x) = w · x.,Experiment/Discussion
"Finally, the model locates score(x) on the real line and returns the appropriate rank as indicated by the boundaries.",Experiment/Discussion
"Formally, the model returns the rank r such that br−1 < score(x) < br.",Experiment/Discussion
"The model is trained with the Perceptron Ranking algorithm (or “PRank algorithm”), which reacts to incorrect predictions on the training set by updating the weight and boundary vectors.",Experiment/Discussion
The PRanking model and algorithm were tested on the EachMovie dataset with a separate ranking model learned for each user in the database.,Experiment/Discussion
An extension of this model is provided by Basilico and Hofmann (2004) in the context of collaborative filtering.,Experiment/Discussion
"Instead of training a separate model for each user, Basilico and Hofmann train a joint ranking model which shares a set of boundaries across all users.",Experiment/Discussion
"In addition to these shared boundaries, userspecific weight vectors are stored.",Experiment/Discussion
"To compute the score for input x and user i, the weight vectors for all users are employed: where 0 < sim(i, j) < 1 is the cosine similarity between users i and j, computed on the entire training set.",Experiment/Discussion
"Once the score has been computed, the prediction rule follows that of the PRanking model.",Experiment/Discussion
"The model is trained using the PRank algorithm, with the exception of the new definition for the scoring function.2 While this model shares information between the different ranking problems, it fails to explicitly model relations between the rank predictions.",Experiment/Discussion
"In contrast, our algorithm uses an agreement model to learn such relations and inform joint predictions.",Experiment/Discussion
The goal of our algorithm is to find a rank assignment that is consistent with predictions of individual rankers and the agreement model.,Experiment/Discussion
"To this end, we develop the Good Grief decoding procedure that minimizes the dissatisfaction (grief) of individual components with a joint prediction.",Experiment/Discussion
"In this section, we formally define the grief of each component, and a mechanism for its minimization.",Experiment/Discussion
We then describe our method for joint training of individual rankers that takes into account the Good Grief decoding procedure.,Experiment/Discussion
"In an m-aspect ranking problem, we are given a training sequence of instance-label pairs (x1, y1), ..., (xt, yt), .... Each instance xt is a feature vector in Rn and the label yt is a vector of m ranks in Ym, where Y = 11, .., k} is the set of possible ranks.",Experiment/Discussion
"The ith component of yt is the rank for the ith aspect, and will be denoted by y[i]t. The goal is to learn a mapping from instances to rank sets, H : X —* Ym, which minimizes the distance between predicted ranks and true ranks.",Experiment/Discussion
"Our m-aspect ranking model contains m+1 components: ((w[1], b[1]), ..., (w[m], b[m]), a).",Experiment/Discussion
"The first m components are individual ranking models, one for each aspect, and the final component is the agreement model.",Experiment/Discussion
"For each aspect i E 1...m, w[i] E Rn is a vector of weights on the input features, and b[i] E Rk−1 is a vector of boundaries which divide the real line into k intervals, corresponding to the k possible ranks.",Experiment/Discussion
The default prediction of the aspect ranking model simply uses the ranking rule of the PRank algorithm.,Experiment/Discussion
"This rule predicts the rank r such that b[i]r−1 < scorei(x) < b[i]r.3 The value scorei(x) can be defined simply as the dot product w[i]·x, or it can take into account the weight vectors for other aspects weighted by a measure of interaspect similarity.",Experiment/Discussion
"We adopt the definition given in equation 1, replacing the user-specific weight vectors with our aspect-specific weight vectors.",Experiment/Discussion
"3More precisely (taking into account the possibility of ties): y[i] = min1E{1,..,k}{r : scorei(x) − b[i],.",Experiment/Discussion
< 01 The agreement model is a vector of weights a E Rn.,Experiment/Discussion
"A value of a · x > 0 predicts that the ranks of all m aspects are equal, and a value of a · x < 0 indicates disagreement.",Experiment/Discussion
The absolute value Ja · xJ indicates the confidence in the agreement prediction.,Experiment/Discussion
The goal of the decoding procedure is to predict a joint rank for the m aspects which satisfies the individual ranking models as well as the agreement model.,Experiment/Discussion
"For a given input x, the individual model for aspect i predicts a default rank y[i] based on its feature weight and boundary vectors (w[i], b[i]).",Experiment/Discussion
"In addition, the agreement model makes a prediction regarding rank consensus based on a · x.",Experiment/Discussion
"However, the default aspect predictions 9[1] ... y[m] may not accord with the agreement model.",Experiment/Discussion
"For example, if a · x > 0, but 9[i] =� y[j] for some i, j E 1...m, then the agreement model predicts complete consensus, whereas the individual aspect models do not.",Experiment/Discussion
We therefore adopt a joint prediction criterion which simultaneously takes into account all model components – individual aspect models as well as the agreement model.,Experiment/Discussion
"For each possible prediction r = (r[1], ..., r[m]) this criterion assesses the level of grief associated with the ith-aspect ranking model, gi(x, r[i]).",Experiment/Discussion
"Similarly, we compute the grief of the agreement model with the joint prediction, ga(x, r) (both gi and ga are defined formally below).",Experiment/Discussion
"The decoder then predicts the m ranks which minimize the overall grief: � (2) If the default rank predictions for the aspect models, y� = (0[1], ..., y[m]), are in accord with the agreement model (both indicating consensus or both indicating contrast), then the grief of all model components will be zero, and we simply output y.",Experiment/Discussion
"On the other hand, if y� indicates disagreement but the agreement model predicts consensus, then we have the option of predicting y� and bearing the grief of the agreement model.",Experiment/Discussion
"Alternatively, we can predict some consensus y0 (i.e. with y0[i] = y0[j], Vi, j) and bear the grief of the component ranking models.",Experiment/Discussion
The decoder H chooses the option with lowest overall grief.4 m Now we formally define the measures of grief used in this criterion.,Experiment/Discussion
"Aspect Model Grief We define the grief of the ithaspect ranking model with respect to a rank r to be the smallest magnitude correction term which places the input’s score into the rth segment of the real line: Agreement Model Grief Similarly, we define the grief of the agreement model with respect to a joint rank r = (r[1], ... , r[m]) as the smallest correction needed to bring the agreement score into accord with the agreement relation between the individual ranks r[1], ... , r[m]: Ranking models Pseudo-code for Good Grief training is shown in Figure 1.",Experiment/Discussion
"This training algorithm is based on PRanking (Crammer and Singer, 2001), an online perceptron algorithm.",Experiment/Discussion
The training is performed by iteratively ranking each training input x and updating the model.,Experiment/Discussion
"If the predicted rank y� is equal to the true rank y, the weight and boundaries vectors remain unchanged.",Experiment/Discussion
"On the other hand, if y� =� y, then the weights and boundaries are updated to improve the prediction for x (step 4.c in Figure 1).",Experiment/Discussion
"See (Crammer and Singer, 2001) for explanation and analysis of this update rule.",Experiment/Discussion
Our algorithm departs from PRanking by conjoining the updates for the m ranking models.,Experiment/Discussion
We achieve this by using Good Grief decoding at each step throughout training.,Experiment/Discussion
Our decoder H(x) (from equation 2) uses all the aspect component models ponent models are comparable.,Experiment/Discussion
"In practice, we take an uncalibrated agreement model a' and reweight it with a tuning parameter: a = αa'.",Experiment/Discussion
The value of α is estimated using the development set.,Experiment/Discussion
We assume that the griefs of the ranking models are comparable since they are jointly trained. as well as the (previously trained) agreement model to determine the predicted rank for each aspect.,Experiment/Discussion
"In concrete terms, for every training instance x, we predict the ranks of all aspects simultaneously (step 2 in Figure 1).",Experiment/Discussion
"Then, for each aspect we make a separate update based on this joint prediction (step 4 in Figure 1), instead of using the individual models’ predictions.",Experiment/Discussion
Agreement model The agreement model a is assumed to have been previously trained on the same training data.,Experiment/Discussion
An instance is labeled with a positive label if all the ranks associated with this instance are equal.,Experiment/Discussion
The rest of the instances are labeled as negative.,Experiment/Discussion
This model can use any standard training algorithm for binary classification such as Perceptron or SVM optimization.,Experiment/Discussion
"Ranking Models Following previous work on sentiment classification (Pang et al., 2002), we represent each review as a vector of lexical features.",Experiment/Discussion
"More specifically, we extract all unigrams and bigrams, discarding those that appear fewer than three times.",Experiment/Discussion
"This process yields about 30,000 features.",Experiment/Discussion
Agreement Model The agreement model also operates over lexicalized features.,Experiment/Discussion
The effectiveness of these features for recognition of discourse relations has been previously shown by Marcu and Echihabi (2002).,Experiment/Discussion
"In addition to unigrams and bigrams, we also introduce a feature that measures the maximum contrastive distance between pairs of words in a review.",Experiment/Discussion
"For example, the presence of “delicious” and “dirty” indicate high contrast, whereas the pair “expensive” and “slow” indicate low contrast.",Experiment/Discussion
The contrastive distance for a pair of words is computed by considering the difference in relative weight assigned to the words in individually trained PRanking models.,Experiment/Discussion
"In this section, we prove that our model is able to perfectly rank a strict superset of the training corpora perfectly rankable by m ranking models individually.",Experiment/Discussion
"We first show that if the independent ranking models can individually rank a training set perfectly, then our model can do so as well.",Experiment/Discussion
"Next, we show that our model is more expressive by providing a simple illustrative example of a training set which can only be perfectly ranked with the inclusion of an agreement model.",Experiment/Discussion
First we introduce some notation.,Experiment/Discussion
"For each training instance (xt, yt), each aspect i  1...m, and each rank r  1...k, define an auxiliary variable y[i]t r with y[i]t r = −1 if y[i]t  r and y[i]t r = 1 if y[i]t > r. In words, y[i]t r indicates whether the true rank y[i]t is to the right or left of a potential rank r. Now suppose that a training set (x1, y1), ..., (xT , yT) is perfectly rankable for each aspect independently.",Experiment/Discussion
"That is, for each aspect i  1...m, there exists some ideal model v[i]∗ = (w[i]∗, b[i]∗) such that the signed distance from the prediction to the rth boundary: w[i]∗ · xt − b[i]∗r has the same sign as the auxiliary variable y[i]tr.",Experiment/Discussion
"In other words, the minimum margin over all training instances and ranks, γ = minr�t{(w[i]∗ · xt − b[i]∗r)y[i]tr}, is no less than zero.",Experiment/Discussion
"Now for the tth training instance, define an agreement auxiliary variable at, where at = 1 when all aspects agree in rank and at = −1 when at least two aspects disagree in rank.",Experiment/Discussion
"First consider the case where the agreement model a perfectly classifies all training instances: (a · xt)at > 0, t.",Experiment/Discussion
"It is clear that Good Grief decoding with the ideal joint model (w[1]∗, b[1]∗, ..., w[m]∗, b[m]∗, a) will produce the same output as the component ranking models run separately (since the grief will always be zero for the default rank predictions).",Experiment/Discussion
Now consider the case where the training data is not linearly separable with regard to agreement classification.,Experiment/Discussion
Define the margin of the worst case error to be β = maxt{|(a·xt) |: (a·xt)at < 0}.,Experiment/Discussion
"If β < γ, then again Good Grief decoding will always produce the default results (since the grief of the agreement model will be at most β in cases of error, whereas the grief of the ranking models for any deviation from their default predictions will be at least γ).",Experiment/Discussion
"On the other hand, if β  γ, then the agreement model errors could potentially disrupt the perfect ranking.",Experiment/Discussion
"However, we need only rescale w∗ := w∗(� � + 0 and b∗ := b∗(� + E) to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error.",Experiment/Discussion
"Thus whenever independent ranking models can perfectly rank a training set, a joint ranking model with Good Grief decoding can do so as well.",Experiment/Discussion
Now we give a simple example of a training set which can only be perfectly ranked with the addition of an agreement model.,Experiment/Discussion
"Consider a training set of four instances with two rank aspects: We can interpret these inputs as feature vectors corresponding to the presence of “good”, “bad”, and “but not” in the following four sentences: The food was good, but not the ambience.",Experiment/Discussion
"The food was good, and so was the ambience.",Experiment/Discussion
"The food was bad, but not the ambience.",Experiment/Discussion
"The food was bad, and so was the ambience.",Experiment/Discussion
"We can further interpret the first rank aspect as the quality of food, and the second as the quality of the ambience, both on a scale of 1-2.",Experiment/Discussion
A simple ranking model which only considers the words “good” and “bad” perfectly ranks the food aspect.,Experiment/Discussion
"However, it is easy to see that no single model perfectly ranks the ambience aspect.",Experiment/Discussion
"Consider any model (w, b = (b)).",Experiment/Discussion
"Note that w · x1 < b and w · x2 > b together imply that w3 < 0, whereas w · x3 > b and w · x4 < b together imply that w3 > 0.",Experiment/Discussion
Thus independent ranking models cannot perfectly rank this corpus.,Experiment/Discussion
"The addition of an agreement model, however, can easily yield a perfect ranking.",Experiment/Discussion
"With a = (0, 0, −5) (which predicts contrast with the presence of the words “but not”) and a ranking model for the ambience aspect such as w = (1, −1, 0), b = (0), the Good Grief decoder will produce a perfect rank.",Experiment/Discussion
We evaluate our multi-aspect ranking algorithm on a corpus of restaurant reviews available on the website http://www.we8there.com.,Experiment/Discussion
"Reviews from this website have been previously used in other sentiment analysis tasks (Higashinaka et al., 2006).",Experiment/Discussion
"Each review is accompanied by a set of five ranks, each on a scale of 1-5, covering food, ambience, service, value, and overall experience.",Experiment/Discussion
These ranks are provided by consumers who wrote original reviews.,Experiment/Discussion
Our corpus does not contain incomplete data points since all the reviews available on this website contain both a review text and the values for all the five aspects.,Experiment/Discussion
"Training and Testing Division Our corpus contains 4,488 reviews, averaging 115 words.",Experiment/Discussion
"We randomly select 3,488 reviews for training, 500 for development and 500 for testing.",Experiment/Discussion
Parameter Tuning We used the development set to determine optimal numbers of training iterations for our model and for the baseline models.,Experiment/Discussion
"Also, given an initial uncalibrated agreement model a', we define our agreement model to be a = αa' for an appropriate scaling factor α.",Experiment/Discussion
We tune the value of α on the development set.,Experiment/Discussion
Corpus Statistics Our training corpus contains 528 among 55 = 3025 possible rank sets.,Experiment/Discussion
"The most frequent rank set (5, 5, 5, 5, 5) accounts for 30.5% of the training set.",Experiment/Discussion
"However, no other rank set comprises more than 5% of the data.",Experiment/Discussion
"To cover 90% of occurrences in the training set, 227 rank sets are required.",Experiment/Discussion
"Therefore, treating a rank tuple as a single label is not a viable option for this task.",Experiment/Discussion
"We also find that reviews with full agreement across rank aspects are quite common in our corpus, accounting for 38% of the training data.",Experiment/Discussion
Thus an agreementbased approach is natural and relevant.,Experiment/Discussion
A rank of 5 is the most common rank for all aspects and thus a prediction of all 5’s gives a MAJORITY baseline and a natural indication of task difficulty.,Experiment/Discussion
"Evaluation Measures We evaluate our algorithm and the baseline using ranking loss (Crammer and Singer, 2001; Basilico and Hofmann, 2004).",Experiment/Discussion
Ranking loss measures the average distance between the true rank and the predicted rank.,Experiment/Discussion
"Formally, given N test instances (x1, y1), ..., (xN, yN) of an m-aspect ranking problem and the corresponding predictions ˆy1, ..., ˆyN, ranking loss is defined as Et,i |y[i]t_ˆy[i]t|.",Experiment/Discussion
Lower values of this measure cormN respond to a better performance of the algorithm.,Experiment/Discussion
"Comparison with Baselines Table 1 shows the performance of the Good Grief training algorithm GG TRAIN+DECODE along with various baselines, including the simple MAJORITY baseline mentioned in section 5.",Experiment/Discussion
"The first competitive baseline, PRANK, learns a separate ranker for each aspect using the PRank algorithm.",Experiment/Discussion
"The second competitive baseline, SIM, shares the weight vectors across aspects using a similarity measure (Basilico and Hofmann, 2004).",Experiment/Discussion
Both of these methods are described in detail in Section 2.,Experiment/Discussion
"In addition, we consider two variants of our algorithm: GG DECODE employs the PRank training algorithm to independently train all component ranking models and only applies Good Grief decoding at test time.",Experiment/Discussion
GG ORACLE uses Good Grief training and decoding but in both cases is given perfect knowledge of whether or not the true ranks all agree (instead of using the trained agreement model).,Experiment/Discussion
"Our model achieves a rank error of 0.632, compared to 0.675 for PRANK and 0.663 for SIM.",Experiment/Discussion
Both of these differences are statistically significant at p < 0.002 by a Fisher Sign Test.,Experiment/Discussion
The gain in performance is observed across all five aspects.,Experiment/Discussion
"Our model also yields significant improvement (p < 0.05) over the decoding-only variant GG DECODE, confirming the importance of joint training.",Experiment/Discussion
"As shown in Figure 2, our model demonstrates consistent improvement over the baselines across all the training rounds.",Experiment/Discussion
Model Analysis We separately analyze our percomputed separately on cases of actual consensus and actual disagreement. formance on the 210 test instances where all the target ranks agree and the remaining 290 instances where there is some contrast.,Experiment/Discussion
"As Table 2 shows, we outperform the PRANK baseline in both cases.",Experiment/Discussion
However on the consensus instances we achieve a relative reduction in error of 21.8% compared to only a 1.1% reduction for the other set.,Experiment/Discussion
"In cases of consensus, the agreement model can guide the ranking models by reducing the decision space to five rank sets.",Experiment/Discussion
"In cases of disagreement, however, our model does not provide sufficient constraints as the vast majority of ranking sets remain viable.",Experiment/Discussion
"This explains the performance of GG ORACLE, the variant of our algorithm with perfect knowledge of agreement/disagreement facts.",Experiment/Discussion
"As shown in Table 1, GG ORACLE yields substantial improvement over our algorithm, but most of this gain comes from consensus instances (see Table 2).",Experiment/Discussion
We also examine the impact of the agreement model accuracy on our algorithm.,Experiment/Discussion
"The agreement model, when considered on its own, achieves classification accuracy of 67% on the test set, compared to a majority baseline of 58%.",Experiment/Discussion
"However, those instances with high confidence |a · x |exhibit substantially higher classification accuracy.",Experiment/Discussion
Figure 3 shows the performance of the agreement model as a function of the confidence value.,Experiment/Discussion
"The 10% of the data with highest confidence values can be classified by the agreement model with 90% accuracy, and the third of the data with highest confidence can be classified at 80% accuracy.",Experiment/Discussion
This property explains why the agreement model helps in joint ranking even though its overall accuracy may seem low.,Experiment/Discussion
"Under the Good Grief criterion, the agreement model’s prediction will only be enforced when its grief outweighs that of the ranking models.",Experiment/Discussion
"Thus in cases where the prediction confidence (|a·x|) is relatively low,6 the agreement model will essentially be ignored.",Experiment/Discussion
We considered the problem of analyzing multiple related aspects of user reviews.,Results/Conclusion
The algorithm presented jointly learns ranking models for individual aspects by modeling the dependencies between assigned ranks.,Results/Conclusion
The strength of our algorithm lies in its ability to guide the prediction of individual rankers using rhetorical relations between aspects such as agreement and contrast.,Results/Conclusion
Our method yields significant empirical improvements over individual rankers as well as a state-of-the-art joint ranking model.,Results/Conclusion
Our current model employs a single rhetorical relation – agreement vs. contrast – to model dependencies between different opinions.,Results/Conclusion
"As our analy6What counts as “relatively low” will depend on both the value of the tuning parameter α and the confidence of the component ranking models for a particular input x. sis shows, this relation does not provide sufficient constraints for non-consensus instances.",Results/Conclusion
An avenue for future research is to consider the impact of additional rhetorical relations between aspects.,Results/Conclusion
We also plan to theoretically analyze the convergence properties of this and other joint perceptron algorithms.,Results/Conclusion
The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168 and grant IIS0415865) and the Microsoft Research Faculty Fellowship.,Results/Conclusion
"Thanks to Michael Collins, Pawan Deshpande, Jacob Eisenstein, Igor Malioutov, Luke Zettlemoyer, and the anonymous reviewers for helpful comments and suggestions.",Results/Conclusion
Thanks also to Vasumathi Raman for programming assistance.,Results/Conclusion
"Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.",Results/Conclusion
