col1,col2
"In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonslrate he use of these networks for word sense disambiguation.",{}
"Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models.",{}
"The automatic construction of VLNNs enables real-size xperiments with neural networks for natural language processing, which in turn provides insight into their behavior and design and can lead to possible improvements.",{}
"Automated language understanding requires the determination f the concept which a given use of a word represents, a process referred to as word sense disambiguation (WSD).","{'title': 'Introduction', 'number': '1'}"
"WSD is typically effected in natural llanguage processing systems by utilizing semantic teature lists for each word in the system's lexicon, together with restriction mechanisms such as case role selection.","{'title': 'Introduction', 'number': '1'}"
"However, it is often impractical to manually encode such information, especially for generalized text where the variety and meaning of words is potentially unrestricted.","{'title': 'Introduction', 'number': '1'}"
"Furthermore, restriction mechanisms usually operate within a single sentence~ and thus the broader context cannot assist in the disambiguation process.","{'title': 'Introduction', 'number': '1'}"
"in this paper, we describe a means tor automatically building Very Large Neural Networks (VLNNs) from definition texts in machine-readable dictionaries, and denmnstrate he use of these networks for WSD.","{'title': 'Introduction', 'number': '1'}"
"Our method brings together two earlier, independent approaches to WSD: the use of machine-readable dictionaries and spreading and activation models.","{'title': 'Introduction', 'number': '1'}"
"The automatic onstruction of VLNNs enables real-size experiments with neural networks, which in turn The authors would like to acknowledge the contributions of St~phanc tlari6 and Gavin Huntlcy to the work presented in this paper.","{'title': 'Introduction', 'number': '1'}"
provides insight into their behavior and design and can lead to possible improvements.,"{'title': 'Introduction', 'number': '1'}"
2.1.,"{'title': 'Previous work. ', 'number': '2'}"
Machine-readable dictionaries Jbr WSD.,"{'title': 'Previous work. ', 'number': '2'}"
"There have been several attempts to exploit the information in maclfine-readable versions of everyday dictionaries ( ee, tor instance, Amsler, 1980; Calzolari, 1984; Chodorow, Byrd and Heidorn, 1985; Markowitz, Ahlswede and Evens, 1986; Byrd et al, 1987; V&onis, Ide and Wurbel, 1989), in which an enormous amount of lexical and semantic knowledge is already ""encoded"".","{'title': 'Previous work. ', 'number': '2'}"
"Such information is not systematic or even complete, and its extraction from machine- readable dictionaries is not always straightforward.","{'title': 'Previous work. ', 'number': '2'}"
"However, it has been shown that even in its base form, information from machine-readable dictionaries can be used, for example, to assist in the disambiguation f prepositional phrase attachment (Jensen and Bluet, 1987), or to find subject domains in texts (Walker and Amsler, 1986).","{'title': 'Previous work. ', 'number': '2'}"
"The most general and well-known attempt to utilize information i machine-readable dictionaries for WSD is that of Lesk (1986), which computes the degree of overlap--that is, number of shared words--in definition texts of words that appear in a ten-word window of 1 389 context.","{'title': 'Previous work. ', 'number': '2'}"
The sense of a word with the greatest number of overlaps with senses of other words in the window is chosen as the correct one.,"{'title': 'Previous work. ', 'number': '2'}"
"For example, consider the definitions of pen and sheep from the Collins English Dictionary, the dictionary used in our experiments, in figure 1.","{'title': 'Previous work. ', 'number': '2'}"
"Figure 1: Definitions of PEN, SHEEP, GOAT and PAGE in the Collins English Dictionary pen 1 1.","{'title': 'Previous work. ', 'number': '2'}"
"an implement for writing or drawing using ink, formerly consisting of a sharpened and split quill, and now of a metal nib attached to a holder.","{'title': 'Previous work. ', 'number': '2'}"
2.,"{'title': 'Previous work. ', 'number': '2'}"
the writing end of such an implement; nib.,"{'title': 'Previous work. ', 'number': '2'}"
3.,"{'title': 'Previous work. ', 'number': '2'}"
style of writing.,"{'title': 'Previous work. ', 'number': '2'}"
4.,"{'title': 'Previous work. ', 'number': '2'}"
the pen.,"{'title': 'Previous work. ', 'number': '2'}"
"a. writing as an occupation, b. the written word.","{'title': 'Previous work. ', 'number': '2'}"
"5, the long horny internal shell of a squid.","{'title': 'Previous work. ', 'number': '2'}"
6.,"{'title': 'Previous work. ', 'number': '2'}"
to write or compose.,"{'title': 'Previous work. ', 'number': '2'}"
pen 2 1.,"{'title': 'Previous work. ', 'number': '2'}"
an enclosure in which domestic animals are kept.,"{'title': 'Previous work. ', 'number': '2'}"
2.any place of confinement.,"{'title': 'Previous work. ', 'number': '2'}"
3.,"{'title': 'Previous work. ', 'number': '2'}"
a dock for servicing submarines.,"{'title': 'Previous work. ', 'number': '2'}"
4.,"{'title': 'Previous work. ', 'number': '2'}"
to enclose or keep in a pen.,"{'title': 'Previous work. ', 'number': '2'}"
pen 3 short for penitentiary.,"{'title': 'Previous work. ', 'number': '2'}"
pen 4 a female swan.,"{'title': 'Previous work. ', 'number': '2'}"
"sheep L any of various bovid mammals of the genus O~is and related genera having transversely ribbed horns and a narrow face, There are many breeds of domestic sheep, raised for their wool and for meat.","{'title': 'Previous work. ', 'number': '2'}"
2.,"{'title': 'Previous work. ', 'number': '2'}"
:Barbary sheep.,"{'title': 'Previous work. ', 'number': '2'}"
3.,"{'title': 'Previous work. ', 'number': '2'}"
a meek or timid person.,"{'title': 'Previous work. ', 'number': '2'}"
4.,"{'title': 'Previous work. ', 'number': '2'}"
"separate the sheep from the goats, to pick out the members of any group who are superior in some respects.","{'title': 'Previous work. ', 'number': '2'}"
goat 1.,"{'title': 'Previous work. ', 'number': '2'}"
"any sure-footed agile bovid mammal of the genus Capra, naturally inhabiting rough stony ground in Europe, Asia, and N Africa, typically having a brown-grey colouring and a beard.","{'title': 'Previous work. ', 'number': '2'}"
"Domesticated varieties (C. hircus) are reared for milk, meat, and wool.","{'title': 'Previous work. ', 'number': '2'}"
3.,"{'title': 'Previous work. ', 'number': '2'}"
a lecherous man. 4.,"{'title': 'Previous work. ', 'number': '2'}"
a bad or inferior member of any group 6.,"{'title': 'Previous work. ', 'number': '2'}"
act (or play) the (giddy) goat.,"{'title': 'Previous work. ', 'number': '2'}"
to fool around.,"{'title': 'Previous work. ', 'number': '2'}"
7.,"{'title': 'Previous work. ', 'number': '2'}"
get (someone's) goat.,"{'title': 'Previous work. ', 'number': '2'}"
to cause annoyance to (someone) page I 1.,"{'title': 'Previous work. ', 'number': '2'}"
"one side of one of the leaves of a book, newspaper, letter, etc. or the written or printed matter it bears.","{'title': 'Previous work. ', 'number': '2'}"
2.,"{'title': 'Previous work. ', 'number': '2'}"
such a leaf considered as a unit 3.,"{'title': 'Previous work. ', 'number': '2'}"
"an episode, phase, or period 4.","{'title': 'Previous work. ', 'number': '2'}"
Printing.,"{'title': 'Previous work. ', 'number': '2'}"
the type as set up for printing a page.,"{'title': 'Previous work. ', 'number': '2'}"
6.,"{'title': 'Previous work. ', 'number': '2'}"
"to look through (a book, report, etc.); leaf through.","{'title': 'Previous work. ', 'number': '2'}"
page 2 1.,"{'title': 'Previous work. ', 'number': '2'}"
"a boy employed to run errands, carry messages, etc., for the guests in a hotel, club, etc. 2.","{'title': 'Previous work. ', 'number': '2'}"
a youth in attendance at official functions or ceremonies.,"{'title': 'Previous work. ', 'number': '2'}"
3.,"{'title': 'Previous work. ', 'number': '2'}"
"a. a boy in training for knighthood in personal attendance on a knight, b. a youth in the personal service of a person of rank.","{'title': 'Previous work. ', 'number': '2'}"
4.,"{'title': 'Previous work. ', 'number': '2'}"
an attendant at Congress or other legislative body.,"{'title': 'Previous work. ', 'number': '2'}"
5.,"{'title': 'Previous work. ', 'number': '2'}"
"a boy or girl employed in the debating chamber of the house of Commons, the Senate, or a legislative assembly to carry messages for members.","{'title': 'Previous work. ', 'number': '2'}"
6.,"{'title': 'Previous work. ', 'number': '2'}"
to call out the name of (a person).,"{'title': 'Previous work. ', 'number': '2'}"
7.,"{'title': 'Previous work. ', 'number': '2'}"
"to call (a person) by an electronic device, such as bleep, g. to act as a page to or attend as a page.","{'title': 'Previous work. ', 'number': '2'}"
"If these two words appear together in context, the appropriate senses of pen (2.1: ""enclosure"") and sheep (1: ""mammal"") will be chosen because the definitions of these two senses have the word domestic in common.","{'title': 'Previous work. ', 'number': '2'}"
"However, with one word as a basis, the relation is tenuous and wholly dependent upon a particular dictionary's wording.","{'title': 'Previous work. ', 'number': '2'}"
The method also fails to take into account less immediate r lationships between words.,"{'title': 'Previous work. ', 'number': '2'}"
"As a result, it will not determine the correct sense of pen in the context of goat.","{'title': 'Previous work. ', 'number': '2'}"
"The correct sense of pen (2.1: enclosure ) and the correct sense of goat (1: mammal ) do not share any words in common in their definitions in the Collins English Dictionary; however, a strategy 390 which takes into account a longer path through definitions will find that animal is in the definition of pen 2.1, each of mammal and animal appear in the definition of the other, and mammal is in the definition of goat 1.","{'title': 'Previous work. ', 'number': '2'}"
"Similarly, Lesk's method would also be unable to determine the correct sense of pen (1.1: writing utensil ) in the context of page, because seven of the thirteen senses of pen have the same number of overlaps with senses of page.","{'title': 'Previous work. ', 'number': '2'}"
"Six of the senses of pen share only the word write with the correct sense of page (1.1: ""leaf of a book"").","{'title': 'Previous work. ', 'number': '2'}"
"However, pen 1.1 also contains words such as draw and ink, and page 1.1 contains book, newspaper, letter, and print.","{'title': 'Previous work. ', 'number': '2'}"
These other words are heavily interconnected in a complex network which cannot be discovered by simply counting overlaps.,"{'title': 'Previous work. ', 'number': '2'}"
"Wilks et al (forthcoming) build on Lesk's method by computing the degree of overlap for related word-sets constructed using co-occurrence data from definition texts, but their method suffers from the same problems, in addition to combinatorial problems thai prevent disambiguating more than one word at a time.","{'title': 'Previous work. ', 'number': '2'}"
2.2.,"{'title': 'Previous work. ', 'number': '2'}"
Neural networks for WSD.,"{'title': 'Previous work. ', 'number': '2'}"
"Neural network approaches to WSD have been suggested (Cottrell and Small, 1983; Waltz and Pollack, 1985).","{'title': 'Previous work. ', 'number': '2'}"
"These models consist of networks in which the nodes (""neurons"") represent words or concepts, connected by ""activatory"" links: the words activate the concepts to which they are semantically related, and vice versa.","{'title': 'Previous work. ', 'number': '2'}"
"In addition, ""lateral"" inhibitory links usually interconnect competing senses of a given word.","{'title': 'Previous work. ', 'number': '2'}"
"Initially, the nodes corresponding tothe words in the sentence to be analyzed are activated.","{'title': 'Previous work. ', 'number': '2'}"
"These words activate their neighbors in the next cycle in turn, these neighbors activate their immediate neighbors, and so on.","{'title': 'Previous work. ', 'number': '2'}"
"After a number of cycles, the network stabilizes in a state in which one sense for each input word is more activated than the others, using a parallel, analog, relaxation process.","{'title': 'Previous work. ', 'number': '2'}"
Neural network approaches to WSD seem able to capture most of what cannot be handled by overlap strategies such as Lesk's.,"{'title': 'Previous work. ', 'number': '2'}"
"However, the networks used in experiments o far are hand-coded and thus necessarily very small (at most, a few dozen words and concepts).","{'title': 'Previous work. ', 'number': '2'}"
"Due to a lack of real-size data, it is not clear that he same neural net models will scale up for realistic application.","{'title': 'Previous work. ', 'number': '2'}"
"Further, some approaches rely on ""context- setting"" nodes to prime particular word senses in order to force 1the correct interpretation?","{'title': 'Previous work. ', 'number': '2'}"
"But as Waltz and Pollack point out, it is possible that such words (e.g., writing in the context of pen ) are not explicitly present in the text under analysis, but may be inferred by the reader from the presence of other, related words (e.g., page, book, inkwell, etc.).","{'title': 'Previous work. ', 'number': '2'}"
"To solve this problem, words in such networks have been represented by sets of semantic ""microfeatures"" (Waltz and Pollack, 1985; Bookman, 1987) which correspond to fundamental semantic distinctions (animate/inanimate, edible/ inedible, threatening/safe, etc.), characteristic duration of events (second, minute, hour, day, etc.), locations (city, country, continent, etc.), and other similar distinctions that humans typically make about situations in the world.","{'title': 'Previous work. ', 'number': '2'}"
"To be comprehensive, the authors uggest that these features must number in the thousands.","{'title': 'Previous work. ', 'number': '2'}"
"Each concept iin the network is linked, via bidirectional activatory or inhibitory links, to only a subset of the complete microfeature s t. A given concept theoretically shares everal microfeatures with concepts to which it is closely related, and will therefore activate the nodes corresponding to closely related concepts when it is activated :itself.","{'title': 'Previous work. ', 'number': '2'}"
"ttowever, such schemes are problematic due to the difficulties of designing an appropriate set of microfeatures, which in essence consists of designing semantic primitives.","{'title': 'Previous work. ', 'number': '2'}"
"This becomes clear when one exmnines the sample microfeatures given by Waltz ~md Pollack: they specify micro.f carfares uch as CASINO and CANYON, but it is obviously questionable whether such concepts constitute fundamental semantic distinctions.","{'title': 'Previous work. ', 'number': '2'}"
"More practically, it is simply difficult to imagine how vectors of several thousands of microfeamrcs for each one of the lens of thousands of words and hundreds of thousands of senses can be realistically encoded by hand.","{'title': 'Previous work. ', 'number': '2'}"
"Our approach to WSD takes advantage of both strategies outlined above, but enables us to address solutions to their shortcomings.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"This work has been carried out in tile context of a joint project of Vassar College and the Groupe Reprdsentation et Traitement des Connaissances of the Centre National de la Recherche Scientifique (CNRS), which is concerned with the construction and exploitation of a large lexical data base of English and French.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"At present, the Vassar/CNRS data base includes, through the courtesy of several editors and research institutions, several English and French dictionaries (the Collins English Dictionary, the Oxford Advanced Learner's Dictionary, the COBUILD Dictionary, the Longman) Dictionary of Contemporary English, theWebster's 9th Dictionary, and the ZYZOMYS CD-ROM dictionary from Hachette Publishers) as well as several other lexical and textual materials (the Brown Corpus of American English, the CNRS BDLex data base, the MRC Psycholinguistic Data Base, etc.).","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
We build VLNNs utilizing definitions in the Collins English Dictionary.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Like Lesk and Wilks, we assume that there are significant semantic relations between a word and the words used to define it.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
The connections in the network reflect these relations.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"All of the knowledge represented in the network is automatically generated from a machine-readable dictionary, and therefore no hand coding is required.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Further, the lexicon m~d the knowledge it contains potentially cover all of English (90,000 words), and as a result this information cml potentially be used to help dismnbiguate unrestricted text.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
3.1.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Topology of the network.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"In our model, words are complex units.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Each word in the input is represented by a word node connected by excitatory links to sense nodes (figure 2) representing the different possible senses tbr that word in the Collins English Dictionary.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Each sense node is in turn connected by excitatory links to word nodes rcpreseming the words in tile definition of that sense.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"This process is repeated a number of times, creating an increasingly complex and interconnected network.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Ideally, the network would include the entire dictionary, but for practical reasons we limit the number of repetitions and thus restrict tile size of the network to a few thousand nodes and 10 to 20 thousand transitions.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"All words in the network are reduced to their lemmas, and grammatical words are excluded.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
The different sense nodes tor a given word are interconnected by lateral inhibitory links.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
3 391 Figure 2.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Topology of the network ~.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
", : ' .i \ [ ~ Word Node Sense Node ~ . Excitatory Link ..........................","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Inhibitory Link When the network is run, the input word nodes are activated first.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Then each input word node sends activation to its sense nodes, which in turn send activation to the word nodes to which they are connected, and so on throughout he network for a number of cycles.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"At each cycle, word and sense nodes receive feedback from connected nodes.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Competing sense nodes send inhibition to one another.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Feedback and inhibition cooperate in a ""winner-take-all"" strategy to activate increasingly related word and sense nodes and deactivate the unrelated or weakly related nodes.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Eventually, after a few dozen cycles, the network stabilizes in a configuration where only the sense nodes with the strongest relations to other nodes in the network are activated.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Because of the ""winner-take-all"" strategy, at most one sense node per word will ultimately be activated.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Our model does not use microfeatures, because, as we will show below, the context is taken into account by the number of nodes in the network and the extent to which they are heavily interconnected.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"So far, we do not consider the syntax of the input sentence, in order to locus on the semantic properties of the model.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"However, it is clear that syntactic information can assist in the disambiguation process in certain cases, and a network including a syntactic layer, such as that proposed by Waltz and Pol lack, would undoubtedly enhance the model's behavior.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
3.2.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
Results.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
The network finds the correct sense in cases where Lesk's strategy succeeds.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"For example, if the input consists of pen and sheep, pen 2.1 and sheep 1 are correct ly act ivated.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"More interestingly, the network selects "" the appropriate senses in cases where Lesk's strategy fails.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Figures 3 and 4 show the state of the network after being run with pen and goat, and pen and page, respectively.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
The figures represent only the most activated part of each network after 100 cycles.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Over the course of the run, the network reinforces only a small cluster of the most semantically relevant words and senses, and filters out tile rest of the thousands of nodes.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"The correct sense for each word in each context (pen 2.1 with goat 1, and pen 1.1 withpage 1.1) is the only one activated at the end of the run.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
This model solves the context-setting problem mentioned above without any use of microfeatures.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Sense 1.1 of pen would also be activated if it appeared in the context of a large number of other words--e.g., book, ink, inkwell, pencil, paper, write, draw, sketch, etc.--which ave a similar semantic relationship to pen.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"For example, figure 5 shows the state of the network after being run with pen and book.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
It is apparent that the subset of nodes activated is similar to those which were activated by page.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
392 4 Figure 3.,"{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"State of the network after being run with ""pen"" and ""goat"" \[ are the most activated } Figure 4.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"State of the network after being run with ""pen"" and ""page"" ~ \[ The darker nodes \] Figure 5.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"State of the network after being run with ""pen"" and ""book"" r The darker nodes \] ~ ~ , ook 393 The examples given here utilize only two words as input, in order to show clearly the behavior of the network.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"In fact, the performance of the network improves with additional input, since additional context can only contribute more to the disambiguation process.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"For example, given the sentence The young page put the sheep in the pen, the network correctly chooses the correct senses of page (2.3: ""a youth in personal service""), sheep (1), and pen (2.1).","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"This example is particularly difficult, because page and sheep compete against each other to activate different senses of pen, as demonstrated in the examples above.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"However, the word young reinforces sense 2.3 of page, which enables sheep to win the struggle.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Inter-sentential context could be used as well, by retaining the most activated nodes within the network during subsequent runs.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"By running various experiments on VLNNs, we have discovered that when the simple models proposed so far are scaled up, several improvements are necessary.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"We have, for instance, discovered that ""gang effects"" appear due to extreme imbalance among words having few senses and hence few connections, and words containing up to 80 senses and several hundred connections, and that therefore dampening is required.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"tn addition, we have found that is is necessary to treat a word node and its sense nodes as a complex, ecological unit rather than as separate ntities.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"In our model, word nodes corttrol the behavior of sense nodes by means of a differential neuron that prevents, for example, a sense node from becoming more activated than its master word node.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"Our experimentation with VLNNs has also shed light on the role of and need for various other parameters, uch as thresholds, decay, etc.","{'title': 'Word sense disambiguation with VLNNs. ', 'number': '3'}"
"The use of word relations implicitly encoded in machine-readable dictionaries, coupled with the neural network strategy, seems to offer a promising approach to WSD.","{'title': 'Conclus ion. ', 'number': '4'}"
"This approach succeeds where the Lesk strategy fails, and it does not require determining and encoding microfeatures or other semantic information.","{'title': 'Conclus ion. ', 'number': '4'}"
"The model is also more robust than the Lesk strategy, since it does not rely on the presence or absence of a particular word or words and can filter out some degree of ""noise"" (such as inclusion of some wrong lemmas due to lack of information about part-of-speech or occasional activation of misleading homographs).","{'title': 'Conclus ion. ', 'number': '4'}"
"How- ever, there are clearly several improvements which can be made: for instance, the part-of-speech for input words and words in definitions can be used to extract only the correct lemmas from the dictionary, the frequency of use for particular senses of each word can be used to help choose among competing senses, and additional knowledge can be extracted from other dictionaries and thesauri.","{'title': 'Conclus ion. ', 'number': '4'}"
"It is also conceivable that the network could ""learn"" by giving more weight to links which have been heavily activated over numerous runs on large samples of text.","{'title': 'Conclus ion. ', 'number': '4'}"
"The model we describe here is only a first step toward a fuller understanding and refinement of the use of VLNNs for language processing, and it opens several interesting avenues for further application and research.","{'title': 'Conclus ion. ', 'number': '4'}"
