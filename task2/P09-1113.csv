col1,col2
Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.,Introduction
"We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size.",Introduction
"Our experiments use Freebase, a large semantic database of several thousand relations, to For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier.",Introduction
"Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).",Introduction
"Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.",Introduction
"We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.",Introduction
"At least three learning paradigms have been applied to the task of extracting relational facts from text (for example, learning that a person is employed by a particular organization, or that a geographic entity is located in a particular region).",Experiment/Discussion
"In supervised approaches, sentences in a corpus are first hand-labeled for the presence of entities and the relations between them.",Experiment/Discussion
"The NIST Automatic Content Extraction (ACE) RDC 2003 and 2004 corpora, for example, include over 1,000 documents in which pairs of entities have been labeled with 5 to 7 major relation types and 23 to 24 subrelations, totaling 16,771 relation instances.",Experiment/Discussion
"ACE systems then extract a wide variety of lexical, syntactic, and semantic features, and use supervised classifiers to label the relation mention holding between a given pair of entities in a test set sentence, optionally combining relation mentions (Zhou et al., 2005; Zhou et al., 2007; Surdeanu and Ciaramita, 2007).",Experiment/Discussion
"Supervised relation extraction suffers from a number of problems, however.",Experiment/Discussion
Labeled training data is expensive to produce and thus limited in quantity.,Experiment/Discussion
"Also, because the relations are labeled on a particular corpus, the resulting classifiers tend to be biased toward that text domain.",Experiment/Discussion
"An alternative approach, purely unsupervised information extraction, extracts strings of words between entities in large amounts of text, and clusters and simplifies these word strings to produce relation-strings (Shinyama and Sekine, 2006; Banko et al., 2007).",Experiment/Discussion
"Unsupervised approaches can use very large amounts of data and extract very large numbers of relations, but the resulting relations may not be easy to map to relations needed for a particular knowledge base.",Experiment/Discussion
"A third approach has been to use a very small number of seed instances or patterns to do bootstrap learning (Brin, 1998; Riloff and Jones, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2005; Pennacchiotti and Pantel, 2006; Bunescu and Mooney, 2007; Rozenfeld and Feldman, 2008).",Experiment/Discussion
"These seeds are used with a large corpus to extract a new set of patterns, which are used to extract more instances, which are used to extract more patterns, in an iterative fashion.",Experiment/Discussion
The resulting patterns often suffer from low precision and semantic drift.,Experiment/Discussion
"We propose an alternative paradigm, distant supervision, that combines some of the advantages of each of these approaches.",Experiment/Discussion
"Distant supervision is an extension of the paradigm used by Snow et al. (2005) for exploiting WordNet to extract hypernym (is-a) relations between entities, and is similar to the use of weakly labeled data in bioinformatics (Craven and Kumlien, 1999; Morgan et al., 2004).",Experiment/Discussion
"Our algorithm uses Freebase (Bollacker et al., 2008), a large semantic database, to provide distant supervision for relation extraction.",Experiment/Discussion
"Freebase contains 116 million instances of 7,300 relations between 9 million entities.",Experiment/Discussion
The intuition of distant supervision is that any sentence that contains a pair of entities that participate in a known Freebase relation is likely to express that relation in some way.,Experiment/Discussion
"Since there may be many sentences containing a given entity pair, we can extract very large numbers of (potentially noisy) features that are combined in a logistic regression classifier.",Experiment/Discussion
"Thus whereas the supervised training paradigm uses a small labeled corpus of only 17,000 relation instances as training data, our algorithm can use much larger amounts of data: more text, more relations, and more instances.",Experiment/Discussion
"We use 1.2 million Wikipedia articles and 1.8 million instances of 102 relations connecting 940,000 entities.",Experiment/Discussion
"In addition, combining vast numbers of features in a large classifier helps obviate problems with bad features.",Experiment/Discussion
"Because our algorithm is supervised by a database, rather than by labeled text, it does not suffer from the problems of overfitting and domain-dependence that plague supervised systems.",Experiment/Discussion
"Supervision by a database also means that, unlike in unsupervised approaches, the output of our classifier uses canonical names for relations.",Experiment/Discussion
Our paradigm offers a natural way of integrating data from multiple sentences to decide if a relation holds between two entities.,Experiment/Discussion
"Because our algorithm can use large amounts of unlabeled data, a pair of entities may occur multiple times in the test set.",Experiment/Discussion
"For each pair of entities, we aggregate the features from the many different sentences in which that pair appeared into a single feature vector, allowing us to provide our classifier with more information, resulting in more accurate labels.",Experiment/Discussion
Table 1 shows examples of relation instances extracted by our system.,Experiment/Discussion
We also use this system to investigate the value of syntactic versus lexical (word sequence) features in relation extraction.,Experiment/Discussion
"While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE.",Experiment/Discussion
"Most previous research in bootstrapping or unsupervised IE has used only simple lexical features, thereby avoiding the computational expense of parsing (Brin, 1998; Agichtein and Gravano, 2000; Etzioni et al., 2005), and the few systems that have used unsupervised IE have not compared the performance of these two types of feature.",Experiment/Discussion
"Except for the unsupervised algorithms discussed above, previous supervised or bootstrapping approaches to relation extraction have typically relied on relatively small datasets, or on only a small number of distinct relations.",Experiment/Discussion
"Approaches based on WordNet have often only looked at the hypernym (is-a) or meronym (part-of) relation (Girju et al., 2003; Snow et al., 2005), while those based on the ACE program (Doddington et al., 2004) have been restricted in their evaluation to a small number of relation instances and corpora of less than a million words.",Experiment/Discussion
Many early algorithms for relation extraction used little or no syntactic information.,Experiment/Discussion
"For example, the DIPRE algorithm by Brin (1998) used string-based regular expressions in order to recognize relations such as author-book, while the SNOWBALL algorithm by Agichtein and Gravano (2000) learned similar regular expression patterns over words and named entity tags.",Experiment/Discussion
Hearst (1992) used a small number of regular expressions over words and part-of-speech tags to find examples of the hypernym relation.,Experiment/Discussion
"The use of these patterns has been widely replicated in successful systems, for example by Etzioni et al. (2005).",Experiment/Discussion
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations.,Experiment/Discussion
"More recent approaches have used deeper syntactic information derived from parses of the input sentences, including work exploiting syntactic dependencies by Lin and Pantel (2001) and Snow et al. (2005), and work in the ACE paradigm such as Zhou et al.",Experiment/Discussion
(2005) and Zhou et al. (2007).,Experiment/Discussion
Perhaps most similar to our distant supervision algorithm is the effective method of Wu and Weld (2007) who extract relations from a Wikipedia page by using supervision from the page’s infobox.,Experiment/Discussion
"Unlike their corpus-specific method, which is specific to a (single) Wikipedia page, our algorithm allows us to extract evidence for a relation from many different documents, and from any genre.",Experiment/Discussion
"Following the literature, we use the term ‘relation’ to refer to an ordered, binary relation between entities.",Experiment/Discussion
We refer to individual ordered pairs in this relation as ‘relation instances’.,Experiment/Discussion
"For example, the person-nationality relation holds between the entities named ‘John Steinbeck’ and ‘United States’, so it has (John Steinbeck, United States) as an instance.",Experiment/Discussion
"We use relations and relation instances from Freebase, a freely available online database of structured semantic data.",Experiment/Discussion
Data in Freebase is collected from a variety of sources.,Experiment/Discussion
One major source is text boxes and other tabular data from Wikipedia.,Experiment/Discussion
"Data is also taken from NNDB (biographical information), MusicBrainz (music), the SEC (financial and corporate data), as well as direct, wiki-style user editing.",Experiment/Discussion
"After some basic processing of the July 2008 link export to convert Freebase’s data representation into binary relations, we have 116 million instances of 7,300 relations between 9 million entities.",Experiment/Discussion
We next filter out nameless and uninteresting entities such as user profiles and music tracks.,Experiment/Discussion
"Freebase also contains the reverses of many of its relations (bookauthor v. author-book), and these are merged.",Experiment/Discussion
"Filtering and removing all but the largest relations leaves us with 1.8 million instances of 102 relations connecting 940,000 entities.",Experiment/Discussion
Examples are shown in Table 2.,Experiment/Discussion
The intuition of our distant supervision approach is to use Freebase to give us a training set of relations and entity pairs that participate in those relations.,Experiment/Discussion
"In the training step, all entities are identified in sentences using a named entity tagger that labels persons, organizations and locations.",Experiment/Discussion
"If a sentence contains two entities and those entities are an instance of one of our Freebase relations, features are extracted from that sentence and are added to the feature vector for the relation.",Experiment/Discussion
"The distant supervision assumption is that if two entities participate in a relation, any sentence that contain those two entities might express that relation.",Experiment/Discussion
"Because any individual sentence may give an incorrect cue, our algorithm trains a multiclass logistic regression classifier, learning weights for each noisy feature.",Experiment/Discussion
"In training, the features for identical tuples (relation, entity1, entity2) from different sentences are combined, creating a richer feature vector.",Experiment/Discussion
"In the testing step, entities are again identified using the named entity tagger.",Experiment/Discussion
"This time, every pair of entities appearing together in a sentence is considered a potential relation instance, and whenever those entities appear together, features are extracted on the sentence and added to a feature vector for that entity pair.",Experiment/Discussion
"For example, if a pair of entities occurs in 10 sentences in the test set, and each sentence has 3 features extracted from it, the entity pair will have 30 associated features.",Experiment/Discussion
"Each entity pair in each sentence in the test corpus is run through feature extraction, and the regression classifier predicts a relation name for each entity pair based on the features from all of the sentences in which it appeared.",Experiment/Discussion
"Consider the location-contains relation, imagining that in Freebase we had two instances of this relation: (Virginia, Richmond) and (France, Nantes).",Experiment/Discussion
"As we encountered sentences like ‘Richmond, the capital of Virginia’ and ‘Henry’s Edict of Nantes helped the Protestants of France’ we would extract features from these sentences.",Experiment/Discussion
"Some features would be very useful, such as the features from the Richmond sentence, and some would be less useful, like those from the Nantes sentence.",Experiment/Discussion
"In testing, if we came across a sentence like ‘Vienna, the capital of Austria’, one or more of its features would match those of the Richmond sentence, providing evidence that (Austria, Vienna) belongs to the locationcontains relation.",Experiment/Discussion
Note that one of the main advantages of our architecture is its ability to combine information from many different mentions of the same relation.,Experiment/Discussion
"Consider the entity pair (Steven Spielberg, Saving Private Ryan) from the following two sentences, as evidence for the film-director relation.",Experiment/Discussion
"The first sentence, while providing evidence for film-director, could instead be evidence for filmwriter orfilm-producer.",Experiment/Discussion
"The second sentence does not mention that Saving Private Ryan is a film, and so could instead be evidence for the CEO relation (consider ‘Robert Mueller directed the FBI’).",Experiment/Discussion
"In isolation, neither of these features is conclusive, but in combination, they are.",Experiment/Discussion
Our features are based on standard lexical and syntactic features from the literature.,Experiment/Discussion
"Each feature describes how two entities are related in a sentence, using either syntactic or non-syntactic information.",Experiment/Discussion
Our lexical features describe specific words between and surrounding the two entities in the sentence in which they appear: Each lexical feature consists of the conjunction of all these components.,Experiment/Discussion
"We generate a conjunctive feature for each k E 10, 1, 21.",Experiment/Discussion
Thus each lexical row in Table 3 represents a single lexical feature.,Experiment/Discussion
"Part-of-speech tags were assigned by a maximum entropy tagger trained on the Penn Treebank, and then simplified into seven categories: nouns, verbs, adverbs, adjectives, numbers, foreign words, and everything else.",Experiment/Discussion
"In an attempt to approximate syntactic features, we also tested variations on our lexical features: (1) omitting all words that are not verbs and (2) omitting all function words.",Experiment/Discussion
"In combination with the other lexical features, they gave a small boost to precision, but not large enough to justify the increased demand on our computational resources.",Experiment/Discussion
In addition to lexical features we extract a number of features based on syntax.,Experiment/Discussion
"In order to generate these features we parse each sentence with the broad-coverage dependency parser MINIPAR (Lin, 1998).",Experiment/Discussion
A dependency parse consists of a set of words and chunks (e.g.,Experiment/Discussion
"‘Edwin Hubble’, ‘Missouri’, ‘born’), linked by directional dependencies (e.g.",Experiment/Discussion
"‘pred’, ‘lex-mod’), as in Figure 1.",Experiment/Discussion
For each sentence we extract a dependency path between each pair of entities.,Experiment/Discussion
"A dependency path consists of a series of dependencies, directions and words/chunks representing a traversal of the parse.",Experiment/Discussion
Part-of-speech tags are not included in the dependency path.,Experiment/Discussion
Our syntactic features are similar to those used in Snow et al. (2005).,Experiment/Discussion
They consist of the conjunction of: A window node is a node connected to one of the two entities and not part of the dependency path.,Experiment/Discussion
"We generate one conjunctive feature for each pair of left and right window nodes, as well as features which omit one or both of them.",Experiment/Discussion
Thus each syntactic row in Table 3 represents a single syntactic feature.,Experiment/Discussion
"Every feature contains, in addition to the content described above, named entity tags for the two entities.",Experiment/Discussion
"We perform named entity tagging using the Stanford four-class named entity tagger (Finkel et al., 2005).",Experiment/Discussion
"The tagger provides each word with a label from {person, location, organization, miscellaneous, none}.",Experiment/Discussion
"Rather than use each of the above features in the classifier independently, we use only conjunctive features.",Experiment/Discussion
"Each feature consists of the conjunction of several attributes of the sentence, plus the named entity tags.",Experiment/Discussion
"For two features to match, all of their conjuncts must match exactly.",Experiment/Discussion
This yields low-recall but high-precision features.,Experiment/Discussion
"With a small amount of data, this approach would be problematic, since most features would only be seen once, rendering them useless to the classifier.",Experiment/Discussion
"Since we use large amounts of data, even complex features appear multiple times, allowing our highprecision features to work as intended.",Experiment/Discussion
Features for a sample sentence are shown in Table 3.,Experiment/Discussion
"For unstructured text we use the Freebase Wikipedia Extraction, a dump of the full text of all Wikipedia articles (not including discussion and user pages) which has been sentence-tokenized by Metaweb Technologies, the developers of Freebase (Metaweb, 2008).",Experiment/Discussion
"This dump consists of approximately 1.8 million articles, with an average of 14.3 sentences per article.",Experiment/Discussion
"The total number of words (counting punctuation marks) is 601,600,703.",Experiment/Discussion
"For our experiments we use about half of the articles: 800,000 for training and 400,000 for testing.",Experiment/Discussion
"We use Wikipedia because it is relatively upto-date, and because its sentences tend to make explicit many facts that might be omitted in newswire.",Experiment/Discussion
"Much of the information in Freebase is derived from tabular data from Wikipedia, meaning that Freebase relations are more likely to appear in sentences in Wikipedia.",Experiment/Discussion
Each sentence of this unstructured text is dependency parsed by MINIPAR to produce a dependency graph.,Experiment/Discussion
"In preprocessing, consecutive words with the same named entity tag are ‘chunked’, so that Edwin/PERSON Hubble/PERSON becomes [Edwin Hubble]/PERSON.",Experiment/Discussion
"This chunking is restricted by the dependency parse of the sentence, however, in that chunks must be contiguous in the parse (i.e., no chunks across subtrees).",Experiment/Discussion
"This ensures that parse tree structure is preserved, since the parses must be updated to reflect the chunking.",Experiment/Discussion
"For held-out evaluation experiments (see section 7.1), half of the instances of each relation are not used in training, and are later used to compare against newly discovered instances.",Experiment/Discussion
"This means that 900,000 Freebase relation instances are used in training, and 900,000 are held out.",Experiment/Discussion
"These experiments used 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase.",Experiment/Discussion
"For human evaluation experiments, all 1.8 million relation instances are used in training.",Experiment/Discussion
"Again, we use 800,000 Wikipedia articles in the training phase and 400,000 different articles in the testing phase.",Experiment/Discussion
"For all our experiments, we only extract relation instances that do not appear in our training data, i.e., instances that are not already in Freebase.",Experiment/Discussion
Our system needs negative training data for the purposes of constructing the classifier.,Experiment/Discussion
"Towards this end, we build a feature vector in the training phase for an ‘unrelated’ relation by randomly selecting entity pairs that do not appear in any Freebase relation and extracting features for them.",Experiment/Discussion
While it is possible that some of these entity pairs on the 102 largest relations we use.,Experiment/Discussion
"Precision for three different feature sets (lexical features, syntactic features, and both) is reported at recall levels from 10 to 100,000.",Experiment/Discussion
"At the 100,000 recall level, we classify most of the instances into three relations: 60% as location-contains, 13% as person-place-of-birth, and 10% as person-nationality. are in fact related but are wrongly omitted from the Freebase data, we expect that on average these false negatives will have a small effect on the performance of the classifier.",Experiment/Discussion
"For performance reasons, we randomly sample 1% of such entity pairs for use as negative training examples.",Experiment/Discussion
"By contrast, in the actual test data, 98.7% of the entity pairs we extract do not possess any of the top 102 relations we consider in Freebase.",Experiment/Discussion
We use a multi-class logistic classifier optimized using L-BFGS with Gaussian regularization.,Experiment/Discussion
"Our classifier takes as input an entity pair and a feature vector, and returns a relation name and a confidence score based on the probability of the entity pair belonging to that relation.",Experiment/Discussion
"Once all of the entity pairs discovered during testing have been classified, they can be ranked by confidence score and used to generate a list of the n most likely new relation instances.",Experiment/Discussion
Table 4 shows some high-weight features learned by our system.,Experiment/Discussion
We discuss the results in the next section.,Experiment/Discussion
"We evaluate labels in two ways: automatically, by holding out part of the Freebase relation data during training, and comparing newly discovered relation instances against this held-out data, and manually, having humans who look at each positively labeled entity pair and mark whether the relation indeed holds between the participants.",Experiment/Discussion
Both evaluations allow us to calculate the precision of the system for the best N instances.,Experiment/Discussion
Figure 2 shows the performance of our classifier on held-out Freebase relation data.,Experiment/Discussion
"While held-out evaluation suffers from false negatives, it gives a rough measure of precision without requiring expensive human evaluation, making it useful for parameter setting.",Experiment/Discussion
"At most recall levels, the combination of syntactic and lexical features offers a substantial improvement in precision over either of these feature sets on its own.",Experiment/Discussion
"Human evaluation was performed by evaluators on Amazon’s Mechanical Turk service, shown to be effective for natural language annotation in Snow et al. (2008).",Experiment/Discussion
We ran three experiments: one using only syntactic features; one using only lexical features; and one using both syntactic and lexical features.,Experiment/Discussion
"For each of the 10 relations that appeared most frequently in our test data (according to our classifier), we took samples from the first 100 and 1000 instances of this relation generated in each experiment, and sent these to Mechanical Turk for results per relation, using stratified samples.",Experiment/Discussion
‘Average’ gives the mean precision of the 10 relations.,Experiment/Discussion
Key: Syn = syntactic features only.,Experiment/Discussion
Lex = lexical features only.,Experiment/Discussion
We use stratified samples because of the overabundance of location-contains instances among our high-confidence results. human evaluation.,Experiment/Discussion
Our sample size was 100.,Experiment/Discussion
Each predicted relation instance was labeled as true or false by between 1 and 3 labelers on Mechanical Turk.,Experiment/Discussion
We assigned the truth or falsehood of each relation according to the majority vote of the labels; in the case of a tie (one vote each way) we assigned the relation as true or false with equal probability.,Experiment/Discussion
"The evaluation of the syntactic, lexical, and combination of features at a recall of 100 and 1000 instances is presented in Table 5.",Experiment/Discussion
"At a recall of 100 instances, the combination of lexical and syntactic features has the best performance for a majority of the relations, while at a recall level of 1000 instances the results are mixed.",Experiment/Discussion
No feature set strongly outperforms any of the others across all relations.,Experiment/Discussion
Our results show that the distant supervision algorithm is able to extract high-precision patterns for a reasonably large number of relations.,Results/Conclusion
The held-out results in Figure 2 suggest that the combination of syntactic and lexical features provides better performance than either feature set on its own.,Results/Conclusion
"In order to understand the role of syntactic features, we examine Table 5, the human evaluation of the most frequent 10 relations.",Results/Conclusion
"For the topranking 100 instances of each relation, most of the best results use syntactic features, either alone or in combination with lexical features.",Results/Conclusion
"For the topranking 1000 instances of each relation, the results are more mixed, but syntactic features still helped in most classifications.",Results/Conclusion
We then examine those relations for which syntactic features seem to help.,Results/Conclusion
"For example, syntactic features consistently outperform lexical features for the director-film and writer-film relations.",Results/Conclusion
"As discussed in section 4, these two relations are particularly ambiguous, suggesting that syntactic features may help tease apart difficult relations.",Results/Conclusion
"Perhaps more telling, we noticed many examples with a long string of words between the director and the film: Back Street is a 1932 film made by Universal Pictures, directed by John M. Stahl, and produced by Carl Laemmle Jr. Sentences like this have very long (and thus rare) lexical features, but relatively short dependency paths.",Results/Conclusion
Syntactic features can more easily abstract from the syntactic modifiers that comprise the extraneous parts of these strings.,Results/Conclusion
"Our results thus suggest that syntactic features are indeed useful in distantly supervised information extraction, and that the benefit of syntax occurs in cases where the individual patterns are particularly ambiguous, and where they are nearby in the dependency structure but distant in terms of words.",Results/Conclusion
"It remains for future work to see whether simpler, chunk-based syntactic features might be able to capture enough of this gain without the overhead of full parsing, and whether coreference resolution could improve performance.",Results/Conclusion
"We would like to acknowledge Sarah Spikes for her help in developing the relation extraction system, Christopher Manning and Mihai Surdeanu for their invaluable advice, and Fuliang Weng and Baoshi Yan for their guidance.",Results/Conclusion
Our research was partially funded by the NSF via award IIS0811974 and by Robert Bosch LLC.,Results/Conclusion
