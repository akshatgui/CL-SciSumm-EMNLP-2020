col1,col2
This paper describes the application of discriminative reranking techniques to the problem of machine translation.,{}
"For each sentence in the source language, we obtain from a baseline statistical machine translation system, a ranked best list of candidate translations in the target language.",{}
We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric.,{}
We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.,{}
We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation.,{}
"The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.","{'title': '1 Introduction', 'number': '1'}"
"Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we introduce two novel machine learning algorithms specialized for the MT task.","{'title': '1 Introduction', 'number': '1'}"
Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance.,"{'title': '1 Introduction', 'number': '1'}"
"Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs).","{'title': '1 Introduction', 'number': '1'}"
"In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years.","{'title': '1 Introduction', 'number': '1'}"
"Based on this work, in this paper, we will present some novel discriminative reranking techniques applied to machine translation.","{'title': '1 Introduction', 'number': '1'}"
"The reranking problem for natural language is neither a classification problem nor a regression problem, and under certain conditions MT reranking turns out to be quite different from parse reranking.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we consider the special issues of applying reranking techniques to the MT task and introduce two perceptron-like reranking algorithms for MT reranking.","{'title': '1 Introduction', 'number': '1'}"
We provide experimental results that show that the proposed algorithms achieve start-of-the-art results on the NIST 2003 Chinese-English large data track evaluation.,"{'title': '1 Introduction', 'number': '1'}"
"The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.","{'title': '1 Introduction', 'number': '1'}"
The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT.,"{'title': '1 Introduction', 'number': '1'}"
"The source and target sentences were treated as the observations, but the alignments were treated as hidden information learned from parallel texts using the EM algorithm.","{'title': '1 Introduction', 'number': '1'}"
"This sourcechannel model treated the task of finding the probability , where is the translation in the target (English) language for a given source (foreign) sentence , as two generative probability models: the language model which is a generative probability over candidate translations and the translation model which is a generative conditional probability of the source sentence given a candidate translation .","{'title': '1 Introduction', 'number': '1'}"
The lexicon of the single-word based IBM models does not take word context into account.,"{'title': '1 Introduction', 'number': '1'}"
This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity.,"{'title': '1 Introduction', 'number': '1'}"
Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.,"{'title': '1 Introduction', 'number': '1'}"
Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.,"{'title': '1 Introduction', 'number': '1'}"
"Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.","{'title': '1 Introduction', 'number': '1'}"
"In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.","{'title': '1 Introduction', 'number': '1'}"
"However, phrase level alignment cannot handle long distance reordering effectively.","{'title': '1 Introduction', 'number': '1'}"
Parse trees have also been used in alignment models.,"{'title': '1 Introduction', 'number': '1'}"
Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.,"{'title': '1 Introduction', 'number': '1'}"
"(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.","{'title': '1 Introduction', 'number': '1'}"
Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.,"{'title': '1 Introduction', 'number': '1'}"
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.,"{'title': '1 Introduction', 'number': '1'}"
The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models.,"{'title': '1 Introduction', 'number': '1'}"
"Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model estimated using a maximum entropy model.","{'title': '1 Introduction', 'number': '1'}"
A small number of feature functions defined on the source and target sentence were used to rerank the translations generated by a baseline MT system.,"{'title': '1 Introduction', 'number': '1'}"
"While the total number of feature functions was small, each feature function was a complex statistical model by itself, as for example, the alignment template feature functions used in this approach.","{'title': '1 Introduction', 'number': '1'}"
Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.,"{'title': '1 Introduction', 'number': '1'}"
The experiments showed that this approach obtains significantly better results than using the maximum mutual information criterion on parameter estimation.,"{'title': '1 Introduction', 'number': '1'}"
"This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).","{'title': '1 Introduction', 'number': '1'}"
"SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.","{'title': '1 Introduction', 'number': '1'}"
More than 450 different feature functions were used in order to improve the syntactic well-formedness of MT output.,"{'title': '1 Introduction', 'number': '1'}"
"By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%.","{'title': '1 Introduction', 'number': '1'}"
"Like machine translation, parsing is another field of natural language processing in which generative models have been widely used.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"In recent years, reranking techniques, especially discriminative reranking, have resulted in significant improvements in parsing.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).","{'title': '2 Ranking and Reranking', 'number': '2'}"
The reranking techniques have resulted in a 13.5% error reduction in labeled recall/precision over the previous best generative parsing models.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
Discriminative reranking methods for parsing typically use the notion of a margin as the distance between the best candidate parse and the rest of the parses.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
The reranking problem is reduced to a classification problem by using pairwise samples.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
"In (Shen and Joshi, 2004), we have introduced a new perceptron-like ordinal regression algorithm for parse reranking.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"In that algorithm, pairwise samples are used for training and margins are defined as the distance between parses of different ranks.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"In addition, the uneven margin technique has been used for the purpose of adapting ordinal regression to reranking tasks.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"In this paper, we apply this algorithm to MT reranking, and we also introduce a new perceptron-like reranking algorithm for MT.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"In the field of machine learning, a class of tasks (called ranking or ordinal regression) are similar to the reranking tasks in NLP.","{'title': '2 Ranking and Reranking', 'number': '2'}"
One of the motivations of this paper is to apply ranking or ordinal regression algorithms to MT reranking.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
"In the previous works on ranking or ordinal regression, the margin is defined as the distance between two consecutive ranks.","{'title': '2 Ranking and Reranking', 'number': '2'}"
Two large margin approaches have been used.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
"One is the PRank algorithm, a variant of the perceptron algorithm, that uses multiple biases to represent the boundaries between every two consecutive ranks (Crammer and Singer, 2001; Harrington, 2003).","{'title': '2 Ranking and Reranking', 'number': '2'}"
"However, as we will show in section 3.7, the PRank algorithm does not work on the reranking tasks due to the introduction of global ranks.","{'title': '2 Ranking and Reranking', 'number': '2'}"
"The other approach is to reduce the ranking problem to a classification problem by using the method of pairwise samples (Herbrich et al., 2000).","{'title': '2 Ranking and Reranking', 'number': '2'}"
The underlying assumption is that the samples of consecutive ranks are separable.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
This may become a problem in the case that ranks are unreliable when ranking does not strongly distinguish between candidates.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
This is just what happens in reranking for machine translation.,"{'title': '2 Ranking and Reranking', 'number': '2'}"
"The reranking approach for MT is defined as follows: First, a baseline system generates -best candidates.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Features that can potentially discriminate between good vs. bad translations are extracted from these -best candidates.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
These features are then used to determine a new ranking for the -best list.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
The new top ranked candidate in this -best list is our new best candidate translation.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Discriminative reranking allows us to use global features which are unavailable for the baseline system.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Second, we can use features of various kinds and need not worry about fine-grained smoothing issues.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Finally, the statistical machine learning approach has been shown to be effective in many NLP tasks.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Reranking enables rapid experimentation with complex feature functions, because the complex decoding steps in SMT are done once to generate the N-best list of translations.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"First, we consider how to apply discriminative reranking to machine translation.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
We may directly use those algorithms that have been successfully used in parse reranking.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"However, we immediately find that those algorithms are not as appropriate for machine translation.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Let be the candidate ranked at the th position for the source sentence, where ranking is defined on the quality of the candidates.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In parse reranking, we look for parallel hyperplanes successfully separating and for all the source sentences, but in MT, for each source sentence, we have a set of reference translations instead of a single gold standard.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"For this reason, it is hard to define which candidate translation is the best.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Suppose we have two translations, one of which is close to reference translation ref while the other is close to reference translation ref .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
It is difficult to say that one candidate is better than the other.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Although we might invent metrics to define the quality of a translation, standard reranking algorithms cannot be directly applied to MT.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In parse reranking, each training sentence has a ranked list of 27 candidates on average (Collins, 2000), but for machine translation, the number of candidate translations in the -best list is much higher.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"(SMT Team, 2003) show that to get a reasonable improvement in the BLEU score at least 1000 candidates need to be considered in the -best list.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In addition, the parallel hyperplanes separating and actually are unable to distinguish good translations from bad translations, since they are not trained to distinguish any translations in .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Furthermore, many good translations in may differ greatly from , since there are multiple references.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
These facts cause problems for the applicability of reranking algorithms.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Our first attempt to handle this problem is to redefine the notion of good translations versus bad translations.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Instead of separating and , we say the top of the -best translations are good translations, and the bottom of the -best translations are bad translations, where .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Then we look for parallel hyperplanes splitting the top translations and bottom translations for each sentence.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Figure 1 illustrates this situation, where and .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Furthermore, if we only look for the hyperplanes to separate the good and the bad translations, we, in fact, discard the order information of translations of the same class.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Maybe knowing that is better than may be useless for training to some extent, but knowing is better than is useful, if .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Although we cannot give an affirmative answer at this time, it is at least reasonable to use the ordering information.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
The problem is how to use the ordering information.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In addition, we only want to maintain the order of two candidates if their ranks are far away from each other.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"On the other hand, we do not care the order of two translations whose ranks are very close, e.g.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
100 and 101.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Thus insensitive ordinal regression is more desirable and is the approach we follow in this paper.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"However, reranking is not an ordinal regression problem.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In reranking evaluation, we are only interested in the quality of the translation with the highest score, and we do not care the order of bad translations.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Therefore we cannot simply regard a reranking problem as an ordinal regression problem, since they have different definitions for the loss function.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"As far as linear classifiers are concerned, we want to maintain a larger margin in translations of high ranks and a smaller margin in translations of low ranks.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"For example, The reason is that the scoring function will be penalized There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987).","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"However, SVMs are extremely slow in training since they need to solve a quadratic programming search.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Its large margin version is able to provide relatively good results in general.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In previous work on the PRank algorithm, ranks are defined on the entire training and test data.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
Thus we can define boundaries between consecutive ranks on the entire data.,"{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"But in MT reranking, ranks are defined over every single source sentence.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"For example, in our data set, the rank of a translation is only the rank among all the translations for the same sentence.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"The training data includes about 1000 sentences, each of which normally has 1000 candidate translations with the exception of short sentences that have a smaller number of candidate translations.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"As a result, we cannot use the PRank algorithm in the reranking task, since there are no global ranks or boundaries for all the samples.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"However, the approach of using pairwise samples does work.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"By pairing up two samples, we compute the relative distance between these two samples in the scoring metric.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In the training phase, we are only interested in whether the relative distance is positive or negative.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"However, the size of generated training samples will be very large.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"For samples, the total number of pairwise samples in (Herbrich et al., 2000) is roughly .","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"In the next section, we will introduce two perceptron-like algorithms that utilize pairwise samples while keeping the complexity of data space unchanged.","{'title': '3 Discriminative Reranking for MT', 'number': '3'}"
"Considering the desiderata discussed in the last section, we present two perceptron-like algorithms for MT reranking.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"The first one is a splitting algorithm specially designed for MT reranking, which has similarities to a classification algorithm.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"We also experimented with an ordinal regression algorithm proposed in (Shen and Joshi, 2004).","{'title': '4 Reranking Algorithms', 'number': '4'}"
"For the sake of completeness, we will briefly describe the algorithm here.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"In this section, we will propose a splitting algorithm which separates translations of each sentence into two parts, the top translations and the bottom translations.","{'title': '4 Reranking Algorithms', 'number': '4'}"
All the separating hyperplanes are parallel by sharing the same weight vector .,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"The margin is defined on the distance between the top items and the bottom items in each cluster, as shown in Figure 1.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"Let be the feature vector of the translation of the sentence, and be the rank for this translation among all the translations for the sentence.","{'title': '4 Reranking Algorithms', 'number': '4'}"
Then the set of training samples is: where is the number of clusters and is the length of ranks for each cluster.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"Let be a linear function, where is the feature vector of a translation, and is a weight vector.","{'title': '4 Reranking Algorithms', 'number': '4'}"
We construct a hypothesis function with as follows. where is a function that takes a list of scores for the candidate translations computed according to the evaluation metric and returns the rank in that list.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
For example .,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"The splitting algorithm searches a linear function that successfully splits the top -ranked and bottom -ranked translations for each sentence, where .","{'title': '4 Reranking Algorithms', 'number': '4'}"
"Formally, let for any linear function .","{'title': '4 Reranking Algorithms', 'number': '4'}"
We look for the function such that which means that can successfully separate the good translations and the bad translations.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"Suppose there exists a linear function satisfying (1) and (2), we say is by given and .","{'title': '4 Reranking Algorithms', 'number': '4'}"
"Furthermore, we can define the splitting margin for the translations of the sentence as follows.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"The minimal splitting margin, , for given and is defined as follows.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"Algorithm 1 splitting Require: , and a positive learning margin .","{'title': '4 Reranking Algorithms', 'number': '4'}"
Algorithm 1 is a perceptron-like algorithm that looks for a function that splits the training data.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
The idea of the algorithm is as follows.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"For every two translations and , if the rank of is higher than or equal to , , the rank of is lower than ,, the weight vector can not successfully separate and with a learning margin , , then we need to update with the addition of .","{'title': '4 Reranking Algorithms', 'number': '4'}"
"However, the updating is not executed until all the inconsistent pairs in a sentence are found for the purpose of speeding up the algorithm.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"When sentence is selected, we first compute and store for all .","{'title': '4 Reranking Algorithms', 'number': '4'}"
Thus we do not need to recompute again in the inner loop.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"Now the complexity of a repeat iteration is , where is the average number of active features in vector .","{'title': '4 Reranking Algorithms', 'number': '4'}"
"If we updated the weight vector whenever an inconsistent pair was found, the complexity of a loop would be .","{'title': '4 Reranking Algorithms', 'number': '4'}"
"The following theorem will show that Algorithm 1 will stop in finite steps, outputting a function that splits the training data with a large margin, if the training data is splittable.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"Due to lack of space, we omit the proof for Theorem 1 in this paper.","{'title': '4 Reranking Algorithms', 'number': '4'}"
Theorem 1 Suppose the training samples Let .,"{'title': '4 Reranking Algorithms', 'number': '4'}"
Then Algorithm 1 makes at most mistakes on the pairwise samples during the Algorithm 2 ordinal regression with uneven margin Require: a positive learning margin .,"{'title': '4 Reranking Algorithms', 'number': '4'}"
"The second algorithm that we will use for MT reranking is the -insensitive ordinal regression with uneven margin, which was proposed in (Shen and Joshi, 2004), as shown in Algorithm 2.","{'title': '4 Reranking Algorithms', 'number': '4'}"
"In Algorithm 2, the function is used to control the level of insensitivity, and the function is used to control the learning margin between pairs of translations with different ranks as described in Section 3.5.","{'title': '4 Reranking Algorithms', 'number': '4'}"
There are many candidates for .,"{'title': '4 Reranking Algorithms', 'number': '4'}"
The following definition for is one of the simplest solutions.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
We will use this function in our experiments on MT reranking.,"{'title': '4 Reranking Algorithms', 'number': '4'}"
We provide experimental results on the NIST 2003 Chinese-English large data track evaluation.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"We use the data set used in (SMT Team, 2003).","{'title': '5 Experiments and Analysis', 'number': '5'}"
"The training data consists of about 170M English words, on which the baseline translation system is trained.","{'title': '5 Experiments and Analysis', 'number': '5'}"
The training data is also used to build language models which are used to define feature functions on various syntactic levels.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The development data consists of 993 Chinese sentences.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
Each Chinese sentence is associated with 1000-best English translations generated by the baseline MT system.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The development data set is used to estimate the parameters for the feature functions for the purpose of reranking.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The test data consists of 878 Chinese sentences.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
Each Chinese sentence is associated with 1000-best English translations too.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The test set is used to assess the quality of the reranking output.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"In (SMT Team, 2003), 450 features were generated.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"Six features from (Och, 2003) were used as baseline features.","{'title': '5 Experiments and Analysis', 'number': '5'}"
Each of the 450 features was evaluated independently by combining it with 6 baseline features and assessing on the test data with the minimum error training.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The baseline BLEU score on the test set is 31.6%.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
Table 1 shows some of the best performing features.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"In (SMT Team, 2003), aggressive search was used to combine features.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"After combining about a dozen features, the BLEU score did not improve any more, and the score was 32.9%.","{'title': '5 Experiments and Analysis', 'number': '5'}"
It was also noticed that the major improvement came from the Model 1 feature.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"By combining the four features, Model 1, matched parentheses, matched quotation marks and POS language model, the system achieved a BLEU score of 32.6%.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"In our experiments, we will use 4 different kinds of feature combinations: Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.","{'title': '5 Experiments and Analysis', 'number': '5'}"
Best Feature: Baseline + IBM Model 1 + matched parentheses + matched quotation marks + POS language model.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
Top Twenty: Baseline + 14 features with individual BLEU score no less than 31.9% with the minimum error training.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
Large Set: Baseline + 50 features with individual BLEU score no less than 31.7% with the minimum error training.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"Since the baseline is 31.6% and the 95% confidence range is 0.9%, most of the features in this set are not individually discriminative with respect to the BLEU metric.","{'title': '5 Experiments and Analysis', 'number': '5'}"
We apply Algorithm 1 and 2 to the four feature sets.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"For algorithm 1, the splitting algorithm, we set in the 1000-best translations given by the baseline MT system.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"For algorithm 2, the ordinal regression algorithm, we set the updating condition as and , which means one’s rank number is at most half of the other’s and there are at least 20 ranks in between.","{'title': '5 Experiments and Analysis', 'number': '5'}"
Figures 2-9 show the results of using Algorithm 1 and 2 with the four feature sets.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
The -axis represents the number of iterations in the training.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"The left -axis stands for the BLEU% score on the test data, and the right -axis stands for log of the loss function on the development data.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"Algorithm 1, the splitting algorithm, converges on the first three feature sets.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"The smaller the feature set is, the faster the algorithm converges.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"It achieves a BLEU score of 31.7% on the Baseline, 32.8% on the Best Feature, but only 32.6% on the Top Twenty features.","{'title': '5 Experiments and Analysis', 'number': '5'}"
However it is within the range of 95% confidence.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"Unfortunately on the Large Set, Algorithm 1 converges very slowly.","{'title': '5 Experiments and Analysis', 'number': '5'}"
In the Top Twenty set there are a fewer number of individually non-discriminative feature making the pool of features “better”.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"In addition, generalization performance in the Top Twenty set is better than the Large Set due to the smaller set of “better” features, cf.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"(Shen and Joshi, 2004).","{'title': '5 Experiments and Analysis', 'number': '5'}"
"If the number of the non-discriminative features is large enough, the data set becomes unsplittable.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.","{'title': '5 Experiments and Analysis', 'number': '5'}"
It converges on the first 3 feature sets too.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"On the Baseline, it achieves 31.4%.","{'title': '5 Experiments and Analysis', 'number': '5'}"
We notice that the model is over-trained on the development data according to the learning curve.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%.","{'title': '5 Experiments and Analysis', 'number': '5'}"
This algorithm does not converge on the Large Set in 10000 iterations.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error training and the regression algorithm tie for first place on feature combinations.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"However, the differences are not significant.","{'title': '5 Experiments and Analysis', 'number': '5'}"
We notice in those separable feature sets the performance on the development data and the test data are tightly consistent.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"Whenever the log-loss on the development set is decreased, and BLEU score on the test set goes up, and vice versa.","{'title': '5 Experiments and Analysis', 'number': '5'}"
"This tells us the merit of these two algorithms; By optimizing on the loss function for the development data, we can improve performance on the test data.","{'title': '5 Experiments and Analysis', 'number': '5'}"
This property is guaranteed by the theoretical analysis and is borne out in the experimental results.,"{'title': '5 Experiments and Analysis', 'number': '5'}"
"In this paper, we have successfully applied the discriminative reranking to machine translation.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
We applied a new perceptron-like splitting algorithm and ordinal regression algorithm with uneven margin to reranking in MT.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
We provide a theoretical justification for the performance of the splitting algorithms.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
Experimental results provided in this paper show that the proposed algorithms provide state-of-the-art performance in the NIST 2003 Chinese-English large data track evaluation.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
This material is based upon work supported by the National Science Foundation under Grant No.,"{'title': 'Acknowledgments', 'number': '7'}"
0121285.,"{'title': 'Acknowledgments', 'number': '7'}"
The first author was partially supported by JHU postworkshop fellowship and NSF Grant ITR-0205456.,"{'title': 'Acknowledgments', 'number': '7'}"
"The second author is partially supported by NSERC, Canada (RGPIN: 264905).","{'title': 'Acknowledgments', 'number': '7'}"
We thank the members of the SMT team of JHU Workshop 2003 for help on the dataset and three anonymous reviewers for useful comments.,"{'title': 'Acknowledgments', 'number': '7'}"
