col1,col2
"Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.",{}
"However, the flexibility of ME models is not without cost.",{}
"While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters.",{}
"In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.",{}
"Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.",{}
"Maximum entropy (ME) models, variously known as log-linear, Gibbs, exponential, and multinomial logit models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to fields as diverse as computer vision and econometrics.","{'title': '1 Introduction', 'number': '1'}"
"In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999).","{'title': '1 Introduction', 'number': '1'}"
"A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.","{'title': '1 Introduction', 'number': '1'}"
"However, the richness of the representations is not without cost.","{'title': '1 Introduction', 'number': '1'}"
Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters.,"{'title': '1 Introduction', 'number': '1'}"
"While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and frequently contain hundreds of thousands of free parameters.","{'title': '1 Introduction', 'number': '1'}"
"Estimation of such large models is not only expensive, but also, due to sparsely distributed features, sensitive to round-off errors.","{'title': '1 Introduction', 'number': '1'}"
"Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.","{'title': '1 Introduction', 'number': '1'}"
"Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.","{'title': '1 Introduction', 'number': '1'}"
Suppose we are given a probability distribution p over a set of events X which are characterized by a d dimensional feature vector function f : X → Rd.,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"In addition, we have also a set of contexts W and a function Y which partitions the members of X.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"In the case of a stochastic context-free grammar, for example, X might be the set of possible trees, the feature vectors might represent the number of times each rule applied in the derivation of each tree, W might be the set of possible strings of words, and Y(w) the set of trees whose yield is w ∈ W. A conditional maximum entropy model qθ(x|w) for p has the parametric form (Berger et al., 1996; Chi, 1998; where θ is a d-dimensional parameter vector and θT f (x) is the inner product of the parameter vector and a feature vector.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Given the parametric form of an ME model in (1), fitting an ME model to a collection of training data entails finding values for the parameter vector θ which minimize the Kullback-Leibler divergence between the model q0 and the empirical distribution p: ratio of Ep[f] to Eq(k)[f], with the restriction that ∑j fj(x) = C for each event x in the training data (a condition which can be easily satisfied by the addition of a correction feature).","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"We can adapt GIS to estimate the model parameters θ rather than the model probabilities q, yielding the update rule: The step size, and thus the rate of convergence, depends on the constant C: the larger the value of C, the smaller the step size.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"In case not all rows of the training data sum to a constant, the addition of a correction feature effectively slows convergence to match the most difficult case.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"To avoid this slowed convergence and the need for a correction feature, Della Pietra et al. (1997) propose an Improved Iterative Scaling (IIS) algorithm, whose update rule is the solution to the equation: The gradient of the log likelihood function, or the Ep[f] = ∑w,x p(w)q(k)(x|w)f(x)exp(M(x)δ(k)) vector of its first derivatives with respect to the parameter θ is: Since the likelihood function (2) is concave over the parameter space, it has a global maximum where the gradient is zero.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Unfortunately, simply setting G(θ) = 0 and solving for θ does not yield a closed form solution, so we proceed iteratively.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"At each step, we adjust an estimate of the parameters θ(k) to a new estimate θ(k+1) based on the divergence between the estimated probability distribution q(k) and the empirical distribution p. We continue until successive improvements fail to yield a sufficiently large decrease in the divergence.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates δ(k) at each search step differs substantially.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"As we shall see, this difference can have a dramatic impact on the number of updates required to reach convergence.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"One popular method for iteratively refining the model parameters is Generalized Iterative Scaling (GIS), due to Darroch and Ratcliff (1972).","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"An extension of Iterative Proportional Fitting (Deming and Stephan, 1940), GIS scales the probability distribution q(k) by a factor proportional to the where M(x) is the sum of the feature values for an event x in the training data.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"This is a polynomial in exp(δ(k)), and the solution can be found straightforwardly using, for example, the Newton-Raphson method.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
Their primary strength is that on each iteration they only require computation of the expected values Eq(k).,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"They do not depend on evaluation of the gradient of the log-likelihood function, which, depending on the distribution, could be prohibitively expensive.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"In the case of ME models, however, the vector of expected values required by iterative scaling essentially is the gradient G. Thus, it makes sense to consider methods which use the gradient directly.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"The most obvious way of making explicit use of the gradient is by Cauchy’s method, or the method of steepest ascent.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
The gradient of a function is a vector which points in the direction in which the function’s value increases most rapidly.,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Since our goal is to maximize the log-likelihood function, a natural strategy is to shift our current estimate of the parameters in the direction of the gradient via the update rule: where the step size α(k) is chosen to maximize L(θ(k) +δ(k)).","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Finding the optimal step size is itself an optimization problem, though only in one dimension and, in practice, only an approximate solution is required to guarantee global convergence.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Since the log-likelihood function is concave, the method of steepest ascent is guaranteed to find the global maximum.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"However, while the steps taken on each iteration are in a very narrow sense locally optimal, the global convergence rate of steepest ascent is very poor.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Each new search direction is orthogonal (or, if an approximate line search is used, nearly so) to the previous direction.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"This leads to a characteristic “zig-zag” ascent, with convergence slowing as the maximum is approached.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
One way of looking at the problem with steepest ascent is that it considers the same search directions many times.,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"We would prefer an algorithm which considered each possible search direction only once, in each iteration taking a step of exactly the right length in a direction orthogonal to all previous search directions.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"This intuition underlies conjugate gradient methods, which choose a search direction which is a linear combination of the steepest ascent direction and the previous search direction.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"The step size is selected by an approximate line search, as in the steepest ascent method.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Several non-linear conjugate gradient methods, such as the Fletcher-Reeves (cg-fr) and the Polak-Ribi`erePositive (cf-prp) algorithms, have been proposed.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"While theoretically equivalent, they use slighly different update rules and thus show different numeric properties.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Another way of looking at the problem with steepest ascent is that while it takes into account the gradient of the log-likelihood function, it fails to take into account its curvature, or the gradient of the gradient.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"The usefulness of the curvature is made clear if we consider a second-order Taylor series approximation of L(θ +δ): where H is Hessian matrix of the log-likelihood function, the d × d matrix of its second partial derivatives with respect to θ.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"If we set the derivative of (4) to zero and solve for δ, we get the update rule for Newton’s method: Newton’s method converges very quickly (for quadratic objective functions, in one step), but it requires the computation of the inverse of the Hessian matrix on each iteration.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"While the log-likelihood function for ME models in (2) is twice differentiable, for large scale problems the evaluation of the Hessian matrix is computationally impractical, and Newton’s method is not competitive with iterative scaling or first order methods.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
Variable metric or quasi-Newton methods avoid explicit evaluation of the Hessian by building up an approximation of it using successive evaluations of the gradient.,"{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"That is, we replace H−1(θ(k)) in (5) with a local approximation of the inverse Hessian B(k): with B(k) a symmatric, positive definite matrix which satisfies the equation: where y(k) = G(θ(k)) − G(θ(k−1)).","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Variable metric methods also show excellent convergence properties and can be much more efficient than using true Newton updates, but for large scale problems with hundreds of thousands of parameters, even storing the approximate Hessian is prohibitively expensive.","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of θ(k) using the previous m values of y(k) and δ(k).","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
"Since in practical applications values of m between 3 and 10 suffice, this can offer a substantial savings in storage requirements over variable metric methods, while still giving favorable convergence properties.1","{'title': '2 Maximum likelihood estimation', 'number': '2'}"
The performance of optimization algorithms is highly dependent on the specific properties of the problem to be solved.,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
Worst-case analysis typically 'Space constraints preclude a more detailed discussion of these methods here.,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For algorithmic details and theoretical analysis of first and second order methods, see, e.g., Nocedal (1997) or Nocedal and Wright (1999). does not reflect the actual behavior on actual problems.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor´e, 2002).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Minka (2001) offers a comparison of iterative scaling with other algorithms for parameter estimation in logistic regression, a problem similar to the one considered here, but it is difficult to transfer Minka’s results to ME models.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For one, he evaluates the algorithms with randomly generated training data.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"However, the performance and accuracy of optimization algorithms can be sensitive to the specific numerical properties of the function being optimized; results based on random data may or may not carry over to more realistic problems.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"And, the test problems Minka considers are relatively small (100–500 dimensions).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"As we have seen, though, algorithms which perform well for small and medium scale problems may not always be applicable to problems with many thousands of dimensions.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"As a basis for the implementation, we have used PETSc (the “Portable, Extensible Toolkit for Scientific Computation”), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"PETSc offers data structures and routines for parallel and sequential storage, manipulation, and visualization of very large sparse matrices.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For any of the estimation techniques, the most expensive operation is computing the probability distribution q and the expectations Eq[f] for each iteration.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"In order to make use of the facilities provided by PETSc, we can store the training data as a (sparse) matrix F, with rows corresponding to events and columns to features.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Then given a parameter vector θ, the unnormalized probabilities ˙q0 are the matrix-vector product: and the feature expectations are the transposed matrix-vector product: By expressing these computations as matrix-vector operations, we can take advantage of the high performance sparse matrix primitives of PETSc.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For the comparison, we implemented both Generalized and Improved Iterative Scaling in C++ using the primitives provided by PETSc.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For the other optimization techniques, we used TAO (the “Toolkit for Advanced Optimization”), a library layered on top of the foundation of PETSc for solving nonlinear optimization problems (Benson et al., 2002).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
TAO offers the building blocks for writing optimization programs (such as line searches and convergence tests) as well as high-quality implementations of standard optimization algorithms (including conjugate gradient and variable metric methods).,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Before turning to the results of the comparison, two additional points need to be made.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"First, in order to assure a consistent comparison, we need to use the same stopping rule for each algorithm.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"That is, each run was stopped when: where the relative tolerance ε = 10−7.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For any particular application, this may or may not be an appropriate stopping rule, but is only used here for purposes of comparison.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
These improvements take advantage of a model’s structure to simplify the evaluation of the denominator in (1).,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Thus, the use of such optimizations is independent of the choice of parameter estimation method.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"To compare the algorithms described in §2, we applied the implementation outlined in the previous section to four training data sets (described in Table 1) drawn from the domain of natural language processing.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"The ‘rules’ and ‘lex’ datasets are examples of stochastic attribute value grammars, one with a small set of SCFG-like features, and with a very large set of fine-grained lexical features (Bouma et al., 2001).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"The ‘summary’ dataset is part of a sentence extraction task (Osborne, to appear), and the ‘shallow’ dataset is drawn from a text chunking application (Osborne, 2002).","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"These datasets vary widely in their size and composition, and are representative of the kinds of datasets typically encountered in applying ME models to NLP classification tasks.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
The results of applying each of the parameter estimation algorithms to each of the datasets is summarized in Table 2.,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For each run, we report the KL divergence between the fitted model and the training data at convergence, the prediction accuracy of fitted model on a held-out test set (the fraction of contexts for which the event with the highest probability under the model also had the highest probability under the reference distribution), the number of iterations required, the number of log-likelihood and gradient evaluations required (algorithms which use a line search may require several function evaluations per iteration), and the total elapsed time (in seconds).2 There are a few things to observe about these results.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"First, while IIS converges in fewer steps the GIS, it takes substantially more time.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"At least for this implementation, the additional bookkeeping overhead required by IIS more than cancels any improvements in speed offered by accelerated convergence.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"This may be a misleading conclusion, however, since a more finely tuned implementation of IIS may well take much less time per iteration than the one used for these experiments.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"However, even if each iteration of IIS could be made as fast as an iteration of GIS (which seems unlikely), the benefits of IIS over GIS would in these cases be quite modest.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Second, note that for three of the four datasets, the KL divergence at convergence is roughly the same for all of the algorithms.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For the ‘summary’ dataset, however, they differ by up to two orders of magnitude.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
This is an indication that the convergence test in (6) is sensitive to the rate of convergence and thus to the choice of algorithm.,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Any degree of precision desired could be reached by any of the algorithms, with the appropriate value of ε.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"However, GIS, say, would require many more iterations than reported in Table 2 to reach the precision achieved by the limited memory variable metric algorithm.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Third, the prediction accuracy is, in most cases, more or less the same for all of the algorithms.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Some variability is to be expected—all of the data sets being considered here are badly ill-conditioned, and many different models will yield the same likelihood.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"In a few cases, however, the prediction accuracy differs more substantially.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"For the two SAVG data sets (‘rules’ and ‘lex’), GIS has a small advantage over the other methods.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"More dramatically, both iterative scaling methods perform very poorly on the ‘shallow’ dataset.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"In this case, the training data is very sparse.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Many features are nearly ‘pseudo-minimal’ in the sense of Johnson et al. (1999), and so receive weights approaching −∞.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
Smoothing the reference probabilities would likely improve the results for all of the methods and reduce the observed differences.,"{'title': '3 Comparing estimation techniques', 'number': '3'}"
"However, this does suggest that gradient-based methods are robust to certain problems with the training data.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"Finally, the most significant lesson to be drawn from these results is that, with the exception of steepest ascent, gradient-based methods outperform iterative scaling by a wide margin for almost all the datasets, as measured by both number of function evaluations and by the total elapsed time.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.","{'title': '3 Comparing estimation techniques', 'number': '3'}"
"In this paper, we have described experiments comparing the performance of a number of different algorithms for estimating the parameters of a conditional ME model.","{'title': '4 Conclusions', 'number': '4'}"
"The results show that variants of iterative scaling, the algorithms which are most widely used in the literature, perform quite poorly when compared to general function optimization algorithms such as conjugate gradient and variable metric methods.","{'title': '4 Conclusions', 'number': '4'}"
"And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.","{'title': '4 Conclusions', 'number': '4'}"
This conclusion has obvious consequences for the field.,"{'title': '4 Conclusions', 'number': '4'}"
"ME modeling is a commonly used machine learning technique, and the application of improved parameter estimation algorithms will it practical to construct larger, more complex models.","{'title': '4 Conclusions', 'number': '4'}"
"And, since the parameters of individual models can be estimated quite quickly, this will further open up the possibility for more sophisticated model and feature selection techniques which compare large numbers of alternative model specifications.","{'title': '4 Conclusions', 'number': '4'}"
This suggests that more comprehensive experiments to compare the convergence rate and accuracy of various algorithms on a wider range of problems is called for.,"{'title': '4 Conclusions', 'number': '4'}"
"In addition, there is a larger lesson to be drawn from these results.","{'title': '4 Conclusions', 'number': '4'}"
We typically think of computational linguistics as being primarily a symbolic discipline.,"{'title': '4 Conclusions', 'number': '4'}"
"However, statistical natural language processing involves non-trivial numeric computations.","{'title': '4 Conclusions', 'number': '4'}"
"As these results show, natural language processing can take great advantage of the algorithms and software libraries developed by and for more quantitatively oriented engineering and computational sciences.","{'title': '4 Conclusions', 'number': '4'}"
The research of Dr. Malouf has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences and by the NWO PIONIER project Algorithms for Linguistic Processing.,"{'title': 'Acknowledgements', 'number': '5'}"
"Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.","{'title': 'Acknowledgements', 'number': '5'}"
