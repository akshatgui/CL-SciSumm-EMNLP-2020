col1,col2
"Named entities are phrases that contain the names of persons, organizations and locations.","{'title': '1 Introduction', 'number': '1'}"
Example: [ORG U.N. ] official [PER Ekeus ] heads for [LOC Baghdad ] .,"{'title': '1 Introduction', 'number': '1'}"
"This sentence contains three named entities: Ekeus is a person, U.N. is a organization and Baghdad is a location.","{'title': '1 Introduction', 'number': '1'}"
Named entity recognition is an important task of information extraction systems.,"{'title': '1 Introduction', 'number': '1'}"
"There has been a lot of work on named entity recognition, especially for English (see Borthwick (1999) for an overview).","{'title': '1 Introduction', 'number': '1'}"
The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition.,"{'title': '1 Introduction', 'number': '1'}"
"They have also produced a scheme for entity annotation (Chinchor et al., 1999).","{'title': '1 Introduction', 'number': '1'}"
"More recently, there have been other system development competitions which dealt with different languages (IREX and CoNLL-2002).","{'title': '1 Introduction', 'number': '1'}"
The shared task of CoNLL-2003 concerns language-independent named entity recognition.,"{'title': '1 Introduction', 'number': '1'}"
"We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups.","{'title': '1 Introduction', 'number': '1'}"
"The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).","{'title': '1 Introduction', 'number': '1'}"
The participants of the 2003 shared task have been offered training and test data for two other European languages: English and German.,"{'title': '1 Introduction', 'number': '1'}"
They have used the data for developing a named-entity recognition system that includes a machine learning component.,"{'title': '1 Introduction', 'number': '1'}"
"The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data.","{'title': '1 Introduction', 'number': '1'}"
"In this section we discuss the sources of the data that were used in this shared task, the preprocessing steps we have performed on the data, the format of the data and the method that was used for evaluating the participating systems.","{'title': '2 Data and Evaluation', 'number': '2'}"
The CoNLL-2003 named entity data consists of eight files covering two languages: English and German1.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"For each of the languages there is a training file, a development file, a test file and a large file with unannotated data.","{'title': '2 Data and Evaluation', 'number': '2'}"
The learning methods were trained with the training data.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The development data could be used for tuning the parameters of the learning methods.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The challenge of this year’s shared task was to incorporate the unannotated data in the learning process in one way or another.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"When the best parameters were found, the method could be trained on the training data and tested on the test data.","{'title': '2 Data and Evaluation', 'number': '2'}"
The results of the different learning methods on the test sets are compared in the evaluation of the shared task.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The split between development data and test data was chosen to avoid systems being tuned to the test data.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The English data was taken from the Reuters Corpus2.,"{'title': '2 Data and Evaluation', 'number': '2'}"
This corpus consists of Reuters news stories between August 1996 and August 1997.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"For the training and development set, ten days’ worth of data were taken from the files representing the end of August 1996.","{'title': '2 Data and Evaluation', 'number': '2'}"
"For the test set, the texts were from December 1996.","{'title': '2 Data and Evaluation', 'number': '2'}"
The preprocessed raw data covers the month of September 1996.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The text for the German data was taken from the ECI Multilingual Text Corpus3.,"{'title': '2 Data and Evaluation', 'number': '2'}"
This corpus consists of texts in many languages.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"The portion of data that was used for this task, was extracted from the German newspaper Frankfurter Rundshau.","{'title': '2 Data and Evaluation', 'number': '2'}"
"All three of the training, development and test sets were taken from articles written in one week at the end of August 1992.","{'title': '2 Data and Evaluation', 'number': '2'}"
The raw data were taken from the months of September to December 1992.,"{'title': '2 Data and Evaluation', 'number': '2'}"
Table 1 contains an overview of the sizes of the data files.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The unannotated data contain 17 million tokens (English) and 14 million tokens (German).,"{'title': '2 Data and Evaluation', 'number': '2'}"
"The participants were given access to the corpus after some linguistic preprocessing had been done: for all data, a tokenizer, part-of-speech tagger, and a chunker were applied to the raw data.","{'title': '2 Data and Evaluation', 'number': '2'}"
We created two basic language-specific tokenizers for this shared task.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"The English data was tagged and chunked by the memory-based MBT tagger (Daelemans et al., 2002).","{'title': '2 Data and Evaluation', 'number': '2'}"
"The German data was lemmatized, tagged and chunked by the decision tree tagger Treetagger (Schmid, 1995).","{'title': '2 Data and Evaluation', 'number': '2'}"
"Named entity tagging of English and German training, development, and test data, was done by hand at the University of Antwerp.","{'title': '2 Data and Evaluation', 'number': '2'}"
"Mostly, MUC conventions were followed (Chinchor et al., 1999).","{'title': '2 Data and Evaluation', 'number': '2'}"
An extra named entity category called MISC was added to denote all names which are not already in the other categories.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"This includes adjectives, like Italian, and events, like 1000 Lakes Rally, making it a very diverse category.","{'title': '2 Data and Evaluation', 'number': '2'}"
All data files contain one word per line with empty lines representing sentence boundaries.,"{'title': '2 Data and Evaluation', 'number': '2'}"
At the end of each line there is a tag which states whether the current word is inside a named entity or not.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The tag also encodes the type of named entity.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"Here is an example sentence: Each line contains four fields: the word, its partof-speech tag, its chunk tag and its named entity tag.","{'title': '2 Data and Evaluation', 'number': '2'}"
Words tagged with O are outside of named entities and the I-XXX tag is used for words inside a named entity of type XXX.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"Whenever two entities of type XXX are immediately next to each other, the first word of the second entity will be tagged B-XXX in order to show that it starts another entity.","{'title': '2 Data and Evaluation', 'number': '2'}"
"The data contains entities of four types: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC).","{'title': '2 Data and Evaluation', 'number': '2'}"
This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus (1995).,"{'title': '2 Data and Evaluation', 'number': '2'}"
We assume that named entities are non-recursive and non-overlapping.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"When a named entity is embedded in another named entity, usually only the top level entity has been annotated.","{'title': '2 Data and Evaluation', 'number': '2'}"
Table 2 contains an overview of the number of named entities in each data file.,"{'title': '2 Data and Evaluation', 'number': '2'}"
The performance in this task is measured with Fβ=1 rate: Table 3: Main features used by the the sixteen systems that participated in the CoNLL-2003 shared task sorted by performance on the English test data.,"{'title': '2 Data and Evaluation', 'number': '2'}"
"Aff: affix information (n-grams); bag: bag of words; cas: global case information; chu: chunk tags; doc: global document information; gaz: gazetteers; lex: lexical features; ort: orthographic information; pat: orthographic patterns (like Aa0); pos: part-of-speech tags; pre: previously predicted NE tags; quo: flag signing that the word is between quotes; tri: trigger words. with β=1 (Van Rijsbergen, 1975).","{'title': '2 Data and Evaluation', 'number': '2'}"
Precision is the percentage of named entities found by the learning system that are correct.,"{'title': '2 Data and Evaluation', 'number': '2'}"
Recall is the percentage of named entities present in the corpus that are found by the system.,"{'title': '2 Data and Evaluation', 'number': '2'}"
A named entity is correct only if it is an exact match of the corresponding entity in the data file.,"{'title': '2 Data and Evaluation', 'number': '2'}"
Sixteen systems have participated in the CoNLL2003 shared task.,"{'title': '3 Participating Systems', 'number': '3'}"
They employed a wide variety of machine learning techniques as well as system combination.,"{'title': '3 Participating Systems', 'number': '3'}"
Most of the participants have attempted to use information other than the available training data.,"{'title': '3 Participating Systems', 'number': '3'}"
"This information included gazetteers and unannotated data, and there was one participant who used the output of externally trained named entity recognition systems.","{'title': '3 Participating Systems', 'number': '3'}"
The most frequently applied technique in the CoNLL-2003 shared task is the Maximum Entropy Model.,"{'title': '3 Participating Systems', 'number': '3'}"
Five systems used this statistical learning method.,"{'title': '3 Participating Systems', 'number': '3'}"
"Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003).","{'title': '3 Participating Systems', 'number': '3'}"
"Two more systems used them in combination with other techniques (Florian et al., 2003; Klein et al., 2003).","{'title': '3 Participating Systems', 'number': '3'}"
Maximum Entropy Models seem to be a good choice for this kind of task: the top three results for English and the top two results for German were obtained by participants who employed them in one way or another.,"{'title': '3 Participating Systems', 'number': '3'}"
"Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al., 2003; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003).","{'title': '3 Participating Systems', 'number': '3'}"
"However, they were always used in combination with other learning techniques.","{'title': '3 Participating Systems', 'number': '3'}"
Klein et al. (2003) also applied the related Conditional Markov Models for combining classifiers.,"{'title': '3 Participating Systems', 'number': '3'}"
Learning methods that were based on connectionist approaches were applied by four systems.,"{'title': '3 Participating Systems', 'number': '3'}"
"Zhang and Johnson (2003) used robust risk minimization, which is a Winnow technique.","{'title': '3 Participating Systems', 'number': '3'}"
Florian et al. (2003) employed the same technique in a combination of learners.,"{'title': '3 Participating Systems', 'number': '3'}"
Voted perceptrons were applied to the shared task data by Carreras et al. (2003a) and Hammerton used a recurrent neural network (Long Short-Term Memory) for finding named entities.,"{'title': '3 Participating Systems', 'number': '3'}"
Other learning approaches were employed less frequently.,"{'title': '3 Participating Systems', 'number': '3'}"
"Two teams used AdaBoost.MH (Carreras et al., 2003b; Wu et al., 2003) and two other groups employed memory-based learning (De Meulder and Daelemans, 2003; Hendrickx and Van den Bosch, 2003).","{'title': '3 Participating Systems', 'number': '3'}"
"Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al., 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.","{'title': '3 Participating Systems', 'number': '3'}"
Combination of different learning systems has proven to be a good method for obtaining excellent results.,"{'title': '3 Participating Systems', 'number': '3'}"
Five participating groups have applied system combination.,"{'title': '3 Participating Systems', 'number': '3'}"
Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best.,"{'title': '3 Participating Systems', 'number': '3'}"
"Klein et al. (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models.","{'title': '3 Participating Systems', 'number': '3'}"
Mayfield et al. (2003) stacked two learners and obtained better performance.,"{'title': '3 Participating Systems', 'number': '3'}"
Wu et al. (2003) applied both stacking and voting to three learners.,"{'title': '3 Participating Systems', 'number': '3'}"
Munro et al. (2003) employed both voting and bagging for combining classifiers.,"{'title': '3 Participating Systems', 'number': '3'}"
The choice of the learning approach is important for obtaining a good system for recognizing named entities.,"{'title': '3 Participating Systems', 'number': '3'}"
"However, in the CoNLL-2002 shared task we found out that choice of features is at least as important.","{'title': '3 Participating Systems', 'number': '3'}"
"An overview of some of the types of features chosen by the shared task participants, can be found in Table 3.","{'title': '3 Participating Systems', 'number': '3'}"
All participants used lexical features (words) except for Whitelaw and Patrick (2003) who implemented a character-based method.,"{'title': '3 Participating Systems', 'number': '3'}"
"Most of the systems employed part-of-speech tags and two of them have recomputed the English tags with better taggers (Hendrickx and Van den Bosch, 2003; Wu et al., 2003).","{'title': '3 Participating Systems', 'number': '3'}"
"Othographic information, affixes, gazetteers and chunk information were also incorporated in most systems although one group reports that the available chunking information did not help (Wu et al., 2003) Other features were used less frequently.","{'title': '3 Participating Systems', 'number': '3'}"
Table 3 does not reveal a single feature that would be ideal for named entity recognition.,"{'title': '3 Participating Systems', 'number': '3'}"
Eleven of the sixteen participating teams have attempted to use information other than the training data that was supplied for this shared task.,"{'title': '3 Participating Systems', 'number': '3'}"
All included gazetteers in their systems.,"{'title': '3 Participating Systems', 'number': '3'}"
"Four groups examined the usability of unannotated data, either for extracting training instances (Bender et al., 2003; Hendrickx and Van den Bosch, 2003) or obtaining extra named entities for gazetteers (De Meulder and Daelemans, 2003; McCallum and Li, 2003).","{'title': '3 Participating Systems', 'number': '3'}"
A reasonable number of groups have also employed unannotated data for obtaining capitalization features for words.,"{'title': '3 Participating Systems', 'number': '3'}"
"One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al., 2003).","{'title': '3 Participating Systems', 'number': '3'}"
"Table 4 shows the error reduction of the systems Table 4: Error reduction for the two development data sets when using extra information like gazetteers (G), unannotated data (U) or externally developed named entity recognizers (E).","{'title': '3 Participating Systems', 'number': '3'}"
The lines have been sorted by the sum of the reduction percentages for the two languages. with extra information compared to while using only the available training data.,"{'title': '3 Participating Systems', 'number': '3'}"
"The inclusion of extra named entity recognition systems seems to have worked well (Florian et al., 2003).","{'title': '3 Participating Systems', 'number': '3'}"
Generally the systems that only used gazetteers seem to gain more than systems that have used unannotated data for other purposes than obtaining capitalization information.,"{'title': '3 Participating Systems', 'number': '3'}"
"However, the gain differences between the two approaches are most obvious for English for which better gazetteers are available.","{'title': '3 Participating Systems', 'number': '3'}"
"With the exception of the result of Zhang and Johnson (2003), there is not much difference in the German results between the gains obtained by using gazetteers and those obtained by using unannotated data.","{'title': '3 Participating Systems', 'number': '3'}"
A baseline rate was computed for the English and the German test sets.,"{'title': '3 Participating Systems', 'number': '3'}"
It was produced by a system which only identified entities which had a unique class in the training data.,"{'title': '3 Participating Systems', 'number': '3'}"
"If a phrase was part of more than one entity, the system would select the longest one.","{'title': '3 Participating Systems', 'number': '3'}"
All systems that participated in the shared task have outperformed the baseline system.,"{'title': '3 Participating Systems', 'number': '3'}"
"For all the F0=1 rates we have estimated significance boundaries by using bootstrap resampling (Noreen, 1989).","{'title': '3 Participating Systems', 'number': '3'}"
"From each output file of a system, 250 random samples of sentences have been chosen and the distribution of the F0=1 rates in these samples is assumed to be the distribution of the performance of the system.","{'title': '3 Participating Systems', 'number': '3'}"
We assume that performance A is significantly different from performance B if A is not within the center 90% of the distribution of B.,"{'title': '3 Participating Systems', 'number': '3'}"
The performances of the sixteen systems on the two test data sets can be found in Table 5.,"{'title': '3 Participating Systems', 'number': '3'}"
"For English, the combined classifier of Florian et al. (2003) achieved the highest overall F0=1 rate.","{'title': '3 Participating Systems', 'number': '3'}"
"However, the difference between their performance and that of the Maximum Entropy approach of Chieu and Ng (2003) is not significant.","{'title': '3 Participating Systems', 'number': '3'}"
"An important feature of the best system that other participants did not use, was the inclusion of the output of two externally trained named entity recognizers in the combination process.","{'title': '3 Participating Systems', 'number': '3'}"
Florian et al. (2003) have also obtained the highest F0=1 rate for the German data.,"{'title': '3 Participating Systems', 'number': '3'}"
Here there is no significant difference between them and the systems of Klein et al. (2003) and Zhang and Johnson (2003).,"{'title': '3 Participating Systems', 'number': '3'}"
We have combined the results of the sixteen system in order to see if there was room for improvement.,"{'title': '3 Participating Systems', 'number': '3'}"
We converted the output of the systems to the same IOB tagging representation and searched for the set of systems from which the best tags for the development data could be obtained with majority voting.,"{'title': '3 Participating Systems', 'number': '3'}"
"The optimal set of systems was determined by performing a bidirectional hill-climbing search (Caruana and Freitag, 1994) with beam size 9, starting from zero features.","{'title': '3 Participating Systems', 'number': '3'}"
"A majority vote of five systems (Chieu and Ng, 2003; Florian et al., 2003; Klein et al., 2003; McCallum and Li, 2003; Whitelaw and Patrick, 2003) performed best on the English development data.","{'title': '3 Participating Systems', 'number': '3'}"
"Another combination of five systems (Carreras et al., 2003b; Mayfield et al., 2003; McCallum and Li, 2003; Munro et al., 2003; Zhang and Johnson, 2003) obtained the best result for the German development data.","{'title': '3 Participating Systems', 'number': '3'}"
We have performed a majority vote with these sets of systems on the related test sets and obtained F0=1 rates of 90.30 for English (14% error reduction compared with the best system) and 74.17 for German (6% error reduction).,"{'title': '3 Participating Systems', 'number': '3'}"
We have described the CoNLL-2003 shared task: language-independent named entity recognition.,"{'title': '4 Concluding Remarks', 'number': '4'}"
Sixteen systems have processed English and German named entity data.,"{'title': '4 Concluding Remarks', 'number': '4'}"
"The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al., 2003).","{'title': '4 Concluding Remarks', 'number': '4'}"
"Apart from the training data, this system also employed gazetteers and the output of two externally trained named entity recognizers.","{'title': '4 Concluding Remarks', 'number': '4'}"
The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al.,"{'title': '4 Concluding Remarks', 'number': '4'}"
(2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.,"{'title': '4 Concluding Remarks', 'number': '4'}"
Eleven teams have incorporated information other than the training data in their system.,"{'title': '4 Concluding Remarks', 'number': '4'}"
Four of them have obtained error reductions of 15% or more for English and one has managed this for German.,"{'title': '4 Concluding Remarks', 'number': '4'}"
"The resources used by these systems, gazetteers and externally trained named entity systems, still require a lot of manual work.","{'title': '4 Concluding Remarks', 'number': '4'}"
"Systems that employed unannotated data, obtained performance gains around 5%.","{'title': '4 Concluding Remarks', 'number': '4'}"
"The search for an excellent method for taking advantage of the fast amount of available raw text, remains open.","{'title': '4 Concluding Remarks', 'number': '4'}"
Tjong Kim Sang is financed by IWT STWW as a researcher in the ATraNoS project.,"{'title': 'Acknowledgements', 'number': '5'}"
De Meulder is supported by a BOF grant supplied by the University of Antwerp.,"{'title': 'Acknowledgements', 'number': '5'}"
