col1,col2
"This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.",{}
"Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.",{}
"In this way, the NER problem can be resolved effectively.",{}
Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.,{}
It shows that the performance is significantly better than reported by any other machine-learning system.,{}
"Moreover, the performance is even consistently better than those based on handcrafted rules.",{}
Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.,"{'title': '1 Introduction', 'number': '1'}"
"In the taxonomy of computational linguistics tasks, it falls under the domain of &quot;information extraction&quot;, which extracts specific kinds of information from documents as opposed to the more general task of &quot;document management&quot; which seeks to extract all of the information found in a document.","{'title': '1 Introduction', 'number': '1'}"
"Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.","{'title': '1 Introduction', 'number': '1'}"
"The atomic elements of information extraction -- indeed, of language as a whole -- could be considered as the &quot;who&quot;, &quot;where&quot; and &quot;how much&quot; in a sentence.","{'title': '1 Introduction', 'number': '1'}"
"NER performs what is known as surface parsing, delimiting sequences of tokens that answer these important questions.","{'title': '1 Introduction', 'number': '1'}"
"NER can also be used as the first step in a chain of processors: a next level of processing could relate two or more NEs, or perhaps even give semantics to that relationship using a verb.","{'title': '1 Introduction', 'number': '1'}"
"In this way, further processing could discover the &quot;what&quot; and &quot;how&quot; of a sentence or body of text.","{'title': '1 Introduction', 'number': '1'}"
"While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance.","{'title': '1 Introduction', 'number': '1'}"
"There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues.","{'title': '1 Introduction', 'number': '1'}"
"During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups.","{'title': '1 Introduction', 'number': '1'}"
"Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher.","{'title': '1 Introduction', 'number': '1'}"
"Typical systems are Univ. of Sheffield's LaSIE-II [Humphreys+98], ISOQuest's NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh's LTG [Mikheev+98] [Mikheev+99] for English NER.","{'title': '1 Introduction', 'number': '1'}"
These systems are mainly rule-based.,"{'title': '1 Introduction', 'number': '1'}"
"However, rule-based approaches lack the ability of coping with the problems of robustness and portability.","{'title': '1 Introduction', 'number': '1'}"
Each new source of text requires significant tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep.,"{'title': '1 Introduction', 'number': '1'}"
"The current trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one.","{'title': '1 Introduction', 'number': '1'}"
The representative machine-learning approaches used in NER are HMM (BBN's IdentiFinder in [Miller+98] [Bikel+99] and KRDL's system [Yu+98] for Chinese NER.,"{'title': '1 Introduction', 'number': '1'}"
"), Maximum Entropy (New York Univ.","{'title': '1 Introduction', 'number': '1'}"
's MEME in [Borthwick+98] [Borthwich99]) and Decision Tree (New York Univ.,"{'title': '1 Introduction', 'number': '1'}"
's system in [Sekine98] and SRA's system in [Bennett+97]).,"{'title': '1 Introduction', 'number': '1'}"
"Besides, a variant of Eric Brill's transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95].","{'title': '1 Introduction', 'number': '1'}"
"Among these approaches, the evaluation performance of HMM is higher than those of others.","{'title': '1 Introduction', 'number': '1'}"
"The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text.","{'title': '1 Introduction', 'number': '1'}"
"Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence.","{'title': '1 Introduction', 'number': '1'}"
"However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b].","{'title': '1 Introduction', 'number': '1'}"
"This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts.","{'title': '1 Introduction', 'number': '1'}"
"As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above.","{'title': '1 Introduction', 'number': '1'}"
The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context.,"{'title': '1 Introduction', 'number': '1'}"
"In order to effectively apply and integrate internal and external evidences, we present a NER system using a HMM.","{'title': '1 Introduction', 'number': '1'}"
"The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].","{'title': '1 Introduction', 'number': '1'}"
"Here, a NE is regarded as a chunk, named &quot;NE-Chunk&quot;.","{'title': '1 Introduction', 'number': '1'}"
"To date, our system has been successfully trained and applied in English NER.","{'title': '1 Introduction', 'number': '1'}"
"To our knowledge, our system outperforms any published machine-learning systems.","{'title': '1 Introduction', 'number': '1'}"
"Moreover, our system even outperforms any published rule-based systems.","{'title': '1 Introduction', 'number': '1'}"
The layout of this paper is as follows.,"{'title': '1 Introduction', 'number': '1'}"
Section 2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.,"{'title': '1 Introduction', 'number': '1'}"
Section 3 explains the word feature used to capture both the internal and external evidences.,"{'title': '1 Introduction', 'number': '1'}"
Section 4 describes the back-off schemes used to tackle the sparseness problem.,"{'title': '1 Introduction', 'number': '1'}"
Section 5 gives the experimental results of our system.,"{'title': '1 Introduction', 'number': '1'}"
Section 6 contains our remarks and possible extensions of the proposed work.,"{'title': '1 Introduction', 'number': '1'}"
"Given a token sequence G1n = g1g2 g , the goal The second item in (2-1) is the mutual information between T1n and n simplify the computation of this item, we assume mutual information independence: The basic premise of this model is to consider the raw text, encountered when decoding, as though it had passed through a noisy channel, where it had been originally marked with NE tags.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.,"{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"It is obvious that our generative model is reverse to the generative model of traditional HMM1, as used in BBN's IdentiFinder, which models the original process that generates the NE-class annotated words from the original NE tags.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
Another difference is that our model assumes mutual information independence (2-2) while traditional HMM assumes conditional probability independence (I-1).,"{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
Assumption (2-2) is much looser than assumption (I-1) because assumption (I-1) has the same effect with the sum of assumptions (2-2) and (I-3)2.,"{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"In this way, our model can apply more context information to determine the tag of current token.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"From equation (2-4), we can see that: We will not discuss both the first and second items further in this paper.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"Ideally, it can be estimated by using the forward-backward algorithm [Rabiner89] recursively for the 1st-order [Rabiner89] or 2nd -order HMMs [Watson+92].","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (  |) Then we assume conditional probability word sequence and F1n = f 1 f2 ... fn is the word-feature sequence.","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence ti−1ti is valid / invalid while &quot;valid on&quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .","{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.,"{'title': '2 HMM-based Chunk Tagger', 'number': '2'}"
"As stated above, token is denoted as ordered pairs of word-feature and word itself: gi =< fi , wi > .","{'title': '3 Determining Word Feature', 'number': '3'}"
"Here, the word-feature is a simple deterministic computation performed on the word and/or word string with appropriate consideration of context as looked up in the lexicon or added to the context.","{'title': '3 Determining Word Feature', 'number': '3'}"
"In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.","{'title': '3 Determining Word Feature', 'number': '3'}"
The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"Our model captures three types of internal sub-features: 1) 1 f : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2: internal semantic feature of important triggers; 3) f 3: internal gazetteer feature. f is the basic sub-feature exploited in this model, as shown in Table 2 with the descending order of priority.","{'title': '3 Determining Word Feature', 'number': '3'}"
"For example, in the case of non-disjoint feature classes such as ContainsDigitAndAlpha and ContainsDigitAndDash, the former will take precedence.","{'title': '3 Determining Word Feature', 'number': '3'}"
"The first eleven features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates.","{'title': '3 Determining Word Feature', 'number': '3'}"
The rest of the features distinguish types of capitalization and all other words such as punctuation marks.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"In particular, the FirstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that AllCaps and CapPeriod are computed before FirstWord, and take precedence.)","{'title': '3 Determining Word Feature', 'number': '3'}"
This sub-feature is language dependent.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"Fortunately, the feature computation is an extremely small part of the implementation.","{'title': '3 Determining Word Feature', 'number': '3'}"
"This kind of internal sub-feature has been widely used in machine-learning systems, such as BBN's IdendiFinder and New York Univ.","{'title': '3 Determining Word Feature', 'number': '3'}"
's MENE.,"{'title': '3 Determining Word Feature', 'number': '3'}"
The rationale behind this sub-feature is clear: a) capitalization gives good evidence of NEs in Roman languages; b) Numeric symbols can automatically be grouped into categories.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"2) 2 f is the semantic classification of important triggers, as seen in Table 3, and is unique to our system.","{'title': '3 Determining Word Feature', 'number': '3'}"
It is based on the intuitions that important triggers are useful for NER and can be classified according to their semantics.,"{'title': '3 Determining Word Feature', 'number': '3'}"
This sub-feature applies to both single word and multiple words.,"{'title': '3 Determining Word Feature', 'number': '3'}"
This set of triggers is collected semi-automatically from the NEs and their local context of the training data. determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive3, a list of 10,000 organization names from websites such as Yahoo4 and a list of 10,000 famous people from websites such as Scope Systems5.","{'title': '3 Determining Word Feature', 'number': '3'}"
Gazetters have been widely used in NER systems to improve performance.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"For external evidence, only one external macro context feature 4 f , as shown in Table 5, is captured in our model.","{'title': '3 Determining Word Feature', 'number': '3'}"
"4 f is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.).","{'title': '3 Determining Word Feature', 'number': '3'}"
This sub-feature is unique to our system.,"{'title': '3 Determining Word Feature', 'number': '3'}"
The intuition behind this is the phenomena of name alias.,"{'title': '3 Determining Word Feature', 'number': '3'}"
"During decoding, the NEs already recognized from the document are stored in a list.","{'title': '3 Determining Word Feature', 'number': '3'}"
"When the system encounters a NE candidate, a name alias algorithm is invoked to dynamically determine its relationship with the NEs in the recognized list.","{'title': '3 Determining Word Feature', 'number': '3'}"
"Initially, we also consider part-of-speech (POS) sub-feature.","{'title': '3 Determining Word Feature', 'number': '3'}"
"However, the experimental result is disappointing that incorporation of POS even decreases the performance by 2%.","{'title': '3 Determining Word Feature', 'number': '3'}"
"This may be because capitalization information of a word is submerged in the muddy of several POS tags and the performance of POS tagging is not satisfactory, especially for unknown capitalized words (since many of NEs include unknown capitalized words.).","{'title': '3 Determining Word Feature', 'number': '3'}"
"Therefore, POS is discarded.","{'title': '3 Determining Word Feature', 'number': '3'}"
"Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.","{'title': '4 Back-off Modeling', 'number': '4'}"
"Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.","{'title': '4 Back-off Modeling', 'number': '4'}"
"In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .","{'title': '4 Back-off Modeling', 'number': '4'}"
"2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .","{'title': '4 Back-off Modeling', 'number': '4'}"
"In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.","{'title': '5 Experimental Results', 'number': '5'}"
"For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.","{'title': '5 Experimental Results', 'number': '5'}"
"For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.","{'title': '5 Experimental Results', 'number': '5'}"
"Here, the precision (P) measures the number of correct NEs in the answer file over the total number of NEs in the answer file and the recall (R) measures the number of correct NEs in the answer file over the total number of NEs in the key file while F-measure is the weighted harmonic mean of precision and recall: with β2 =1.","{'title': '5 Experimental Results', 'number': '5'}"
It shows that the performance is significantly better than reported by any other machine-learning system.,"{'title': '5 Experimental Results', 'number': '5'}"
"Moreover, the performance is consistently better than those based on handcrafted rules.","{'title': '5 Experimental Results', 'number': '5'}"
"With any learning technique, one important question is how much training data is required to achieve acceptable performance.","{'title': '5 Experimental Results', 'number': '5'}"
More generally how does the performance vary as the training data size changes?,"{'title': '5 Experimental Results', 'number': '5'}"
The result is shown in Figure 2 for MUC-7 NE task.,"{'title': '5 Experimental Results', 'number': '5'}"
It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.,"{'title': '5 Experimental Results', 'number': '5'}"
It also shows that our system still has some room for performance improvement.,"{'title': '5 Experimental Results', 'number': '5'}"
This may be because of the complex word feature and the corresponding sparseness problem existing in our system.,"{'title': '5 Experimental Results', 'number': '5'}"
Another important question is about the effect of different sub-features.,"{'title': '5 Experimental Results', 'number': '5'}"
Table 8 answers the question on MUC-7 NE task: 1) Applying only 1 f gives our system the performance of 77.6%.,"{'title': '5 Experimental Results', 'number': '5'}"
2) 2 f is very useful for NER and increases the performance further by 10% to 87.4%.,"{'title': '5 Experimental Results', 'number': '5'}"
3) 4 f is impressive too with another 5.5% performance improvement.,"{'title': '5 Experimental Results', 'number': '5'}"
"4) However, 3 f contributes only further 1.2% to the performance.","{'title': '5 Experimental Results', 'number': '5'}"
This may be because information included in 3 f has already been captured by 2 f and f4 .,"{'title': '5 Experimental Results', 'number': '5'}"
"Actually, the experiments show that the contribution of 3 f comes from where there is no explicit indicator information in/around the NE and there is no reference to other NEs in the macro context of the document.","{'title': '5 Experimental Results', 'number': '5'}"
"The NEs contributed by 3 f are always well-known ones, e.g.","{'title': '5 Experimental Results', 'number': '5'}"
"Microsoft, IBM and Bach (a composer), which are introduced in texts without much helpful context.","{'title': '5 Experimental Results', 'number': '5'}"
"This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.","{'title': '6 Conclusion', 'number': '6'}"
"Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.","{'title': '6 Conclusion', 'number': '6'}"
It also shows that our NER system can reach &quot;near human performance&quot;.,"{'title': '6 Conclusion', 'number': '6'}"
"To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.","{'title': '6 Conclusion', 'number': '6'}"
"While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.","{'title': '6 Conclusion', 'number': '6'}"
"In the near feature, we would like to incorporate the following into our system:","{'title': '6 Conclusion', 'number': '6'}"
