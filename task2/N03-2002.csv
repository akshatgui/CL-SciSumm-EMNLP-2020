col1,col2
We introduce factored language models (FLMs) and generalized parallel backoff (GPB).,{}
"An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.",{}
"), and induces a probability model covering sequences of bundles rather than just words.",{}
"GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.",{}
These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.,{}
This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.,{}
"Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.",{}
"In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.",{}
The art of statistical language modeling (LM) is to create probability models over words and sentences that tradeoff statistical prediction with parameter variance.,"{'title': '1 Introduction', 'number': '1'}"
"The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few.","{'title': '1 Introduction', 'number': '1'}"
"Many models are simply smoothed conditional probability distributions for a word given its preceding history, typically the two preceding words.","{'title': '1 Introduction', 'number': '1'}"
"In this work, we introduce two new methods for language modeling: factored language model (FLM) and generalized parallel backoff (GPB).","{'title': '1 Introduction', 'number': '1'}"
"An FLM considers a word as a bundle of features, and GPB is a technique that generalized backoff to arbitrary conditional probability tables.","{'title': '1 Introduction', 'number': '1'}"
"While these techniques can be considered in isolation, the two methods seem particularly suited to each other — in particular, the method of GPB can greatly facilitate the production of FLMs with better performance.","{'title': '1 Introduction', 'number': '1'}"
"In a factored language model, a word is viewed as a vector of k factors, so that wt ≡ {f1t , f2t , ... , fKt }.","{'title': '2 Factored Language Models', 'number': '2'}"
"Factors can be anything, including morphological classes, stems, roots, and other such features in highly inflected languages (e.g., Arabic, German, Finnish, etc.","{'title': '2 Factored Language Models', 'number': '2'}"
"), or data-driven word classes or semantic features useful for sparsely inflected languages (e.g., English).","{'title': '2 Factored Language Models', 'number': '2'}"
"Clearly, a two-factor FLM generalizes standard class-based language models, where one factor is the word class and the other is words themselves.","{'title': '2 Factored Language Models', 'number': '2'}"
"An FLM is a model over factors, i.e., p(ft:K|ft1 F'1K that can be factored as a product of probabilities of the form p(f|f1, f2, ... , fN).","{'title': '2 Factored Language Models', 'number': '2'}"
"Our task is twofold: 1) find an appropriate set of factors, and 2) induce an appropriate statistical model over those factors (i.e., the structure learning problem in graphical models (Bilmes, 2003; Friedman and Koller, 2001)).","{'title': '2 Factored Language Models', 'number': '2'}"
"An individual FLM probability model can be seen as a directed graphical model over a set of N + 1 random variables, with child variable F and N parent variables F1 through FN (if factors are words, then F = Wt and Fi = Wt−i).","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"Two features make an FLM distinct from a standard language model: 1) the variables {F, F1, ... , FN} can be heterogeneous (e.g., words, word clusters, morphological classes, etc.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"); and 2) there is no obvious natural (e.g., temporal) backoff order as in standard wordbased language models.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"With word-only models, backoff proceeds by dropping first the oldest word, then the next oldest, and so on until only the unigram remains.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"In p(f|f1, f2,.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
".. , fN), however, many of the parent variables might be the same age.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"Even if the variables have differing seniorities, it is not necessarily best to drop the oldest variable first.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"We introduce the notion of a backoff graph (Figure 1) to depict this issue, which shows the various backoff paths from the all-parents case (top graph node) to the unigram (bottom graph node).","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
Many possible backoff paths could be taken.,"{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"For example, when all variables are words, the path A − B − E − H corresponds to trigram with standard oldest-first backoff order.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
The path A − D − G − H is a reverse-time backoff model.,"{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"This can be seen as a generalization of lattice-based language modeling (Dupont and Rosenfeld, 1997) where factors consist of words and hierarchically derived word classes.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"In our GPB procedure, either a single distinct path is chosen for each gram or multiple parallel paths are used simultaneously.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"In either case, the set of backoff path(s) that are chosen are determined dynamically (at “run-time”) based on the current values of the variables.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"For example, a path might consist of nodes A − (BCD) − (EF) − G where node A backs off in parallel to the three nodes BCD, node B backs off to nodes (EF), C backs off to (E), and D backs off to (F).","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
This can be seen as a generalization of the standard backoff equation.,"{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"In the two parents case, this becomes: where dN(f,f1,f2) is a standard discount (determining the smoothing method), pML is the maximum likelihood distribution, α(f1, f2) are backoff weights, and g(f, f1, f2) is an arbitrary non-negative backofffunction of its three factor arguments.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"Standard backoff occurs with g(f, f1, f2) = pBO(f|f1), but the GPB procedures can be obtained by using different g-functions.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"For example, g(f, f1, f2) = pBO(f|f2) corresponds to a different backoff path, and parallel backoff is obtained by using an appropriate g (see below).","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"As long as g is non-negative, the backoff weights are defined as follows: This equation is non-standard only in the denominator, where one may no longer sum over the factors f only with counts greater than T. This is because g is not necessarily a distribution (i.e., does not sum to unity).","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"Therefore, backoff weight computation can indeed be more expensive for certain g functions, but this appears not to be prohibitive as demonstrated in the next few sections.","{'title': '3 Generalized Parallel Backoff', 'number': '3'}"
"During the recent 2002 JHU workshop (Kirchhoff et al., 2003), significant extensions were made to the SRI language modeling toolkit (Stolcke, 2002) to support arbitrary FLMs and GPB procedures.","{'title': '4 SRILM-FLM extensions', 'number': '4'}"
"This uses a graphicalmodel like specification language, and where many different backoff functions (19 in total) were implemented.","{'title': '4 SRILM-FLM extensions', 'number': '4'}"
"Other features include: 1) all SRILM smoothing methods at every node in a backoff graph; 2) graph level skipping; and 3) up to 32 possible parents (e.g., 33-gram).","{'title': '4 SRILM-FLM extensions', 'number': '4'}"
Two of the backoff functions are (in the three parents case): where (call this g2) where N() is the count function.,"{'title': '4 SRILM-FLM extensions', 'number': '4'}"
"Implemented backoff functions include maximum/min (normalized) counts/backoff probabilities, products, sums, mins, maxs, (weighted) averages, and geometric means.","{'title': '4 SRILM-FLM extensions', 'number': '4'}"
GPB-FLMs were applied to two corpora and their perplexity was compared with standard optimized vanilla biand trigram language models.,"{'title': '5 Results', 'number': '5'}"
"In the following, we consider as a “bigram” a language model with a temporal history that includes information from no longer than one previous time-step into the past.","{'title': '5 Results', 'number': '5'}"
"Therefore, if factors are deterministically derivable from words, a “bigram” might include both the previous words and previous factors as a history.","{'title': '5 Results', 'number': '5'}"
"From a decoding state-space perspective, any such bigram would be relatively cheap.","{'title': '5 Results', 'number': '5'}"
"In CallHome-Arabic, words are accompanied with deterministically derived factors: morphological class (M), stems (S), roots (R), and patterns (P).","{'title': '5 Results', 'number': '5'}"
Training data consisted of official training portions of the LDC CallHome ECA corpus plus the CallHome ECA supplement (100 conversations).,"{'title': '5 Results', 'number': '5'}"
For testing we used the official 1996 evaluation set.,"{'title': '5 Results', 'number': '5'}"
"Results are given in Table 1 and show perplexity for: 1) the baseline 3-gram; 2) a FLM 3-gram using morphs and stems; 3) a GPB-FLM 3-gram using morphs, stems and backoff function g1; 4) the baseline 2-gram; 5) an FLM 2-gram using morphs; 6) an FLM 2-gram using morphs and stems; and 7) an GPB-FLM 2-gram using morphs and stems.","{'title': '5 Results', 'number': '5'}"
Backoffpath(s) are depicted by listing the parent number(s) in backoff order.,"{'title': '5 Results', 'number': '5'}"
"As can be seen, the FLM alone might increase perplexity, but the GPB-FLM decreases it.","{'title': '5 Results', 'number': '5'}"
"Also, it is possible to obtain a 2-gram with lower perplexity than the optimized baseline 3-gram.","{'title': '5 Results', 'number': '5'}"
The Wall Street Journal (WSJ) data is from the Penn Treebank 2 tagged (’88-’89) WSJ collection.,"{'title': '5 Results', 'number': '5'}"
Word and POS tag information (Tt) was extracted.,"{'title': '5 Results', 'number': '5'}"
The sentence order was randomized to produce 5-fold crossvalidation results using (4/5)/(1/5) training/testing sizes.,"{'title': '5 Results', 'number': '5'}"
"Other factors included the use of a simple deterministic tagger obtained by mapping a word to its most frequent tag (Ft), and word classes obtained using SRILM’s ngram-class tool with 50 (Ct) and 500 (Dt) classes.","{'title': '5 Results', 'number': '5'}"
Results are given in Table 2.,"{'title': '5 Results', 'number': '5'}"
"The table shows the baseline 3-gram and 2-gram perplexities, and three GPB-FLMs.","{'title': '5 Results', 'number': '5'}"
Model A uses the true by-hand tag information from the Treebank.,"{'title': '5 Results', 'number': '5'}"
"To simulate conditions during first-pass decoding, Model B shows the results using the most frequent tag, and Model C uses only the two data-driven word classes.","{'title': '5 Results', 'number': '5'}"
"As can be seen, the bigram perplexities are significantly reduced relative to the baseline, almost matching that of the baseline trigram.","{'title': '5 Results', 'number': '5'}"
Note that none of these reduced perplexity bigrams were possible without using one of the novel backoff functions.,"{'title': '5 Results', 'number': '5'}"
The improved perplexity bigram results mentioned above should ideally be part of a first-pass recognition step of a multi-pass speech recognition system.,"{'title': '6 Discussion', 'number': '6'}"
"With a bigram, the decoder search space is not large, so any appreciable LM perplexity reductions should yield comparable word error reductions for a fixed set of acoustic scores in a fzrstpass.","{'title': '6 Discussion', 'number': '6'}"
"For N-best or lattice generation, the oracle error should similarly improve.","{'title': '6 Discussion', 'number': '6'}"
"The use of an FLM with GPB in such a first pass, however, requires a decoder that supports such language models.","{'title': '6 Discussion', 'number': '6'}"
"Therefore, FLMs with GPB will be incorporated into GMTK (Bilmes, 2002), a general purpose graphical model toolkit for speech recognition and language processing.","{'title': '6 Discussion', 'number': '6'}"
"The authors thank Dimitra Vergyri, Andreas Stolcke, and Pat Schone for useful discussions during the JHU’02 workshop.","{'title': '6 Discussion', 'number': '6'}"
