col1,col2
"Named Entity (NE) recognition is a task in whichproper nouns and numerical information are extracted from documents and are classified into cat egories such as person, organization, and date.",Introduction
It is a key technology of Information Extraction and Open-Domain Question Answering.,Introduction
"First, we showthat an NE recognizer based on Support Vector Ma chines (SVMs) gives better scores than conventional systems.",Introduction
"However, off-the-shelf SVM classifiers are too inefficient for this task.",Introduction
"Therefore, we present a method that makes the system substantially faster.This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging.",Introduction
We also present an SVM-based feature selec tion method and an efficient training method.,Introduction
"Named Entity (NE) recognition is a task in whichproper nouns and numerical information in a docu ment are detected and classified into categories suchas person, organization, and date.",Experiment/Discussion
"It is a key technol ogy of Information Extraction and Open-Domain Question Answering (Voorhees and Harman, 2000).",Experiment/Discussion
We are building a trainable Open-Domain Question Answering System called SAIQA-II.,Experiment/Discussion
"In this paper, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores thanconventional systems.",Experiment/Discussion
"SVMs have given high per formance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001).",Experiment/Discussion
"However, it turned out that off-the-shelf SVM classifiers are too inefficient for NE recognition.",Experiment/Discussion
"The recognizer runs at a rate of only 85 bytes/sec on an Athlon 1.3 GHz Linux PC, while rule-based systems (e.g., Isozaki, (2001)) can process several kilobytes in a second.",Experiment/Discussion
The major reason is the inefficiency of SVM classifiers.,Experiment/Discussion
There are otherreports on the slowness of SVM classifiers.,Experiment/Discussion
"Another SVM-based NE recognizer (Yamada and Mat sumoto, 2001) is 0.8 sentences/sec on a Pentium III 933 MHz PC.",Experiment/Discussion
An SVM-based part-of-speech (POS).,Experiment/Discussion
"tagger (Nakagawa et al, 2001) is 20 tokens/sec on an Alpha 21164A 500 MHz processor.",Experiment/Discussion
It is difficult to use such slow systems in practical applications.,Experiment/Discussion
"In this paper, we present a method that makes the NE system substantially faster.",Experiment/Discussion
This method can also be applied to other tasks in natural languageprocessing such as chunking and POS tagging.,Experiment/Discussion
Another problem with SVMs is its incomprehensibil ity.,Experiment/Discussion
It is not clear which features are important or how they work.,Experiment/Discussion
The above method is also useful for finding useless features.,Experiment/Discussion
We also mention a method to reduce training time.,Experiment/Discussion
1.1 Support Vector Machines.,Experiment/Discussion
"Suppose we have a set of training data for a two class problem:     , where   ffflfi is a feature vector of the ffi -th sample in the training data and   !$#%# is the label forthe sample.",Experiment/Discussion
"The goal is to find a decision func tion that accurately predicts for unseen  . A non-linear SVM classifier gives a decision function ( ) * sign ,+-) for an input vector  where +-) .* / 0 21)3 546879: !6; Here, () *=!$# means  is a member of a cer tain class and () $* # means  is not a mem ber.",Experiment/Discussion
7 s are called support vectors and are repre sentatives of training examples.,Experiment/Discussion
 is the numberof support vectors.,Experiment/Discussion
"Therefore, computational com plexity of +?) is proportional to  . Support vectorsand other constants are determined by solving a cer tain quadratic programming problem.",Experiment/Discussion
4687@ is akernel that implicitly maps vectors into a higher di mensional space.,Experiment/Discussion
"Typical kernels use dot products: 4687@ A*CBED7@ . A polynomial kernel of degree Fis given by BG? *HI#J!KG L . We can use vari MM M M N M M M M M M M M M N M O O O O O N O O O O O O O O O O O O M : positive example, O : negative example N M , N O : support vectors Figure 1: Support Vector Machine ous kernels, and the design of an appropriate kernel for a particular application is an important research issue.Figure 1 shows a linearly separable case.",Experiment/Discussion
The de cision hyperplane defined by +-) P*RQ separatespositive and negative examples by the largest mar gin.,Experiment/Discussion
The solid line indicates the decision hyperplaneand two parallel dotted lines indicate the margin be tween positive and negative examples.,Experiment/Discussion
"Since such aseparating hyperplane may not exist, a positive pa rameter S is introduced to allow misclassifications.",Experiment/Discussion
See Vapnik (1995).,Experiment/Discussion
1.2 SVM-based NE recognition.,Experiment/Discussion
"As far as we know, the first SVM-based NE system was proposed by Yamada et al (2001) for Japanese.His system is an extension of Kudo?s chunking sys tem (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks.",Experiment/Discussion
"In theirsystem, every word in a sentence is classified sequentially from the beginning or the end of a sen tence.",Experiment/Discussion
"However, since Yamada has not compared it with other methods under the same conditions, it is not clear whether his NE system is better or not.",Experiment/Discussion
"Here, we show that our SVM-based NE system ismore accurate than conventional systems.",Experiment/Discussion
"Our sys tem uses the Viterbi search (Allen, 1995) instead of sequential determination.For training, we use ?CRL data?, which was prepared for IREX (Information Retrieval and Extrac tion Exercise1, Sekine and Eriguchi (2000)).",Experiment/Discussion
"It has about 19,000 NEs in 1,174 articles.",Experiment/Discussion
We also use additional data by Isozaki (2001).,Experiment/Discussion
Both datasets are based on Mainichi Newspaper?s 1994 and 1995 CD-ROMs.,Experiment/Discussion
"We use IREX?s formal test data calledGENERAL that has 1,510 named entities in 71 ar ticles from Mainichi Newspaper of 1999.",Experiment/Discussion
Systems are compared in terms of GENERAL?s F-measure 1http://cs.nyu.edu/cs/projects/proteus/irexwhich is the harmonic mean of ?recall?,Experiment/Discussion
and ?preci sion?,Experiment/Discussion
and is defined as follows.,Experiment/Discussion
"Recall = M/(the number of correct NEs), Precision = M/(the number of NEs extracted by a system), where M is the number of NEs correctly extracted and classified by the system.We developed an SVM-based NE system by following our NE system based on maximum entropy (ME) modeling (Isozaki, 2001).",Experiment/Discussion
We sim ply replaced the ME model with SVM classifiers.The above datasets are processed by a morphological analyzer ChaSen 2.2.12.,Experiment/Discussion
It tokenizes a sen tence into words and adds POS tags.,Experiment/Discussion
ChaSen uses about 90 POS tags such as common-noun and location-name.,Experiment/Discussion
"Since most unknown words are proper nouns, ChaSen?s parameters for unknownwords are modified for better results.",Experiment/Discussion
"Then, a char acter type tag is added to each word.",Experiment/Discussion
It uses 17character types such as all-kanji and small integer.,Experiment/Discussion
See Isozaki (2001) for details.,Experiment/Discussion
"Now, Japanese NE recognition is solved by theclassification of words (Sekine et al, 1998; Borth wick, 1999; Uchimoto et al, 2000).",Experiment/Discussion
"For instance, the words in ?President George Herbert Bush saidClinton is . . .",Experiment/Discussion
are classified as follows: ?President?,Experiment/Discussion
"= OTHER, ?George?",Experiment/Discussion
"= PERSON-BEGIN, ?Her bert?",Experiment/Discussion
"= PERSON-MIDDLE, ?Bush?",Experiment/Discussion
"= PERSON-END, ?said?",Experiment/Discussion
"= OTHER, ?Clinton?",Experiment/Discussion
"= PERSON-SINGLE, ?is? = OTHER.",Experiment/Discussion
"In this way, the first word of a person?s name is labeled as PERSON-BEGIN.",Experiment/Discussion
The last word is labeled as PERSON-END.,Experiment/Discussion
Other words in the nameare PERSON-MIDDLE.,Experiment/Discussion
"If a person?s name is expressed by a single word, it is labeled as PERSON SINGLE.",Experiment/Discussion
"If a word does not belong to any namedentities, it is labeled as OTHER.",Experiment/Discussion
"Since IREX de fines eight NE classes, words are classified into 33 ( *UTWVEX!K# ) categories.Each sample is represented by 15 features be cause each word has three features (part-of-speech tag, character type, and the word itself), and two preceding words and two succeeding words are also used for context dependence.",Experiment/Discussion
"Although infrequent features are usually removed to prevent overfitting, we use all features because SVMs are robust.",Experiment/Discussion
"Each sample is represented by a long binary vector, i.e., a sequence of 0 (false) and 1 (true).",Experiment/Discussion
"For instance, ?Bush?",Experiment/Discussion
in the above example is represented by a 2http://chasen.aist-nara.ac.jp/ vector P*YG[Z\#^]_ G[Z `a] described below.,Experiment/Discussion
Only 15 elements are 1.,Experiment/Discussion
bdcfe8ghji // Current word is not ?Alice?,Experiment/Discussion
bdc klghme // Current word is ?Bush?,Experiment/Discussion
bdc nghji // Current word is not ?Charlie?,Experiment/Discussion
: bdcfe^opikpqpghme // Current POS is a proper noun bdcfe^opinipghji // Current POS is not a verb : bdc nqre^sre ghji // Previous word is not ?Henry?,Experiment/Discussion
bdc nqre^skghme // Previous word is ?Herbert?,Experiment/Discussion
":Here, we have to consider the following problems.",Experiment/Discussion
"First, SVMs can solve only a two-class problem.",Experiment/Discussion
"Therefore, we have to reduce the above multi class problem to a group of two-class problems.",Experiment/Discussion
"Second, we have to consider consistency among word classes in a sentence.",Experiment/Discussion
"For instance, a word classified as PERSON-BEGIN should be followed by PERSON-MIDDLE or PERSON-END.",Experiment/Discussion
"It impliesthat the system has to determine the best combina tions of word classes from numerous possibilities.Here, we solve these problems by combining exist ing methods.",Experiment/Discussion
There are a few approaches to extend SVMs to cover t -class problems.,Experiment/Discussion
"Here, we employ the ?oneclass versus all others?",Experiment/Discussion
approach.,Experiment/Discussion
"That is, each clas sifier (%u ) is trained to distinguish members of a class v from non-members.",Experiment/Discussion
"In this method, two or more classifiers may give !$# to an unseen vector or no classifier may give !$# . One common way to avoid such situations is to compare + u ) values and to choose the class index v of the largest + u ) . The consistency problem is solved by the Viterbi search.",Experiment/Discussion
"Since SVMs do not output probabilities, we use the SVM+sigmoid method (Platt, 2000).",Experiment/Discussion
"That is, we use a sigmoid function wxG? J*y#zI#{!",Experiment/Discussion
|l}~ {G to map + u ) to a probability-like value.,Experiment/Discussion
The output of the Viterbi search is adjusted by a postprocessor for wrong word boundaries.,Experiment/Discussion
"The adjustment rules are also statistically determined (Isozaki, 2001).",Experiment/Discussion
1.3 Comparison of NE recognizers.,Experiment/Discussion
We use a fixed value ?* #Q9Q . F-measures are not very sensitive to  unless  is too small.,Experiment/Discussion
"Whenwe used 1,038,986 training vectors, GENERAL?s F measure was 89.64% for ?*?Q?# and 90.03% for 6*?#Q9Q . We employ the quadratic kernel ( F *Y? ) because it gives the best results.",Experiment/Discussion
"Polynomial kernels of degree 1, 2, and 3 resulted in 83.03%, 88.31%, F-measure (%) ? ?",Experiment/Discussion
RG+DT ? ?,Experiment/Discussion
ME ? ?,Experiment/Discussion
SVM 0 20 40 60 80 100 120 CRL data ???E? ?^??:???,Experiment/Discussion
76 78 80 82 84 86 88 90 Number of NEs in training data ( ??,Experiment/Discussion
") Figure 2: F-measures of NE systems and 87.04% respectively when we used 569,994 training vectors.",Experiment/Discussion
Figure 2 compares NE recognizers in terms ofGENERAL?s F-measures.,Experiment/Discussion
?SVM?,Experiment/Discussion
in the figure in dicates F-measures of our system trained by Kudo?s TinySVM-0.073 with S?*?Q?# . It attained 85.04% when we used only CRL data.,Experiment/Discussion
?ME? indicates our ME system and ?RG+DT?,Experiment/Discussion
"indicates a rule-basedmachine learning system (Isozaki, 2001).",Experiment/Discussion
"According to this graph, ?SVM?",Experiment/Discussion
"is better than the other sys tems.However, SVM classifiers are too slow.",Experiment/Discussion
"Fa mous SVM-Light 3.50 (Joachims, 1999) took 1.2 days to classify 569,994 vectors derived from 2 MB documents.",Experiment/Discussion
"That is, it runs at only 19 bytes/sec.",Experiment/Discussion
"TinySVM?s classifier seems best optimized among publicly available SVM toolkits, but it still works at only 92 bytes/sec.",Experiment/Discussion
"In this section, we investigate the cause of this in efficiency and propose a solution.",Experiment/Discussion
"All experiments are conducted for training data of 569,994 vectors.",Experiment/Discussion
"The total size of the original news articles was 2 MB and the number of NEs was 39,022.",Experiment/Discussion
"According to the definition of +-) , a classifier has to process support vectors for each  . Table 1 shows  s for different word classes.",Experiment/Discussion
"According to this table, classi fication of one word requires  ?s dot products with 228,306 support vectors in 33 classifiers.",Experiment/Discussion
"Therefore, the classifiers are very slow.",Experiment/Discussion
We have never seensuch large  s in SVM literature on pattern recogni tion.,Experiment/Discussion
The reason for the large  s is word features.,Experiment/Discussion
"Inother domains such as character recognition, dimen 3http://cl.aist-nara.ac.jp/?taku-ku/software/TinySVM sion ` is usually fixed.",Experiment/Discussion
"However, in the NE task, ` increases monotonically with respect to the size of the training data.",Experiment/Discussion
"Since SVMs learn combinations of features,  tends to be very large.",Experiment/Discussion
"This tendencywill hold for other tasks of natural language pro cessing, too.",Experiment/Discussion
"Here, we focus on the quadratic kernel BG * I#!?G ?",Experiment/Discussion
that yielded the best score in the above experiments.,Experiment/Discussion
Suppose ?* G[Z\#^]_ G[Z `a] hasonly ?,Experiment/Discussion
(=15) non-zero elements.,Experiment/Discussion
The dot prod uct of  and 7  * 5?,Experiment/Discussion
 Z\#^]_ ?  Z `] is given by ? fi ? 1) G[Z??,Experiment/Discussion
 Z??,Experiment/Discussion
"] . Hence, I#!??D?7  ? *?#!W? fi 0 ? 1) G?Z??",Experiment/Discussion
 Z???]!? fi 0 ? 1) G?Z??,Experiment/Discussion
 Z???] ?  We can rewrite +-) as follows.,Experiment/Discussion
fi 0 ? 1) _?  Z??,Experiment/Discussion
]?G[Z???]?!m? ? Z???]?G[Z???],Experiment/Discussion
fi.?,Experiment/Discussion
 0 ? 1) fi 0 ? 1 ??,Experiment/Discussion
 ???rZ??? B@]?G[Z??,Experiment/Discussion
]?G?Z?B@]_ where ? ?,Experiment/Discussion
/ ?1) 3 ? ??Z??,Experiment/Discussion
/ ?1) 3 5? Z??,Experiment/Discussion
]_ ? ?,Experiment/Discussion
Z??,Experiment/Discussion
]?* ? / ?1) 3 ??p8Z??,Experiment/Discussion
]??% ?P?rZ?? B@]?* ? ?,Experiment/Discussion
/ ?1) 3  ?  Z??,Experiment/Discussion
" Z?B@]_ For binary vectors, it can be simplified as +-) .*??",Experiment/Discussion
"0 ??,?9?",Experiment/Discussion
?l? 1) _?C?  Z???],Experiment/Discussion
"0 ?-?,????%?",Experiment/Discussion
?9?,Experiment/Discussion
1) ? ?,Experiment/Discussion
Z?? B@]  where ? ?,Experiment/Discussion
 Z???]?* ?  Z???],Experiment/Discussion
!m? ? Z???]Y* ? 0  ???5?,Experiment/Discussion
"?l? 1) 3   ???9Z??? B@]?* ? 0  ?,???_? ?l? 1 ?????",Experiment/Discussion
"1) 3   Now, +?) can be given by summing up ? ?",Experiment/Discussion
 Z???],Experiment/Discussion
for every non-zero element G?Z??,Experiment/Discussion
] and ? ?,Experiment/Discussion
Z?? B@] for every non-zero pair G?Z??,Experiment/Discussion
"]?G[Z?B@] . Accordingly, we only need to add #W!???!??j?R?# z%?",Experiment/Discussion
"(=121) con stants to get +-) . Therefore, we can expect thismethod to be much faster than a na??ve implementa tion that computes tens of thousands of dot products at run time.",Experiment/Discussion
We call this method ?XQK?,Experiment/Discussion
(eXpand the Quadratic Kernel).,Experiment/Discussion
Table 1 compares TinySVM and XQK in terms of CPU time taken to apply 33 classifiers to process the training data.,Experiment/Discussion
Classes are sorted by  . Small numbers in parentheses indicate the initializationtime for reading support vectors 7   and allocat ing memory.,Experiment/Discussion
XQK requires a longer initialization time in order to prepare ? ?,Experiment/Discussion
 and ???,Experiment/Discussion
"For instance,TinySVM took 11,490.26 seconds (3.2 hours) in to tal for applying OTHER?s classifier to all vectors in the training data.",Experiment/Discussion
"Its initialization phase took 2.13 seconds and all vectors in the training data were classified in 11,488.13 ( *=#9#%X?%Q??9????x?#p? ) sec onds.",Experiment/Discussion
"On the other hand, XQK took 225.28 secondsin total and its initialization phase took 174.17 sec onds.",Experiment/Discussion
"Therefore, 569,994 vectors were classified in51.11 seconds.",Experiment/Discussion
The initialization time can be disre garded because we can reuse the above coefficents.,Experiment/Discussion
"Consequently, XQK is 224.8 (=11,488.13/51.11) times faster than TinySVM for OTHER.",Experiment/Discussion
"TinySVM took 6 hours to process all the word classes, whereas XQK took only 17 minutes.",Experiment/Discussion
XQK is 102 times faster than SVM-Light 3.50 which took 1.2 days.,Experiment/Discussion
"XQK makes the classifiers faster, but mem ory requirement increases from ? ? / ?1) ?  to ? ? / ?1) ?  ?",Experiment/Discussion
" !fl# z%?r where ? (=15) is the num ber of non-zero elements in 7  . Therefore, removal.",Experiment/Discussion
of useless features would be beneficial.,Experiment/Discussion
"Conven tional SVMs do not tell us how an individual feature works because weights are given not to features but to 4687  . However, the above weights ( ? ?",Experiment/Discussion
 and ???,Experiment/Discussion
) clarify how a feature or a feature pair works.,Experiment/Discussion
We can use this fact for feature selection after the training.,Experiment/Discussion
We simplify ( ) by removing all features ? that satisfy ??,Experiment/Discussion
} 8???,Experiment/Discussion
Z??,Experiment/Discussion
]?f???,Experiment/Discussion
} ? ?????rZ??? B@]?f ??,Experiment/Discussion
} ? ???P?rZ?B- ?]??,Experiment/Discussion
K???,Experiment/Discussion
The largest ? that does not change the number of misclassifications for the training data is found by using the binary searchfor each word class.,Experiment/Discussion
We call this method ?XQKFS?,Experiment/Discussion
(XQK with Feature Selection).,Experiment/Discussion
This approx imation slightly degraded GENERAL?s F-measure from 88.31% to 88.03%.Table 2 shows the reduction of features that ap pear in support vectors.,Experiment/Discussion
Classes are sorted by the numbers of original features.,Experiment/Discussion
"For instance, OTHERhas 56,220 features in its support vectors.",Experiment/Discussion
"Accord ing to the binary search, its performance did notchange even when the number of features was re duced to 21,852 at ?*KQ?Qr?9?r?%?",Experiment/Discussion
"Table 1: Reduction of CPU time (in seconds) by XQK word class  TinySVM (init) XQK (init) speed up SVM-Light OTHER 64,970 11,488.13 (2.13) 51.11 (174.17) 224.8 29,986.52 ARTIFACT-MIDDLE 14,171 1,372.85 (0.51) 41.32 (14.98) 33.2 6,666.26 LOCATION-SINGLE 13,019 1,209.29 (0.47) 38.24 (11.41) 31.6 6,100.54 ORGANIZ..-MIDDLE 12,050 987.39 (0.44) 37.93 (11.70) 26.0 5,570.82 : : : : : : TOTAL 228,306 21,754.23 (9.83) 1,019.20 (281.28) 21.3 104,466.31 Table 2: Reduction of features by XQK-FS word class number of features number of non-zero weights seconds OTHER 56,220 ? 21,852 (38.9%) 1,512,827 ? 892,228 (59.0%) 42.31 ARTIFIFACT-MIDDLE 22,090 ? 4,410 (20.0%) 473,923 ? 164,632 (34.7%) 30.47 LOCATION-SINGLE 17,169 ? 3,382 (19.7%) 366,961 ? 123,808 (33.7%) 27.72 ORGANIZ..-MIDDLE 17,123 ? 9,959 (58.2%) 372,784 ? 263,695 (70.7%) 31.02 ORGANIZ..-END 15,214 ? 3,073 (20.2%) 324,514 ? 112,307 (34.6%) 26.87 : : : : TOTAL 307,721 ? 75,455 (24.5%) 6,669,664 ? 2,650,681 (39.7%) 763.10 The total number of features was reduced by 75%and that of weights was reduced by 60%.",Experiment/Discussion
The ta ble also shows CPU time for classification by the selected features.,Experiment/Discussion
XQK-FS is 28.5 (=21754.23/ 763.10) times faster than TinySVM.,Experiment/Discussion
"Although the reduction of features is significant, the reduction of CPU time is moderate, because most of the reducedfeatures are infrequent ones.",Experiment/Discussion
"However, simple re duction of infrequent features without consideringweights damages the system?s performance.",Experiment/Discussion
"For instance, when we removed 5,066 features that ap peared four times or less in the training data, themodified classifier for ORGANIZATION-END misclassified 103 training examples, whereas the origi nal classifier misclassified only 19 examples.",Experiment/Discussion
"On theother hand, XQK-FS removed 12,141 features with out an increase in misclassifications for the training data.",Experiment/Discussion
XQK can be easily extended to a more generalquadratic kernel BG? ?*??vl??!?v  G ?,Experiment/Discussion
and to nonbinary sparse vectors.,Experiment/Discussion
XQK-FS can be used to se lect useful features before training by other kernels.,Experiment/Discussion
"As mentioned above, we conducted an experiment for the cubic kernel ( F *??",Experiment/Discussion
") by using all features.When we trained the cubic kernel classifiers by us ing only features selected by XQK-FS, TinySVM?s classification time was reduced by 40% because  was reduced by 38%.",Experiment/Discussion
GENERAL?s F-measure was slightly improved from 87.04% to 87.10%.,Experiment/Discussion
"Onthe other hand, when we trained the cubic ker nel classifiers by using only features that appeared three times or more (without considering weights), TinySVM?s classification time was reduced by only 14% and the F-measure was slightly degraded to86.85%.",Experiment/Discussion
"Therefore, we expect XQK-FS to be use ful as a feature selection method for other kernels when such kernels give much better results than the quadratic kernel.",Experiment/Discussion
"Since training of 33 classifiers also takes a longtime, it is difficult to try various combinations of pa rameters and features.",Experiment/Discussion
"Here, we present a solution for this problem.",Experiment/Discussion
"In the training time, calculation of B???Dr   B??$Dr ?  B??D@  for various  ? s is dominant.",Experiment/Discussion
Conventional systems save time by caching the results.,Experiment/Discussion
"By analyzing TinySVM?s classifier, we found that they can be calculated more efficiently.",Experiment/Discussion
"For sparse vectors, most SVM classifiers (e.g., SVM-Light) use a sparse dot product algorithm (Platt, 1999) that compares non-zero elements of  and those of 7  to get BED7  in +-) . However,  is common to all dot products in B?D7   BD 7/ . Therefore, we can implement a faster classifierthat calculates them concurrently.",Experiment/Discussion
TinySVM?s clas sifier prepares a list fi2si Z??,Experiment/Discussion
] that contains all 7  s whose ? -th coordinates are not zero.,Experiment/Discussion
"In addition, counters for ?D%7  p ?D%7 / are prepared because dot products of binary vectors are integers.",Experiment/Discussion
"Then, for each non-zero G[Z??",Experiment/Discussion
"] , the counters are incremented for all 7   fi2si Z???]",Experiment/Discussion
By checking only members of fi2si Z??,Experiment/Discussion
] for non-zero G[Z??,Experiment/Discussion
"] , the classifier is not bothered by fruitless cases: G?Z??",Experiment/Discussion
"]?*?Q ?8Z???]??*YQ orG[Z???]W?*?Q ? ?Z???]?*yQ . Therefore, TinySVM?s clas sifier is faster than other classifiers.",Experiment/Discussion
This method is applicable to any kernels based on dot products.,Experiment/Discussion
"For the training phase, we can build fi2si ? Z???]",Experiment/Discussion
that contains all   s whose ? -th coordinates are notzero.,Experiment/Discussion
"Then, B??D   B???D  can be efficiently calculated because ??",Experiment/Discussion
is common.,Experiment/Discussion
This im provement is effective especially when the cache is small and/or the training data is large.,Experiment/Discussion
"When we used a 200 MB cache, the improved system took only 13 hours for training by the CRL data, while TinySVM and SVM-Light took 30 hours and 46hours respectively for the same cache size.",Experiment/Discussion
"Al though we have examined other SVM toolkits, we could not find any system that uses this approach in the training phase.",Experiment/Discussion
The above methods can also be applied to othertasks in natural language processing such as chunk ing and POS tagging because the quadratic kernels give good results.,Results/Conclusion
"Utsuro et al (2001) report that a combination of two NE recognizers attained F = 84.07%, butwrong word boundary cases are excluded.",Results/Conclusion
Our system attained 85.04% and word boundaries are auto matically adjusted.,Results/Conclusion
"Yamada (Yamada et al, 2001) also reports that F*??",Results/Conclusion
is best.,Results/Conclusion
"Although his sys tem attained F = 83.7% for 5-fold cross-validation of the CRL data (Yamada and Matsumoto, 2001), our system attained 86.8%.",Results/Conclusion
"Since we followedIsozaki?s implementation (Isozaki, 2001), our system is different from Yamada?s system in the fol lowing points: 1) adjustment of word boundaries, 2)ChaSen?s parameters for unknown words, 3) char acter types, 4) use of the Viterbi search.",Results/Conclusion
"For efficient classification, Burges and Scho?lkopf (1997) propose an approximation method that uses ?reduced set vectors?",Results/Conclusion
instead of support vectors.,Results/Conclusion
"Since the size of the reduced set vectors is smaller than  , classifiers become more efficient, but the computational cost to determine the vectors is verylarge.",Results/Conclusion
Osuna and Girosi (1999) propose two meth ods.,Results/Conclusion
"The first method approximates +-) by support vector regression, but this method is applicable onlywhen S is large enough.",Results/Conclusion
The second method reformulates the training phase.,Results/Conclusion
Our approach is sim pler than these methods.,Results/Conclusion
"Downs et al (Downs et al, 2001) try to reduce the number of support vectors by using linear dependence.",Results/Conclusion
"We can also reduce the run-time complexity of a multi-class problem by cascading SVMs in the form of a binary tree (Schwenker, 2001) or a directacyclic graph (Platt et al, 2000).",Results/Conclusion
Yamada and Mat sumoto (2001) applied such a method to their NEsystem and reduced its CPU time by 39%.,Results/Conclusion
This ap proach can be combined with our SVM classifers.NE recognition can be regarded as a variablelength multi-class problem.,Results/Conclusion
"For this kind of prob lem, probability-based kernels are studied for more theoretically well-founded methods (Jaakkola and Haussler, 1998; Tsuda et al, 2001; Shimodaira et al., 2001).",Results/Conclusion
Our SVM-based NE recognizer attained F = 90.03%.,Results/Conclusion
"This is the best score, as far as we know.",Results/Conclusion
"Since it was too slow, we made SVMs faster.",Results/Conclusion
The improved classifier is 21 times faster than TinySVMand 102 times faster than SVM-Light.,Results/Conclusion
The im proved training program is 2.3 times faster than TinySVM and 3.5 times faster than SVM-Light.,Results/Conclusion
We also presented an SVM-based feature selectionmethod that removed 75% of features.,Results/Conclusion
These methods can also be applied to other tasks such as chunk ing and POS tagging.,Results/Conclusion
AcknowledgmentWe would like to thank Yutaka Sasaki for the training data.,Results/Conclusion
We thank members of Knowledge Pro cessing Research Group for valuable comments and discussion.,Results/Conclusion
We also thank Shigeru Katagiri and Ken-ichiro Ishii for their support.,Results/Conclusion
