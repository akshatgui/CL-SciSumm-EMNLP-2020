col1,col2
Named Entity recognition (NER) is an important part of many natural language processing tasks.,{}
Most current approaches employ machine learning techniques and require supervised data.,{}
"However, many languages lack such resources.",{}
"This paper presents an algorithm to automatically discover Named Entities (NEs) in a resource free language, given a bilingual corpora in which it is weakly temporally aligned with a resource rich language.",{}
"We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively.",{}
"The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.",{}
"We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.",{}
"Named Entity recognition has been getting much attention in NLP research in recent years, since it is seen as a significant component of higher level NLP tasks such as information distillation and question answering, and an enabling technology for better information access.","{'title': '1 Introduction', 'number': '1'}"
"Most successful approaches to NER employ machine learning techniques, which require supervised training data.","{'title': '1 Introduction', 'number': '1'}"
"However, for many languages, these resources do not exist.","{'title': '1 Introduction', 'number': '1'}"
"Moreover, it is often difficult to find experts in these languages both for the expensive annotation effort and even for language specific clues.","{'title': '1 Introduction', 'number': '1'}"
"On the other hand, comparable multilingual data (such as multilingual news streams) are increasingly available (see section 4).","{'title': '1 Introduction', 'number': '1'}"
"In this work, we make two independent observations about Named Entities encountered in such corpora, and use them to develop an algorithm that extracts pairs of NEs across languages.","{'title': '1 Introduction', 'number': '1'}"
"Specifically, given a bilingual corpora that is weakly temporally aligned, and a capability to annotate the text in one of the languages with NEs, our algorithm identifies the corresponding NEs in the second language text, and annotates them with the appropriate type, as in the source text.","{'title': '1 Introduction', 'number': '1'}"
The first observation is that NEs in one language in such corpora tend to co-occur with their counterparts in the other.,"{'title': '1 Introduction', 'number': '1'}"
"E.g., Figure 1 shows a histogram of the number of occurrences of the word Hussein and its Russian transliteration in our bilingual news corpus spanning years 2001 through late 2005.","{'title': '1 Introduction', 'number': '1'}"
"One can see several common peaks in the two histograms, largest one being around the time of the beginning of the war in Iraq.","{'title': '1 Introduction', 'number': '1'}"
"The word Russia, on the other hand, has a distinctly different temporal signature.","{'title': '1 Introduction', 'number': '1'}"
We can exploit such weak synchronicity of NEs across languages as a way to associate them.,"{'title': '1 Introduction', 'number': '1'}"
"In order to score a pair of entities across languages, we compute the similarity of their time distributions.","{'title': '1 Introduction', 'number': '1'}"
"The second observation is that NEs are often transliterated or have a common etymological origin across languages, and thus are phonetically similar.","{'title': '1 Introduction', 'number': '1'}"
Figure 2 shows an example list of NEs and their possible Russian transliterations.,"{'title': '1 Introduction', 'number': '1'}"
Approaches that attempt to use these two characteristics separately to identify NEs across languages would have significant shortcomings.,"{'title': '1 Introduction', 'number': '1'}"
"Transliteration based approaches require a good model, typically handcrafted or trained on a clean set of transliteration pairs.","{'title': '1 Introduction', 'number': '1'}"
"On the other hand, time sequence similarity based approaches would incorrectly match words which happen to have similar time signatures (e.g.","{'title': '1 Introduction', 'number': '1'}"
Taliban and Afghanistan in recent news).,"{'title': '1 Introduction', 'number': '1'}"
We introduce an algorithm we call co-ranking which exploits these observations simultaneously to match NEs on one side of the bilingual corpus to their counterparts on the other.,"{'title': '1 Introduction', 'number': '1'}"
"We use a Discrete Fourier Transform (Arfken, 1985) based metric for computing similarity of time distributions, and we score NEs similarity with a linear transliteration model.","{'title': '1 Introduction', 'number': '1'}"
"For a given NE in one language, the transliteration model chooses a top ranked list of candidates in another language.","{'title': '1 Introduction', 'number': '1'}"
Time sequence scoring is then used to re-rank the candidates and choose the one best temporally aligned with the NE.,"{'title': '1 Introduction', 'number': '1'}"
"That is, we attempt to choose a candidate which is both a good transliteration (according to the current model) and is well aligned with the NE.","{'title': '1 Introduction', 'number': '1'}"
"Finally, pairs of NEs and the best candidates are used to iteratively train the transliteration model.","{'title': '1 Introduction', 'number': '1'}"
A major challenge inherent in discovering transliterated NEs is the fact that a single entity may be represented by multiple transliteration strings.,"{'title': '1 Introduction', 'number': '1'}"
One reason is language morphology.,"{'title': '1 Introduction', 'number': '1'}"
"For example, in Russian, depending on a case being used, the same noun may appear with various endings.","{'title': '1 Introduction', 'number': '1'}"
Another reason is the lack of transliteration standards.,"{'title': '1 Introduction', 'number': '1'}"
"Again, in Russian, several possible transliterations of an English entity may be acceptable, as long as they are phonetically similar to the source.","{'title': '1 Introduction', 'number': '1'}"
"Thus, in order to rely on the time sequences we obtain, we need to be able to group variants of the same NE into an equivalence class, and collect their aggregate mention counts.","{'title': '1 Introduction', 'number': '1'}"
We would then score time sequences of these equivalence classes.,"{'title': '1 Introduction', 'number': '1'}"
"For instance, we would like to count the aggregate number of occurrences of Herzegovina, Hercegovina on the English side in order to map it accurately to the equivalence class of that NE’s variants we may see on the Russian side of our corpus (e.g.","{'title': '1 Introduction', 'number': '1'}"
).,"{'title': '1 Introduction', 'number': '1'}"
One of the objectives for this work was to use as little of the knowledge of both languages as possible.,"{'title': '1 Introduction', 'number': '1'}"
"In order to effectively rely on the quality of time sequence scoring, we used a simple, knowledge poor approach to group NE variants for Russian.","{'title': '1 Introduction', 'number': '1'}"
"In the rest of the paper, whenever we refer to a Named Entity, we imply an NE equivalence class.","{'title': '1 Introduction', 'number': '1'}"
"Note that although we expect that better use of language specific knowledge would improve the results, it would defeat one of the goals of this work.","{'title': '1 Introduction', 'number': '1'}"
There has been other work to automatically discover NE with minimal supervision.,"{'title': '2 Previous Work', 'number': '2'}"
"Both (Cucerzan and Yarowsky, 1999) and (Collins and Singer, 1999) present algorithms to obtain NEs from untagged corpora.","{'title': '2 Previous Work', 'number': '2'}"
"However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language.","{'title': '2 Previous Work', 'number': '2'}"
"The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new.","{'title': '2 Previous Work', 'number': '2'}"
"(Hetland, 2004) surveys recent methods for scoring time sequences for similarity.","{'title': '2 Previous Work', 'number': '2'}"
"(Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources.","{'title': '2 Previous Work', 'number': '2'}"
A large amount of previous work exists on transliteration models.,"{'title': '2 Previous Work', 'number': '2'}"
"Most are generative and consider the task of producing an appropriate transliteration for a given word, and thus require considerable knowledge of the languages.","{'title': '2 Previous Work', 'number': '2'}"
"For example, (AbdulJaleel and Larkey, 2003; Jung et al., 2000) train English-Arabic and English-Korean generative transliteration models, respectively.","{'title': '2 Previous Work', 'number': '2'}"
"(Knight and Graehl, 1997) build a generative model for backward transliteration from Japanese to English.","{'title': '2 Previous Work', 'number': '2'}"
"While generative models are often robust, they tend to make independence assumptions that do not hold in data.","{'title': '2 Previous Work', 'number': '2'}"
"The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).","{'title': '2 Previous Work', 'number': '2'}"
"We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language.","{'title': '2 Previous Work', 'number': '2'}"
"In essence, the algorithm we present uses temporal alignment as a supervision signal to iteratively train a discriminative transliteration model, which can be viewed as a distance metric between and English NE and a potential transliteration.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"On each iteration, it selects a set of transliteration candidates for each NE according to the current model (line 6).","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"It then uses temporal alignment (with thresholding) to select the best transliteration candidate for the next round of training (lines 8, and 9).","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"Once the training is complete, lines 4 through 10 are executed without thresholding for each NE in to discover its counterpart in .","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"In order to generate time sequence for a word, we divide the corpus into a sequence of temporal bins, and count the number of occurrences of the word in each bin.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
We then normalize the sequence.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"We use a method called the F-index (Hetland, 2004) to implement the similarity function on line 8 of the algorithm.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
We first run a Discrete Fourier Transform on a time sequence to extract its Fourier expansion coefficients.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
The score of a pair of time sequences is then computed as a Euclidian distance between their expansion coefficient vectors.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"As we mentioned earlier, an NE in one language may map to multiple morphological variants and transliterations in another.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Identification of the entity’s equivalence class of transliterations is important for obtaining its accurate time sequence.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"In order to keep to our objective of requiring as little language knowledge as possible, we took a rather simplistic approach to take into account morphological ambiguities of NEs in Russian.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Two words were considered variants of the same NE if they share a prefix of size five or longer.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"At this point, our algorithm takes a simplistic approach also for the English side of the corpus – each unique word had its own equivalence class although, in principle, we can incorporate works such as (Li et al., 2004) into the algorithm.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
A cumulative distribution was then collected for such equivalence classes.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"Unlike most of the previous work to transliteration, that consider generative transliteration models, we take a discriminative approach.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
We train a linear model to decide whether a word is a transliteration of an NE .,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
The words in the pair are partitioned into a set of substrings and up to a particular length (including the empty string ).,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Couplings of the substrings from both sets produce features we use for training.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Note that couplings with the empty string represent insertions/omissions.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"Consider the following example: ( ,) We build a feature vector by coupling substrings from the two sets: We use the observation that transliteration tends to preserve phonetic sequence to limit the number of couplings.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"For example, we can disallow the coupling of substrings whose starting positions are too far apart: thus, we might not consider a pairing in the above example.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"In our experiments, we paired substrings if their positions in their respective words differed by -1, 0, or 1.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"We use the perceptron (Rosenblatt, 1958) algorithm to train the model.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
The model activation provides the score we use to select best transliterations on line 6.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Our version of perceptron takes examples with a variable number of features; each example is a set of all features seen so far that are active in the input.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"As the iterative algorithm observes more data, it discovers and makes use of more features.","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
"This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version in SNoW (Roth, 1998).","{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
Negative examples are English non-NEs paired with random Russian words.,"{'title': '3 Co-ranking: An Algorithm for NE Discovery', 'number': '3'}"
We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru).,"{'title': '4 Experimental Study', 'number': '4'}"
The site provides loose translations of (and pointers to) the original English texts.,"{'title': '4 Experimental Study', 'number': '4'}"
We collected pairs of articles spanning from 1/1/2001 through 12/24/2004.,"{'title': '4 Experimental Study', 'number': '4'}"
"The corpus consists of 2,022 documents with 0-8 documents per day.","{'title': '4 Experimental Study', 'number': '4'}"
The corpus is available on our web page at http://L2R.cs.uiuc.edu/ cogcomp/.,"{'title': '4 Experimental Study', 'number': '4'}"
"The English side was tagged with a publicly available NER system based on the SNoW learning architecture (Roth, 1998), that is available at the same site.","{'title': '4 Experimental Study', 'number': '4'}"
This set of English NEs was hand-pruned to remove incorrectly classified words to obtain 978 single word NEs.,"{'title': '4 Experimental Study', 'number': '4'}"
"In order to reduce running time, some limited preprocessing was done on the Russian side.","{'title': '4 Experimental Study', 'number': '4'}"
"All classes, whose temporal distributions were close to uniform (i.e. words with a similar likelihood of occurrence throughout the corpus) were deemed common and not considered as NE candidates.","{'title': '4 Experimental Study', 'number': '4'}"
"Unique words were grouped into 15,594 equivalence classes, and 1,605 of those classes were discarded using this method.","{'title': '4 Experimental Study', 'number': '4'}"
Insertions/omissions features were not used in the experiments as they provided no tangible benefit for the languages of our corpus.,"{'title': '4 Experimental Study', 'number': '4'}"
"Unless mentioned otherwise, the transliteration model was initialized with a subset of 254 pairs of NEs and their transliteration equivalence classes.","{'title': '4 Experimental Study', 'number': '4'}"
Negative examples here and during the rest of the training were pairs of randomly selected non-NE English and Russian words.,"{'title': '4 Experimental Study', 'number': '4'}"
"In each iteration, we used the current transliter= (powell, pauel).","{'title': '4 Experimental Study', 'number': '4'}"
"We build a feature vector from this example in the following manner: First, we split both words into all possible substrings of up to size two: ation model to find a list of 30 best transliteration equivalence classes for each NE.","{'title': '4 Experimental Study', 'number': '4'}"
We then computed time sequence similarity score between NE and each class from its list to find the one with the best matching time sequence.,"{'title': '4 Experimental Study', 'number': '4'}"
"If its similarity score surpassed a set threshold, it was added to the list of positive examples for the next round of training.","{'title': '4 Experimental Study', 'number': '4'}"
Positive examples were constructed by pairing each English NE with each of the transliterations from the best equivalence class that surpasses the threshold.,"{'title': '4 Experimental Study', 'number': '4'}"
We used the same number of positive and negative examples.,"{'title': '4 Experimental Study', 'number': '4'}"
"For evaluation, random 727 of the total of 978 NE pairs matched by the algorithm were selected and checked by a language expert.","{'title': '4 Experimental Study', 'number': '4'}"
Accuracy was computed as the percentage of those NEs correctly discovered by the algorithm.,"{'title': '4 Experimental Study', 'number': '4'}"
Figure 3 shows the proportion of correctly discovered NE transliteration equivalence classes throughout the run of the algorithm.,"{'title': '4 Experimental Study', 'number': '4'}"
The figure also shows the accuracy if transliterations are selected according to the current transliteration model (top scoring candidate) and sequence matching alone.,"{'title': '4 Experimental Study', 'number': '4'}"
"The transliteration model alone achieves an accuracy of about 47%, while the time sequence alone gets about 41%.","{'title': '4 Experimental Study', 'number': '4'}"
"The combined algorithm achieves about 66%, giving a significant improvement.","{'title': '4 Experimental Study', 'number': '4'}"
"In order to understand what happens to the transliteration model as the algorithm proceeds, let us consider the following example.","{'title': '4 Experimental Study', 'number': '4'}"
Figure 4 shows parts of transliteration lists for NE forsyth for two iterations of the algorithm.,"{'title': '4 Experimental Study', 'number': '4'}"
The weak transliteration model selects the correct transliteration (italicized) as the 24th best transliteration in the first iteration.,"{'title': '4 Experimental Study', 'number': '4'}"
Time sequence scoring function chooses it to be one of the training examples for the next round of training of the model.,"{'title': '4 Experimental Study', 'number': '4'}"
"By the eighth iteration, the model has improved to select it as a best transliteration.","{'title': '4 Experimental Study', 'number': '4'}"
Not all correct transliterations make it to the top of the candidates list (transliteration model by itself is never as accurate as the complete algorithm on Figure 3).,"{'title': '4 Experimental Study', 'number': '4'}"
"That is not required, however, as the model only needs to be good enough to place the correct transliteration anywhere in the candidate list.","{'title': '4 Experimental Study', 'number': '4'}"
"Not surprisingly, some of the top transliteration candidates start sounding like the NE itself, as training progresses.","{'title': '4 Experimental Study', 'number': '4'}"
"On Figure 4, candidates for forsyth on iteration 7 include fross and fossett.","{'title': '4 Experimental Study', 'number': '4'}"
We ran a series of experiments to see how the size of the initial training set affects the accuracy of the model as training progresses (Figure 5).,"{'title': '4 Experimental Study', 'number': '4'}"
"Although the performance of the early iterations is significantly affected by the size of the initial training example set, the algorithm quickly improves its performance.","{'title': '4 Experimental Study', 'number': '4'}"
"As we decrease the size from 254, to 127, to 85 examples, the accuracy of the first iteration drops by roughly 10% each time.","{'title': '4 Experimental Study', 'number': '4'}"
"However, starting at the 6th iteration, the three are with 3% of one another.","{'title': '4 Experimental Study', 'number': '4'}"
These numbers suggest that we only need a few initial positive examples to bootstrap the transliteration model.,"{'title': '4 Experimental Study', 'number': '4'}"
The intuition is the following: the few examples in the initial training set produce features corresponding to substring pairs characteristic for English-Russian transliterations.,"{'title': '4 Experimental Study', 'number': '4'}"
Model trained on these (few) examples chooses other transliterations containing these same substring pairs.,"{'title': '4 Experimental Study', 'number': '4'}"
"In turn, the chosen positive examples contain other characteristic substring pairs, which will be used by the model to select more positive examples on the next round, and so on.","{'title': '4 Experimental Study', 'number': '4'}"
We have proposed a novel algorithm for cross lingual NE discovery in a bilingual weakly temporally aligned corpus.,"{'title': '5 Conclusions', 'number': '5'}"
We have demonstrated that using two independent sources of information (transliteration and temporal similarity) together to guide NE extraction gives better performance than using either of them alone (see Figure 3).,"{'title': '5 Conclusions', 'number': '5'}"
"We developed a linear discriminative transliteration model, and presented a method to automatically generate features.","{'title': '5 Conclusions', 'number': '5'}"
"For time sequence matching, we used a scoring metric novel in this domain.","{'title': '5 Conclusions', 'number': '5'}"
"As supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (Salton and McGill, 1986)) when corpora are not well temporally aligned.","{'title': '5 Conclusions', 'number': '5'}"
"In keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration equivalence classes, which sometimes produced erroneous groupings (e.g. an equivalence class for NE lincoln in Russian included both lincoln and lincolnshire on Figure 6).","{'title': '5 Conclusions', 'number': '5'}"
"This approach is specific to Russian morphology, and would have to be altered for other languages.","{'title': '5 Conclusions', 'number': '5'}"
"For example, for Arabic, a small set of prefixes can be used to group most NE variants.","{'title': '5 Conclusions', 'number': '5'}"
We expect that language specific knowledge used to discover accurate equivalence classes would result in performance improvements.,"{'title': '5 Conclusions', 'number': '5'}"
Michael Collins and Yoram Singer.,"{'title': '5 Conclusions', 'number': '5'}"
1999.,"{'title': '5 Conclusions', 'number': '5'}"
Unsupervised models for named entity classification.,"{'title': '5 Conclusions', 'number': '5'}"
In Proc. of the Conference on Empirical Methods for Natural Language Processing (EMNLP).,"{'title': '5 Conclusions', 'number': '5'}"
"In this work, we only consider single word Named Entities.","{'title': '6 Future Work', 'number': '6'}"
A subject of future work is to extend the algorithm to the multi-word setting.,"{'title': '6 Future Work', 'number': '6'}"
Many of the multi-word NEs are translated as well as transliterated.,"{'title': '6 Future Work', 'number': '6'}"
"For example, Mount in Mount Rainier will probably be translated, and Rainier - transliterated.","{'title': '6 Future Work', 'number': '6'}"
"If a dictionary exists for the two languages, it can be consulted first, and, if a match is found, transliteration model can be bypassed.","{'title': '6 Future Work', 'number': '6'}"
The algorithm can be naturally extended to comparable corpora of more than two languages.,"{'title': '6 Future Work', 'number': '6'}"
Pairwise time sequence scoring and transliteration models should give better confidence in NE matches.,"{'title': '6 Future Work', 'number': '6'}"
It seems plausible to suppose that phonetic features (if available) would help learning our transliteration model.,"{'title': '6 Future Work', 'number': '6'}"
We would like to verify if this is indeed the case.,"{'title': '6 Future Work', 'number': '6'}"
The ultimate goal of this work is to automatically tag NEs so that they can be used for training of an NER system for a new language.,"{'title': '6 Future Work', 'number': '6'}"
"To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.","{'title': '6 Future Work', 'number': '6'}"
"We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.","{'title': '7 Acknowledgments', 'number': '7'}"
This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.,"{'title': '7 Acknowledgments', 'number': '7'}"
