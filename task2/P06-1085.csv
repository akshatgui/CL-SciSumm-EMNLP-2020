col1,col2
"Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.",{}
We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.,{}
"The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.",{}
We also show that previous probabilistic models rely crucially on suboptimal search procedures.,{}
"Word segmentation, i.e., discovering word boundaries in continuous text or speech, is of interest for both practical and theoretical reasons.","{'title': '1 Introduction', 'number': '1'}"
"It is the first step of processing orthographies without explicit word boundaries, such as Chinese.","{'title': '1 Introduction', 'number': '1'}"
It is also one of the key problems that human language learners must solve as they are learning language.,"{'title': '1 Introduction', 'number': '1'}"
"Many previous methods for unsupervised word segmentation are based on the observation that transitions between units (characters, phonemes, or syllables) within words are generally more predictable than transitions across word boundaries.","{'title': '1 Introduction', 'number': '1'}"
"Statistics that have been proposed for measuring these differences include “successor frequency” (Harris, 1954), “transitional probabilities” (Saffran et al., 1996), mutual information (Sun et al., ∗This work was partially supported by the following grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF IGERT-DGE-9870676, and the DARPA CALO project.","{'title': '1 Introduction', 'number': '1'}"
"1998), “accessor variety” (Feng et al., 2004), and boundary entropy (Cohen and Adams, 2001).","{'title': '1 Introduction', 'number': '1'}"
"While methods based on local statistics are quite successful, here we focus on approaches based on explicit probabilistic models.","{'title': '1 Introduction', 'number': '1'}"
Formulating an explicit probabilistic model permits us to cleanly separate assumptions about the input and properties of likely segmentations from details of algorithms used to find such solutions.,"{'title': '1 Introduction', 'number': '1'}"
"Specifically, this paper demonstrates the importance of contextual dependencies for word segmentation by comparing two probabilistic models that differ only in that the first assumes that the probability of a word is independent of its local context, while the second incorporates bigram dependencies between adjacent words.","{'title': '1 Introduction', 'number': '1'}"
"The algorithms we use to search for likely segmentations do differ, but so long as the segmentations they produce are close to optimal we can be confident that any differences in the segmentations reflect differences in the probabilistic models, i.e., in the kinds of dependencies between words.","{'title': '1 Introduction', 'number': '1'}"
We are not the first to propose explicit probabilistic models of word segmentation.,"{'title': '1 Introduction', 'number': '1'}"
Two successful word segmentation systems based on explicit probabilistic models are those of Brent (1999) and Venkataraman (2001).,"{'title': '1 Introduction', 'number': '1'}"
Brent’s ModelBased Dynamic Programming (MBDP) system assumes a unigram word distribution.,"{'title': '1 Introduction', 'number': '1'}"
"Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS).","{'title': '1 Introduction', 'number': '1'}"
"Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar.","{'title': '1 Introduction', 'number': '1'}"
"Moreover, the segmentation accuracy of the NGS unigram, bigram, and trigram models hardly differ, suggesting that contextual dependencies are irrelevant to word segmentation.","{'title': '1 Introduction', 'number': '1'}"
"However, the segmentations produced by both these methods depend crucially on properties of the search procedures they employ.","{'title': '1 Introduction', 'number': '1'}"
We show this by exhibiting for each model a segmentation that is less accurate but more probable under that model.,"{'title': '1 Introduction', 'number': '1'}"
"In this paper, we present an alternative framework for word segmentation based on the Dirichlet process, a distribution used in nonparametric Bayesian statistics.","{'title': '1 Introduction', 'number': '1'}"
This framework allows us to develop extensible models that are amenable to standard inference procedures.,"{'title': '1 Introduction', 'number': '1'}"
"We present two such models incorporating unigram and bigram word dependencies, respectively.","{'title': '1 Introduction', 'number': '1'}"
We use Gibbs sampling to sample from the posterior distribution of possible segmentations under these models.,"{'title': '1 Introduction', 'number': '1'}"
The plan of the paper is as follows.,"{'title': '1 Introduction', 'number': '1'}"
"In the next section, we describe MBDP and NGS in detail.","{'title': '1 Introduction', 'number': '1'}"
"In Section 3 we present the unigram version of our own model, the Gibbs sampling procedure we use for inference, and experimental results.","{'title': '1 Introduction', 'number': '1'}"
"Section 4 extends that model to incorporate bigram dependencies, and Section 5 concludes the paper.","{'title': '1 Introduction', 'number': '1'}"
"The NGS and MBDP systems are similar in some ways: both are designed to find utterance boundaries in a corpus of phonemically transcribed utterances, with known utterance boundaries.","{'title': '2 NGS and MBDP', 'number': '2'}"
"Both also use approximate online search procedures, choosing and fixing a segmentation for each utterance before moving onto the next.","{'title': '2 NGS and MBDP', 'number': '2'}"
"In this section, we focus on the very different probabilistic models underlying the two systems.","{'title': '2 NGS and MBDP', 'number': '2'}"
"We show that the optimal solution under the NGS model is the unsegmented corpus, and suggest that this problem stems from the fact that the model assumes a uniform prior over hypotheses.","{'title': '2 NGS and MBDP', 'number': '2'}"
"We then present the MBDP model, which uses a non-uniform prior but is difficult to extend beyond the unigram case.","{'title': '2 NGS and MBDP', 'number': '2'}"
NGS assumes that each utterance is generated independently via a standard n-gram model.,"{'title': '2 NGS and MBDP', 'number': '2'}"
"For simplicity, we will discuss the unigram version of the model here, although our argument is equally applicable to the bigram and trigram versions.","{'title': '2 NGS and MBDP', 'number': '2'}"
"The unigram model generates an utterance u according to the grammar in Figure 1, so where u consists of the words w1 ... wn and p$ is the probability of the utterance boundary marker $.","{'title': '2 NGS and MBDP', 'number': '2'}"
"This model can be used to find the highest probability segmentation hypothesis h given the data d by using Bayes’ rule: NGS assumes a uniform prior P(h) over hypotheses, so its goal is to find the solution that maximizes the likelihood P(djh).","{'title': '2 NGS and MBDP', 'number': '2'}"
"Using this model, NGS’s approximate search technique delivers competitive results.","{'title': '2 NGS and MBDP', 'number': '2'}"
"However, the true maximum likelihood solution is not competitive, since it contains no utterance-internal word boundaries.","{'title': '2 NGS and MBDP', 'number': '2'}"
"To see why not, consider the solution in which p$ = 1 and each utterance is a single ‘word’, with probability equal to the empirical probability of that utterance.","{'title': '2 NGS and MBDP', 'number': '2'}"
Any other solution will match the empirical distribution of the data less well.,"{'title': '2 NGS and MBDP', 'number': '2'}"
"In particular, a solution with additional word boundaries must have 1 − p$ > 0, which means it wastes probability mass modeling unseen data (which can now be generated by concatenating observed utterances together).","{'title': '2 NGS and MBDP', 'number': '2'}"
"Intuitively, the NGS model considers the unsegmented solution to be optimal because it ranks all hypotheses equally probable a priori.","{'title': '2 NGS and MBDP', 'number': '2'}"
"We know, however, that hypotheses that memorize the input data are unlikely to generalize to unseen data, and are therefore poor solutions.","{'title': '2 NGS and MBDP', 'number': '2'}"
"To prevent memorization, we could restrict our hypothesis space to models with fewer parameters than the number of utterances in the data.","{'title': '2 NGS and MBDP', 'number': '2'}"
"A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.","{'title': '2 NGS and MBDP', 'number': '2'}"
"This is in fact the route taken by Brent in his MBDP model, as we shall see in the following section.","{'title': '2 NGS and MBDP', 'number': '2'}"
"MBDP assumes a corpus of utterances is generated as a single probabilistic event with four steps: In a final deterministic step, the ordered tokens are concatenated to create an unsegmented corpus.","{'title': '2 NGS and MBDP', 'number': '2'}"
"This means that certain segmented corpora will produce the observed data with probability 1, and all others will produce it with probability 0.","{'title': '2 NGS and MBDP', 'number': '2'}"
"The posterior probability of a segmentation given the data is thus proportional to its prior probability under the generative model, and the best segmentation is that with the highest prior probability.","{'title': '2 NGS and MBDP', 'number': '2'}"
There are two important points to note about the MBDP model.,"{'title': '2 NGS and MBDP', 'number': '2'}"
"First, the distribution over L assigns higher probability to models with fewer lexical items.","{'title': '2 NGS and MBDP', 'number': '2'}"
"We have argued that this is necessary to avoid memorization, and indeed the unsegmented corpus is not the optimal solution under this model, as we will show in Section 3.","{'title': '2 NGS and MBDP', 'number': '2'}"
"Second, the factorization into four separate steps makes it theoretically possible to modify each step independently in order to investigate the effects of the various modeling assumptions.","{'title': '2 NGS and MBDP', 'number': '2'}"
"However, the mathematical statement of the model and the approximations necessary for the search procedure make it unclear how to modify the model in any interesting way.","{'title': '2 NGS and MBDP', 'number': '2'}"
"In particular, the fourth step uses a uniform distribution, which creates a unigram constraint that cannot easily be changed.","{'title': '2 NGS and MBDP', 'number': '2'}"
"Since our research aims to investigate the effects of different modeling assumptions on lexical acquisition, we develop in the following sections a far more flexible model that also incorporates a preference for sparse solutions.","{'title': '2 NGS and MBDP', 'number': '2'}"
"Our goal is a model of language that prefers sparse solutions, allows independent modification of components, and is amenable to standard search procedures.","{'title': '3 Unigram Model', 'number': '3'}"
"We achieve this goal by basing our model on the Dirichlet process (DP), a distribution used in nonparametric Bayesian statistics.","{'title': '3 Unigram Model', 'number': '3'}"
Our unigram model of word frequencies is defined as where the concentration parameter α0 and the base distribution P0 are parameters of the model.,"{'title': '3 Unigram Model', 'number': '3'}"
"Each word wi in the corpus is drawn from a distribution G, which consists of a set of possible words (the lexicon) and probabilities associated with those words.","{'title': '3 Unigram Model', 'number': '3'}"
"G is generated from a DP(α0, P0) distribution, with the items in the lexicon being sampled from P0 and their probabilities being determined by α0, which acts like the parameter of an infinite-dimensional symmetric Dirichlet distribution.","{'title': '3 Unigram Model', 'number': '3'}"
We provide some intuition for the roles of α0 and P0 below.,"{'title': '3 Unigram Model', 'number': '3'}"
"Although the DP model makes the distribution G explicit, we never deal with G directly.","{'title': '3 Unigram Model', 'number': '3'}"
"We take a Bayesian approach and integrate over all possible values of G. The conditional probability of choosing to generate a word from a particular lexical entry is then given by a simple stochastic process known as the Chinese restaurant process (CRP) (Aldous, 1985).","{'title': '3 Unigram Model', 'number': '3'}"
"Imagine a restaurant with an infinite number of tables, each with infinite seating capacity.","{'title': '3 Unigram Model', 'number': '3'}"
Customers enter the restaurant and seat themselves.,"{'title': '3 Unigram Model', 'number': '3'}"
Let zi be the table chosen by the ith customer.,"{'title': '3 Unigram Model', 'number': '3'}"
"Then where z−i = z1 ... zi−1, n(z−i) kis the number of customers already sitting at table k, and K(z−i) is the total number of occupied tables.","{'title': '3 Unigram Model', 'number': '3'}"
"In our model, the tables correspond to (possibly repeated) lexical entries, having labels generated from the distribution P0.","{'title': '3 Unigram Model', 'number': '3'}"
"The seating arrangement thus specifies a distribution over word tokens, with each customer representing one token.","{'title': '3 Unigram Model', 'number': '3'}"
"This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.","{'title': '3 Unigram Model', 'number': '3'}"
Our model can be viewed intuitively as a cache model: each word in the corpus is either retrieved from a cache or generated anew.,"{'title': '3 Unigram Model', 'number': '3'}"
Summing over all the tables labeled with the same word yields the probability distribution for the ith word given previously observed words w−i: where n(w−i) w is the number of instances of w observed in w−i.,"{'title': '3 Unigram Model', 'number': '3'}"
"The first term is the probability of generating w from the cache (i.e., sitting at an occupied table), and the second term is the probability of generating it anew (sitting at an unoccupied table).","{'title': '3 Unigram Model', 'number': '3'}"
"The actual table assignments z−i only become important later, in the bigram model.","{'title': '3 Unigram Model', 'number': '3'}"
There are several important points to note about this model.,"{'title': '3 Unigram Model', 'number': '3'}"
"First, the probability of generating a particular word from the cache increases as more instances of that word are observed.","{'title': '3 Unigram Model', 'number': '3'}"
"This richget-richer process creates a power-law distribution on word frequencies (Goldwater et al., 2006), the same sort of distribution found empirically in natural language.","{'title': '3 Unigram Model', 'number': '3'}"
"Second, the parameter α0 can be used to control how sparse the solutions found by the model are.","{'title': '3 Unigram Model', 'number': '3'}"
"This parameter determines the total probability of generating any novel word, a probability that decreases as more data is observed, but never disappears.","{'title': '3 Unigram Model', 'number': '3'}"
"Finally, the parameter P0 can be used to encode expectations about the nature of the lexicon, since it defines a probability distribution across different novel words.","{'title': '3 Unigram Model', 'number': '3'}"
"The fact that this distribution is defined separately from the distribution on word frequencies gives the model additional flexibility, since either distribution can be modified independently of the other.","{'title': '3 Unigram Model', 'number': '3'}"
"Since the goal of this paper is to investigate the role of context in word segmentation, we chose the simplest possible model for P0, i.e. a unigram phoneme distribution: where word w consists of the phonemes m1 ... mn, and p# is the probability of the word boundary #.","{'title': '3 Unigram Model', 'number': '3'}"
"For simplicity we used a uniform distribution over phonemes, and experimented with different fixed values of p#.1 A final detail of our model is the distribution on utterance lengths, which is geometric.","{'title': '3 Unigram Model', 'number': '3'}"
"That is, we assume a grammar similar to the one shown in Figure 1, with the addition of a symmetric Beta(τ2 ) prior over the probability of the U productions,2 and the substitution of the DP for the standard multinomial distribution over the W productions.","{'title': '3 Unigram Model', 'number': '3'}"
"Having defined our generative model, we are left with the problem of inference: we must determine the posterior distribution of hypotheses given our input corpus.","{'title': '3 Unigram Model', 'number': '3'}"
"To do so, we use Gibbs sampling, a standard Markov chain Monte Carlo method (Gilks et al., 1996).","{'title': '3 Unigram Model', 'number': '3'}"
Gibbs sampling is an iterative procedure in which variables are repeatedly sampled from their conditional posterior distribution given the current values of all other variables in the model.,"{'title': '3 Unigram Model', 'number': '3'}"
"The sampler defines a Markov chain whose stationary distribution is P(h|d), so after convergence samples are from this distribution.","{'title': '3 Unigram Model', 'number': '3'}"
"Our Gibbs sampler considers a single possible boundary point at a time, so each sample is from a set of two hypotheses, h1 and h2.","{'title': '3 Unigram Model', 'number': '3'}"
"These hypotheses contain all the same boundaries except at the one position under consideration, where h2 has a boundary and h1 does not.","{'title': '3 Unigram Model', 'number': '3'}"
The structures are shown in Figure 2.,"{'title': '3 Unigram Model', 'number': '3'}"
"In order to sample a hypothesis, we need only calculate the relative probabilities of h1 and h2.","{'title': '3 Unigram Model', 'number': '3'}"
"Since h1 and h2 are the same except for a few rules, this is straightforward.","{'title': '3 Unigram Model', 'number': '3'}"
"Let h− be all of the structure shared by the two hypotheses, including n− words, and let d be the observed data.","{'title': '3 Unigram Model', 'number': '3'}"
"Then where the second line follows from Equation 3 and the properties of the CRP (in particular, that it is exchangeable, with the probability of a seating configuration not depending on the order in which customers arrive (Aldous, 1985)).","{'title': '3 Unigram Model', 'number': '3'}"
"Also, where nr is the number of branching rules r = U —* W U in h−, and I(.) is an indicator function taking on the value 1 when its argument is true, and 0 otherwise.","{'title': '3 Unigram Model', 'number': '3'}"
"The nr term is derived by integrating over all possible values of pg, and noting that the total number of U productions in h− is n− + 1.","{'title': '3 Unigram Model', 'number': '3'}"
"Using these equations we can simply proceed through the data, sampling each potential boundary point in turn.","{'title': '3 Unigram Model', 'number': '3'}"
"Once the Gibbs sampler converges, these samples will be drawn from the posterior distribution P(h1d).","{'title': '3 Unigram Model', 'number': '3'}"
"In our experiments, we used the same corpus that NGS and MBDP were tested on.","{'title': '3 Unigram Model', 'number': '3'}"
"The corpus, supplied to us by Brent, consists of 9790 transcribed utterances (33399 words) of childdirected speech from the Bernstein-Ratner corpus (Bernstein-Ratner, 1987) in the CHILDES database (MacWhinney and Snow, 1985).","{'title': '3 Unigram Model', 'number': '3'}"
"The utterances have been converted to a phonemic representation using a phonemic dictionary, so that each occurrence of a word has the same phonemic transcription.","{'title': '3 Unigram Model', 'number': '3'}"
Utterance boundaries are given in the input to the system; other word boundaries are not.,"{'title': '3 Unigram Model', 'number': '3'}"
"Because our Gibbs sampler is slow to converge, we used annealing to speed inference.","{'title': '3 Unigram Model', 'number': '3'}"
We began with a temperature of -y = 10 and decreased -y in 10 increments to a final value of 1.,"{'title': '3 Unigram Model', 'number': '3'}"
A temperature of -y corresponds to raising the probabilities of h1 and h2 to the power of γ 1 prior to sampling.,"{'title': '3 Unigram Model', 'number': '3'}"
"We ran our Gibbs sampler for 20,000 iterations through the corpus (with -y = 1 for the final 2000) and evaluated our results on a single sample at that point.","{'title': '3 Unigram Model', 'number': '3'}"
"We calculated precision (P), recall (R), and F-score (F) on the word tokens in the corpus, where both boundaries of a word must be correct to count the word as correct.","{'title': '3 Unigram Model', 'number': '3'}"
"The induced lexicon was also scored for accuracy using these metrics (LP, LR, LF).","{'title': '3 Unigram Model', 'number': '3'}"
"Recall that our DP model has three parameters: T, p#, and α0.","{'title': '3 Unigram Model', 'number': '3'}"
"Given the large number of known utterance boundaries, we expect the value of T to have little effect on our results, so we simply fixed T = 2 for all experiments.","{'title': '3 Unigram Model', 'number': '3'}"
"Figure 3 shows the effects of varying of p# and α0.3 Lower values of p# cause longer words, which tends to improve recall (and thus F-score) in the lexicon, but decrease token accuracy.","{'title': '3 Unigram Model', 'number': '3'}"
"Higher values of α0 allow more novel words, which also improves lexicon recall, as a function of p#, with α0 = 20 and (b) as a function of α0, with p# = .5. but begins to degrade precision after a point.","{'title': '3 Unigram Model', 'number': '3'}"
"Due to the negative correlation between token accuracy and lexicon accuracy, there is no single best value for either p# or α0; further discussion refers to the solution for p# = .5, α0 = 20 (though others are qualitatively similar).","{'title': '3 Unigram Model', 'number': '3'}"
"In Table 1(a), we compare the results of our system to those of MBDP and NGS.4 Although our system has higher lexicon accuracy than the others, its token accuracy is much worse.","{'title': '3 Unigram Model', 'number': '3'}"
This result occurs because our system often mis-analyzes frequently occurring words.,"{'title': '3 Unigram Model', 'number': '3'}"
"In particular, many of these words occur in common collocations such as what’s that and do you, which the system interprets as a single words.","{'title': '3 Unigram Model', 'number': '3'}"
It turns out that a full 31% of the proposed lexicon and nearly 30% of tokens consist of these kinds of errors.,"{'title': '3 Unigram Model', 'number': '3'}"
"Upon reflection, it is not surprising that a unigram language model would segment words in this way.","{'title': '3 Unigram Model', 'number': '3'}"
"Collocations violate the unigram assumption in the model, since they exhibit strong word-toword dependencies.","{'title': '3 Unigram Model', 'number': '3'}"
The only way the model can capture these dependencies is by assuming that these collocations are in fact words themselves.,"{'title': '3 Unigram Model', 'number': '3'}"
Why don’t the MBDP and NGS unigram models exhibit these problems?,"{'title': '3 Unigram Model', 'number': '3'}"
We have already shown that NGS’s results are due to its search procedure rather than its model.,"{'title': '3 Unigram Model', 'number': '3'}"
The same turns out to be true for MBDP.,"{'title': '3 Unigram Model', 'number': '3'}"
"Table 2 shows the probabilider each model of the true solution, the solution with no utterance-internal boundaries, and the solutions found by each algorithm.","{'title': '3 Unigram Model', 'number': '3'}"
Best solutions under each model are bold. ties under each model of various segmentations of the corpus.,"{'title': '3 Unigram Model', 'number': '3'}"
"From these figures, we can see that the MBDP model assigns higher probability to the solution found by our Gibbs sampler than to the solution found by Brent’s own incremental search algorithm.","{'title': '3 Unigram Model', 'number': '3'}"
"In other words, Brent’s model does prefer the lower-accuracy collocation solution, but his search algorithm instead finds a higher-accuracy but lower-probability solution.","{'title': '3 Unigram Model', 'number': '3'}"
We performed two experiments suggesting that our own inference procedure does not suffer from similar problems.,"{'title': '3 Unigram Model', 'number': '3'}"
"First, we initialized our Gibbs sampler in three different ways: with no utteranceinternal boundaries, with a boundary after every character, and with random boundaries.","{'title': '3 Unigram Model', 'number': '3'}"
Our results were virtually the same regardless of initialization.,"{'title': '3 Unigram Model', 'number': '3'}"
"Second, we created an artificial corpus by randomly permuting the words in the true corpus, leaving the utterance lengths the same.","{'title': '3 Unigram Model', 'number': '3'}"
"The artificial corpus adheres to the unigram assumption of our model, so if our inference procedure works correctly, we should be able to correctly identify the words in the permuted corpus.","{'title': '3 Unigram Model', 'number': '3'}"
"This is exactly what we found, as shown in Table 1(b).","{'title': '3 Unigram Model', 'number': '3'}"
"While all three models perform better on the artificial corpus, the improvements of the DP model are by far the most striking.","{'title': '3 Unigram Model', 'number': '3'}"
The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.,"{'title': '4 Bigram Model', 'number': '4'}"
"To test this hypothesis, we extended our model to incorporate bigram dependencies using a hierarchical Dirichlet process (HDP) (Teh et al., 2005).","{'title': '4 Bigram Model', 'number': '4'}"
"Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).","{'title': '4 Bigram Model', 'number': '4'}"
"The HDP is appropriate for situations in which there are multiple distributions over similar sets of outcomes, and the distributions are believed to be similar.","{'title': '4 Bigram Model', 'number': '4'}"
"In our case, we define a bigram model by assuming each word has a different distribution over the words that follow it, but all these distributions are linked.","{'title': '4 Bigram Model', 'number': '4'}"
"The definition of our bigram language model as an HDP is That is, P(wi|wi−1 = w) is distributed according to Hw, a DP specific to word w. Hw is linked to the DPs for all other words by the fact that they share a common base distribution G, which is generated from another DP.5 As in the unigram model, we never deal with Hw or G directly.","{'title': '4 Bigram Model', 'number': '4'}"
"By integrating over them, we get a distribution over bigram frequencies that can be understood in terms of the CRP.","{'title': '4 Bigram Model', 'number': '4'}"
"Now, each word type w is associated with its own restaurant, which represents the distribution over words that follow w. Different restaurants are not completely independent, however: the labels on the tables in the restaurants are all chosen from a common base distribution, which is another CRP.","{'title': '4 Bigram Model', 'number': '4'}"
"To understand the HDP model in terms of a grammar, we consider $ as a special word type, so that wi ranges over E∗ U J$J.","{'title': '4 Bigram Model', 'number': '4'}"
"After observing w−i, the HDP grammar is as shown in Figure 4, where h−i = (w−i, z−i); t$, tE∗, and twi are the total number of tables (across all words) labeled with $, non-$, and wi, respectively; t = t$ + tE∗ is the total number of tables; and n(wi−1,wi) is the number of occurrences of the bigram (wi−1, wi).","{'title': '4 Bigram Model', 'number': '4'}"
We have suppressed the superscript (w−i) notation in all cases.,"{'title': '4 Bigram Model', 'number': '4'}"
"The base distribution shared by all bigrams is given by P1, which can be viewed as a unigram backoff where the unigram probabilities are learned from the bigram table labels.","{'title': '4 Bigram Model', 'number': '4'}"
We can perform inference on this HDP bigram model using a Gibbs sampler similar to our unigram sampler.,"{'title': '4 Bigram Model', 'number': '4'}"
Details appear in the Appendix.,"{'title': '4 Bigram Model', 'number': '4'}"
We used the same basic setup for our experiments with the HDP model as we used for the DP model.,"{'title': '4 Bigram Model', 'number': '4'}"
"We experimented with different values of α0 and α1, keeping p# = .5 throughout.","{'title': '4 Bigram Model', 'number': '4'}"
Some results of these experiments are plotted in Figure 5.,"{'title': '4 Bigram Model', 'number': '4'}"
"With appropriate parameter settings, both lexicon and token accuracy are higher than in the unigram model (dramatically so, for tokens), and there is no longer a negative correlation between the two.","{'title': '4 Bigram Model', 'number': '4'}"
"Only a few collocations remain in the lexicon, and most lexicon errors are on low-frequency words.","{'title': '4 Bigram Model', 'number': '4'}"
"The best values of α0 are much larger than in the unigram model, presumably because all unique word types must be generated via P0, but in the bigram model there is an additional level of discounting (the unigram process) before reaching P0.","{'title': '4 Bigram Model', 'number': '4'}"
Smaller values of α0 lead to fewer word types with fewer characters on average.,"{'title': '4 Bigram Model', 'number': '4'}"
"Table 3 compares the optimal results of the HDP model to the only previous model incorporating bigram dependencies, NGS.","{'title': '4 Bigram Model', 'number': '4'}"
"Due to search, the performance of the bigram NGS model is not much different from that of the unigram model.","{'title': '4 Bigram Model', 'number': '4'}"
"In Figure 5: Word (F) and lexicon (LF) F-score (a) as a function of α0, with α1 = 10 and (b) as a function of α1, with α0 = 1000. in bold.","{'title': '4 Bigram Model', 'number': '4'}"
"HDP results are with p# = .5, α0 = 1000, and α1 = 10. contrast, our HDP model performs far better than our DP model, leading to the highest published accuracy for this corpus on both tokens and lexical items.","{'title': '4 Bigram Model', 'number': '4'}"
"Overall, these results strongly support our hypothesis that modeling bigram dependencies is important for accurate word segmentation.","{'title': '4 Bigram Model', 'number': '4'}"
"In this paper, we have introduced a new modelbased approach to word segmentation that draws on techniques from Bayesian statistics, and we have developed models incorporating unigram and bigram dependencies.","{'title': '5 Conclusion', 'number': '5'}"
The use of the Dirichlet process as the basis of our approach yields sparse solutions and allows us the flexibility to modify individual components of the models.,"{'title': '5 Conclusion', 'number': '5'}"
"We have presented a method of inference using Gibbs sampling, which is guaranteed to converge to the posterior distribution over possible segmentations of a corpus.","{'title': '5 Conclusion', 'number': '5'}"
Our approach to word segmentation allows us to investigate questions that could not be addressed satisfactorily in earlier work.,"{'title': '5 Conclusion', 'number': '5'}"
"We have shown that the search algorithms used with previous models of word segmentation do not achieve their objectives, which has led to misleading results.","{'title': '5 Conclusion', 'number': '5'}"
"In particular, previous work suggested that the use of word-to-word dependencies has little effect on word segmentation.","{'title': '5 Conclusion', 'number': '5'}"
Our experiments indicate instead that bigram dependencies can be crucial for avoiding under-segmentation of frequent collocations.,"{'title': '5 Conclusion', 'number': '5'}"
"Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.","{'title': '5 Conclusion', 'number': '5'}"
"J. Saffran, E. Newport, and R. Aslin.","{'title': 'References', 'number': '6'}"
1996.,"{'title': 'References', 'number': '6'}"
Word segmentation: The role of distributional cues.,"{'title': 'References', 'number': '6'}"
"Journal of Memory and Language, 35:606–621.","{'title': 'References', 'number': '6'}"
"M. Sun, D. Shen, and B. Tsou.","{'title': 'References', 'number': '6'}"
1998.,"{'title': 'References', 'number': '6'}"
Chinese word segmentation without using lexicon and hand-crafted training data.,"{'title': 'References', 'number': '6'}"
In Proceedings of COLING-ACL.,"{'title': 'References', 'number': '6'}"
"Y. Teh, M. Jordan, M. Beal, and D. Blei.","{'title': 'References', 'number': '6'}"
2005.,"{'title': 'References', 'number': '6'}"
Hierarchical Dirichlet processes.,"{'title': 'References', 'number': '6'}"
In Advances in Neural Information Processing Systems 17.,"{'title': 'References', 'number': '6'}"
"MIT Press, Cambridge, MA.","{'title': 'References', 'number': '6'}"
Y. Teh.,"{'title': 'References', 'number': '6'}"
2006.,"{'title': 'References', 'number': '6'}"
A Bayesian interpretation of interpolated kneser-ney.,"{'title': 'References', 'number': '6'}"
"Technical Report TRA2/06, National University of Singapore, School of Computing.","{'title': 'References', 'number': '6'}"
A. Venkataraman.,"{'title': 'References', 'number': '6'}"
2001.,"{'title': 'References', 'number': '6'}"
A statistical model for word discovery in transcribed speech.,"{'title': 'References', 'number': '6'}"
"Computational Linguistics, 27(3):351–372.","{'title': 'References', 'number': '6'}"
