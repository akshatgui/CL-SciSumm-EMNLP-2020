col1,col2
"Sentiment Classification seeks to identify a piece of text according to its author’s general feeling toward their subject, be it positive or negative.",{}
"Traditional machine learning techniques have been applied to this problem with reasonable success, but they have been shown to work well only when there is a good match between the training and test data with respect to topic.",{}
"This paper demonstrates that match with respect to domain and time is also important, and presents preliminary experiments with training data labeled with emoticons, which has the potential of being independent of domain, topic and time.",{}
Recent years have seen an increasing amount of research effort expended in the area of understanding sentiment in textual resources.,"{'title': '1 Introduction', 'number': '1'}"
A sub-topic of this research is that of Sentiment Classification.,"{'title': '1 Introduction', 'number': '1'}"
"That is, given a problem text, can computational methods determine if the text is generally positive or generally negative?","{'title': '1 Introduction', 'number': '1'}"
"Several diverse applications exist for this potential technology, ranging from the automatic filtering of abusive messages (Spertus, 1997) to an in-depth analysis of market trends and consumer opinions (Dave et al., 2003).","{'title': '1 Introduction', 'number': '1'}"
"This is a complex and challenging task for a computer to achieve — consider the difficulties involved in instructing a computer to recognise sarcasm, for example.","{'title': '1 Introduction', 'number': '1'}"
Previous work has shown that traditional text classification approaches can be quite effective when applied to the sentiment analysis problem.,"{'title': '1 Introduction', 'number': '1'}"
"Models such as Naive Bayes (NB), Maximum Entropy (ME) and Support Vector Machines (SVM) can determine the sentiment of texts.","{'title': '1 Introduction', 'number': '1'}"
Pang et al. (2002) used a bagof-features framework (based on unigrams and bigrams) to train these models from a corpus of movie reviews labelled as positive or negative.,"{'title': '1 Introduction', 'number': '1'}"
"The best accuracy achieved was 82.9%, using an SVM trained on unigram features.","{'title': '1 Introduction', 'number': '1'}"
"A later study (Pang and Lee, 2004) found that performance increased to 87.2% when considering only those portions of the text deemed to be subjective.","{'title': '1 Introduction', 'number': '1'}"
"However, Engstr¨om (2004) showed that the bagof-features approach is topic-dependent.","{'title': '1 Introduction', 'number': '1'}"
A classifier trained on movie reviews is unlikely to perform as well on (for example) reviews of automobiles.,"{'title': '1 Introduction', 'number': '1'}"
"Turney (2002) noted that the unigram unpredictable might have a positive sentiment in a movie review (e.g. unpredictable plot), but could be negative in the review of an automobile (e.g. unpredictable steering).","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we demonstrate how the models are also domain-dependent — how a classifier trained on product reviews is not effective when evaluating the sentiment of newswire articles, for example.","{'title': '1 Introduction', 'number': '1'}"
"Furthermore, we show how the models are temporally-dependent — how classifiers are biased by the trends of sentiment apparent during the time-period represented by the training data.","{'title': '1 Introduction', 'number': '1'}"
We propose a novel source of training data based on the language used in conjunction with emoticons in Usenet newsgroups.,"{'title': '1 Introduction', 'number': '1'}"
"Training a classifier using this data provides a breadth of features that, while it does not perform to the state-of-the-art, could function independent of domain, topic and time.","{'title': '1 Introduction', 'number': '1'}"
"In this section, we describe experiments we have carried out to determine the influence of domain, topic and time on machine learning based sentiment classification.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
The experiments use our own implementation of a Naive Bayes classifier and Joachim’s (1999) 5VMlight implementation of a Support Vector Machine classifier.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"The models were trained using unigram features, accounting for the presence of feature types in a document, rather than the frequency, as Pang et al. (2002) found that this is the most effective strategy for sentiment classification.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"When training and testing on the same set, the mean accuracy is determined using three-fold crossvalidation.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"In each case, we use a paired-sample t-test over the set of test documents to determine whether the results produced by one classifier are statistically significantly better than those from another, at a confidence interval of at least 95%.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
Engstr¨om (2004) demonstrated how machinelearning techniques for sentiment classification can be topic dependent.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"However, that study focused on a three-way classification (positive, negative and neutral).","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"In this paper, for uniformity across different data sets, we focus on only positive and negative sentiment.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
This experiment also provides an opportunity to evaluate the Naive Bayes classifier as the previous work used SVMs.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"We use subsets of a Newswire dataset (kindly provided by Roy Lipski of Infonic Ltd.) that relate to the topics of Finance (FIN), Mergers and Aquisitions (M&A) and a mixture of both topics (MIX).","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"Each subset contains further subsets of articles of positive and negative sentiment (selected by independent trained annotators), each containing 100 stories.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
We trained a model on a dataset relating to one topic and tested that model using the other topics.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
Figure 1 shows the results of this experiment.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
The tendency seems to be that performance in a given topic is best if the training data is from the same topic.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"For example, the Finance-trained SVM classifier achieved an accuracy of 78.8% against articles from Finance, but only 72.7% when predicting the sentiment of articles from M&A.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"However, statistical testing showed that the results are not significantly different when training on one topic and testing on another.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"It is interesting to note, though, that providing a dataset of mixed topics (the sub-corpus MIX) does not necessarily reduce topic dependency.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"Indeed, the performance of the classifiers suffers a great deal when training on mixed data (confidence interval 95%).","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
We conducted an experiment to compare the accuracy when training a classifier on one domain (newswire articles or movie reviews from the Polarity 1.0 dataset used by Pang et al. (2002)) and testing on the other domain.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"In Figure 2, we see a clear indication that models trained on one domain do not perform as well on another domain.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
All differences are significant at a confidence interval of 99.9%.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"To investigate the effect of time on sentiment classification, we constructed a new set of movie reviews, following the same approach used by Pang et al. (2002) when they created the Polarity 1.0 dataset.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
The data source was the Internet Movie Review Database archive1 of movie reviews.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
The reviews were categorised as positive or negative using automatically extracted ratings.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
A review was ignored if it was not written in 2003 or 2004 (ensuring that the review was written after any in the Polarity 1.0 dataset).,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"This procedure yielded a corpus of 716 negative and 2,669 positive reviews.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"To create the Polarity 20042 dataset we randomly selected 700 negative reviews and 700 positive reviews, matching the size and distribution of the Polarity 1.0 dataset.","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
The next experiment evaluated the performance of the models first against movie reviews from the same time-period as the training set and then against reviews from the other time-period.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
Figure 3 shows the resulting accuracies.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"These results show that while the models perform well on reviews from the same time-period as the training set, they are not so effective on reviews from other time-periods (confidence interval 95%).","{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
It is also apparent that the Polarity 2004 dataset performs worse than the Polarity 1.0 dataset (confidence interval 99.9%).,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
A possible reason for this is that Polarity 2004 data is from a much smaller time-period than that represented by Polarity 1.0.,"{'title': '2 Dependencies in Sentiment Classification', 'number': '2'}"
"One way of overcoming the domain, topic and time problems we have demonstrated above would be to find a source of much larger and diverse amounts of general text, annotated for sentiment.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
Users of electronic methods of communication have developed visual cues that are associated with emotional states in an attempt to state the emotion that their text represents.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"These have become known as smileys or emoticons and are glyphs constructed using the characters available on a standard keyboard, representing a facial expression of emotion — see Figure 4 for some examples.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"When the author of an electronic communication uses an emoticon, they are effectively marking up their own text with an emotional state.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
This marked-up text can be used to train a sentiment classifier if we assume that a smile indicates generally positive text and a frown indicates generally negative text.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
We collected a corpus of text marked-up with emoticons by downloading Usenet newsgroups and saving an article if it contained an emoticon listed in Figure 4.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"This process resulted in 766,730 articles being stored, from 10,682,455 messages in 49,759 newsgroups inspected.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"Figure 4 also lists the percentage of documents containing each emoticon type, as observed in the Usenet newsgroups.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
We automatically extracted the paragraph(s) containing the emoticon of interest (a smile or a frown) from each message and removed any superfluous formatting characters (such as those used to indicate article quotations in message threads).,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"In order to prevent quoted text from being considered more than once, any paragraph that began with exactly the same thirty characters as a previously observed paragraph was disregarded.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"Finally, we used the classifier developed by Cavnar and Trenkle (1994) to filter out any paragraphs of non-English text.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"This process yielded a corpus of 13,000 article extracts containing frown emoticons.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"As investigating skew between positive and negative distributions is outside the scope of this work, we also extracted 13,000 article extracts containing smile emoticons.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"The dataset is referred to throughout this paper as Emoticons and contains 748,685 words.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
This section describes how the Emoticons corpus3 was optimised for use as sentiment classification training data.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"2,000 articles containing smiles and 2,000 articles containing frowns were held-out as optimising test data.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"We took increasing amounts of articles from the remaining dataset (from 2,000 to 22,000 in increments of 1,000, an equal number being taken from the positive and negative sets) as optimising training data.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"For each set of training data we extracted a context of an increasing number of tokens (from 10 to 1,000 in increments of 10) both before and in a window4 around the smile or frown emoticon.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
The models were trained using this extracted context and tested on the held-out dataset.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"The optimisation process revealed that the bestperforming settings for the Naive Bayes classifier was a window context of 130 tokens taken from the largest training set of 22,000 articles.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"Similarly, the best performance for the SVM classifier was found using a window context of 150 tokens taken from 20,000 articles.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
The classifiers’ performance in predicting the smiles and frowns of article extracts was verified using these optimised parameters and ten-fold crossvalidation.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"The mean accuracy of the Naive Bayes classifier was 61.5%, while the SVM classifier was 70.1%.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
Using these same classifiers to predict the sentiment of movie reviews in Polarity 1.0 resulted in accuracies of 59.1% (Naive Bayes) and 52.1% (SVM).,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"We repeated the optimisation process using a held-out set of 100 positive and 100 negative reviews from the Polarity 1.0 dataset, as it is possible that this test needs different parameter settings.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"This revealed an optimum context of a window of 50 tokens taken from a training set of 21,000 articles for the Naive Bayes classifier.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"Interestingly, the optimum context for the SVM classifier appeared to be a window of only 20 tokens taken from a mere 2,000 training examples.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"This is clearly an anomaly, as these parameters resulted in an accuracy of 48.9% when testing against the reserved reviews of Polarity 1.0.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"We attribute this to the presence of noise, both in the training set and in the held-out set, and discuss this below (Section 4.2).","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"The second-best parameters according to the optimisation process were a context of 510 tokens taken before an emoticon, from a training set of 20,000 examples.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
We used these optimised parameters to evaluate the sentiments of texts in the test sets used to evaluate dependency in Section 2.,"{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"Figures 5, 6 and 7 show the final, optimised results across topics, domains and time-periods respectively.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"These tables report the average accuracies over three folds, with the standard deviation as a measure of error.","{'title': '3 Sentiment Classification using Emoticons', 'number': '3'}"
"The emoticon-trained classifiers perform well (up to 70% accuracy) when predicting the sentiment of article extracts from the Emoticons dataset, which is encouraging when one considers the high level of noise that is likely to be present in the dataset.","{'title': '4 Discussion', 'number': '4'}"
"However, they perform only a little better than one would expect by chance when classifying movie reviews, and are not effective in predicting the sentiment of newswire articles.","{'title': '4 Discussion', 'number': '4'}"
"This is perhaps due to the nature of the datasets — one would expect language to be informal in movie reviews, and even more so in Usenet articles.","{'title': '4 Discussion', 'number': '4'}"
"In contrast, language in newswire articles is far more formal.","{'title': '4 Discussion', 'number': '4'}"
"We might therefore infer a further type of dependence in sentiment classification, that of language-style dependency.","{'title': '4 Discussion', 'number': '4'}"
"Also, note that neither machine-learning model consistently out-performs the other.","{'title': '4 Discussion', 'number': '4'}"
"We speculate that this, and the generally mediocre performance of the classifiers, is due (at least) to two factors; poor coverage of the features found in the test domains and a high level of noise found in Usenet article extracts.","{'title': '4 Discussion', 'number': '4'}"
We investigate these factors below.,"{'title': '4 Discussion', 'number': '4'}"
Figure 8 shows the coverage of the Emoticon-trained classifiers on the various test sets.,"{'title': '4 Discussion', 'number': '4'}"
"In these experiments, we are interested in the coverage in terms of unique token types rather than the frequency of features, as this more closely reflects the training of the models (see Section 2.1).","{'title': '4 Discussion', 'number': '4'}"
The mean coverage of the Polarity 1.0 dataset during three-fold crossvalidation is also listed as an example of the coverage one would expect from a better-performing sentiment classifier.,"{'title': '4 Discussion', 'number': '4'}"
The Emoticon-trained classifier has much worse coverage in the test sets.,"{'title': '4 Discussion', 'number': '4'}"
We analysed the change in coverage of the Emoticon-trained classifiers on the Polarity 1.0 dataset.,"{'title': '4 Discussion', 'number': '4'}"
"We found that the coverage continued to improve as more training data was provided; the coverage of unique token types was improving by about 0.6% per 1,000 training examples when the Emoticons dataset was exhausted.","{'title': '4 Discussion', 'number': '4'}"
It appears possible that more training data will improve the performance of the Emoticon-trained classifiers by increasing the coverage.,"{'title': '4 Discussion', 'number': '4'}"
"Potential sources for this include online bulletin boards, chat forums, and further newsgroup data from Usenet and Google Groups5.","{'title': '4 Discussion', 'number': '4'}"
Future work will utilise these sources to collect more examples of emoticon use and analyse any improvement in coverage and accuracy.,"{'title': '4 Discussion', 'number': '4'}"
The article extracts collected in the Emoticons dataset may be noisy with respect to sentiment.,"{'title': '4 Discussion', 'number': '4'}"
The SVM classifier seems particularly affected by this noise.,"{'title': '4 Discussion', 'number': '4'}"
Figure 9 depicts the change in performance of the SVM classifier when varying the training set size and size of context extracted.,"{'title': '4 Discussion', 'number': '4'}"
"There are significant spikes apparent for the training sizes of 2,000, 3,000 and 6,000 article extracts (as noted in Section 3.2), where the accuracy suddenly increases for the training set size, then quickly decreases for the next set size.","{'title': '4 Discussion', 'number': '4'}"
"This implies that the classifier is discovering features that are useful in classifying the heldout set, but the addition of more, noisy, texts soon makes the information redundant.","{'title': '4 Discussion', 'number': '4'}"
"Some examples of noise taken from the Emoticons dataset are: mixed sentiment, e.g.","{'title': '4 Discussion', 'number': '4'}"
“Sorry about venting my frustration here but I just lost it.,"{'title': '4 Discussion', 'number': '4'}"
":-( Happy thanks giving everybody :-)”, sarcasm, e.g.","{'title': '4 Discussion', 'number': '4'}"
"“Thank you so much, that’s really encouraging :-(”, and spelling mistakes, e.g.","{'title': '4 Discussion', 'number': '4'}"
“The movies where for me a major desapointment :-(”.,"{'title': '4 Discussion', 'number': '4'}"
In future work we will investigate ways to remove noisy data from the Emoticons dataset.,"{'title': '4 Discussion', 'number': '4'}"
"This paper has demonstrated that dependency in sentiment classification can take the form of domain, topic, temporal and language style.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
One might suppose that dependency is occurring because classifiers are learning the semantic sentiment of texts rather than the general sentiment of language used.,"{'title': '5 Conclusions and Future Work', 'number': '5'}"
"That is, the classifiers could be learning authors’ sentiment towards named entities (e.g. actors, directors, companies, etc.).","{'title': '5 Conclusions and Future Work', 'number': '5'}"
"However, this does not seem to be the case.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
"In a small experiment, we part-ofspeech tagged the Polarity 2004 dataset and automatically replaced proper nouns with placeholders.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
Retraining on this modified text did not significantly affect performance.,"{'title': '5 Conclusions and Future Work', 'number': '5'}"
But it may be that something more subtle is happening.,"{'title': '5 Conclusions and Future Work', 'number': '5'}"
"Possibly, the classifiers are learning the words associated with the semantic sentiment of entities.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
"For example, suppose that there has been a well-received movie about mountaineering.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
"During this movie, there is a particularly stirring scene involving an ice-axe and most of the reviewers mention this scene.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
"During training, the word ‘ice-axe’ would become associated with a positive sentiment, whereas one would suppose that this word does not in general express any kind of sentiment.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
In future work we will perform further tests to determine the nature of dependency in machine learning techniques for sentiment classification.,"{'title': '5 Conclusions and Future Work', 'number': '5'}"
One way of evaluating the ‘ice-axe’ effect could be to build a ‘pseudo-ontology’ of the movie reviews — a map of the sentiment-bearing relations that would enable the analysis of the dependencies created by the training process.,"{'title': '5 Conclusions and Future Work', 'number': '5'}"
"Other extensions of this work are to collect more text marked-up with emoticons, and to experiment with techniques to automatically remove noisy examples from the training data.","{'title': '5 Conclusions and Future Work', 'number': '5'}"
This research was funded by a UK EPSRC studentship.,"{'title': 'Acknowledgements', 'number': '6'}"
"I am very grateful to Thorsten Joachims, Roy Lipski, Bo Pang and John Trenkle for kindly making their data or software available, and to the anonymous reviewers for their constructive comments.","{'title': 'Acknowledgements', 'number': '6'}"
Thanks also to Nick Jacobi for his discussion of the ‘ice-axe’ effect.,"{'title': 'Acknowledgements', 'number': '6'}"
"Special thanks to my supervisor, John Carroll, for his continued advice and encouragement.","{'title': 'Acknowledgements', 'number': '6'}"
