col1,col2
Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks.,Introduction
We present a semi-automatic method for extracting fine-grained semantic relations between verbs.,Introduction
"We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.",Introduction
"On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.",Introduction
Analysis of types shows that on the relation achieved 75% accuracy.,Introduction
We provide the called for download at,Introduction
"Many NLP tasks, such as question answering, summarization, and machine translation could benefit from broad-coverage semantic resources such as WordNet (Miller 1990) and EVCA (English Verb Classes and Alternations) (Levin 1993).",Experiment/Discussion
These extremely useful resources have very high precision entries but have important limitations when used in real-world NLP tasks due to their limited coverage and prescriptive nature (i.e. they do not include semantic relations that are plausible but not guaranteed).,Experiment/Discussion
"For example, it may be valuable to know that if someone has bought an item, they may sell it at a later time.",Experiment/Discussion
WordNet does not include the relation &quot;X buys Y&quot; happens-before &quot;X sells Y&quot; since it is possible to sell something without having bought it (e.g. having manufactured or stolen it).,Experiment/Discussion
Verbs are the primary vehicle for describing events and expressing relations between entities.,Experiment/Discussion
"Hence, verb semantics could help in many natural language processing (NLP) tasks that deal with events or relations between entities.",Experiment/Discussion
"For tasks which require canonicalization of natural language statements or derivation of plausible inferences from such statements, a particularly valuable resource is one which (i) relates verbs to one another and (ii) provides broad coverage of the verbs in the target language.",Experiment/Discussion
"In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns.",Experiment/Discussion
"The verb relations we discover are similarity, strength, antonymy, enablement, and temporal relations.",Experiment/Discussion
"Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN.",Experiment/Discussion
Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002).,Experiment/Discussion
We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern.,Experiment/Discussion
"In this section, we describe application domains that can benefit from a resource of verb semantics.",Experiment/Discussion
We then introduce some existing resources and describe previous attempts at mining semantics from text.,Experiment/Discussion
Question answering is often approached by canonicalizing the question text and the answer text into logical forms.,Experiment/Discussion
"This approach is taken, inter alia, by a top-performing system (Moldovan et al. 2002).",Experiment/Discussion
"In discussing future work on the system's logical form matching component, Rus (2002 p. 143) points to incorporating entailment and causation verb relations to improve the matcher's performance.",Experiment/Discussion
"In other work, Webber et al. (2002) have argued that successful question answering depends on lexical reasoning, and that lexical reasoning in turn requires fine-grained verb semantics in addition to troponymy (is-a relations between verbs) and antonymy.",Experiment/Discussion
"In multi-document summarization, knowing verb similarities is useful for sentence compression and for determining sentences that have the same meaning (Lin 1997).",Experiment/Discussion
Knowing that a particular action happens before another or is enabled by another is also useful to determine the order of the events (Barzilay et al. 2002).,Experiment/Discussion
"For example, to order summary sentences properly, it may be useful to know that selling something can be preceded by either buying, manufacturing, or stealing it.",Experiment/Discussion
"Furthermore, knowing that a particular verb has a meaning stronger than another (e.g. rape vs. abuse and renovate vs. upgrade) can help a system pick the most general sentence.",Experiment/Discussion
"In lexical selection of verbs in machine translation and in work on document classification, practitioners have argued for approaches that depend on wide-coverage resources indicating verb similarity and membership of a verb in a certain class.",Experiment/Discussion
"In work on translating verbs with many counterparts in the target language, Palmer and Wu (1995) discuss inherent limitations of approaches which do not examine a verb's class membership, and put forth an approach based on verb similarity.",Experiment/Discussion
"In document classification, Klavans and Kan (1998) demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class (Levin 1993).",Experiment/Discussion
"In discussing future work, Klavans and Kan point to extending coverage of the manually constructed EVCA resource as a way of improving the performance of the system.",Experiment/Discussion
A widecoverage repository of verb relations including verbs linked by the similarity relation will provide a way to automatically extend the existing verb classes to cover more of the English lexicon.,Experiment/Discussion
Some existing broad-coverage resources on verbs have focused on organizing verbs into classes or annotating their frames or thematic roles.,Experiment/Discussion
EVCA (English Verb Classes and Alternations) (Levin 1993) organizes verbs by similarity and participation / nonparticipation in alternation patterns.,Experiment/Discussion
It contains 3200 verbs classified into 191 classes.,Experiment/Discussion
"Additional manually constructed resources include PropBank (Kingsbury et al. 2002), FrameNet (Baker et al.",Experiment/Discussion
"1998), VerbNet (Kipper et al. 2000), and the resource on verb selectional restrictions developed by Gomez (2001).",Experiment/Discussion
Our approach differs from the above in its focus.,Experiment/Discussion
We relate verbs to each other rather than organize them into classes or identify their frames or thematic roles.,Experiment/Discussion
"WordNet does provide relations between verbs, but at a coarser level.",Experiment/Discussion
"We provide finer-grained relations such as strength, enablement and temporal information.",Experiment/Discussion
"Also, in contrast with WordNet, we cover more than the prescriptive cases.",Experiment/Discussion
Previous web mining work has rarely addressed extracting many different semantic relations from Web-sized corpus.,Experiment/Discussion
Most work on extracting semantic information from large corpora has largely focused on the extraction of is-a relations between nouns.,Experiment/Discussion
Hearst (1992) was the first followed by recent larger-scale and more fully automated efforts (Pantel and Ravichandran 2004; Etzioni et al. 2004; Ravichandran and Hovy 2002).,Experiment/Discussion
"Recently, Moldovan et al. (2004) present a learning algorithm to detect 35 fine-grained noun phrase relations.",Experiment/Discussion
"Turney (2001) studied word relatedness and synonym extraction, while Lin et al. (2003) present an algorithm that queries the Web using lexical patterns for distinguishing noun synonymy and antonymy.",Experiment/Discussion
Our approach addresses verbs and provides for a richer and finer-grained set of semantics.,Experiment/Discussion
Reliability of estimating bigram counts on the web via search engines has been investigated by Keller and Lapata (2003).,Experiment/Discussion
Semantic networks have also been extracted from dictionaries and other machine-readable resources.,Experiment/Discussion
MindNet (Richardson et al. 1998) extracts a collection of triples of the type &quot;ducks have wings&quot; and &quot;duck capable-of flying&quot;.,Experiment/Discussion
"This resource, however, does not relate verbs to each other or provide verb semantics.",Experiment/Discussion
"In this section, we introduce and motivate the specific relations that we extract.",Experiment/Discussion
"Whilst the natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977), large-coverage manually created semantic resources typically only organize verbs into a flat or shallow hierarchy of classes (such as those described in Section 2.2).",Experiment/Discussion
"WordNet identifies synonymy, antonymy, troponymy, and cause.",Experiment/Discussion
"As summarized in Figure 1, Fellbaum (1998) discusses a finer-grained analysis of entailment, while the WordNet database does not distinguish between, e.g., backward presupposition (forget :: know, where know must have happened before forget) from proper temporal inclusion (walk :: step).",Experiment/Discussion
"In formulating our set of relations, we have relied on the finer-grained analysis, explicitly breaking out the temporal precedence between entities.",Experiment/Discussion
"In selecting the relations to identify, we aimed at both covering the relations described in WordNet and covering the relations present in our collection of strongly associated verb pairs.",Experiment/Discussion
"We relied on the strongly associated verb pairs, described in Section 4.4, for computational efficiency.",Experiment/Discussion
The relations we identify were experimentally found to cover 99 out of 100 randomly selected verb pairs.,Experiment/Discussion
Our algorithm identifies six semantic relations between verbs.,Experiment/Discussion
These are summarized in Table 1 along with their closest corresponding WordNet category and the symmetry of the relation (whether VI rel V2 is equivalent to V2 rel VI).,Experiment/Discussion
Similarity.,Experiment/Discussion
"As Fellbaum (1998) and the tradition of organizing verbs into similarity classes indicate, verbs do not neatly fit into a unified is-a (troponymy) hierarchy.",Experiment/Discussion
"Rather, verbs are often similar or related.",Experiment/Discussion
"Similarity between action verbs, for example, can arise when they differ in connotations about manner or degree of action.",Experiment/Discussion
"Examples extracted by our system include maximize :: enhance, produce :: create, reduce :: restrict.",Experiment/Discussion
Strength.,Experiment/Discussion
"When two verbs are similar, one may denote a more intense, thorough, comprehensive or absolute action.",Experiment/Discussion
"In the case of change-of-state verbs, one may denote a more complete change.",Experiment/Discussion
We identify this as the strength relation.,Experiment/Discussion
"Sample verb pairs extracted by our system, in the order weak to strong, are: taint :: poison, permit :: authorize, surprise :: startle, startle :: shock.",Experiment/Discussion
Some instances of strength sometimes map to WordNet's troponymy relation.,Experiment/Discussion
"Strength, a subclass of similarity, has not been identified in broad-coverage networks of verbs, but may be of particular use in natural language generation and summarization applications.",Experiment/Discussion
Antonymy.,Experiment/Discussion
"Also known as semantic opposition, antonymy between verbs has several distinct subtypes.",Experiment/Discussion
"As discussed by Fellbaum (1998), it can arise from switching thematic roles associated with the verb (as in buy :: sell, lend :: borrow).",Experiment/Discussion
"There is also antonymy between stative verbs (live :: die, differ :: equal) and antonymy between sibling verbs which share a parent (walk :: run) or an entailed verb (fail :: succeed both entail try).",Experiment/Discussion
Antonymy also systematically interacts with the happens-before relation in the case of restitutive opposition (Cruse 1986).,Experiment/Discussion
"This subtype is exemplified by damage :: repair, wrap :: unwrap.",Experiment/Discussion
"In terms of the relations we recognize, it can be stated that restitutive-opposition(V1, V2) = happensbefore(V1, V2), and antonym(V1, V2).",Experiment/Discussion
"Examples of antonymy extracted by our system include: assemble :: dismantle; ban :: allow; regard :: condemn, roast :: fry.",Experiment/Discussion
Enablement.,Experiment/Discussion
This relation holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2.,Experiment/Discussion
Enablement is classified as a type of causal relation by Barker and Szpakowicz (1995).,Experiment/Discussion
Examples of enablement extracted by our system include: assess :: review and accomplish :: complete.,Experiment/Discussion
Happens-before.,Experiment/Discussion
This relation indicates that the two verbs refer to two temporally disjoint intervals or instances.,Experiment/Discussion
"WordNet's cause relation, between a causative and a resultative verb (as in buy :: own), would be tagged as instances of happens-before by our system.",Experiment/Discussion
"Examples of the happens-before relation identified by our system include marry :: divorce, detain :: prosecute, enroll :: graduate, schedule :: reschedule, tie :: untie.",Experiment/Discussion
We discover the semantic relations described above by querying the Web with Google for lexico-syntactic patterns indicative of each relation.,Experiment/Discussion
Our approach has two stages.,Experiment/Discussion
"First, we identify pairs of highly associated verbs co-occurring on the Web with sufficient frequency using previous work by Lin and Pantel (2001), as described in Section 4.4.",Experiment/Discussion
"Next, for each verb pair, we tested lexico-syntactic patterns, calculating a score for each possible semantic relation as described in Section 4.2.",Experiment/Discussion
"Finally, as described in Section 4.3, we compare the strengths of the individual semantic relations and, preferring the most specific and then strongest relations, output a consistent set as the final output.",Experiment/Discussion
"As a guide to consistency, we use a simple theory of semantics indicating which semantic relations are subtypes of other ones, and which are compatible and which are mutually exclusive.",Experiment/Discussion
The lexico-syntactic patterns were manually selected by examining pairs of verbs in known semantic relations.,Experiment/Discussion
They were refined to decrease capturing wrong parts of speech or incorrect semantic relations.,Experiment/Discussion
We used 50 verb pairs and the overall process took about 25 hours.,Experiment/Discussion
"We use a total of 35 patterns, which are listed in Table 2 along with the estimated frequency of hits.",Experiment/Discussion
"Y or at least X 1,016,905 Yed or at least Xed not only Xed but Yed not just Xed but Yed The probabilities in the denominator are difficult to calculate directly from search engine results.",Experiment/Discussion
"For a given lexico-syntactic pattern, we need to estimate the frequency of the pattern instantiated with appropriately conjugated verbs.",Experiment/Discussion
"For verbs, we need to estimate the frequency of the verbs, but avoid counting other parts-of-speech (e.g. chair as a noun or painted as an adjective).",Experiment/Discussion
"Another issue is that some relations are symmetric (similarity and antonymy), while others are not (strength, enablement, happens-before).",Experiment/Discussion
"For symmetric relations only, the verbs can fill the lexico-syntactic pattern in either order.",Experiment/Discussion
"To address these issues, we estimate Sp(V1,V2) using: *narrow- and broad- similarity overlap in their coverage and are treated as a single category, similarity, when postprocessed.",Experiment/Discussion
Narrow similarity tests for rare patterns and hitsest for it had to be approximated rather than estimated from the smaller corpus.,Experiment/Discussion
Note that our patterns specify the tense of the verbs they accept.,Experiment/Discussion
"When instantiating these patterns, we conjugate as needed.",Experiment/Discussion
"For example, &quot;both Xed and Yed&quot; instantiates on sing and dance as &quot;both sung and danced&quot;.",Experiment/Discussion
"In this section, we describe how the presence of a semantic relation is detected.",Experiment/Discussion
We test the relations with patterns exemplified in Table 2.,Experiment/Discussion
"We adopt an approach inspired by mutual information to measure the strength of association, denoted Sp(V1, V2), between three entities: a verb pair V1 and V2 and a lexico-syntactic pattern p: As a result of tuning the system on a tuning set of 50 verb pairs, = 8.5.",Experiment/Discussion
Additional test for asymmetric relations.,Experiment/Discussion
"For the asymmetric relations, we require not only that exceed a certain threshold, but that there be strong asymmetry of the relation: for symmetric relations.",Experiment/Discussion
"Here, hits(S) denotes the number of documents containing the string S, as returned by Google.",Experiment/Discussion
N is the number of words indexed by the search engine is a correction factor to obtain the frequency of the verb V in all tenses from the frequency of the pattern &quot;to V&quot;.,Experiment/Discussion
"Based on several verbs, we have estimated = 8.5.",Experiment/Discussion
"Because pattern counts, when instantiated with verbs, could not be estimated directly, we have computed the frequencies of the patterns in tagged word corpus and used it to estimate the expected number of hits for each pattern.",Experiment/Discussion
We estimated the a similar method.,Experiment/Discussion
"We say that the semantic relation Given a pair of semantic relations from the set we identify, one of three cases can arise: (i) one relation is more specific (strength is more specific than similarity, enablement is more specific than happens-before), (ii) the relations are compatible (antonymy and happens-before), where presence of one does not imply or rule out presence of the other, and (iii) the relations are incompatible (similarity and antonymy).",Experiment/Discussion
"It is not uncommon for our algorithm to identify presence of several relations, with different strengths.",Experiment/Discussion
"To produce the most likely output, we use semantics of compatibility of the relations to output the most likely one(s).",Experiment/Discussion
"The rules are as follows: If the frequency was too low (less than 10 on the pattern &quot;X * Y&quot; OR &quot;Y * X&quot; OR &quot;X * * Y&quot; OR &quot;Y * * X&quot;), output that the statements are unrelated and stop.",Experiment/Discussion
"If happens-before is detected, output presence of happens-before (additional relation may still be output, if detected).",Experiment/Discussion
"If happens-before is not detected, ignore detection of enablement (because enablement is more specific than happens-before, but is sometimes falsely detected in the absence of happens-before).",Experiment/Discussion
"If strength is detected, score of similarity is ignored (because strength is more specific than similarity).",Experiment/Discussion
"Of the relations strength, similarity, opposition and enablement which were detected (and not ignored), output the one with highest Sp.",Experiment/Discussion
"If nothing has been output to this point, output unrelated.",Experiment/Discussion
"To exhaustively test the more than 64 million unordered verb pairs for WordNet's more than 11,000 verbs would be computationally intractable.",Experiment/Discussion
"Instead, we use a set of highly associated verb pairs output by a paraphrasing algorithm called DIRT (Lin and Pantel 2001).",Experiment/Discussion
"Since we are able to test up to 4000 verb pairs per day on a single machine (we issue at most 40 queries per test and each query takes approximately 0.5 seconds), we are able to test several dozen associated verbs for each verb in WordNet in a matter of weeks.",Experiment/Discussion
Lin and Pantel (2001) describe an algorithm called DIRT (Discovery of Inference Rules from Text) that automatically learns paraphrase expressions from text.,Experiment/Discussion
It is a generalization of previous algorithms that use the distributional hypothesis (Harris 1985) for finding similar words.,Experiment/Discussion
"Instead of applying the hypothesis to words, Lin and Pantel applied it to paths in dependency trees.",Experiment/Discussion
"Essentially, if two paths tend to link the same sets of words, they hypothesized that the meanings of the corresponding paths are similar.",Experiment/Discussion
It is from paths of the form subject-verb-object that we extract our set of associated verb pairs.,Experiment/Discussion
"Hence, this paper is concerned only with relations between transitive verbs.",Experiment/Discussion
"A path, extracted from a parse tree, is an expression that represents a binary relation between two nouns.",Experiment/Discussion
A set of paraphrases was generated for each pair of associated paths.,Experiment/Discussion
"For example, using a 1.5GB newspaper corpus, here are the 20 most associated paths to &quot;X solves Y&quot; generated by DIRT: This list of associated paths looks tantalizingly close to the kind of axioms that would prove useful in an inference system.",Experiment/Discussion
"However, DIRT only outputs pairs of paths that have some semantic relation.",Experiment/Discussion
We used these as our set to extract finergrained relations.,Experiment/Discussion
"In this section, we empirically evaluate the accuracy of VERBOCEAN1.",Experiment/Discussion
"We studied 29,165 pairs of verbs.",Experiment/Discussion
"Applying DIRT to a 1.5GB newspaper corpus2, we extracted 4000 paths that consisted of single verbs in the relation subject-verb-object (i.e. paths of the form &quot;X verb Y&quot;) whose verbs occurred in at least 150 documents on the Web.",Experiment/Discussion
"For example, from the 20 most associated paths to &quot;X solves Y&quot; shown in Section 4.4, the following verb pairs were extracted: We classified each verb pair according to the semantic relations described in Section 2.",Experiment/Discussion
"If the system does not identify any semantic relation for a verb pair, then the system tags the pair as having no relation.",Experiment/Discussion
"To evaluate the accuracy of the system, we randomly sampled 100 of these verb pairs, and presented the classifications to two human judges.",Experiment/Discussion
The adjudicators were asked to judge whether or not the system classification was acceptable (i.e. whether or not the relations output by the system were correct).,Experiment/Discussion
"Since the semantic relations are not disjoint (e.g. mop is both stronger than and similar to sweep), multiple relations may be appropriately acceptable for a given verb pair.",Experiment/Discussion
The judges were also asked to identify their preferred semantic relations (i.e. those relations which seem most plausible).,Experiment/Discussion
Table 3 shows five randomly selected pairs along with the judges' responses.,Experiment/Discussion
The Appendix shows sample relationships discovered by the system.,Experiment/Discussion
Table 4 shows the accuracy of the system.,Experiment/Discussion
"The baseline system consists of labeling each pair with the most common semantic relation, similarity, which occurs 33 times.",Experiment/Discussion
The Tags Correct column represents the percentage of verb pairs whose system output relations were deemed correct.,Experiment/Discussion
The Preferred Tags Correct column gives the percentage of verb pairs whose system output relations matched exactly the human's preferred relations.,Experiment/Discussion
The Kappa statistic (Siegel and Castellan 1988) for the task of judging system tags as correct and incorrect is κ = 0.78 whereas the task of identifying the preferred semantic relation has κ = 0.72.,Experiment/Discussion
"For the latter task, the two judges agreed on 73 of the 100 semantic relations.",Experiment/Discussion
73% gives an idea of an upper bound for humans on this task.,Experiment/Discussion
"On these 73 relations, the system achieved a higher accuracy of 70.0%.",Experiment/Discussion
The system is allowed to output the happens-before relation in combination with other relations.,Experiment/Discussion
"On the 17 happens-before relations output by the system, 67.6% were judged correct.",Experiment/Discussion
"Ignoring the happens-before relations, we achieved a Tags Correct precision of 68%.",Experiment/Discussion
Table 5 shows the accuracy of the system on each of the relations.,Experiment/Discussion
The stronger-than relation is a subset of the similarity relation.,Experiment/Discussion
"Considering a coarser extraction where stronger-than relations are merged with similarity, the task of judging system tags and the task of identifying the preferred semantic relation both jump to 68.2% accuracy.",Experiment/Discussion
"Also, the overall accuracy of the system climbs to 68.5%.",Experiment/Discussion
"As described in Section 2, WordNet contains verb semantic relations.",Experiment/Discussion
A significant percentage of our discovered relations are not covered by WordNet's coarser classifications.,Experiment/Discussion
"Of the 40 verb pairs whose system relation was tagged as correct by both judges in our accuracy experiments and whose tag was not `no relation', only 22.5% of them existed in a WordNet relation.",Experiment/Discussion
The experience of extracting these semantic relations has clarified certain important challenges.,Experiment/Discussion
"While relying on a search engine allows us to query a corpus of nearly a trillion words, some issues arise: (i) the number of instances has to be approximated by the number of hits (documents); (ii) the number of hits for the same query may fluctuate over time; and (iii) some needed counts are not directly available.",Experiment/Discussion
We addressed the latter issue by approximating these counts using a smaller corpus.,Experiment/Discussion
We do not detect entailment with lexicosyntactic patterns.,Experiment/Discussion
"In fact, we propose that whether the entailment relation holds between V1 and V2 depends on the absence of another verb V1' in the same relationship with V2.",Experiment/Discussion
"For example, given the relation marry happens-before divorce, we can conclude that divorce entails marry.",Experiment/Discussion
"But, given the relation buy happens-before sell, we cannot conclude entailment since manufacture can also happen before sell.",Experiment/Discussion
This also applies to the enablement and strength relations.,Experiment/Discussion
"Corpus-based methods, including ours, hold the promise of wide coverage but are weak on discriminating senses.",Experiment/Discussion
"While we hope that applications will benefit from this resource as is, an interesting next step would be to augment it with sense information.",Experiment/Discussion
There are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs.,Experiment/Discussion
"One avenue would be to automatically learn or manually craft more patterns and to extend the pattern vocabulary (when developing the system, we have noticed that different registers and verb types require different patterns).",Experiment/Discussion
Another possibility would be to use more relaxed patterns when the part of speech confusion is not likely (e.g.,Experiment/Discussion
"&quot;eat&quot; is a common verb which does not have a noun sense, and patterns need not protect against noun senses when testing such verbs).",Experiment/Discussion
Our approach can potentially be extended to multiword paths.,Experiment/Discussion
"DIRT actually provides two orders of magnitude more relations than the 29,165 single verb relations (subject-verb-object) we extracted.",Experiment/Discussion
"On the same 1GB corpus described in Section 5.1, DIRT extracted over 200K paths and 6M unique paraphrases.",Experiment/Discussion
"These provide an opportunity to create a much larger corpus of semantic relations, or to construct smaller, in-depth resources for selected subdomains.",Experiment/Discussion
"For example, we could extract that take a trip to is similar to travel to, and that board a plane happens before deplane.",Experiment/Discussion
"If the entire database is viewed as a graph, we currently leverage and enforce only local consistency.",Experiment/Discussion
"It would be useful to enforce global consistency, e.g.",Experiment/Discussion
"V1 stronger-than V2, and V2 strongerthan V3 indicates that V1 stronger-than V3, which may be leveraged to identify additional relations or inconsistent relations (e.g.",Experiment/Discussion
V3 stronger-than V1).,Experiment/Discussion
"Finally, as discussed in Section 5.3, entailment relations may be derivable by processing the complete graph of the identified semantic relation.",Experiment/Discussion
"We have demonstrated that certain fine-grained semantic relations between verbs are present on the Web, and are extractable with a simple patternbased approach.",Results/Conclusion
"In addition to discovering relations identified in WordNet, such as opposition and enablement, we obtain strong results on strength relations (for which no wide-coverage resource is available).",Results/Conclusion
"On a set of 29,165 associated verb pairs, experimental results show an accuracy of 65.5% in assigning similarity, strength, antonymy, enablement, and happens-before.",Results/Conclusion
Further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.,Results/Conclusion
"We hope to open the way to inferring implied, but not stated assertions and to benefit applications such as question answering, information retrieval, and summarization.",Results/Conclusion
The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.,Results/Conclusion
This research was partly supported by NSF grant #EIA-0205111.,Results/Conclusion
