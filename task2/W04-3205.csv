col1,col2
Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks.,{}
We present a semi-automatic method for extracting fine-grained semantic relations between verbs.,{}
"We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.",{}
"On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.",{}
Analysis of types shows that on the relation achieved 75% accuracy.,{}
We provide the called for download at,{}
"Many NLP tasks, such as question answering, summarization, and machine translation could benefit from broad-coverage semantic resources such as WordNet (Miller 1990) and EVCA (English Verb Classes and Alternations) (Levin 1993).","{'title': '1 Introduction', 'number': '1'}"
These extremely useful resources have very high precision entries but have important limitations when used in real-world NLP tasks due to their limited coverage and prescriptive nature (i.e. they do not include semantic relations that are plausible but not guaranteed).,"{'title': '1 Introduction', 'number': '1'}"
"For example, it may be valuable to know that if someone has bought an item, they may sell it at a later time.","{'title': '1 Introduction', 'number': '1'}"
WordNet does not include the relation &quot;X buys Y&quot; happens-before &quot;X sells Y&quot; since it is possible to sell something without having bought it (e.g. having manufactured or stolen it).,"{'title': '1 Introduction', 'number': '1'}"
Verbs are the primary vehicle for describing events and expressing relations between entities.,"{'title': '1 Introduction', 'number': '1'}"
"Hence, verb semantics could help in many natural language processing (NLP) tasks that deal with events or relations between entities.","{'title': '1 Introduction', 'number': '1'}"
"For tasks which require canonicalization of natural language statements or derivation of plausible inferences from such statements, a particularly valuable resource is one which (i) relates verbs to one another and (ii) provides broad coverage of the verbs in the target language.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns.","{'title': '1 Introduction', 'number': '1'}"
"The verb relations we discover are similarity, strength, antonymy, enablement, and temporal relations.","{'title': '1 Introduction', 'number': '1'}"
"Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN.","{'title': '1 Introduction', 'number': '1'}"
Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002).,"{'title': '1 Introduction', 'number': '1'}"
We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern.,"{'title': '1 Introduction', 'number': '1'}"
"In this section, we describe application domains that can benefit from a resource of verb semantics.","{'title': '2 Relevant Work', 'number': '2'}"
We then introduce some existing resources and describe previous attempts at mining semantics from text.,"{'title': '2 Relevant Work', 'number': '2'}"
Question answering is often approached by canonicalizing the question text and the answer text into logical forms.,"{'title': '2 Relevant Work', 'number': '2'}"
"This approach is taken, inter alia, by a top-performing system (Moldovan et al. 2002).","{'title': '2 Relevant Work', 'number': '2'}"
"In discussing future work on the system's logical form matching component, Rus (2002 p. 143) points to incorporating entailment and causation verb relations to improve the matcher's performance.","{'title': '2 Relevant Work', 'number': '2'}"
"In other work, Webber et al. (2002) have argued that successful question answering depends on lexical reasoning, and that lexical reasoning in turn requires fine-grained verb semantics in addition to troponymy (is-a relations between verbs) and antonymy.","{'title': '2 Relevant Work', 'number': '2'}"
"In multi-document summarization, knowing verb similarities is useful for sentence compression and for determining sentences that have the same meaning (Lin 1997).","{'title': '2 Relevant Work', 'number': '2'}"
Knowing that a particular action happens before another or is enabled by another is also useful to determine the order of the events (Barzilay et al. 2002).,"{'title': '2 Relevant Work', 'number': '2'}"
"For example, to order summary sentences properly, it may be useful to know that selling something can be preceded by either buying, manufacturing, or stealing it.","{'title': '2 Relevant Work', 'number': '2'}"
"Furthermore, knowing that a particular verb has a meaning stronger than another (e.g. rape vs. abuse and renovate vs. upgrade) can help a system pick the most general sentence.","{'title': '2 Relevant Work', 'number': '2'}"
"In lexical selection of verbs in machine translation and in work on document classification, practitioners have argued for approaches that depend on wide-coverage resources indicating verb similarity and membership of a verb in a certain class.","{'title': '2 Relevant Work', 'number': '2'}"
"In work on translating verbs with many counterparts in the target language, Palmer and Wu (1995) discuss inherent limitations of approaches which do not examine a verb's class membership, and put forth an approach based on verb similarity.","{'title': '2 Relevant Work', 'number': '2'}"
"In document classification, Klavans and Kan (1998) demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class (Levin 1993).","{'title': '2 Relevant Work', 'number': '2'}"
"In discussing future work, Klavans and Kan point to extending coverage of the manually constructed EVCA resource as a way of improving the performance of the system.","{'title': '2 Relevant Work', 'number': '2'}"
A widecoverage repository of verb relations including verbs linked by the similarity relation will provide a way to automatically extend the existing verb classes to cover more of the English lexicon.,"{'title': '2 Relevant Work', 'number': '2'}"
Some existing broad-coverage resources on verbs have focused on organizing verbs into classes or annotating their frames or thematic roles.,"{'title': '2 Relevant Work', 'number': '2'}"
EVCA (English Verb Classes and Alternations) (Levin 1993) organizes verbs by similarity and participation / nonparticipation in alternation patterns.,"{'title': '2 Relevant Work', 'number': '2'}"
It contains 3200 verbs classified into 191 classes.,"{'title': '2 Relevant Work', 'number': '2'}"
"Additional manually constructed resources include PropBank (Kingsbury et al. 2002), FrameNet (Baker et al.","{'title': '2 Relevant Work', 'number': '2'}"
"1998), VerbNet (Kipper et al. 2000), and the resource on verb selectional restrictions developed by Gomez (2001).","{'title': '2 Relevant Work', 'number': '2'}"
Our approach differs from the above in its focus.,"{'title': '2 Relevant Work', 'number': '2'}"
We relate verbs to each other rather than organize them into classes or identify their frames or thematic roles.,"{'title': '2 Relevant Work', 'number': '2'}"
"WordNet does provide relations between verbs, but at a coarser level.","{'title': '2 Relevant Work', 'number': '2'}"
"We provide finer-grained relations such as strength, enablement and temporal information.","{'title': '2 Relevant Work', 'number': '2'}"
"Also, in contrast with WordNet, we cover more than the prescriptive cases.","{'title': '2 Relevant Work', 'number': '2'}"
Previous web mining work has rarely addressed extracting many different semantic relations from Web-sized corpus.,"{'title': '2 Relevant Work', 'number': '2'}"
Most work on extracting semantic information from large corpora has largely focused on the extraction of is-a relations between nouns.,"{'title': '2 Relevant Work', 'number': '2'}"
Hearst (1992) was the first followed by recent larger-scale and more fully automated efforts (Pantel and Ravichandran 2004; Etzioni et al. 2004; Ravichandran and Hovy 2002).,"{'title': '2 Relevant Work', 'number': '2'}"
"Recently, Moldovan et al. (2004) present a learning algorithm to detect 35 fine-grained noun phrase relations.","{'title': '2 Relevant Work', 'number': '2'}"
"Turney (2001) studied word relatedness and synonym extraction, while Lin et al. (2003) present an algorithm that queries the Web using lexical patterns for distinguishing noun synonymy and antonymy.","{'title': '2 Relevant Work', 'number': '2'}"
Our approach addresses verbs and provides for a richer and finer-grained set of semantics.,"{'title': '2 Relevant Work', 'number': '2'}"
Reliability of estimating bigram counts on the web via search engines has been investigated by Keller and Lapata (2003).,"{'title': '2 Relevant Work', 'number': '2'}"
Semantic networks have also been extracted from dictionaries and other machine-readable resources.,"{'title': '2 Relevant Work', 'number': '2'}"
MindNet (Richardson et al. 1998) extracts a collection of triples of the type &quot;ducks have wings&quot; and &quot;duck capable-of flying&quot;.,"{'title': '2 Relevant Work', 'number': '2'}"
"This resource, however, does not relate verbs to each other or provide verb semantics.","{'title': '2 Relevant Work', 'number': '2'}"
"In this section, we introduce and motivate the specific relations that we extract.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Whilst the natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977), large-coverage manually created semantic resources typically only organize verbs into a flat or shallow hierarchy of classes (such as those described in Section 2.2).","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"WordNet identifies synonymy, antonymy, troponymy, and cause.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"As summarized in Figure 1, Fellbaum (1998) discusses a finer-grained analysis of entailment, while the WordNet database does not distinguish between, e.g., backward presupposition (forget :: know, where know must have happened before forget) from proper temporal inclusion (walk :: step).","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"In formulating our set of relations, we have relied on the finer-grained analysis, explicitly breaking out the temporal precedence between entities.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"In selecting the relations to identify, we aimed at both covering the relations described in WordNet and covering the relations present in our collection of strongly associated verb pairs.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"We relied on the strongly associated verb pairs, described in Section 4.4, for computational efficiency.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
The relations we identify were experimentally found to cover 99 out of 100 randomly selected verb pairs.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
Our algorithm identifies six semantic relations between verbs.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
These are summarized in Table 1 along with their closest corresponding WordNet category and the symmetry of the relation (whether VI rel V2 is equivalent to V2 rel VI).,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
Similarity.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"As Fellbaum (1998) and the tradition of organizing verbs into similarity classes indicate, verbs do not neatly fit into a unified is-a (troponymy) hierarchy.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Rather, verbs are often similar or related.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Similarity between action verbs, for example, can arise when they differ in connotations about manner or degree of action.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Examples extracted by our system include maximize :: enhance, produce :: create, reduce :: restrict.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
Strength.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"When two verbs are similar, one may denote a more intense, thorough, comprehensive or absolute action.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"In the case of change-of-state verbs, one may denote a more complete change.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
We identify this as the strength relation.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Sample verb pairs extracted by our system, in the order weak to strong, are: taint :: poison, permit :: authorize, surprise :: startle, startle :: shock.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
Some instances of strength sometimes map to WordNet's troponymy relation.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Strength, a subclass of similarity, has not been identified in broad-coverage networks of verbs, but may be of particular use in natural language generation and summarization applications.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
Antonymy.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Also known as semantic opposition, antonymy between verbs has several distinct subtypes.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"As discussed by Fellbaum (1998), it can arise from switching thematic roles associated with the verb (as in buy :: sell, lend :: borrow).","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"There is also antonymy between stative verbs (live :: die, differ :: equal) and antonymy between sibling verbs which share a parent (walk :: run) or an entailed verb (fail :: succeed both entail try).","{'title': '3 Semantic relations among verbs', 'number': '3'}"
Antonymy also systematically interacts with the happens-before relation in the case of restitutive opposition (Cruse 1986).,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"This subtype is exemplified by damage :: repair, wrap :: unwrap.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"In terms of the relations we recognize, it can be stated that restitutive-opposition(V1, V2) = happensbefore(V1, V2), and antonym(V1, V2).","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Examples of antonymy extracted by our system include: assemble :: dismantle; ban :: allow; regard :: condemn, roast :: fry.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
Enablement.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
This relation holds between two verbs V1 and V2 when the pair can be glossed as V1 is accomplished by V2.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
Enablement is classified as a type of causal relation by Barker and Szpakowicz (1995).,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
Examples of enablement extracted by our system include: assess :: review and accomplish :: complete.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
Happens-before.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
This relation indicates that the two verbs refer to two temporally disjoint intervals or instances.,"{'title': '3 Semantic relations among verbs', 'number': '3'}"
"WordNet's cause relation, between a causative and a resultative verb (as in buy :: own), would be tagged as instances of happens-before by our system.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
"Examples of the happens-before relation identified by our system include marry :: divorce, detain :: prosecute, enroll :: graduate, schedule :: reschedule, tie :: untie.","{'title': '3 Semantic relations among verbs', 'number': '3'}"
We discover the semantic relations described above by querying the Web with Google for lexico-syntactic patterns indicative of each relation.,"{'title': '4 Approach', 'number': '4'}"
Our approach has two stages.,"{'title': '4 Approach', 'number': '4'}"
"First, we identify pairs of highly associated verbs co-occurring on the Web with sufficient frequency using previous work by Lin and Pantel (2001), as described in Section 4.4.","{'title': '4 Approach', 'number': '4'}"
"Next, for each verb pair, we tested lexico-syntactic patterns, calculating a score for each possible semantic relation as described in Section 4.2.","{'title': '4 Approach', 'number': '4'}"
"Finally, as described in Section 4.3, we compare the strengths of the individual semantic relations and, preferring the most specific and then strongest relations, output a consistent set as the final output.","{'title': '4 Approach', 'number': '4'}"
"As a guide to consistency, we use a simple theory of semantics indicating which semantic relations are subtypes of other ones, and which are compatible and which are mutually exclusive.","{'title': '4 Approach', 'number': '4'}"
The lexico-syntactic patterns were manually selected by examining pairs of verbs in known semantic relations.,"{'title': '4 Approach', 'number': '4'}"
They were refined to decrease capturing wrong parts of speech or incorrect semantic relations.,"{'title': '4 Approach', 'number': '4'}"
We used 50 verb pairs and the overall process took about 25 hours.,"{'title': '4 Approach', 'number': '4'}"
"We use a total of 35 patterns, which are listed in Table 2 along with the estimated frequency of hits.","{'title': '4 Approach', 'number': '4'}"
"Y or at least X 1,016,905 Yed or at least Xed not only Xed but Yed not just Xed but Yed The probabilities in the denominator are difficult to calculate directly from search engine results.","{'title': '4 Approach', 'number': '4'}"
"For a given lexico-syntactic pattern, we need to estimate the frequency of the pattern instantiated with appropriately conjugated verbs.","{'title': '4 Approach', 'number': '4'}"
"For verbs, we need to estimate the frequency of the verbs, but avoid counting other parts-of-speech (e.g. chair as a noun or painted as an adjective).","{'title': '4 Approach', 'number': '4'}"
"Another issue is that some relations are symmetric (similarity and antonymy), while others are not (strength, enablement, happens-before).","{'title': '4 Approach', 'number': '4'}"
"For symmetric relations only, the verbs can fill the lexico-syntactic pattern in either order.","{'title': '4 Approach', 'number': '4'}"
"To address these issues, we estimate Sp(V1,V2) using: *narrow- and broad- similarity overlap in their coverage and are treated as a single category, similarity, when postprocessed.","{'title': '4 Approach', 'number': '4'}"
Narrow similarity tests for rare patterns and hitsest for it had to be approximated rather than estimated from the smaller corpus.,"{'title': '4 Approach', 'number': '4'}"
Note that our patterns specify the tense of the verbs they accept.,"{'title': '4 Approach', 'number': '4'}"
"When instantiating these patterns, we conjugate as needed.","{'title': '4 Approach', 'number': '4'}"
"For example, &quot;both Xed and Yed&quot; instantiates on sing and dance as &quot;both sung and danced&quot;.","{'title': '4 Approach', 'number': '4'}"
"In this section, we describe how the presence of a semantic relation is detected.","{'title': '4 Approach', 'number': '4'}"
We test the relations with patterns exemplified in Table 2.,"{'title': '4 Approach', 'number': '4'}"
"We adopt an approach inspired by mutual information to measure the strength of association, denoted Sp(V1, V2), between three entities: a verb pair V1 and V2 and a lexico-syntactic pattern p: As a result of tuning the system on a tuning set of 50 verb pairs, = 8.5.","{'title': '4 Approach', 'number': '4'}"
Additional test for asymmetric relations.,"{'title': '4 Approach', 'number': '4'}"
"For the asymmetric relations, we require not only that exceed a certain threshold, but that there be strong asymmetry of the relation: for symmetric relations.","{'title': '4 Approach', 'number': '4'}"
"Here, hits(S) denotes the number of documents containing the string S, as returned by Google.","{'title': '4 Approach', 'number': '4'}"
N is the number of words indexed by the search engine is a correction factor to obtain the frequency of the verb V in all tenses from the frequency of the pattern &quot;to V&quot;.,"{'title': '4 Approach', 'number': '4'}"
"Based on several verbs, we have estimated = 8.5.","{'title': '4 Approach', 'number': '4'}"
"Because pattern counts, when instantiated with verbs, could not be estimated directly, we have computed the frequencies of the patterns in tagged word corpus and used it to estimate the expected number of hits for each pattern.","{'title': '4 Approach', 'number': '4'}"
We estimated the a similar method.,"{'title': '4 Approach', 'number': '4'}"
"We say that the semantic relation Given a pair of semantic relations from the set we identify, one of three cases can arise: (i) one relation is more specific (strength is more specific than similarity, enablement is more specific than happens-before), (ii) the relations are compatible (antonymy and happens-before), where presence of one does not imply or rule out presence of the other, and (iii) the relations are incompatible (similarity and antonymy).","{'title': '4 Approach', 'number': '4'}"
"It is not uncommon for our algorithm to identify presence of several relations, with different strengths.","{'title': '4 Approach', 'number': '4'}"
"To produce the most likely output, we use semantics of compatibility of the relations to output the most likely one(s).","{'title': '4 Approach', 'number': '4'}"
"The rules are as follows: If the frequency was too low (less than 10 on the pattern &quot;X * Y&quot; OR &quot;Y * X&quot; OR &quot;X * * Y&quot; OR &quot;Y * * X&quot;), output that the statements are unrelated and stop.","{'title': '4 Approach', 'number': '4'}"
"If happens-before is detected, output presence of happens-before (additional relation may still be output, if detected).","{'title': '4 Approach', 'number': '4'}"
"If happens-before is not detected, ignore detection of enablement (because enablement is more specific than happens-before, but is sometimes falsely detected in the absence of happens-before).","{'title': '4 Approach', 'number': '4'}"
"If strength is detected, score of similarity is ignored (because strength is more specific than similarity).","{'title': '4 Approach', 'number': '4'}"
"Of the relations strength, similarity, opposition and enablement which were detected (and not ignored), output the one with highest Sp.","{'title': '4 Approach', 'number': '4'}"
"If nothing has been output to this point, output unrelated.","{'title': '4 Approach', 'number': '4'}"
"To exhaustively test the more than 64 million unordered verb pairs for WordNet's more than 11,000 verbs would be computationally intractable.","{'title': '4 Approach', 'number': '4'}"
"Instead, we use a set of highly associated verb pairs output by a paraphrasing algorithm called DIRT (Lin and Pantel 2001).","{'title': '4 Approach', 'number': '4'}"
"Since we are able to test up to 4000 verb pairs per day on a single machine (we issue at most 40 queries per test and each query takes approximately 0.5 seconds), we are able to test several dozen associated verbs for each verb in WordNet in a matter of weeks.","{'title': '4 Approach', 'number': '4'}"
Lin and Pantel (2001) describe an algorithm called DIRT (Discovery of Inference Rules from Text) that automatically learns paraphrase expressions from text.,"{'title': '4 Approach', 'number': '4'}"
It is a generalization of previous algorithms that use the distributional hypothesis (Harris 1985) for finding similar words.,"{'title': '4 Approach', 'number': '4'}"
"Instead of applying the hypothesis to words, Lin and Pantel applied it to paths in dependency trees.","{'title': '4 Approach', 'number': '4'}"
"Essentially, if two paths tend to link the same sets of words, they hypothesized that the meanings of the corresponding paths are similar.","{'title': '4 Approach', 'number': '4'}"
It is from paths of the form subject-verb-object that we extract our set of associated verb pairs.,"{'title': '4 Approach', 'number': '4'}"
"Hence, this paper is concerned only with relations between transitive verbs.","{'title': '4 Approach', 'number': '4'}"
"A path, extracted from a parse tree, is an expression that represents a binary relation between two nouns.","{'title': '4 Approach', 'number': '4'}"
A set of paraphrases was generated for each pair of associated paths.,"{'title': '4 Approach', 'number': '4'}"
"For example, using a 1.5GB newspaper corpus, here are the 20 most associated paths to &quot;X solves Y&quot; generated by DIRT: This list of associated paths looks tantalizingly close to the kind of axioms that would prove useful in an inference system.","{'title': '4 Approach', 'number': '4'}"
"However, DIRT only outputs pairs of paths that have some semantic relation.","{'title': '4 Approach', 'number': '4'}"
We used these as our set to extract finergrained relations.,"{'title': '4 Approach', 'number': '4'}"
"In this section, we empirically evaluate the accuracy of VERBOCEAN1.","{'title': '5 Experimental results', 'number': '5'}"
"We studied 29,165 pairs of verbs.","{'title': '5 Experimental results', 'number': '5'}"
"Applying DIRT to a 1.5GB newspaper corpus2, we extracted 4000 paths that consisted of single verbs in the relation subject-verb-object (i.e. paths of the form &quot;X verb Y&quot;) whose verbs occurred in at least 150 documents on the Web.","{'title': '5 Experimental results', 'number': '5'}"
"For example, from the 20 most associated paths to &quot;X solves Y&quot; shown in Section 4.4, the following verb pairs were extracted: We classified each verb pair according to the semantic relations described in Section 2.","{'title': '5 Experimental results', 'number': '5'}"
"If the system does not identify any semantic relation for a verb pair, then the system tags the pair as having no relation.","{'title': '5 Experimental results', 'number': '5'}"
"To evaluate the accuracy of the system, we randomly sampled 100 of these verb pairs, and presented the classifications to two human judges.","{'title': '5 Experimental results', 'number': '5'}"
The adjudicators were asked to judge whether or not the system classification was acceptable (i.e. whether or not the relations output by the system were correct).,"{'title': '5 Experimental results', 'number': '5'}"
"Since the semantic relations are not disjoint (e.g. mop is both stronger than and similar to sweep), multiple relations may be appropriately acceptable for a given verb pair.","{'title': '5 Experimental results', 'number': '5'}"
The judges were also asked to identify their preferred semantic relations (i.e. those relations which seem most plausible).,"{'title': '5 Experimental results', 'number': '5'}"
Table 3 shows five randomly selected pairs along with the judges' responses.,"{'title': '5 Experimental results', 'number': '5'}"
The Appendix shows sample relationships discovered by the system.,"{'title': '5 Experimental results', 'number': '5'}"
Table 4 shows the accuracy of the system.,"{'title': '5 Experimental results', 'number': '5'}"
"The baseline system consists of labeling each pair with the most common semantic relation, similarity, which occurs 33 times.","{'title': '5 Experimental results', 'number': '5'}"
The Tags Correct column represents the percentage of verb pairs whose system output relations were deemed correct.,"{'title': '5 Experimental results', 'number': '5'}"
The Preferred Tags Correct column gives the percentage of verb pairs whose system output relations matched exactly the human's preferred relations.,"{'title': '5 Experimental results', 'number': '5'}"
The Kappa statistic (Siegel and Castellan 1988) for the task of judging system tags as correct and incorrect is κ = 0.78 whereas the task of identifying the preferred semantic relation has κ = 0.72.,"{'title': '5 Experimental results', 'number': '5'}"
"For the latter task, the two judges agreed on 73 of the 100 semantic relations.","{'title': '5 Experimental results', 'number': '5'}"
73% gives an idea of an upper bound for humans on this task.,"{'title': '5 Experimental results', 'number': '5'}"
"On these 73 relations, the system achieved a higher accuracy of 70.0%.","{'title': '5 Experimental results', 'number': '5'}"
The system is allowed to output the happens-before relation in combination with other relations.,"{'title': '5 Experimental results', 'number': '5'}"
"On the 17 happens-before relations output by the system, 67.6% were judged correct.","{'title': '5 Experimental results', 'number': '5'}"
"Ignoring the happens-before relations, we achieved a Tags Correct precision of 68%.","{'title': '5 Experimental results', 'number': '5'}"
Table 5 shows the accuracy of the system on each of the relations.,"{'title': '5 Experimental results', 'number': '5'}"
The stronger-than relation is a subset of the similarity relation.,"{'title': '5 Experimental results', 'number': '5'}"
"Considering a coarser extraction where stronger-than relations are merged with similarity, the task of judging system tags and the task of identifying the preferred semantic relation both jump to 68.2% accuracy.","{'title': '5 Experimental results', 'number': '5'}"
"Also, the overall accuracy of the system climbs to 68.5%.","{'title': '5 Experimental results', 'number': '5'}"
"As described in Section 2, WordNet contains verb semantic relations.","{'title': '5 Experimental results', 'number': '5'}"
A significant percentage of our discovered relations are not covered by WordNet's coarser classifications.,"{'title': '5 Experimental results', 'number': '5'}"
"Of the 40 verb pairs whose system relation was tagged as correct by both judges in our accuracy experiments and whose tag was not `no relation', only 22.5% of them existed in a WordNet relation.","{'title': '5 Experimental results', 'number': '5'}"
The experience of extracting these semantic relations has clarified certain important challenges.,"{'title': '5 Experimental results', 'number': '5'}"
"While relying on a search engine allows us to query a corpus of nearly a trillion words, some issues arise: (i) the number of instances has to be approximated by the number of hits (documents); (ii) the number of hits for the same query may fluctuate over time; and (iii) some needed counts are not directly available.","{'title': '5 Experimental results', 'number': '5'}"
We addressed the latter issue by approximating these counts using a smaller corpus.,"{'title': '5 Experimental results', 'number': '5'}"
We do not detect entailment with lexicosyntactic patterns.,"{'title': '5 Experimental results', 'number': '5'}"
"In fact, we propose that whether the entailment relation holds between V1 and V2 depends on the absence of another verb V1' in the same relationship with V2.","{'title': '5 Experimental results', 'number': '5'}"
"For example, given the relation marry happens-before divorce, we can conclude that divorce entails marry.","{'title': '5 Experimental results', 'number': '5'}"
"But, given the relation buy happens-before sell, we cannot conclude entailment since manufacture can also happen before sell.","{'title': '5 Experimental results', 'number': '5'}"
This also applies to the enablement and strength relations.,"{'title': '5 Experimental results', 'number': '5'}"
"Corpus-based methods, including ours, hold the promise of wide coverage but are weak on discriminating senses.","{'title': '5 Experimental results', 'number': '5'}"
"While we hope that applications will benefit from this resource as is, an interesting next step would be to augment it with sense information.","{'title': '5 Experimental results', 'number': '5'}"
There are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs.,"{'title': '6 Future work', 'number': '6'}"
"One avenue would be to automatically learn or manually craft more patterns and to extend the pattern vocabulary (when developing the system, we have noticed that different registers and verb types require different patterns).","{'title': '6 Future work', 'number': '6'}"
Another possibility would be to use more relaxed patterns when the part of speech confusion is not likely (e.g.,"{'title': '6 Future work', 'number': '6'}"
"&quot;eat&quot; is a common verb which does not have a noun sense, and patterns need not protect against noun senses when testing such verbs).","{'title': '6 Future work', 'number': '6'}"
Our approach can potentially be extended to multiword paths.,"{'title': '6 Future work', 'number': '6'}"
"DIRT actually provides two orders of magnitude more relations than the 29,165 single verb relations (subject-verb-object) we extracted.","{'title': '6 Future work', 'number': '6'}"
"On the same 1GB corpus described in Section 5.1, DIRT extracted over 200K paths and 6M unique paraphrases.","{'title': '6 Future work', 'number': '6'}"
"These provide an opportunity to create a much larger corpus of semantic relations, or to construct smaller, in-depth resources for selected subdomains.","{'title': '6 Future work', 'number': '6'}"
"For example, we could extract that take a trip to is similar to travel to, and that board a plane happens before deplane.","{'title': '6 Future work', 'number': '6'}"
"If the entire database is viewed as a graph, we currently leverage and enforce only local consistency.","{'title': '6 Future work', 'number': '6'}"
"It would be useful to enforce global consistency, e.g.","{'title': '6 Future work', 'number': '6'}"
"V1 stronger-than V2, and V2 strongerthan V3 indicates that V1 stronger-than V3, which may be leveraged to identify additional relations or inconsistent relations (e.g.","{'title': '6 Future work', 'number': '6'}"
V3 stronger-than V1).,"{'title': '6 Future work', 'number': '6'}"
"Finally, as discussed in Section 5.3, entailment relations may be derivable by processing the complete graph of the identified semantic relation.","{'title': '6 Future work', 'number': '6'}"
"We have demonstrated that certain fine-grained semantic relations between verbs are present on the Web, and are extractable with a simple patternbased approach.","{'title': '7 Conclusions', 'number': '7'}"
"In addition to discovering relations identified in WordNet, such as opposition and enablement, we obtain strong results on strength relations (for which no wide-coverage resource is available).","{'title': '7 Conclusions', 'number': '7'}"
"On a set of 29,165 associated verb pairs, experimental results show an accuracy of 65.5% in assigning similarity, strength, antonymy, enablement, and happens-before.","{'title': '7 Conclusions', 'number': '7'}"
Further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.,"{'title': '7 Conclusions', 'number': '7'}"
"We hope to open the way to inferring implied, but not stated assertions and to benefit applications such as question answering, information retrieval, and summarization.","{'title': '7 Conclusions', 'number': '7'}"
The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.,"{'title': 'Acknowledgments', 'number': '8'}"
This research was partly supported by NSF grant #EIA-0205111.,"{'title': 'Acknowledgments', 'number': '8'}"
