col1,col2
Paraphrase recognition is a critical step for natural language interpretation.,{}
"Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.",{}
"However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.",{}
We present a fully unsupervised learning algorithm for Web-based extraction an extended model of paraphrases.,{}
"We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.",{}
Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.,{}
"Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.",{}
Modeling semantic variability in language has drawn a lot of attention in recent years.,"{'title': '1 Introduction', 'number': '1'}"
"Many applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al., 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms.","{'title': '1 Introduction', 'number': '1'}"
"Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed.","{'title': '1 Introduction', 'number': '1'}"
"Following (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g.","{'title': '1 Introduction', 'number': '1'}"
"(Moldovan and Rus, 2001)).","{'title': '1 Introduction', 'number': '1'}"
"These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other.","{'title': '1 Introduction', 'number': '1'}"
For example “X acquired Y” entails “X owns Y”.,"{'title': '1 Introduction', 'number': '1'}"
"These relations provide a broad framework for representing and recognizing semantic variability, as proposed in (Dagan and Glickman, 2004).","{'title': '1 Introduction', 'number': '1'}"
"For example, if a QA system has to answer the question “Who owns Overture?” and the corpus includes the phrase “Yahoo acquired Overture”, the system can use the known entailment relation to conclude that this phrase really indicates the desired answer.","{'title': '1 Introduction', 'number': '1'}"
"More examples of entailment relations, acquired by our method, can be found in Table 1 (section 4).","{'title': '1 Introduction', 'number': '1'}"
"To perform such inferences at a broad scale, applications need to possess a large knowledge base (KB) of entailment patterns.","{'title': '1 Introduction', 'number': '1'}"
"We estimate such a KB should contain from between a handful to a few dozens of relations per meaning, which may sum to a few hundred thousands of relations for a broad domain, given that a typical lexicon includes tens of thousands of words.","{'title': '1 Introduction', 'number': '1'}"
Our research goal is to approach unsupervised acquisition of such a full scale KB.,"{'title': '1 Introduction', 'number': '1'}"
"We focus on developing methods that acquire entailment relations from the Web, the largest available resource.","{'title': '1 Introduction', 'number': '1'}"
To this end substantial improvements are needed in order to promote scalability relative to current Webbased approaches.,"{'title': '1 Introduction', 'number': '1'}"
"In particular, we address two major goals: reducing dramatically the complexity of required auxiliary inputs, thus enabling to apply the methods at larger scales, and generalizing the types of structures that can be acquired.","{'title': '1 Introduction', 'number': '1'}"
The algorithms described in this paper were applied for acquiring entailment relations for verb-based expressions.,"{'title': '1 Introduction', 'number': '1'}"
They successfully discovered several relations on average per each randomly selected expression.,"{'title': '1 Introduction', 'number': '1'}"
"This section provides a qualitative view of prior work, emphasizing the perspective of aiming at a full-scale paraphrase resource.","{'title': '2 Background and Motivations', 'number': '2'}"
"As there are still no standard benchmarks, current quantitative results are not comparable in a consistent way.","{'title': '2 Background and Motivations', 'number': '2'}"
"The major idea in paraphrase acquisition is often to find linguistic structures, here termed templates, that share the same anchors.","{'title': '2 Background and Motivations', 'number': '2'}"
Anchors are lexical elements describing the context of a sentence.,"{'title': '2 Background and Motivations', 'number': '2'}"
"Templates that are extracted from different sentences and connect the same anchors in these sentences, are assumed to paraphrase each other.","{'title': '2 Background and Motivations', 'number': '2'}"
"For example, the sentences “Yahoo bought Overture” and “Yahoo acquired Overture” share the anchors {X=Yahoo, Y=Overture}, suggesting that the templates ‘X buy Y’ and ‘X acquire Y’ paraphrase each other.","{'title': '2 Background and Motivations', 'number': '2'}"
"Algorithms for paraphrase acquisition address two problems: (a) finding matching anchors and (b) identifying template structure, as reviewed in the next two subsections.","{'title': '2 Background and Motivations', 'number': '2'}"
"The prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event.","{'title': '2 Background and Motivations', 'number': '2'}"
"To facilitate finding many matching sentences, highly redundant comparable corpora have been used.","{'title': '2 Background and Motivations', 'number': '2'}"
"These include multiple translations of the same text (Barzilay and McKeown, 2001) and corresponding articles from multiple news sources (Shinyama et al., 2002; Pang et al., 2003; Barzilay and Lee, 2003).","{'title': '2 Background and Motivations', 'number': '2'}"
"While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability.","{'title': '2 Background and Motivations', 'number': '2'}"
"Avoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus.","{'title': '2 Background and Motivations', 'number': '2'}"
"Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required.","{'title': '2 Background and Motivations', 'number': '2'}"
"Naturally, the largest available corpus is the Web.","{'title': '2 Background and Motivations', 'number': '2'}"
"Since exhaustive processing of the Web is not feasible, (Duclaye et al., 2002) and (Ravichandran and Hovy, 2002) attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of (Riloff and Jones, 1999).","{'title': '2 Background and Motivations', 'number': '2'}"
These methods start with a provided known set of anchors for a target meaning.,"{'title': '2 Background and Motivations', 'number': '2'}"
"For example, the known anchor set {Mozart, 1756} is given as input in order to find paraphrases for the template ‘X born in Y’.","{'title': '2 Background and Motivations', 'number': '2'}"
"Web searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one (“born in”).","{'title': '2 Background and Motivations', 'number': '2'}"
"These new templates are then exploited to get new anchor sets, which are subsequently processed as the initial {Mozart, 1756}.","{'title': '2 Background and Motivations', 'number': '2'}"
"Eventually, the overall procedure results in an iterative process able to induce templates from anchor sets and vice versa.","{'title': '2 Background and Motivations', 'number': '2'}"
The limitation of this approach is the requirement for one input anchor set per target meaning.,"{'title': '2 Background and Motivations', 'number': '2'}"
Preparing such input for all possible meanings in broad domains would be a huge task.,"{'title': '2 Background and Motivations', 'number': '2'}"
"As will be explained below, our method avoids this limitation by finding all anchor sets automatically in an unsupervised manner.","{'title': '2 Background and Motivations', 'number': '2'}"
"Finally, (Lin and Pantel, 2001) present a notably different approach that relies on matching separately single anchors.","{'title': '2 Background and Motivations', 'number': '2'}"
They limit the allowed structure of templates only to paths in dependency parses connecting two anchors.,"{'title': '2 Background and Motivations', 'number': '2'}"
"The algorithm constructs for each possible template two feature vectors, representing its co-occurrence statistics with the two anchors.","{'title': '2 Background and Motivations', 'number': '2'}"
Two templates with similar vectors are suggested as paraphrases (termed inference rule).,"{'title': '2 Background and Motivations', 'number': '2'}"
Matching of single anchors relies on the general distributional similarity principle and unlike the other methods does not require redundancy of sets of multiple anchors.,"{'title': '2 Background and Motivations', 'number': '2'}"
"Consequently, a much larger number of paraphrases can be found in a regular corpus.","{'title': '2 Background and Motivations', 'number': '2'}"
"Lin and Pantel report experiments for 9 templates, in which their system extracted 10 correct inference rules on average per input template, from 1GB of news data.","{'title': '2 Background and Motivations', 'number': '2'}"
"Yet, this method also suffers from certain limitations: (a) it identifies only templates with pre-specified structures; (b) accuracy seems more limited, due to the weaker notion of similarity; and (c) coverage is limited to the scope of an available corpus.","{'title': '2 Background and Motivations', 'number': '2'}"
"To conclude, several approaches exhaustively process different types of corpora, obtaining varying scales of output.","{'title': '2 Background and Motivations', 'number': '2'}"
"On the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints.","{'title': '2 Background and Motivations', 'number': '2'}"
Paraphrasing approaches learn different kinds of template structures.,"{'title': '2 Background and Motivations', 'number': '2'}"
"Interesting algorithms are presented in (Pang et al., 2003; Barzilay and Lee, 2003).","{'title': '2 Background and Motivations', 'number': '2'}"
They learn linear patterns within similar contexts represented as finite state automata.,"{'title': '2 Background and Motivations', 'number': '2'}"
"Three classes of syntactic template learning approaches are presented in the literature: learning ofpredicate argument templates (Yangarber et al., 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al., 2003).","{'title': '2 Background and Motivations', 'number': '2'}"
The last approach is the most general with respect to the template form.,"{'title': '2 Background and Motivations', 'number': '2'}"
"However, its processing time increases exponentially with the size of the templates.","{'title': '2 Background and Motivations', 'number': '2'}"
"As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process.","{'title': '2 Background and Motivations', 'number': '2'}"
"Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of meanings, while requiring minimal input and (b) Keeping template structures as general as possible.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
To address the first goal we require as input only a phrasal lexicon of the relevant domain (including single words and multiword expressions).,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Broad coverage lexicons are widely available or may be constructed using known term acquisition techniques, making it a feasible and scalable input requirement.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We then aim to acquire entailment relations that include any of the lexicon’s entries.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The second goal is addressed by a novel algorithm for extracting the most general templates being justified by the data.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"For each lexicon entry, denoted a pivot, our extraction method performs two phases: (a) extract promising anchor sets for that pivot (ASE, Section 3.1), and (b) from sentences containing the anchor sets, extract templates for which an entailment relation holds with the pivot (TE, Section 3.2).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Examples for verb pivots are: ‘acquire’, ‘fall to’, ‘prevent’.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We will use the pivot ‘prevent’ for examples through this section.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Before presenting the acquisition method we first define its output.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"A template is a dependency parsetree fragment, with variable slots at some tree nodes (e.g.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
‘X s+_ prevent � Y’).,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"An entailment relation between two templates T1 and T2 holds if the meaning of T2 can be inferred from the meaning of T1 (or vice versa) in some contexts, but not necessarily all, under the same variable instantiation.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"For example, ‘X s+ prevent 0* Y’ entails ‘X s+_ reduce -* Y risk’ because the sentence “aspirin reduces heart attack risk” can be inferred from “aspirin prevents a first heart attack”.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Our output consists of pairs of templates for which an entailment relation holds.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The goal of this phase is to find a substantial number of promising anchor sets for each pivot.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
A good anchor-set should satisfy a proper balance between specificity and generality.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"On one hand, an anchor set should correspond to a sufficiently specific setting, so that entailment would hold between its different occurrences.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"On the other hand, it should be sufficiently frequent to appear with different entailing templates.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Finding good anchor sets based on just the input pivot is a hard task.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Most methods identify good repeated anchors “in retrospect”, that is after processing a full corpus, while previous Web-based methods require at least one good anchor set as input.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Given our minimal input, we needed refined criteria that identify a priori the relatively few promising anchor sets within a sample of pivot occurrences.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The ASE algorithm (presented in Figure 1) performs 4 main steps.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (1) creates a complete template, called the pivot template and denoted Tp, for the input pivot, denoted P. Variable slots are added for the major types of syntactic relations that interact with P, based on its syntactic type.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
These slots enable us to later match Tp with other templates.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"For verbs, we add slots for a subject and for an object or a modifier (e.g.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
‘X s+_ prevent � Y’).,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (2) constructs asample corpus, denoted S, for the pivot template.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (2.A) utilizes a Web search engine to initialize S by retrieving sentences containing P. The sentences are parsed by the MINIPAR dependency parser (Lin, 1998), keeping only sentences that contain the complete syntactic template Tp (with all the variables instantiated).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (2.B) identifies phrases that are statistically associated with Tp in S. We test all noun-phrases in S , discarding phrases that are too common on the Web (absolute frequency higher than a threshold MAXPHRASEF), such as “desire”.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Then we select the N phrases with highest tf ·idf score1.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"These phrases have a strong collocation relationship with the pivot P and are likely to indicate topical (rather than anecdotal) occurrences of P. For example, the phrases “patient” and “American Dental Association”, which indicate contexts of preventing health problems, were selected for the pivot ‘prevent’.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Fi1Here, tf ·idf = freqS(X) · log (freqN (X) ) where freqS(X) is the number of occurrences in S containing X, N is the total number of Web documents, and freqW (X) is the number of Web documents containing X. nally, STEP (2.C) expands S by querying the Web with the both P and each of the associated phrases, adding the retrieved sentences to S as in step (2.a).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
STEP (3) extracts candidate anchor sets for Tp.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"From each sentence in S we try to generate one candidate set, containing noun phrases whose Web frequency is lower than MAXPHRASEF.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
STEP (3.A) extracts slot anchors – phrases that instantiate the slot variables of Tp.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Each anchor is marked with the corresponding slot.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"For example, the anchors {antibioticssubj← , miscarriage obj←} were extracted from the sentence “antibiotics in pregnancy prevent miscarriage”.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (3.B) tries to extend each candidate set with one additional context anchor, in order to improve its specificity.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"This anchor is chosen as the highest tf ·idf scoring phrase in the sentence, if it exists.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"In the previous example, ‘pregnancy’ is selected.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
STEP (4) filters out bad candidate anchor sets by two different criteria.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (4.A) maintains only candidates with absolute Web frequency within a threshold range [MINSETF, MAXSETF], to guarantee an appropriate specificity-generality level.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"STEP (4.B) guarantees sufficient (directional) association between the candidate anchor set c and Tp, by estimating where freqW is Web frequency and P is the pivot.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"We maintain only candidates for which this probability falls within a threshold range [SETMINP, SETMAXP].","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Higher probability often corresponds to a strong linguistic collocation between the candidate and Tp, without any semantic entailment.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Lower probability indicates coincidental cooccurrence, without a consistent semantic relation.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"The remaining candidates in S become the input anchor-sets for the template extraction phase, for example, {Aspirinsubj← , heart attackobj←} for ‘prevent’.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The Template Extraction algorithm accepts as its input a list of anchor sets extracted from ASE for each pivot template.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Then, TE generates a set of syntactic templates which are supposed to maintain an entailment relationship with the initial pivot template.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"TE performs three main steps, described in the following subsections: For each input anchor set, TE acquires from the Web a sample corpus of sentences containing it.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"For example, a sentence from the sample corpus for {aspirin, heart attack} is: “Aspirin stops heart attack?”.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"All of the sample sentences are then parsed with MINIPAR (Lin, 1998), which generates from each sentence a syntactic directed acyclic graph (DAG) representing the dependency structure of the sentence.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Each vertex in this graph is labeled with a word and some morphological information; each graph edge is labeled with the syntactic relation between the words it connects.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
TE then substitutes each slot anchor (see section 3.1) in the parse graphs with its corresponding slot variable.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Therefore, “Aspirin stops heart attack?” will be transformed into ‘X stop Y’.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
This way all the anchors for a certain slot are unified under the same variable name in all sentences.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"The parsed sentences related to all of the anchor sets are subsequently merged into a single set of parse graphs S = {P1, P2, ... , Pn} (see P1 and P2 in Figure 2).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The core of TE is a General Structure Learning algorithm (GSL) that is applied to the set of parse graphs S resulting from the previous step.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"GSL extracts single-rooted syntactic DAGs, which are named spanning templates since they must span at least over Na slot variables, and should also appear in at least Nr sentences from S (In our experiments we set Na=2 and Nr=2).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"GSL learns maximal most general templates: they are spanning templates which, at the same time, (a) cannot be generalized by further reduction and (b) cannot be further extended keeping the same generality level.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"In order to properly define the notion of maximal most general templates, we introduce some formal definitions and notations.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"DEFINITION: For a spanning template t we define a sentence set, denoted with σ(t), as the set of all parsed sentences in S containing t. For each pair of templates t1 and t2, we use the notation t1 :� t2 to denote that t1 is included as a subgraph or is equal to t2.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We use the notation t1 ≺ t2 when such inclusion holds strictly.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We define T(S) as the set of all spanning templates in the sample S. DEFINITION: A spanning template t E T (S) is maximal most general if and only if both of the following conditions hold: Condition A ensures that the extracted templates do not contain spanning sub-structures that are more ”general” (i.e. having a larger sentence set); condition B ensures that the template cannot be further enlarged without reducing its sentence set.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
GSL performs template extraction in two main steps: (1) build a compact graph representation of all the parse graphs from S; (2) extract templates from the compact representation.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
A compact graph representation is an aggregate graph which joins all the sentence graphs from S ensuring that all identical spanning sub-structures from different sentences are merged into a single one.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Therefore, each vertex v (respectively, edge e) in the aggregate graph is either a copy of a corresponding vertex (edge) from a sentence graph Pi or it represents the merging of several identically labeled vertices (edges) from different sentences in S. The set of such sentences is defined as the sentence set of v (e), and is represented through the set of index numbers of related sentences (e.g.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"“(1,2)” in the third tree of Figure 2).","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We will denote with Gi the compact graph representation of the first i sentences in S. The parse trees P1 and P2 of two sentences and their related compact representation G2 are shown in Figure 2.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Building the compact graph representation The compact graph representation is built incrementally.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The algorithm starts with an empty aggregate graph G0 and then merges the sentence graphs from S one at a time into the aggregate structure.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Let’s denote the current aggregate graph with Gi_1(Vg, Eg) and let Pi(Vp, Ep) be the parse graph which will be merged next.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
Note that the sentence set of Pi is a single element set W. During each iteration a new graph is created as the union of both input graphs: Gi = Gi_1 U Pi.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Then, the following merging procedure is performed on the elements of Gi ated and added to Gi.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The new vertex takes the same label and holds a sentence set which is formed from the sentence set of vg by adding i to it.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Still with reference to Figure 2, the generalized vertices in G2 are ‘X’, ‘Y’ and ‘stop’.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
The algorithm connects the generalized vertex vnew g with all the vertices which are connected with vg and vp.,"{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"As an optimization step, we merge only vertices and edges that are included in equal spanning templates.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Extracting the templates GSL extracts all maximal most general templates from the final compact representation Gn using the following sub-algorithm: In Figure 2 the maximal most general template in obj As a last step, names and numbers are filtered out from the templates.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Moreover, TE removes those templates which are very long or which appear with just one anchor set and in less than four sentences.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
"Finally, the templates are sorted first by the number of anchor sets with which each template appeared, and then by the number of sentences in which they appeared.","{'title': '3 The TE/ASE Acquisition Method', 'number': '3'}"
We evaluated the results of the TE/ASE algorithm on a random lexicon of verbal forms and then assessed its performance on the extracted data through human-based judgments.,"{'title': '4 Evaluation', 'number': '4'}"
The test set for human evaluation was generated by picking out 53 random verbs from the 1000 most frequent ones found in a subset of the Reuters corpus2.,"{'title': '4 Evaluation', 'number': '4'}"
"For each verb entry in the lexicon, we provided the judges with the corresponding pivot template and the list of related candidate entailment templates found by the system.","{'title': '4 Evaluation', 'number': '4'}"
"The judges were asked to evaluate entailment for a total of 752 templates, extracted for 53 pivot lexicon entries; Table 1 shows a sample of the evaluated templates; all of them are clearly good and were judged as correct ones. included in the evaluation test set.","{'title': '4 Evaluation', 'number': '4'}"
"Concerning the ASE algorithm, threshold parameters3 were set as PHRASEMAXF=107, SETMINF=102, SETMAXF=105, SETMINP=0.066, and SETMAXP=0.666.","{'title': '4 Evaluation', 'number': '4'}"
An upper limit of 30 was imposed on the number of possible anchor sets used for each pivot.,"{'title': '4 Evaluation', 'number': '4'}"
"Since this last value turned out to be very conservative with respect to system coverage, we subsequently attempted to relax it to 50 (see Discussion in Section 4.3).","{'title': '4 Evaluation', 'number': '4'}"
Further post-processing was necessary over extracted data in order to remove syntactic variations referring to the same candidate template (typically passive/active variations).,"{'title': '4 Evaluation', 'number': '4'}"
Three possible judgment categories have been considered: Correct if an entailment relationship in at least one direction holds between the judged template and the pivot template in some non-bizarre context; Incorrect if there is no reasonable context and variable instantiation in which entailment holds; No Evaluation if the judge cannot come to a definite conclusion.,"{'title': '4 Evaluation', 'number': '4'}"
"Each of the three assessors (referred to as J#1, J#2, and J#3) issued judgments for the 752 different templates.","{'title': '4 Evaluation', 'number': '4'}"
"Correct templates resulted to be 283, 313, and 295 with respect to the three judges.","{'title': '4 Evaluation', 'number': '4'}"
"No evaluation’s were 2, 0, and 16, while the remaining templates were judged Incorrect.","{'title': '4 Evaluation', 'number': '4'}"
"For each verb, we calculate Yield as the absolute number of Correct templates found and Precision as the percentage of good templates out of all extracted templates.","{'title': '4 Evaluation', 'number': '4'}"
"Obtained Precision is 44.15%, averaged over the 53 verbs and the 3 judges.","{'title': '4 Evaluation', 'number': '4'}"
"Considering Low Majority on judges, the precision value is 42.39%.","{'title': '4 Evaluation', 'number': '4'}"
Average Yield was 5.5 templates per verb.,"{'title': '4 Evaluation', 'number': '4'}"
"These figures may be compared (informally, as data is incomparable) with average yield of 10.1 and average precision of 50.3% for the 9 “pivot” templates of (Lin and Pantel, 2001).","{'title': '4 Evaluation', 'number': '4'}"
The comparison suggests that it is possible to obtain from the (very noisy) web a similar range of precision as was obtained from a clean news corpus.,"{'title': '4 Evaluation', 'number': '4'}"
"It also indicates that there is potential for acquiring additional templates per pivot, which would require further research on broadening efficiently the search for additional web data per pivot.","{'title': '4 Evaluation', 'number': '4'}"
"Agreement among judges is measured by the Kappa value, which is 0.55 between J#1 and J#2, 0.57 between J#2 and J#3, and 0.63 between J#1 and J#3.","{'title': '4 Evaluation', 'number': '4'}"
Such Kappa values correspond to moderate agreement for the first two pairs and substantial agreement for the third one.,"{'title': '4 Evaluation', 'number': '4'}"
"In general, unanimous agreement among all of the three judges has been reported on 519 out of 752 templates, which corresponds to 69%.","{'title': '4 Evaluation', 'number': '4'}"
"Our algorithm obtained encouraging results, extracting a considerable amount of interesting templates and showing inherent capability of discovering complex semantic relations.","{'title': '4 Evaluation', 'number': '4'}"
"Concerning overall coverage, we managed to find correct templates for 86% of the verbs (46 out of 53).","{'title': '4 Evaluation', 'number': '4'}"
"Nonetheless, presented results show a substantial margin of possible improvement.","{'title': '4 Evaluation', 'number': '4'}"
"In fact yield values (5.5 Low Majority, up to 24 in best cases), which are our first concern, are inherently dependent on the breadth of Web search performed by the ASE algorithm.","{'title': '4 Evaluation', 'number': '4'}"
"Due to computational time, the maximal number of anchor sets processed for each verb was held back to 30, significantly reducing the amount of retrieved data.","{'title': '4 Evaluation', 'number': '4'}"
"In order to further investigate ASE potential, we subsequently performed some extended experiment trials raising the number of anchor sets per pivot to 50.","{'title': '4 Evaluation', 'number': '4'}"
This time we randomly chose a subset of 10 verbs out of the less frequent ones in the original main experiment.,"{'title': '4 Evaluation', 'number': '4'}"
Results for these verbs in the main experiment were an average Yield of 3 and an average Precision of 45.19%.,"{'title': '4 Evaluation', 'number': '4'}"
"In contrast, the extended experiments on these verbs achieved a 6.5 Yield and 59.95% Precision (average values).","{'title': '4 Evaluation', 'number': '4'}"
"These results are indeed promising, and the substantial growth in Yield clearly indicates that the TE/ASE algorithms can be further improved.","{'title': '4 Evaluation', 'number': '4'}"
"We thus suggest that the feasibility of our approach displays the inherent scalability of the TE/ASE process, and its potential to acquire a large entailment relation KB using a full scale lexicon.","{'title': '4 Evaluation', 'number': '4'}"
A further improvement direction relates to template ranking and filtering.,"{'title': '4 Evaluation', 'number': '4'}"
"While in this paper we considered anchor sets to have equal weights, we are also carrying out experiments with weights based on cross-correlation between anchor sets.","{'title': '4 Evaluation', 'number': '4'}"
We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.,"{'title': '5 Conclusions', 'number': '5'}"
"This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.","{'title': '5 Conclusions', 'number': '5'}"
This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.,"{'title': '5 Conclusions', 'number': '5'}"
The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.,"{'title': '5 Conclusions', 'number': '5'}"
"These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.","{'title': '5 Conclusions', 'number': '5'}"
"From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.","{'title': '5 Conclusions', 'number': '5'}"
"Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.","{'title': '5 Conclusions', 'number': '5'}"
"Together with the fact that we expect to find entailment relations for about 85% of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.","{'title': '5 Conclusions', 'number': '5'}"
"In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.","{'title': '5 Conclusions', 'number': '5'}"
"We also plan to support noun phrases as input, in addition to verb phrases.","{'title': '5 Conclusions', 'number': '5'}"
"Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.","{'title': '5 Conclusions', 'number': '5'}"
"Like (Lin and Pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.","{'title': '5 Conclusions', 'number': '5'}"
"As stated, we learn entailment relations holding for some, but not necessarily all, contexts.","{'title': '5 Conclusions', 'number': '5'}"
In future work we also plan to find the valid contexts for entailment relations.,"{'title': '5 Conclusions', 'number': '5'}"
"The authors would like to thank Oren Glickman (Bar Ilan University) for helpful discussions and assistance in the evaluation, Bernardo Magnini for his scientific supervision at ITC-irst, Alessandro Vallin and Danilo Giampiccolo (ITC-irst) for their help in developing the human based evaluation, and Prof. Yossi Matias (Tel-Aviv University) for supervising the first author.","{'title': 'Acknowledgements', 'number': '6'}"
"This work was partially supported by the MOREWEB project, financed by Provincia Autonoma di Trento.","{'title': 'Acknowledgements', 'number': '6'}"
"It was also partly carried out within the framework of the ITC-IRST (TRENTO, ITALY) – UNIVERSITY OF HAIFA (ISRAEL) collaboration project.","{'title': 'Acknowledgements', 'number': '6'}"
For data visualization and analysis the authors intensively used the CLARK system (www.bultreebank.org) developed at the Bulgarian Academy of Sciences.,"{'title': 'Acknowledgements', 'number': '6'}"
