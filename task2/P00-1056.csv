col1,col2
"In this paper, we present and compare various single-word based alignment models for statistical machine translation.",Introduction
"We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.",Introduction
We present different methods to combine alignments.,Introduction
As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.,Introduction
"We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.",Introduction
In statistical machine translation we set up a statistical translation model Pr(fillef) which describes the relationship between a source language (SL) string f and a target language (TL) string ef.,Experiment/Discussion
"In (statistical) alignment models Pr(fil , aflef), a 'hidden' alignment a is introduced which describes a mapping from source word fi to a target word Ca,.",Experiment/Discussion
"We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).",Experiment/Discussion
"The different alignment models we present provide different decompositions of Pr(fil ,41e{).",Experiment/Discussion
An alignment Ã¢ for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model.,Experiment/Discussion
"So far, no well established evaluation criterion exists in the literature for these alignment models.",Experiment/Discussion
"For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.",Experiment/Discussion
"Using translation quality is problematic, as translation quality is not well defined and as there are additional influences such as language model or decoder properties.",Experiment/Discussion
We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.,Experiment/Discussion
"This allows an automatic evaluation, once a reference alignment has been produced.",Experiment/Discussion
"In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models.",Experiment/Discussion
"In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).",Experiment/Discussion
"All these models provide different decompositions of the probability Pr(fil ,41e{).",Experiment/Discussion
The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.,Experiment/Discussion
All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment.,Experiment/Discussion
We now sketch the structure of the six models: cient as they waste probability mass on non-strings.,Experiment/Discussion
IBM-5 is a reformulation of IBM-4 with a suitably refined alignment model in order to avoid deficiency.,Experiment/Discussion
"So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.",Experiment/Discussion
"For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.",Experiment/Discussion
"In the HMM alignment model we allow for a dependence from the class E = C(ea,_,).",Experiment/Discussion
"Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).",Experiment/Discussion
"The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).",Experiment/Discussion
"The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .",Results/Conclusion
.,Results/Conclusion
.,Results/Conclusion
",S .",Results/Conclusion
"In the Estep the counts for one sentence pair (f, e) are calculated.",Results/Conclusion
"For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.",Results/Conclusion
"The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-step it is possible to efficiently consider all alignments.",Results/Conclusion
"For the HMM we do this using the Baum-Welch algorithm (Baum, 1972).",Results/Conclusion
"Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments.",Results/Conclusion
"For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments.",Results/Conclusion
In order to keep the training fast we can take into account only a small fraction of all alignments.,Results/Conclusion
We will compare three different possibilities of using subsets of different size:,Results/Conclusion
"This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268).",Results/Conclusion
