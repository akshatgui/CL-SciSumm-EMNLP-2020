col1,col2
"In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.",{}
"We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.",{}
We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.,{}
The evaluation shows that our model achieves better readability scores than a set of baseline systems.,{}
Sentence simplification transforms long and dif ficult sentences into shorter and more readable ones.,"{'title': 'Introduction', 'number': '1'}"
This helps humans read texts more easilyand faster.,"{'title': 'Introduction', 'number': '1'}"
"Reading assistance is thus an important application of sentence simplification, espe cially for people with reading disabilities (Carrollet al, 1999; Inui et al, 2003), low-literacy read ers (Watanabe et al, 2009), or non-native speakers (Siddharthan, 2002).Not only human readers but also NLP applications can benefit from sentence simplification.","{'title': 'Introduction', 'number': '1'}"
"The original motivation for sentence simplification is using it as a preprocessor to facili tate parsing or translation tasks (Chandrasekar et al., 1996).","{'title': 'Introduction', 'number': '1'}"
Complex sentences are considered as stumbling blocks for such systems.,"{'title': 'Introduction', 'number': '1'}"
"More recently,sentence simplification has also been shown help ful for summarization (Knight and Marcu, 2000), ? This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) underthe grant No.","{'title': 'Introduction', 'number': '1'}"
"GU 798/3-1, and by the Volkswagen Founda tion as part of the Lichtenberg-Professorship Program under the grant No.","{'title': 'Introduction', 'number': '1'}"
"I/82806.sentence fusion (Filippova and Strube, 2008b), se mantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009).At sentence level, reading difficulty stems either from lexical or syntactic complexity.","{'title': 'Introduction', 'number': '1'}"
"Sen tence simplification can therefore be classifiedinto two types: lexical simplification and syntac tic simplification (Carroll et al, 1999).","{'title': 'Introduction', 'number': '1'}"
These two types of simplification can be further implemented by a set of simplification operations.,"{'title': 'Introduction', 'number': '1'}"
"Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations.","{'title': 'Introduction', 'number': '1'}"
The splitting operation splits a long sentence intoseveral shorter sentences to decrease the complex ity of the long sentence.,"{'title': 'Introduction', 'number': '1'}"
The dropping operation further removes unimportant parts of a sentence to make it more concise.,"{'title': 'Introduction', 'number': '1'}"
"The reordering operationinterchanges the order of the split sentences (Sid dharthan, 2006) or parts in a sentence (Watanabeet al, 2009).","{'title': 'Introduction', 'number': '1'}"
"Finally, the substitution operation re places difficult phrases or words with their simpler synonyms.In most cases, different simplification operations happen simultaneously.","{'title': 'Introduction', 'number': '1'}"
It is therefore nec essary to consider the simplification process as a combination of different operations and treatthem as a whole.,"{'title': 'Introduction', 'number': '1'}"
"However, most of the existing models only consider one of these operations.","{'title': 'Introduction', 'number': '1'}"
"Siddharthan (2006) and Petersen and Ostendorf (2007) focus on sentence splitting, while sen tence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation.","{'title': 'Introduction', 'number': '1'}"
"As faras lexical simplification is concerned, word substitution is usually done by selecting simpler syn onyms from Wordnet based on word frequency (Carroll et al, 1999).In this paper, we propose a sentence simplifica tion model by tree transformation which is based 1353 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada andKnight, 2002; Graehl et al, 2008).","{'title': 'Introduction', 'number': '1'}"
"Our model in tegrally covers splitting, dropping, reordering and phrase/word substitution.","{'title': 'Introduction', 'number': '1'}"
The parameters of ourmodel can be efficiently learned from complex simple parallel datasets.,"{'title': 'Introduction', 'number': '1'}"
The transformation froma complex sentence to a simple sentence is con ducted by applying a sequence of simplification operations.,"{'title': 'Introduction', 'number': '1'}"
An expectation maximization (EM) algorithm is used to iteratively train our model.,"{'title': 'Introduction', 'number': '1'}"
We also propose a method based on monolingualword mapping which speeds up the training pro cess significantly.,"{'title': 'Introduction', 'number': '1'}"
"Finally, a decoder is designed to generate the simplified sentences using a greedy strategy and integrates language models.In order to train our model, we further com pile a large-scale complex-simple parallel dataset(PWKP) from Simple English Wikipedia1 and En glish Wikipedia2, as such datasets are rare.We organize the remainder of the paper as follows: Section 2 describes the PWKP dataset.","{'title': 'Introduction', 'number': '1'}"
Sec tion 3 presents our TSM model.,"{'title': 'Introduction', 'number': '1'}"
"Sections 4 and 5 are devoted to training and decoding, respectively.","{'title': 'Introduction', 'number': '1'}"
Section 6 details the evaluation.,"{'title': 'Introduction', 'number': '1'}"
The conclusions follow in the final section.,"{'title': 'Introduction', 'number': '1'}"
We collected a paired dataset from the English Wikipedia and Simple English Wikipedia.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
The targeted audience of Simple Wikipedia includes?children and adults who are learning English lan guage?.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
The authors are requested to ?use easy words and short sentences?,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
to compose articles.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"We processed the dataset as follows: Article Pairing 65,133 articles from SimpleWikipedia3 and Wikipedia4 were paired by fol lowing the ?language link?","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
using the dump filesin Wikimedia.5 Administration articles were fur ther removed.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"Plain Text Extraction We use JWPL (Zesch etal., 2008) to extract plain texts from Wikipedia ar ticles by removing specific Wiki tags.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"Pre-processing including sentence boundary detection and tokenization with the Stanford 1http://simple.wikipedia.org 2http://en.wikipedia.org 3As of Aug 17th, 2009 4As of Aug 22nd, 2009 5http://download.wikimedia.org Parser package (Klein and Manning, 2003), and lemmatization with the TreeTagger (Schmid, 1994).","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"Monolingual Sentence Alignment As we need a parallel dataset algned at the sentence level,we further applied monolingual sentence align ment on the article pairs.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"In order to achieve the best sentence alignment on our dataset, we tested three similarity measures: (i) sentence-level TF*IDF (Nelken and Shieber, 2006), (ii) word overlap (Barzilay and Elhadad, 2003) and (iii)word-based maximum edit distance (MED) (Lev enshtein, 1966) with costs of insertion, deletionand substitution set to 1.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
To evaluate their perfor mance we manually annotated 120 sentence pairs from the article pairs.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
Tab.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
1 reports the precision and recall of these three measures.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
We manually adjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre viously adopted by Nelken and Shieber (2006).,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
Similarity Precision Recall TF*IDF 91.3% 55.4% Word Overlap 50.5% 55.1% MED 13.9% 54.7% Table 1: Monolingual Sentence Alignment The results in Tab.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"1 show that sentence-levelTF*IDF clearly outperforms the other two mea sures, which is consistent with the results reported by Nelken and Shieber (2006).","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
We henceforth chose sentence-level TF*IDF to align our dataset.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
As shown in Tab.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"2, PWKP contains more than 108k sentence pairs.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
The sentences from Wikipedia and Simple Wikipedia are considered as ?complex?,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
and ?simple?,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
respectively.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"Both the average sentence length and average token length in Simple Wikipedia are shorter than those inWikipedia, which is in compliance with the pur pose of Simple Wikipedia.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
Avg.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
Sen. Len Avg.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
Tok.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"Len #Sen.Pairscomplex simple complex simple 25.01 20.87 5.06 4.89 108,016 Table 2: Statistics for the PWKP datasetIn order to account for sentence splitting, we al low 1 to n sentence alignment to map one complexsentence to several simple sentences.","{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
We first per form 1 to 1 mapping with sentence-level TF*IDF and then combine the pairs with the same complex sentence and adjacent simple sentences.,"{'title': 'Wikipedia Dataset: PWKP. ', 'number': '2'}"
"We apply the following simplification operations to the parse tree of a complex sentence: splitting, 1354dropping, reordering and substitution.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In this sec tion, we use a running example to illustrate thisprocess.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
c is the complex sentence to be simpli fied in our example.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Fig.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
1 shows the parse tree of c (we skip the POS level).c: August was the sixth month in the ancient Ro man calendar which started in 735BC.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar whichthe Roman month Figure 1: Parse Tree of c 3.1 Splitting.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"The first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen tation, which decides where and whether to split a sentence and (ii) completion, which makes the new split sentences complete.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"First, we decide where we can split a sentence.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our model, the splitting point is judged by the syntactic constituent of the split boundary word in the complex sentence.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The decision whether a sentence should be split is based on the length of the complex sentence.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The features used in the segmentation step are shown in Tab.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
3.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Word Constituent iLength isSplit Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
?which?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
SBAR 1 true 0.0016 ?which?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
SBAR 1 false 0.9984 ?which?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
SBAR 2 true 0.0835 ?which?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"SBAR 2 false 0.9165 Table 3: Segmentation Feature Table (SFT) Actually, we do not use the direct constituent of a word in the parse tree.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our example, the directconstituent of the word ?which?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
is ?WHNP?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In stead, we use Alg.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
1 to calculate the constituentof a word.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Alg.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
1 returns ?SBAR?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
as the adjusted constituent for ?which?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"Moreover, di rectly using the length of the complex sentenceis affected by the data sparseness problem.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In stead, we use iLength as the feature which is calculated as iLength = ceiling( comLengthavgSimLength), where comLength is the length of the complex sentence and avgSimLength is the average length of simple sentences in the training dataset.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The ?Prob.?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
column shows the probabilities obtained after training on our dataset.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"Algorithm 1 adjustConstituent(word, tree) constituent?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
word.father; father ? constituent.father; while father 6= NULL AND constituent is the most left child of father do constituent?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"father; father ? father.father; end while return constituent; In our model, one complex sentence can be split into two or more sentences.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"Since many splitting operations are possible, we need to select the mostlikely one.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The probability of a segmentation op eration is calculated as: P (seg|c) = ? w:c SFT (w|c) (1) where w is a word in the complex sentence c and SFT (w|c) is the probability of the word w in the Segmentation Feature Table (SFT); Fig.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
2 shows a possible segmentation result of our example.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
NP VP S August was NPinsixththe SBAR NP NP PP WHNP S VP started PP in 735BC ancient calendar which the Roman month Figure 2: Segmentation The second step is completion.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In this step, we try to make the split sentences complete and grammatical.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our example, to make the second sentence ?which started in 735BC?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
complete and grammatical we should first drop the border word ?which?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
and then copy the dependent NP ?the ancient Roman calendar?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
to the left of ?started?to obtain the complete sentence ?the ancient Ro man calendar started in 735BC?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our model, whether the border word should be dropped or retained depends on two features of the border word: the direct constituent of the word and the word itself, as shown in Tab.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
4.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Const.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Word isDropped Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"WHNP which True 1.0 WHNP which False Prob.Min Table 4: Border Drop Feature Table (BDFT) In order to copy the necessary parts to complete the new sentences, we must decide which parts should be copied and where to put these parts in the new sentences.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our model, this is judged by two features: the dependency relation and theconstituent.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
We use the Stanford Parser for parsing the dependencies.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our example, the de 1355pendency relation between ?calendar?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
in the com plex sentence and the verb ?started?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
in the secondsplit sentence is ?gov nsubj?.6 The direct constituent of ?started?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
is ?VP?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
and the word ?calen dar?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
should be put on the ?left?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"of ?started?, see Tab.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
5.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Dep.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Const.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
isCopied Pos.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"gov nsubj VP(VBD) True left 0.9000 gov nsubj VP(VBD) True right 0.0994 gov nsubj VP(VBD) False - 0.0006 Table 5: Copy Feature Table (CFT) For dependent NPs, we copy the whole NP phrase rather than only the head noun.7 In ourexample, we copy the whole NP phrase ?the an cient Roman calendar?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
to the new position rather than only the word ?calendar?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The probability of a completion operation can be calculated as P (com|seg) = Y bw:s BDFT (bw|s) Y w:s Y dep:w CFT (dep).,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"where s are the split sentences, bw is a border word in s, w is a word in s, dep is a dependency of w which is out of the scope of s. Fig.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
3 shows the most likely result of the completion operation for our example.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendarthe RomanNP ancient calendarthe Roman month Figure 3: Completion 3.2 Dropping and Reordering.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
We first apply dropping and then reordering to each non-terminal node in the parse tree from topto bottom.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"We use the same features for both drop ping and reordering: the node?s direct constituent and its children?s constituents pattern, see Tab.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
6 and Tab.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
7.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Constituent Children Drop Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"NP DT JJ NNP NN 1101 7.66E-4 NP DT JJ NNP NN 0001 1.26E-7 Table 6: Dropping Feature Table (DFT) 6With Stanford Parser, ?which?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
is a referent of ?calender?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
and the nsubj of ?started?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
?calender?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
thus can be considered to be the nsubj of ?started?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
with ?started?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
as the governor.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
7The copied NP phrase can be further simplified in the following steps.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Constituent Children Reorder Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
NP DT JJ NN 012 0.8303 NP DT JJ NN 210 0.0039 Table 7: Reordering Feature Table (RFT)The bits ?1?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
and ?0?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
in the ?Drop?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
column indicate whether the corresponding constituent is re tained or dropped.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The number in the ?Reorder?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
column represents the new order for the children.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The probabilities of the dropping and reordering operations can be calculated as Equ.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
2 and Equ.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
3.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"P (dp|node) = DFT (node) (2) P (ro|node) = RFT (node) (3) In our example, one of the possible results is dropping the NNP ?Roman?, as shown in Fig.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
4.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC ancient calendartheNP ancient calendarthe month Figure 4: Dropping & Reordering 3.3 Substitution.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
3.3.1 Word SubstitutionWord substitution only happens on the termi nal nodes of the parse tree.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In our model, the conditioning features include the original word and the substitution.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The substitution for a word can be another word or a multi-word expression(see Tab.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
8).,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The probability of a word substitu tion operation can be calculated as P (sub|w) = SubFT (Substitution|Origin).,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Origin Substitution Prob.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
ancient ancient 0.963 ancient old 0.0183 ancient than transport 1.83E-102 old ancient 0.005 Table 8: Substitution Feature Table (SubFT) 3.3.2 Phrase SubstitutionPhrase substitution happens on the non terminal nodes and uses the same conditioningfeatures as word substitution.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The ?Origin?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
con sists of the leaves of the subtree rooted at the node.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"When we apply phrase substitution on anon-terminal node, then any simplification operation (including dropping, reordering and substitu tion) cannot happen on its descendants any more 1356 because when a node has been replaced then its descendants are no longer existing.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"Therefore, for each non-terminal node we must decide whether a substitution should take place at this node or at itsdescendants.","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
We perform substitution for a non terminal node if the following constraint is met: Max(SubFT (?|node)) ? Y ch:node Max(SubFT (?|ch)).,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
where ch is a child of the node.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
canbe any substitution in the SubFT.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
The proba bility of the phrase substitution is calculated as P (sub|node) = SubFT (Substitution|Origin).Fig.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
5 shows one of the possible substitution re sults for our example where ?ancient?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
is replaced by ?old?.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"NP VP pt1 August was NPinsixththe NP NP PPpt2 VP started PP in 735BC old calendartheNP old calendarthe month Figure 5: Substitution As a result of all the simplification operations, we obtain the following two sentences: s1 = Str(pt1)=?August was the sixth month in the old calendar.?","{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
and s2 = Str(pt2)=?The old calendar started in 735BC.?,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
3.4 The Probabilistic Model.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
Our model can be formalized as a direct translation model from complex to simple P (s|c) multi plied by a language model P (s) as shown in Equ.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
4.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
s = argmax s P (s|c)P (s) (4) We combine the parts described in the previous sections to get the direct translation model: P (s|c) = ? ?:Str(?(c))=s (P (seg|c)P (com|seg) (5) ? node P (dp|node)P (ro|node)P (sub|node) ? w (sub|w)).,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
where ? is a sequence of simplification operationsand Str(?(c)) corresponds to the leaves of a simplified tree.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
There can be many sequences of op erations that result in the same simplified sentence and we sum up all of their probabilities.,"{'title': 'The Simplification Model: TSM. ', 'number': '3'}"
"In this section, we describe how we train the prob abilities in the tables.","{'title': 'Training. ', 'number': '4'}"
"Following the work of Yamada and Knight (2001), we train our model by maximizing P (s|c) over the training corpuswith the EM algorithm described in Alg.","{'title': 'Training. ', 'number': '4'}"
"2, us ing a constructed graph structure.","{'title': 'Training. ', 'number': '4'}"
We develop the Training Tree (Fig.,"{'title': 'Training. ', 'number': '4'}"
6) to calculate P (s|c).,"{'title': 'Training. ', 'number': '4'}"
P (s|c) is equal to the inside probability of the root in theTraining Tree.,"{'title': 'Training. ', 'number': '4'}"
Alg.,"{'title': 'Training. ', 'number': '4'}"
3 and Alg.,"{'title': 'Training. ', 'number': '4'}"
4 are used to calculate the inside and outside probabilities.,"{'title': 'Training. ', 'number': '4'}"
We re fer readers to Yamada and Knight (2001) for more details.,"{'title': 'Training. ', 'number': '4'}"
"Algorithm 2 EM Training (dataset)Initialize all probability tables using the uniform distribu tion; for several iterations do reset al cnt = 0; for each sentence pair < c, s > in dataset do tt = buildTrainingTree(< c, s >); calcInsideProb(tt); calcOutsideProb(tt); update cnt for each conditioning feature in each node of tt: cnt = cnt + node.insideProb ? node.outsideProb/root.insideProb; end for updateProbability(); end for root sp sp_res1 sp_res2 dp ro mp mp_res1 mp_res2 sub mp mp_res subsub dp ro mp_res root sp sp_res sp_res dp ro ro_res ro_res sub ro_res subsub dp ro ro_res sub_res sub_res sub_res Figure 6: Training Tree (Left) and Decoding Tree (Right) We illustrate the construction of the training tree with our running example.","{'title': 'Training. ', 'number': '4'}"
There are two kinds of nodes in the training tree: data nodes in rectangles and operation nodes in circles.,"{'title': 'Training. ', 'number': '4'}"
Data nodes contain data and operation nodes execute operations.,"{'title': 'Training. ', 'number': '4'}"
The training is a supervised learning 1357 process with the parse tree of c as input and the two strings s1 and s2 as the desired output.,"{'title': 'Training. ', 'number': '4'}"
root stores the parse tree of c and also s1 and s2.,"{'title': 'Training. ', 'number': '4'}"
"sp, ro, mp and sub are splitting, reordering, mapping and substitution operations.","{'title': 'Training. ', 'number': '4'}"
sp res and mp res store the results of sp and mp.,"{'title': 'Training. ', 'number': '4'}"
"In our example, sp splits the parse tree into two parse trees pt1 and pt2 (Fig.","{'title': 'Training. ', 'number': '4'}"
3).,"{'title': 'Training. ', 'number': '4'}"
sp res1 contains pt1 and s1.,"{'title': 'Training. ', 'number': '4'}"
sp res2 contains pt2 and s2.,"{'title': 'Training. ', 'number': '4'}"
"Then dp, ro and mp are iteratively applied to each non-terminal node at each level of pt1 and pt2 from top to down.","{'title': 'Training. ', 'number': '4'}"
This process continues until the terminal nodesare reached or is stopped by a sub node.,"{'title': 'Training. ', 'number': '4'}"
The function of mp operation is similar to the word mapping operation in the string-based machine trans lation.,"{'title': 'Training. ', 'number': '4'}"
It maps substrings in the complex sentence which are dominated by the children of the current node to proper substrings in the simple sentences.,"{'title': 'Training. ', 'number': '4'}"
Speeding Up The example above is only oneof the possible paths.,"{'title': 'Training. ', 'number': '4'}"
We try all of the promis ing paths in training.,"{'title': 'Training. ', 'number': '4'}"
Promising paths are thepaths which are likely to succeed in transform ing the parse tree of c into s1 and s2.,"{'title': 'Training. ', 'number': '4'}"
We select the promising candidates using monolingual word mapping as shown in Fig.,"{'title': 'Training. ', 'number': '4'}"
7.,"{'title': 'Training. ', 'number': '4'}"
"In this example,only the word ?which?","{'title': 'Training. ', 'number': '4'}"
can be a promising can didate for splitting.,"{'title': 'Training. ', 'number': '4'}"
"We can select the promisingcandidates for the dropping, reordering and map ping operations similarly.","{'title': 'Training. ', 'number': '4'}"
"With this improvement, we can train on the PWKP dataset within 1 hour excluding the parsing time taken by the Stanford Parser.","{'title': 'Training. ', 'number': '4'}"
We initialize the probabilities with the uniform distribution.,"{'title': 'Training. ', 'number': '4'}"
"The binary features, such as SFT and BDFT, are assigned the initial value of 0.5.","{'title': 'Training. ', 'number': '4'}"
"For DFT and RFT, the initial probability is 1N!","{'title': 'Training. ', 'number': '4'}"
", whereN is the number of the children.","{'title': 'Training. ', 'number': '4'}"
CFT is initial ized as 0.25.,"{'title': 'Training. ', 'number': '4'}"
SubFT is initialized as 1.0 for anysubstitution at the first iteration.,"{'title': 'Training. ', 'number': '4'}"
"After each itera tion, the updateProbability function recalculatesthese probabilities based on the cnt for each fea ture.","{'title': 'Training. ', 'number': '4'}"
"Algorithm 3 calcInsideProb (TrainingTree tt) for each node from level = N to root of tt do if node is a sub node then node.insideProb = P (sub|node); else if node is a mp OR sp node then node.insideProb =Qchild child.insideProb;else node.insideProb =Pchild child.insideProb;end if end for Algorithm 4 calcOutsideProb (TrainingTree tt) for each node from root to level = N of tt do if node is the root then node.outsideProb = 1.0; else if node is a sp res OR mp res node then {COMMENT: father are the fathers of the current node, sibling are the children of father excluding the current node} node.outsideProb = P father father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then node.outsideProb = father.outsideProb ? 1.0; else if node is a sp, ro, dp or sub node then node.outsideProb = father.outsideProb ? P (sp or ro or dp or sub|node); end if end for August was the sixth in the ancient Roman calendar statedwhich in 735BC August was the sixth in the old Roman calendar stated in 735BCThe old calendar.","{'title': 'Training. ', 'number': '4'}"
Complex sentence Simple sentences month month Figure 7: Monolingual Word Mapping,"{'title': 'Training. ', 'number': '4'}"
"For decoding, we construct the decoding tree(Fig.","{'title': 'Decoding. ', 'number': '5'}"
6) similarly to the construction of the training tree.,"{'title': 'Decoding. ', 'number': '5'}"
The decoding tree does not have mp op erations and there can be more than one sub nodes attached to a single ro res.,"{'title': 'Decoding. ', 'number': '5'}"
The root contains the parse tree of the complex sentence.,"{'title': 'Decoding. ', 'number': '5'}"
"Due to space limitations, we cannot provide all the details of the decoder.We calculate the inside probability and out side probability for each node in the decoding tree.","{'title': 'Decoding. ', 'number': '5'}"
"When we simplify a complex sentence, we start from the root and greedily select the branchwith the highest outside probability.","{'title': 'Decoding. ', 'number': '5'}"
"For the sub stitution operation, we also integrate a trigram language model to make the generated sentences more fluent.","{'title': 'Decoding. ', 'number': '5'}"
"We train the language model with SRILM (Stolcke, 2002).","{'title': 'Decoding. ', 'number': '5'}"
"All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB.","{'title': 'Decoding. ', 'number': '5'}"
Our evaluation dataset consists of 100 complex sentences and 131 parallel simple sentences from PWKP.,"{'title': 'Evaluation. ', 'number': '6'}"
They have not been used for training.Four baseline systems are compared in our eval uation.,"{'title': 'Evaluation. ', 'number': '6'}"
The first is Moses which is a state of the art SMT system widely used as a baseline in MT community.,"{'title': 'Evaluation. ', 'number': '6'}"
"Obviously, the purpose of Mosesis cross-lingual translation rather than monolin 1358 gual simplification.","{'title': 'Evaluation. ', 'number': '6'}"
The goal of our comparison is therefore to assess how well a standard SMT system may perform simplification when fed with a proper training dataset.,"{'title': 'Evaluation. ', 'number': '6'}"
We train Moses with the same part of PWKP as our model.,"{'title': 'Evaluation. ', 'number': '6'}"
"The secondbaseline system is a sentence compression sys tem (Filippova and Strube, 2008a) whose demo system is available online.8 As the compressionsystem can only perform dropping, we further ex tend it to our third and fourth baseline systems, in order to make a reasonable comparison.","{'title': 'Evaluation. ', 'number': '6'}"
"In our third baseline system, we substitute the words in the output of the compression system with their simpler synonyms.","{'title': 'Evaluation. ', 'number': '6'}"
This is done by looking up the synonyms in Wordnet and selecting the mostfrequent synonym for replacement.,"{'title': 'Evaluation. ', 'number': '6'}"
The word fre quency is counted using the articles from Simple Wikipedia.,"{'title': 'Evaluation. ', 'number': '6'}"
The fourth system performs sentence splitting on the output of the third system.,"{'title': 'Evaluation. ', 'number': '6'}"
"This is simply done by splitting the sentences at ?and?,?or?, ?but?, ?which?, ?who?","{'title': 'Evaluation. ', 'number': '6'}"
"and ?that?, and dis carding the border words.","{'title': 'Evaluation. ', 'number': '6'}"
"In total, there are 5systems in our evaluation: Moses, the MT system; C, the compression system; CS, the compression+substitution system; CSS, the compres sion+substitution+split system; TSM, our model.We also provide evaluation measures for the sen tences in the evaluation dataset: CW: complexsentences from Normal Wikipedia and SW: par allel simple sentences from Simple Wikipedia.","{'title': 'Evaluation. ', 'number': '6'}"
6.1 Basic Statistics and Examples.,"{'title': 'Evaluation. ', 'number': '6'}"
The first three columns in Tab.,"{'title': 'Evaluation. ', 'number': '6'}"
9 present the ba sic statistics for the evaluation sentences and theoutput of the five systems.,"{'title': 'Evaluation. ', 'number': '6'}"
tokenLen is the aver age length of tokens which may roughly reflect the lexical difficulty.,"{'title': 'Evaluation. ', 'number': '6'}"
TSM achieves an average token length which is the same as the Simple Wikipedia (SW).,"{'title': 'Evaluation. ', 'number': '6'}"
"senLen is the average number of tokens inone sentence, which may roughly reflect the syn tactic complexity.","{'title': 'Evaluation. ', 'number': '6'}"
Both TSM and CSS produce shorter sentences than SW.,"{'title': 'Evaluation. ', 'number': '6'}"
Moses is very close to CW.,"{'title': 'Evaluation. ', 'number': '6'}"
#sen gives the number of sentences.,"{'title': 'Evaluation. ', 'number': '6'}"
"Moses, C and CS cannot split sentences and thus produce about the same number of sentences as available in CW.","{'title': 'Evaluation. ', 'number': '6'}"
Here are two example results obtained with our TSM system.Example 1.,"{'title': 'Evaluation. ', 'number': '6'}"
CW: ?Genetic engineering has ex panded the genes available to breeders to utilize in creating desired germlines for new crops.?,"{'title': 'Evaluation. ', 'number': '6'}"
SW: 8http://212.126.215.106/compression/?New plants were created with genetic engineer ing.?,"{'title': 'Evaluation. ', 'number': '6'}"
TSM: ?Engineering has expanded the genes available to breeders to use in making germlines for new crops.?,"{'title': 'Evaluation. ', 'number': '6'}"
Example 2.,"{'title': 'Evaluation. ', 'number': '6'}"
"CW: ?An umbrella term is a word thatprovides a superset or grouping of related con cepts, also called a hypernym.?","{'title': 'Evaluation. ', 'number': '6'}"
SW: ?An umbrellaterm is a word that provides a superset or group ing of related concepts.?,"{'title': 'Evaluation. ', 'number': '6'}"
TSM: ?An umbrella term is a word.,"{'title': 'Evaluation. ', 'number': '6'}"
"A word provides a superset of related concepts, called a hypernym.?In the first example, both substitution and dropping happen.","{'title': 'Evaluation. ', 'number': '6'}"
TSM replaces ?utilize?,"{'title': 'Evaluation. ', 'number': '6'}"
and ?cre ating?,"{'title': 'Evaluation. ', 'number': '6'}"
with ?use?,"{'title': 'Evaluation. ', 'number': '6'}"
and ?making?.,"{'title': 'Evaluation. ', 'number': '6'}"
?Genetic?,"{'title': 'Evaluation. ', 'number': '6'}"
isdropped.,"{'title': 'Evaluation. ', 'number': '6'}"
"In the second example, the complex sen tence is split and ?also?","{'title': 'Evaluation. ', 'number': '6'}"
is dropped.,"{'title': 'Evaluation. ', 'number': '6'}"
6.2 Translation Assessment.,"{'title': 'Evaluation. ', 'number': '6'}"
"In this part of the evaluation, we use traditional measures used for evaluating MT systems.","{'title': 'Evaluation. ', 'number': '6'}"
Tab.,"{'title': 'Evaluation. ', 'number': '6'}"
9 shows the BLEU and NIST scores.,"{'title': 'Evaluation. ', 'number': '6'}"
We use ?mteval-v11b.pl?9 as the evaluation tool.,"{'title': 'Evaluation. ', 'number': '6'}"
CWand SW are used respectively as source and ref erence sentences.,"{'title': 'Evaluation. ', 'number': '6'}"
TSM obtains a very high BLEU score (0.38) but not as high as Moses (0.55).,"{'title': 'Evaluation. ', 'number': '6'}"
"However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences.","{'title': 'Evaluation. ', 'number': '6'}"
We also find that most of the sentences generated by Moses are exactly the same as those in CW:this shows that Moses only performs few modi fications to the original complex sentences.,"{'title': 'Evaluation. ', 'number': '6'}"
"This is confirmed by MT evaluation measures: if we set CW as both source and reference, the BLEU score obtained by Moses is 0.78.","{'title': 'Evaluation. ', 'number': '6'}"
TSM gets 0.55 in the same setting which is significantly smaller than Moses and demonstrates that TSM is able to generate simplifications with a greater amount of variation from the original sentence.,"{'title': 'Evaluation. ', 'number': '6'}"
As shown inthe ?#Same?,"{'title': 'Evaluation. ', 'number': '6'}"
column of Tab.,"{'title': 'Evaluation. ', 'number': '6'}"
"9, 25 sentences generated by Moses are exactly identical to the com plex sentences, while the number for TSM is 2 which is closer to SW.","{'title': 'Evaluation. ', 'number': '6'}"
It is however not clear how well BLEU and NIST discriminate simplification systems.,"{'title': 'Evaluation. ', 'number': '6'}"
"As discussed in Jurafsky and Martin (2008), ?BLEU does poorly at comparing systems with radically different architectures and is most appropriate when evaluating incremental changes with similar architectures.?","{'title': 'Evaluation. ', 'number': '6'}"
"In our case, TSM andCSS can be considered as having similar architec tures as both of them can do splitting, dropping 9http://www.statmt.org/moses/ 1359 TokLen SenLen #Sen BLEU NIST #Same Flesch Lix(Grade) OOV% PPL CW 4.95 27.81 100 0.50 6.89 100 49.1 53.0 (10) 52.9 384 SW 4.76 17.86 131 1.00 10.98 3 60.4 (PE) 44.1 (8) 50.7 179 Moses 4.81 26.08 100 0.55 7.47 25 54.8 48.1 (9) 52.0 363 C 4.98 18.02 103 0.28 5.37 1 56.2 45.9 (8) 51.7 481 CS 4.90 18.11 103 0.19 4.51 0 59.1 45.1 (8) 49.5 616 CSS 4.98 10.20 182 0.18 4.42 0 65.5 (PE) 38.3 (6) 53.4 581 TSM 4.76 13.57 180 0.38 6.21 2 67.4 (PE) 36.7 (5) 50.8 353 Table 9: Evaluation and substitution.","{'title': 'Evaluation. ', 'number': '6'}"
But Moses mostly cannot split and drop.,"{'title': 'Evaluation. ', 'number': '6'}"
We may conclude that TSM and Moses have different architectures and BLEU or NIST isnot suitable for comparing them.,"{'title': 'Evaluation. ', 'number': '6'}"
"Here is an exam ple to illustrate this: (CW): ?Almost as soon as heleaves, Annius and the guard Publius arrive to es cort Vitellia to Titus, who has now chosen her as his empress.?","{'title': 'Evaluation. ', 'number': '6'}"
"(SW): ?Almost as soon as he leaves,Annius and the guard Publius arrive to take Vitellia to Titus, who has now chosen her as his empress.?","{'title': 'Evaluation. ', 'number': '6'}"
(Moses): The same as (SW).,"{'title': 'Evaluation. ', 'number': '6'}"
(TSM): ?An nius and the guard Publius arrive to take Vitellia to Titus.,"{'title': 'Evaluation. ', 'number': '6'}"
"Titus has now chosen her as his empress.?In this example, Moses generates an exactly iden tical sentence to SW, thus the BLUE and NIST scores of Moses is the highest.","{'title': 'Evaluation. ', 'number': '6'}"
"TSM simplifies the complex sentence by dropping, splitting and substitution, which results in two sentences that are quite different from the SW sentence and thus gets lower BLUE and NIST scores.","{'title': 'Evaluation. ', 'number': '6'}"
"Nevertheless, the sentences generated by TSM seem better than Moses in terms of simplification.","{'title': 'Evaluation. ', 'number': '6'}"
6.3 Readability Assessment.,"{'title': 'Evaluation. ', 'number': '6'}"
"Intuitively, readability scores should be suitable metrics for simplification systems.","{'title': 'Evaluation. ', 'number': '6'}"
We use the Linux ?style?,"{'title': 'Evaluation. ', 'number': '6'}"
command to calculate the Fleschand Lix readability scores.,"{'title': 'Evaluation. ', 'number': '6'}"
The results are pre sented in Tab.,"{'title': 'Evaluation. ', 'number': '6'}"
9.,"{'title': 'Evaluation. ', 'number': '6'}"
?PE?,"{'title': 'Evaluation. ', 'number': '6'}"
in the Flesch column standsfor ?Plain English?,"{'title': 'Evaluation. ', 'number': '6'}"
and the ?Grade?,"{'title': 'Evaluation. ', 'number': '6'}"
in Lix repre sents the school year.,"{'title': 'Evaluation. ', 'number': '6'}"
TSM achieves significantly better scores than Moses which has the best BLEUscore.,"{'title': 'Evaluation. ', 'number': '6'}"
This implies that good monolingual trans lation is not necessarily good simplification.,"{'title': 'Evaluation. ', 'number': '6'}"
OOVis the percentage of words that are not in the Ba sic English BE850 list.10 TSM is ranked as the second best system for this criterion.The perplexity (PPL) is a score of text probability measured by a language model and normal ized by the number of words in the text (Equ.,"{'title': 'Evaluation. ', 'number': '6'}"
6).,"{'title': 'Evaluation. ', 'number': '6'}"
10http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlistPPL can be used to measure how tight the language model fits the text.,"{'title': 'Evaluation. ', 'number': '6'}"
"Language models constitute an important feature for assessing readabil ity (Schwarm and Ostendorf, 2005).","{'title': 'Evaluation. ', 'number': '6'}"
We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM.,"{'title': 'Evaluation. ', 'number': '6'}"
TSM gets the best PPL score.,"{'title': 'Evaluation. ', 'number': '6'}"
"From this table, we can conclude that TSM achieves better overall readability than the baseline systems.","{'title': 'Evaluation. ', 'number': '6'}"
PPL(text) = P (w1w2...wN )?,"{'title': 'Evaluation. ', 'number': '6'}"
1 N (6)There are still some important issues to be con sidered in future.,"{'title': 'Evaluation. ', 'number': '6'}"
"Based on our observations, the current model performs well for word substitution and segmentation.","{'title': 'Evaluation. ', 'number': '6'}"
But the completion of the new sentences is still problematic.,"{'title': 'Evaluation. ', 'number': '6'}"
"For example, we copy the dependent NP to the new sentences.","{'title': 'Evaluation. ', 'number': '6'}"
This may break the coherence between sentences.,"{'title': 'Evaluation. ', 'number': '6'}"
Abetter solution would be to use a pronoun to replace the NP.,"{'title': 'Evaluation. ', 'number': '6'}"
"Sometimes, excessive droppings oc cur, e.g., ?older?","{'title': 'Evaluation. ', 'number': '6'}"
and ?twin?,"{'title': 'Evaluation. ', 'number': '6'}"
are dropped in ?She has an older brother and a twin brother...?.,"{'title': 'Evaluation. ', 'number': '6'}"
This results in a problematic sentence: ?She has anbrother and a brother...?.,"{'title': 'Evaluation. ', 'number': '6'}"
There are also some er rors which stem from the dependency parser.,"{'title': 'Evaluation. ', 'number': '6'}"
"InExample 2, ?An umbrella term?","{'title': 'Evaluation. ', 'number': '6'}"
should be a dependency of ?called?.,"{'title': 'Evaluation. ', 'number': '6'}"
But the parser returns ?su perset?,"{'title': 'Evaluation. ', 'number': '6'}"
as the dependency.,"{'title': 'Evaluation. ', 'number': '6'}"
"In the future, we will investigate more sophisticated features and rules to enhance TSM.","{'title': 'Evaluation. ', 'number': '6'}"
"In this paper, we presented a novel large-scale par allel dataset PWKP for sentence simplification.","{'title': 'Conclusions. ', 'number': '7'}"
"We proposed TSM, a tree-based translation model for sentence simplification which covers splitting, dropping, reordering and word/phrase substitution integrally for the first time.","{'title': 'Conclusions. ', 'number': '7'}"
We also described anefficient training method with speeding up tech niques for TSM.,"{'title': 'Conclusions. ', 'number': '7'}"
The evaluation shows that TSM can achieve better overall readability scores than a set of baseline systems.,"{'title': 'Conclusions. ', 'number': '7'}"
1360,"{'title': 'Conclusions. ', 'number': '7'}"
