col1,col2
"In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.",{}
We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.,{}
We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.,{}
We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.,{}
A language model is a statistical model that gives a probability distribution over possible sequences of words.,"{'title': '1 Introduction', 'number': '1'}"
It computes the probability of producing a given word w1 given all the words that precede it in the sentence.,"{'title': '1 Introduction', 'number': '1'}"
"An n-gram language model is an n-th order Markov model where the probability of generating a given word depends only on the last n âˆ’ 1 words immediately preceding it and is given by the following equation: where k >= n. N-gram language models have been successfully used in Automatic Speech Recognition (ASR) as was first proposed by (Bahl et al., 1983).","{'title': '1 Introduction', 'number': '1'}"
They play an important role in selecting among several candidate word realization of a given acoustic signal.,"{'title': '1 Introduction', 'number': '1'}"
"N-gram language models have also been used in Statistical Machine Translation (SMT) as proposed by (Brown et al., 1990; Brown et al., 1993).","{'title': '1 Introduction', 'number': '1'}"
The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding.,"{'title': '1 Introduction', 'number': '1'}"
There is a fundamental difference between decoding for machine translation and decoding for speech recognition.,"{'title': '1 Introduction', 'number': '1'}"
"When decoding a speech signal, words are generated in the same order in which their corresponding acoustic signal is consumed.","{'title': '1 Introduction', 'number': '1'}"
"However, that is not necessarily the case in MT due to the fact that different languages have different word order requirements.","{'title': '1 Introduction', 'number': '1'}"
"For example, in Spanish and Arabic adjectives are mainly noun post-modifiers, whereas in English adjectives are noun pre-modifiers.","{'title': '1 Introduction', 'number': '1'}"
"Therefore, when translating between Spanish and English, words must usually be reordered.","{'title': '1 Introduction', 'number': '1'}"
Existing statistical machine translation decoders have mostly relied on language models to select the proper word order among many possible choices when translating between two languages.,"{'title': '1 Introduction', 'number': '1'}"
"In this paper, we argue that a language model is not sufficient to adequately address this issue, especially when translating between languages that have very different word orders as suggested by our experimental results in Section 5.","{'title': '1 Introduction', 'number': '1'}"
We propose a new distortion model that can be used as an additional component in SMT decoders.,"{'title': '1 Introduction', 'number': '1'}"
"This new model leads to significant improvements in MT quality as measured by BLEU (Papineni et al., 2002).","{'title': '1 Introduction', 'number': '1'}"
The experimental results we report in this paper are for Arabic-English machine translation of news stories.,"{'title': '1 Introduction', 'number': '1'}"
We also present a novel method for measuring word order similarity (or differences) between any given pair of languages based on word alignments as described in Section 3.,"{'title': '1 Introduction', 'number': '1'}"
The rest of this paper is organized as follows.,"{'title': '1 Introduction', 'number': '1'}"
Section 2 presents a review of related work.,"{'title': '1 Introduction', 'number': '1'}"
In Section 3 we propose a method for measuring the distortion between any given pair of languages.,"{'title': '1 Introduction', 'number': '1'}"
"In Section 4, we present our proposed distortion model.","{'title': '1 Introduction', 'number': '1'}"
"In Section 5, we present some empirical results that show the utility of our distortion model for statistical machine translation systems.","{'title': '1 Introduction', 'number': '1'}"
"Then, we conclude this paper with a discussion in Section 6.","{'title': '1 Introduction', 'number': '1'}"
Different languages have different word order requirements.,"{'title': '2 Related Work', 'number': '2'}"
SMT decoders attempt to generate translations in the proper word order by attempting many possible word reorderings during the translation process.,"{'title': '2 Related Work', 'number': '2'}"
"Trying all possible word reordering is an NP-Complete problem as shown in (Knight, 1999), which makes searching for the optimal solution among all possible permutations computationally intractable.","{'title': '2 Related Work', 'number': '2'}"
"Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions.","{'title': '2 Related Work', 'number': '2'}"
"Reordering restrictions for word-based SMT decoders were introduced by (Berger et al., 1996) and (Wu, 1996).","{'title': '2 Related Work', 'number': '2'}"
"(Berger et al., 1996) allow only reordering of at most n words at any given time.","{'title': '2 Related Work', 'number': '2'}"
"(Wu, 1996) propose using contiguity restrictions on the reordering.","{'title': '2 Related Work', 'number': '2'}"
"For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003).","{'title': '2 Related Work', 'number': '2'}"
A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence.,"{'title': '2 Related Work', 'number': '2'}"
Monotone decoding translates words in the same order they appear in the source language.,"{'title': '2 Related Work', 'number': '2'}"
"Hence, the input and output sentences have the same word order.","{'title': '2 Related Work', 'number': '2'}"
Monotone decoding is very efficient since the optimal decoding can be found in polynomial time.,"{'title': '2 Related Work', 'number': '2'}"
"(Tillmann et al., 1997) proposed a DP-based monotone search algorithm for SMT.","{'title': '2 Related Work', 'number': '2'}"
Their proposed solution to address the necessary word reordering is to rewrite the input sentence such that it has a similar word order to the desired target sentence.,"{'title': '2 Related Work', 'number': '2'}"
The paper suggests that reordering the input reduces the translation error rate.,"{'title': '2 Related Work', 'number': '2'}"
"However, it does not provide a methodology on how to perform this reordering.","{'title': '2 Related Work', 'number': '2'}"
"(Xia and McCord, 2004) propose a method to automatically acquire rewrite patterns that can be applied to any given input sentence so that the rewritten source and target sentences have similar word order.","{'title': '2 Related Work', 'number': '2'}"
These rewrite patterns are automatically extracted by parsing the source and target sides of the training parallel corpus.,"{'title': '2 Related Work', 'number': '2'}"
Their approach show a statistically-significant improvement over a phrase-based monotone decoder.,"{'title': '2 Related Work', 'number': '2'}"
Their experiments also suggest that allowing the decoder to consider some word order permutations in addition to the rewrite patterns already applied to the source sentence actually decreases the BLEU score.,"{'title': '2 Related Work', 'number': '2'}"
Rewriting the input sentence whether using syntactic rules or heuristics makes hard decisions that can not be undone by the decoder.,"{'title': '2 Related Work', 'number': '2'}"
"Hence, reordering is better handled during the search algorithm and as part of the optimization function.","{'title': '2 Related Work', 'number': '2'}"
Phrase-based monotone decoding does not directly address word order issues.,"{'title': '2 Related Work', 'number': '2'}"
"Indirectly, however, the phrase dictionary1 in phrase-based decoders typically captures local reorderings that were seen in the training data.","{'title': '2 Related Work', 'number': '2'}"
"However, it fails to generalize to word reorderings that were never seen in the training data.","{'title': '2 Related Work', 'number': '2'}"
"For example, a phrase-based decoder might translate the Arabic phrase AlwlAyAt AlmtHdp2 correctly into English as the United States if it was seen in its training data, was aligned correctly, and was added to the phrase dictionary.","{'title': '2 Related Work', 'number': '2'}"
"However, if the phrase Almmlkp AlmtHdp is not in the phrase dictionary, it will not be translated correctly by a monotone phrase decoder even if the individual units of the phrase Almmlkp and AlmtHdp, and their translations (Kingdom and United, respectively) are in the phrase dictionary since that would require swapping the order of the two words.","{'title': '2 Related Work', 'number': '2'}"
"(Och et al., 1999; Tillmann and Ney, 2003) relax the monotonicity restriction in their phrase-based decoder by allowing a restricted set of word reorderings.","{'title': '2 Related Work', 'number': '2'}"
"For their translation task, word reordering is done only for words belonging to the verb group.","{'title': '2 Related Work', 'number': '2'}"
The context in which they report their results is a Speech-to-Speech translation from German to English.,"{'title': '2 Related Work', 'number': '2'}"
"(Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence.","{'title': '2 Related Work', 'number': '2'}"
They reported results that are better than word-based IBM4-like decoder.,"{'title': '2 Related Work', 'number': '2'}"
"However, their decoder is outperformed by phrase-based decoders such as (Koehn, 2004), (Och et al., 1999), and (Tillmann and Ney, 2003) .","{'title': '2 Related Work', 'number': '2'}"
Phrase-based SMT decoders mostly rely on the language model to select among possible word order choices.,"{'title': '2 Related Work', 'number': '2'}"
"However, in our experiments we show that the language model is not reliable enough to make the choices that lead to a better MT quality.","{'title': '2 Related Work', 'number': '2'}"
"This observation is also reported by (Xia and McCord, 2004).We argue that the distortion model we propose leads to a better translation as measured by BLEU.","{'title': '2 Related Work', 'number': '2'}"
"Distortion models were first proposed by (Brown et al., 1993) in the so-called IBM Models.","{'title': '2 Related Work', 'number': '2'}"
"IBM Models 2 and 3 define the distortion parameters in terms of the word positions in the sentence pair, not the actual words at those positions.","{'title': '2 Related Work', 'number': '2'}"
Distortion probability is also conditioned on the source and target sentence lengths.,"{'title': '2 Related Work', 'number': '2'}"
These models do not generalize well since their parameters are tied to absolute word position within sentences which tend to be different for the same words across sentences.,"{'title': '2 Related Work', 'number': '2'}"
IBM Models 4 and 5 alleviate this limitation by replacing absolute word positions with relative positions.,"{'title': '2 Related Work', 'number': '2'}"
The latter models define the distortion parameters for a cept (one or more words).,"{'title': '2 Related Work', 'number': '2'}"
This models phrasal movement better since words tend to move in blocks and not independently.,"{'title': '2 Related Work', 'number': '2'}"
The distortion is conditioned on classes of the aligned source and target words.,"{'title': '2 Related Work', 'number': '2'}"
"The entire source and target vocabularies are reduced to a small number of classes (e.g., 50) for the purpose of estimating those parameters.","{'title': '2 Related Work', 'number': '2'}"
"Similarly, (Koehn et al., 2003) propose a relative distortion model to be used with a phrase decoder.","{'title': '2 Related Work', 'number': '2'}"
The model is defined in terms of the difference between the position of the current phrase and the position of the previous phrase in the source sentence.,"{'title': '2 Related Work', 'number': '2'}"
It does not conillustrate word positions in the sentence.,"{'title': '2 Related Work', 'number': '2'}"
The indices in the reordered English denote word position in the original English order. sider the words in those positions.,"{'title': '2 Related Work', 'number': '2'}"
The distortion model we propose assigns a probability distribution over possible relative jumps conditioned on source words.,"{'title': '2 Related Work', 'number': '2'}"
Conditioning on the source words allows for a much more fine-grained model.,"{'title': '2 Related Work', 'number': '2'}"
"For instance, words that tend to act as modifers (e.g., adjectives) would have a different distribution than verbs or nouns.","{'title': '2 Related Work', 'number': '2'}"
Our modelâ€™s parameters are directly estimated from word alignments as we will further explain in Section 4.,"{'title': '2 Related Work', 'number': '2'}"
We will also show how to generalize this word distortion model to a phrase-based model.,"{'title': '2 Related Work', 'number': '2'}"
"(Och et al., 2004; Tillman, 2004) propose orientation-based distortion models lexicalized on the phrase level.","{'title': '2 Related Work', 'number': '2'}"
There are two important distinctions between their models and ours.,"{'title': '2 Related Work', 'number': '2'}"
"First, they lexicalize their model on the phrases, which have many more parameters and hence would require much more data to estimate reliably.","{'title': '2 Related Work', 'number': '2'}"
"Second, their models consider only the direction (i.e., orientation) and not the relative jump.","{'title': '2 Related Work', 'number': '2'}"
We are not aware of any work on measuring word order differences between a given language pair in the context of statistical machine translation.,"{'title': '2 Related Work', 'number': '2'}"
"In this section, we propose a simple, novel method for measuring word order similarity (or differences) between any given language pair.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
This method is based on word-alignments and the BLEU metric.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
We assume that we have word-alignments for a set of sentence pairs.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"We first reorder words in the target sentence (e.g., English when translating from Arabic to English) according to the order in which they are aligned to the source words as shown in Table 1.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"If a target word is not aligned, then, we assume that it is aligned to the same source word that the preceding aligned target word is aligned to.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"Once the reordered target (here English) sentences are generated, we measure the distortion between the language pair by computing the BLEU3 score between the original target and reordered target, treating the original target as the reference.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
Table 2 shows these scores for Arabic-English and 3the BLEU scores reported throughout this paper are for case-sensitive BLEU.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"The number of references used is also reported (e.g., BLEUr1n4c: r1 means 1 reference, n4 means upto 4-gram are considred, c means case sensitive).","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
Chinese-English.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
The word alignments we use are both annotated manually by human annotators.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
The ArabicEnglish test set is the NIST MT Evaluation 2003 test set.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"It contains 663 segments (i.e., sentences).","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"The Arabic side consists of 16,652 tokens and the English consists of 19,908 tokens.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
The Chinese-English test set contains 260 segments.,"{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"As suggested by the BLEU scores reported in Table 2, Arabic-English has more word order differences than Chinese-English.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"The difference in n-gPrec is bigger for smaller values of n, which suggests that ArabicEnglish has more local word order differences than in Chinese-English.","{'title': '3 Measuring Word Order Similarity Between Two Language', 'number': '3'}"
"The distortion model we are proposing consists of three components: outbound, inbound, and pair distortion.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
Intuitively our distortion models attempt to capture the order in which source words need to be translated.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
Do we tend to translate words that precede it or succeed it?,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
Which word position to translate next?,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Any aligner such as (Al-Onaizan et al., 1999) or (Vogel et al., 1996) can be used to obtain word alignments.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in (Ge, 2004).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
We will illustrate the components of our model with a partial word alignment.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Let us assume that our source sentences is (f10, f250 )6, and our target sentence is (e410, e20), and their word alignment is a = ((f10, e410), (f300, e20)).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Word Alignment a can be rewritten as a1 = 1 and a2 = 3 (i.e., the second target word is aligned to the third source word).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"From this partial alignment we increase the counts for the following outbound, inbound, and pair distortions: Po(Î´ = +2|f10), Pi(Î´ = +2|f300). and Pp(Î´ = +2|f10, f300).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Formally, our distortion model components are defined as follows: where fi is a foreign word (i.e., Arabic in our case), Î´ is the step size, and C(Î´|fi) is the observed count of this parameter over all word alignments in the training data.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"The value for Î´, in theory, ranges from âˆ’max to +max (where max is the maximum source sentence length observed), but in practice only a small number of those step sizes are observed in the training data, and hence, have non-zero value).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"In order to use these probability distributions in our decoder, they are then turned into costs.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
The outbound distortion cost is defined as: where P3(Î´) is a smoothing distribution 7 and Î± is a linear-mixture parameter 8.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"The inbound and pair costs (Ci(Î´|fi) and Cp(Î´|fi, fj)) can be defined in a similar fashion.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"So far, our distortion cost is defined in terms of words, not phrases.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Therefore, we need to generalize the distortion cost in order to use it in a phrasebased decoder.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
This generalization is defined in terms of the internal word alignment within phrases (we used the Viterbi word alignment).,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
We illustrate this with an example: Suppose the last position translated in the source sentence so far is n and we are to cover a source phrase p=wlAyp wA$nTn that begins at position m in the source sentence.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"Also, suppose that our phrase dictionary provided the translation Washington State, with internal word alignment a = (a1 = 2, a2 = 1) (i.e., a=(<Washington,wA$nTn>, <State,wlAyp>), then the outbound phrase cost is defined as: where l is the length of the target phrase, a is the internal word alignment, fn is source word at position n (in the sentence), and fa, is the source word that is aligned to the i-th word in the target side of the phrase (not the sentence).","{'title': '4 Proposed Distortion Model', 'number': '4'}"
"The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion.","{'title': '4 Proposed Distortion Model', 'number': '4'}"
The above distortion costs are used in conjunction with other cost components used in our decoder.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
The ultimate word order choice made is influenced by both the language model cost as well as the distortion cost.,"{'title': '4 Proposed Distortion Model', 'number': '4'}"
"The phrase-based decoder we use is inspired by the decoder described in (Tillmann and Ney, 2003) and similar to that described in (Koehn, 2004).","{'title': '5 Experimental Results', 'number': '5'}"
"It is a multistack, multi-beam search decoder with n stacks (where n is the length of the source sentence being decoded) The input is the reordered English in the reference.","{'title': '5 Experimental Results', 'number': '5'}"
"The 95% Confidence u ranges from 0.011 to 0.016 and a beam associated with each stack as described in (Al-Onaizan, 2005).","{'title': '5 Experimental Results', 'number': '5'}"
The search is done in n time steps.,"{'title': '5 Experimental Results', 'number': '5'}"
"In time step i, only hypotheses that cover exactly i source words are extended.","{'title': '5 Experimental Results', 'number': '5'}"
"The beam search algorithm attempts to find the translation (i.e., hypothesis that covers all source words) with the minimum cost as in (Tillmann and Ney, 2003) and (Koehn, 2004) .","{'title': '5 Experimental Results', 'number': '5'}"
The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost.,"{'title': '5 Experimental Results', 'number': '5'}"
A hypothesis covers a subset of the source words.,"{'title': '5 Experimental Results', 'number': '5'}"
The final translation is a hypothesis that covers all source words and has the minimum cost among all possible 9 hypotheses that cover all source words.,"{'title': '5 Experimental Results', 'number': '5'}"
"A hypothesis h is extended by matching the phrase dictionary against source word sequences in the input sentence that are not covered in h. The cost of the new hypothesis C(hn w) = C(h) + C(e), where C(e) is the cost of this extension.","{'title': '5 Experimental Results', 'number': '5'}"
"The main components of the cost of extension e can be defined by the following equation: where CLM(e) is the language model cost, CTM (e) is the translation model cost, and CD(e) is the distortion cost.","{'title': '5 Experimental Results', 'number': '5'}"
"The extension cost depends on the hypothesis being extended, the phrase being used in the extension, and the source word positions being covered.","{'title': '5 Experimental Results', 'number': '5'}"
"The word reorderings that are explored by the search algorithm are controlled by two parameters s and w as described in (Tillmann and Ney, 2003).","{'title': '5 Experimental Results', 'number': '5'}"
"The first parameter s denotes the number of source words that are temporarily skipped (i.e., temporarily left uncovered) during the search to cover a source word to the right of the skipped words.","{'title': '5 Experimental Results', 'number': '5'}"
"The second parameter is the window width w, which is defined as the distance (in number of source words) between the left-most uncovered source word and the right-most covered source word.","{'title': '5 Experimental Results', 'number': '5'}"
"To illustrate these restrictions, let us assume the input sentence consists of the following sequence (f1, f2, f3, f4).","{'title': '5 Experimental Results', 'number': '5'}"
"For s=1 and w=2, the permissible permutations are (f1,f2, f3, f4), (f2, f1, f3, f4), The experiments reported in this section are in the context of SMT from Arabic into English.","{'title': '5 Experimental Results', 'number': '5'}"
The training data is a 500K sentence-pairs subsample of the 2005 Large Track Arabic-English Data for NIST MT Evaluation.,"{'title': '5 Experimental Results', 'number': '5'}"
"The language model used is an interpolated trigram model described in (Bahl et al., 1983).","{'title': '5 Experimental Results', 'number': '5'}"
The language model is trained on the LDC English GigaWord Corpus.,"{'title': '5 Experimental Results', 'number': '5'}"
The test set used in the experiments in this section is the 2003 NIST MT Evaluation test set (which is not part of the training data).,"{'title': '5 Experimental Results', 'number': '5'}"
"In the experiments in this section, we show the utility of a trigram language model in restoring the correct word order for English.","{'title': '5 Experimental Results', 'number': '5'}"
"The task is a simplified translation task, where the input is reordered English (English written in Arabic word order) and the output is English in the correct order.","{'title': '5 Experimental Results', 'number': '5'}"
The source sentence is a reordered English sentence in the same manner we described in Section 3.,"{'title': '5 Experimental Results', 'number': '5'}"
The objective of the decoder is to recover the correct English order.,"{'title': '5 Experimental Results', 'number': '5'}"
"We use the same phrase-based decoder we use for our SMT experiments, except that only the language model cost is used here.","{'title': '5 Experimental Results', 'number': '5'}"
"Also, the phrase dictionary used is a one-to-one function that maps every English word in our vocabulary to itself.","{'title': '5 Experimental Results', 'number': '5'}"
The language model we use for the experiments reported here is the same as the one used for other experiments reported in this paper.,"{'title': '5 Experimental Results', 'number': '5'}"
"The results in Table 3 illustrate how the language model performs reasonably well for local reorderings (e.g., for s = 3 and w = 4), but its perfromance deteriorates as we relax the reordering restrictions by increasing the reordering window size (w).","{'title': '5 Experimental Results', 'number': '5'}"
"Table 4 shows some examples of original English, English in Arabic order, and the decoder output for two different sets of reordering parameters.","{'title': '5 Experimental Results', 'number': '5'}"
The phrases in the phrase dictionary we use in the experiments reported here are a combination of phrases automatically extracted from maximumposterior alignments and maximum entropy alignments.,"{'title': '5 Experimental Results', 'number': '5'}"
"Only phrases that conform to the so-called consistent alignment restrictions (Och et al., 1999) are extracted.","{'title': '5 Experimental Results', 'number': '5'}"
"Table 5 shows BLEU scores for our SMT decoder with different parameter settings for skip s, window width w, with and without our distortion model.","{'title': '5 Experimental Results', 'number': '5'}"
The BLEU scores reported in this table are based on 4 reference translations.,"{'title': '5 Experimental Results', 'number': '5'}"
"The language model, phrase dictionary, and other decoder tuning parameters remain the same in all experiments reported in this table.","{'title': '5 Experimental Results', 'number': '5'}"
"Table 5 clearly shows that as we open the search and consider wider range of word reorderings, the BLEU score decreases in the absence of our distortion model when we rely solely on the language model.","{'title': '5 Experimental Results', 'number': '5'}"
Wrong reorderings look attractive to the decoder via the language model which suggests that we need a richer model with more parameter.,"{'title': '5 Experimental Results', 'number': '5'}"
"In the absence of richer models such as the proposed distortion model, our results suggest that it is best to decode monotonically and only allow local reorderings that are captured in our phrase dictionary.","{'title': '5 Experimental Results', 'number': '5'}"
"However, when the distortion model is used, we see statistically significant increases in the BLEU score as we consider more word reorderings.","{'title': '5 Experimental Results', 'number': '5'}"
"The best BLEU score achieved when using the distortion model is 0.4792 , compared to a best BLEU score of 0.4468 when the distortion model is not used.","{'title': '5 Experimental Results', 'number': '5'}"
"Our results on the 2004 and 2005 NIST MT Evaluation test sets using the distortion model are 0.4497 and 0.464610, respectively.","{'title': '5 Experimental Results', 'number': '5'}"
Table 6 shows some Arabic-English translation examples using our decoder with and without the distortion model.,"{'title': '5 Experimental Results', 'number': '5'}"
We presented a new distortion model that can be integrated with existing phrase-based SMT decoders.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
The proposed model shows statistically significant improvement over a state-of-the-art phrase-based SMT decoder.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
We also showed that n-gram language modlations.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Output 2 is decoding with the distortion model and (s=3, w=8), which corresponds to 0.4792 BLEU score.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
The sentences presented here are much shorter than the average in our test set.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
The average length of the arabic sentence in the MT03 test set is â€” 24.7. els are not sufficient to model word movement in translation.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
Our proposed distortion model addresses this weakness of the n-gram language model.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
We also propose a novel metric to measure word order similarity (or differences) between any pair of languages based on word alignments.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
Our metric shows that Chinese-English have a closer word order than Arabic-English.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
Our proposed distortion model relies solely on word alignments and is conditioned on the source words.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
The majority of word movement in translation is mainly due to syntactic differences between the source and target language.,"{'title': '6 Conclusion and Future Work', 'number': '6'}"
"For example, Arabic is verb-initial for the most part.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"So, when translating into English, one needs to move the verb after the subject, which is often a long compounded phrase.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
"Therefore, we would like to incorporate syntactic or part-of-speech information in our distortion model.","{'title': '6 Conclusion and Future Work', 'number': '6'}"
This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.,"{'title': 'Acknowledgment', 'number': '7'}"
It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.,"{'title': 'Acknowledgment', 'number': '7'}"
