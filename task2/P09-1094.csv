col1,col2
Paraphrase generation (PG) is important in plenty of NLP applications.,{}
"However, the research of PG is far from enough.",{}
"In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.",{}
"In our experiments, we use the proposed method to generate paraphrases for three different applications.",{}
The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.,{}
Paraphrases are alternative ways that convey the same meaning.,"{'title': '1 Introduction', 'number': '1'}"
"There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG).","{'title': '1 Introduction', 'number': '1'}"
Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application.,"{'title': '1 Introduction', 'number': '1'}"
"PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboue and Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al., 1991), text simplification in computer-aided reading (Carroll et al., 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al., 2006).","{'title': '1 Introduction', 'number': '1'}"
This paper presents a method for statistical paraphrase generation (SPG).,"{'title': '1 Introduction', 'number': '1'}"
"As far as we know, this is the first statistical model specially designed for paraphrase generation.","{'title': '1 Introduction', 'number': '1'}"
It’s distinguishing feature is that it achieves various applications with a uniform model.,"{'title': '1 Introduction', 'number': '1'}"
"In addition, it exploits multiple resources, including paraphrase phrases, patterns, and collocations, to resolve the data shortage problem and generate more varied paraphrases.","{'title': '1 Introduction', 'number': '1'}"
"We consider three paraphrase applications in our experiments, including sentence compression, sentence simplification, and sentence similarity computation.","{'title': '1 Introduction', 'number': '1'}"
The proposed method generates paraphrases for the input sentences in each application.,"{'title': '1 Introduction', 'number': '1'}"
"The generated paraphrases are then manually scored based on adequacy, fluency, and usability.","{'title': '1 Introduction', 'number': '1'}"
"The results show that the proposed method is promising, which generates useful paraphrases for the given applications.","{'title': '1 Introduction', 'number': '1'}"
"In addition, comparison experiments show that our method outperforms a conventional SMT-based PG method.","{'title': '1 Introduction', 'number': '1'}"
"Conventional methods for paraphrase generation can be classified as follows: Rule-based methods: Rule-based PG methods build on a set of paraphrase rules or patterns, which are either hand crafted or automatically collected.","{'title': '2 Related Work', 'number': '2'}"
"In the early rule-based PG research, the paraphrase rules are generally manually written (McKeown, 1979; Zong et al., 2001), which is expensive and arduous.","{'title': '2 Related Work', 'number': '2'}"
"Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods.","{'title': '2 Related Work', 'number': '2'}"
"However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase patterns are long or complicated (Quirk et al., 2004).","{'title': '2 Related Work', 'number': '2'}"
"Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006).","{'title': '2 Related Work', 'number': '2'}"
"This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation.","{'title': '2 Related Work', 'number': '2'}"
"In the first phase, it extracts all synonyms from a thesaurus, such as WordNet, for the words to be substituted.","{'title': '2 Related Work', 'number': '2'}"
"In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access.","{'title': '2 Related Work', 'number': '2'}"
"However, it cannot generate other types of paraphrases but only synonym substitution.","{'title': '2 Related Work', 'number': '2'}"
"NLG-based methods: NLG-based methods (Kozlowski et al., 2003; Power and Scott, 2005) generally involve two stages.","{'title': '2 Related Work', 'number': '2'}"
"In the first one, the source sentence s is transformed into its semantic representation r by undertaking a series of NLP processing, including morphology analyzing, syntactic parsing, semantic role labeling, etc.","{'title': '2 Related Work', 'number': '2'}"
"In the second stage, a NLG system is employed to generate a sentence t from r. s and t are paraphrases as they are both derived from r. The NLG-based methods simulate human paraphrasing behavior, i.e., understanding a sentence and presenting the meaning in another way.","{'title': '2 Related Work', 'number': '2'}"
"However, deep analysis of sentences is a big challenge.","{'title': '2 Related Work', 'number': '2'}"
"Moreover, developing a NLG system is also not trivial.","{'title': '2 Related Work', 'number': '2'}"
"SMT-based methods: SMT-based methods viewed PG as monolingual MT, i.e., translating s into t that are in the same language.","{'title': '2 Related Work', 'number': '2'}"
"Researchers employ the existing SMT models for PG (Quirk et al., 2004).","{'title': '2 Related Work', 'number': '2'}"
"Similar to typical SMT, a large parallel corpus is needed as training data in the SMT-based PG.","{'title': '2 Related Work', 'number': '2'}"
"However, such data are difficult to acquire compared with the SMT data.","{'title': '2 Related Work', 'number': '2'}"
"Therefore, data shortage becomes the major limitation of the method.","{'title': '2 Related Work', 'number': '2'}"
"To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).","{'title': '2 Related Work', 'number': '2'}"
There have been researchers trying to propose uniform PG methods for multiple applications.,"{'title': '2 Related Work', 'number': '2'}"
"But they are either rule-based (Murata and Isahara, 2001; Takahashi et al., 2001) or thesaurusbased (Bolshakov and Gelbukh, 2004), thus they have some limitations as stated above.","{'title': '2 Related Work', 'number': '2'}"
"Furthermore, few of them conducted formal experiments to evaluate the proposed methods.","{'title': '2 Related Work', 'number': '2'}"
"Despite the similarity between PG and MT, the statistical model used in SMT cannot be directly The SPG method proposed in this work contains three components, i.e., sentence preprocessing, paraphrase planning, and paraphrase generation (Figure 1).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Sentence preprocessing mainly includes POS tagging and dependency parsing for the input sentences, as POS tags and dependency information are necessary for matching the paraphrase pattern and collocation resources in the following stages.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase planning (Section 3.3) aims to select the units to be paraphrased (called source units henceforth) in an input sentence and the candidate paraphrases for the source units (called target units) from multiple resources according to the given application A.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase generation (Section 3.4) is designed to generate paraphrases for the input sentences by selecting the optimal target units with a statistical model.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In this work, the multiple paraphrase resources are stored in paraphrase tables (PTs).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"A paraphrase table is similar to a phrase table in SMT, which contains fine-grained paraphrases, such as paraphrase phrases, patterns, or collocations.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
The PTs used in this work are constructed using different corpora and different score functions (Section 3.5).,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"If the applications are not considered, all units of an input sentence that can be paraphrased using the PTs will be extracted as source units.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Accordingly, all paraphrases for the source units will be extracted as target units.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"However, when a certain application is given, only the source and target units that can achieve the application will be kept.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"We call this process paraphrase planning, which is formally defined as in Figure 2.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
An example is depicted in Figure 3.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
The application in this example is sentence compression.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"All source and target units are listed below the input sentence, in which the first two source units are phrases, while the third and fourth are a pattern and a collocation, respectively.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"As can be seen, the first and fourth source units are filtered in paraphrase planning, since none of their paraphrases achieve the application (i.e., shorter in bytes than the source).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"The second and third source units are kept, but some of their paraphrases are filtered.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control the adequacy, fluency, and usability of the paraphrases, respectively1.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase Model: Paraphrase generation is a decoding process.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"The input sentence s is first segmented into a sequence of I units sI1, which are then paraphrased to a sequence of units �tI1.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Let (si, ti) be a pair of paraphrase units, their paraphrase likelihood is computed using a score function Opm(�si, ti).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Thus the paraphrase score ppm(9I1, �tI1) between s and t is decomposed into: where Apm is the weight of the paraphrase model.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In practice, the units of a sentence may be paraphrased using different PTs.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Suppose we have K PTs, (ski, tki) is a pair of paraphrase units from the k-th PT with the score function Ok(ski, �tki), then Equation (1) can be rewritten as: where Ak is the weight for Ok(Ski, tki).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Equation (2) assumes that a pair of paraphrase units is from only one paraphrase table.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"However, 1The SPG model applies monotone decoding, which does not contain a reordering sub-model that is often used in SMT.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Instead, we use the paraphrase patterns to achieve word reordering in paraphrase generation. we find that about 2% of the paraphrase units appear in two or more PTs.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In this case, we only count the PT that provides the largest paraphrase score, i.e., kˆ = arg maxk{φk(¯si, ¯ti)λk}.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In addition, note that there may be some units that cannot be paraphrased or prefer to keep unchanged during paraphrasing.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Therefore, we have a self-paraphrase table in the K PTs, which paraphrases any separate word w into itself with a constant score c: φself(w, w) = c (we set c = e−1).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Language Model: We use a tri-gram language model in this work.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"The language model based score for the paraphrase t is computed as: where J is the length of t, tj is the j-th word of t, and λlm is the weight for the language model.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Usability Model: The usability model prefers paraphrase units that can better achieve the application.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
The usability of t depends on paraphrase units it contains.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Hence the usability model where λum is the weight for the usability model and pum(¯si, ¯ti) is defined as follows: We consider three applications, including sentence compression, simplification, and similarity computation. µ(¯si, ¯ti) is defined separately for each: only the target units that can enhance the similarity to the reference sentence are kept in planning.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"We define µ(si, ti) = sim(�ti, s')− sim(si, s'), where sim(·, ·) is simply computed as the count of overlapping words.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"We combine the three sub-models based on a log-linear framework and get the SPG model: We use five PTs in this work (except the selfparaphrase table), in which each pair of paraphrase units has a score assigned by the score function of the corresponding method.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase phrases (PT-1 to PT-3): Paraphrase phrases are extracted from three corpora: lel translations of the same foreign novel).,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In our experiments, PT-1 is the largest, which contains 3,041,822 pairs of paraphrase phrases.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"PT-2 and PT-3 contain 92,358, and 17,668 pairs of paraphrase phrases, respectively.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase patterns (PT-4): Paraphrase patterns are also extracted from Corp-1.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"We applied the approach proposed in (Zhao et al., 2008b).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Its basic assumption is that if two English patterns e1 and e2 are aligned with the same foreign pattern f, then e1 and e2 are possible paraphrases.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"One can refer to (Zhao et al., 2008b) for the details.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"PT-4 contains 1,018,371 pairs of paraphrase patterns.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Paraphrase collocations (PT-5): Collocations4 can cover long distance dependencies in sentences.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
Thus paraphrase collocations are useful for SPG.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
We extract collocations from a monolingual corpus and use a binary classifier to recognize if any two collocations are paraphrases.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Due to the space limit, we cannot introduce the detail of the approach.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
We assign the score “1” for any pair of paraphrase collocations.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"PT-5 contains 238,882 pairs of paraphrase collocations.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"To estimate parameters λk(1 < k < K), λlm, and λum, we adopt the approach of minimum error rate training (MERT) that is popular in SMT (Och, 2003).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"In SMT, however, the optimization objective function in MERT is the MT evaluation criteria, such as BLEU.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"As we analyzed above, the BLEU-style criteria cannot be adapted in SPG.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
We therefore introduce a new optimization objective function in this paper.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
The basic assumption is that a paraphrase should contain as many correct unit replacements as possible.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Accordingly, we design the following criteria: Replacement precision (rp): rp assesses the precision of the unit replacements, which is defined as rp = cdev(+r)/cdev(r), where cdev(r) is the total number of unit replacements in the generated paraphrases on the development set. cdev(+r) is the number of the correct replacements.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Replacement rate (rr): rr measures the paraphrase degree on the development set, i.e., the percentage of words that are paraphrased.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"We define rr as: rr = wdev(r)/wdev(s), where wdev(r) is the total number of words in the replaced units on the development set, and wdev(s) is the number of words of all sentences on the development set.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Replacement f-measure (rf): We use rf as the optimization objective function in MERT, which is similar to the conventional f-measure and leverages rp and rr: rf = (2 x rp x rr)/(rp + rr).","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
We estimate parameters for each paraphrase application separately.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"For each application, we first ask two raters to manually label all possible unit replacements on the development set as correct or incorrect, so that rp, rr, and rf can be automatically computed under each set of parameters.","{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
The parameters that result in the highest rf on the development set are finally selected.,"{'title': '3 Statistical Paraphrase Generation', 'number': '3'}"
"Our SPG decoder is developed by remodeling Moses that is widely used in SMT (Hoang and Koehn, 2008).","{'title': '4 Experimental Setup', 'number': '4'}"
"The POS tagger and dependency parser for sentence preprocessing are SVMTool (Gimenez and Marquez, 2004) and MSTParser (McDonald et al., 2006).","{'title': '4 Experimental Setup', 'number': '4'}"
The language model is trained using a 9 GB English corpus.,"{'title': '4 Experimental Setup', 'number': '4'}"
Our method is not restricted in domain or sentence style.,"{'title': '4 Experimental Setup', 'number': '4'}"
Thus any sentence can be used in development and test.,"{'title': '4 Experimental Setup', 'number': '4'}"
"However, for the sentence similarity computation purpose in our experiments, we want to evaluate if the method can enhance the stringlevel similarity between two paraphrase sentences.","{'title': '4 Experimental Setup', 'number': '4'}"
"Therefore, for each input sentence s, we need a reference sentence s' for similarity computation.","{'title': '4 Experimental Setup', 'number': '4'}"
"Based on the above consideration, we acquire experiment data from the human references of the MT evaluation, which provide several human translations for each foreign sentence.","{'title': '4 Experimental Setup', 'number': '4'}"
"In detail, we use the first translation of a foreign sentence as the source s and the second translation as the reference s' for similarity computation.","{'title': '4 Experimental Setup', 'number': '4'}"
"In our experiments, the development set contains 200 sentences and the test set contains 500 sentences, both of which are randomly selected from the human translations of 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.","{'title': '4 Experimental Setup', 'number': '4'}"
"The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).","{'title': '4 Experimental Setup', 'number': '4'}"
"The generated paraphrases are manually evaluated based on three criteria, i.e., adequacy, fluency, and usability, each of which has three scales from 1 to 3.","{'title': '4 Experimental Setup', 'number': '4'}"
Here is a brief description of the different scales for the criteria:,"{'title': '4 Experimental Setup', 'number': '4'}"
We use our method to generate paraphrases for the three applications.,"{'title': '5 Results and Analysis', 'number': '5'}"
"Results show that the percentages of test sentences that can be paraphrased are 97.2%, 95.4%, and 56.8% for the applications of sentence compression, simplification, and similarity computation, respectively.","{'title': '5 Results and Analysis', 'number': '5'}"
"The reason why the last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit replacements from the PTs that improve the similarity to the reference sentences.","{'title': '5 Results and Analysis', 'number': '5'}"
"For the other applications, only some very short sentences cannot be paraphrased.","{'title': '5 Results and Analysis', 'number': '5'}"
"Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation.","{'title': '5 Results and Analysis', 'number': '5'}"
It also indicates that sentence similarity computation is more difficult than the other two applications.,"{'title': '5 Results and Analysis', 'number': '5'}"
We ask two raters to label the paraphrases based on the criteria defined in Section 4.2.,"{'title': '5 Results and Analysis', 'number': '5'}"
The labeling results are shown in the upper part of Table 1.,"{'title': '5 Results and Analysis', 'number': '5'}"
"We can see that for adequacy and fluency, the paraphrases in sentence similarity computation get the highest scores.","{'title': '5 Results and Analysis', 'number': '5'}"
About 70% of the paraphrases are labeled “3”.,"{'title': '5 Results and Analysis', 'number': '5'}"
"This is because in sentence similarity computation, only the target units appearing in the reference sentences are kept in paraphrase planning.","{'title': '5 Results and Analysis', 'number': '5'}"
This constraint filters most of the noise.,"{'title': '5 Results and Analysis', 'number': '5'}"
The adequacy and fluency scores of the other two applications are not high.,"{'title': '5 Results and Analysis', 'number': '5'}"
The percentages of label “3” are around 30%.,"{'title': '5 Results and Analysis', 'number': '5'}"
The main reason is that the average numbers of unit replacements for these two applications are much larger than sentence similarity computation.,"{'title': '5 Results and Analysis', 'number': '5'}"
"It is thus more likely to bring in incorrect unit replacements, which influence the quality of the generated paraphrases.","{'title': '5 Results and Analysis', 'number': '5'}"
"The usability is needed to be manually labeled only for sentence simplification, since it can be automatically labeled in the other two applications.","{'title': '5 Results and Analysis', 'number': '5'}"
"As shown in Table 1, for sentence simplification, most paraphrases are labeled “2” in usability, while merely less than 20% are labeled “3”.","{'title': '5 Results and Analysis', 'number': '5'}"
We conjecture that it is because the raters are not sensitive to the slight change of the simplification degree.,"{'title': '5 Results and Analysis', 'number': '5'}"
Thus they labeled “2” in most cases.,"{'title': '5 Results and Analysis', 'number': '5'}"
We compute the kappa statistic between the raters.,"{'title': '5 Results and Analysis', 'number': '5'}"
"Kappa is defined as K = P(A)−P(E) (Car1−P(E)letta, 1996), where P(A) is the proportion of times that the labels agree, and P(E) is the proportion of times that they may agree by chance.","{'title': '5 Results and Analysis', 'number': '5'}"
"We define P(E) = 13 , as the labeling is based on three point scales.","{'title': '5 Results and Analysis', 'number': '5'}"
"The results show that the kappa statistics for adequacy and fluency are 0.6560 and 0.6500, which indicates a substantial agreement (K: 0.610.8) according to (Landis and Koch, 1977).","{'title': '5 Results and Analysis', 'number': '5'}"
"The kappa statistic for usability is 0.5849, which is only moderate (K: 0.41-0.6).","{'title': '5 Results and Analysis', 'number': '5'}"
Table 2 shows an example of the generated paraphrases.,"{'title': '5 Results and Analysis', 'number': '5'}"
"A source sentence s is paraphrased in each application and we can see that: (1) for sentence compression, the paraphrase t is 8 bytes shorter than s; (2) for sentence simplification, the words wealth and part in t are easier than their sources asset and proportion, especially for the non-native speakers; (3) for sentence similarity computation, the reference sentence s' is listed below t, in which the words appearing in t but not in s are highlighted in blue.","{'title': '5 Results and Analysis', 'number': '5'}"
"In our experiments, we implement two baseline methods for comparison: Baseline-1: Baseline-1 follows the method proposed in (Quirk et al., 2004), which generates paraphrases using typical SMT tools.","{'title': '5 Results and Analysis', 'number': '5'}"
"Similar to Quirk et al.’s method, we extract a paraphrase table for the SMT model from a monolingual comparable corpus (PT-2 described above).","{'title': '5 Results and Analysis', 'number': '5'}"
The SMT decoder used in Baseline-1 is Moses.,"{'title': '5 Results and Analysis', 'number': '5'}"
Baseline-2: Baseline-2 extends Baseline-1 by combining multiple resources.,"{'title': '5 Results and Analysis', 'number': '5'}"
It exploits all PTs introduced above in the same way as our proposed method.,"{'title': '5 Results and Analysis', 'number': '5'}"
The difference from our method is that Baseline-2 does not take different applications into consideration.,"{'title': '5 Results and Analysis', 'number': '5'}"
Thus it contains no paraphrase planning stage or the usability sub-model.,"{'title': '5 Results and Analysis', 'number': '5'}"
We tune the parameters for the two baselines using the development data as described in Section 3.6 and evaluate them with the test data.,"{'title': '5 Results and Analysis', 'number': '5'}"
"Since paraphrase applications are not considered by the baselines, each baseline method outputs a single best paraphrase for each test sentence.","{'title': '5 Results and Analysis', 'number': '5'}"
The generation results show that 93% and 97.8% of the test sentences can be paraphrased by Baseline-1 and Baseline-2.,"{'title': '5 Results and Analysis', 'number': '5'}"
"The average number of unit replacements per sentence is 4.23 and 5.95, respectively.","{'title': '5 Results and Analysis', 'number': '5'}"
"This result suggests that Baseline-1 is less capable than Baseline-2, which is mainly because its paraphrase resource is limited.","{'title': '5 Results and Analysis', 'number': '5'}"
The generated paraphrases are also labeled by our two raters and the labeling results can be found in the lower part of Table 1.,"{'title': '5 Results and Analysis', 'number': '5'}"
"As can be seen, Baseline-1 performs poorly compared with our method and Baseline-2, as the percentage of label “1” is the highest for both adequacy and fluency.","{'title': '5 Results and Analysis', 'number': '5'}"
This result demonstrates that it is necessary to combine multiple paraphrase resources to improve the paraphrase generation performance.,"{'title': '5 Results and Analysis', 'number': '5'}"
Table 1 also shows that Baseline-2 performs comparably with our method except that it does not consider paraphrase applications.,"{'title': '5 Results and Analysis', 'number': '5'}"
"However, we are interested how many paraphrases generated by Baseline-2 can achieve the given applications by chance.","{'title': '5 Results and Analysis', 'number': '5'}"
"After analyzing the results, we find that 24.95%, 8.79%, and 7.16% of the paraphrases achieve sentence compression, simplification, and similarity computation, respectively, which are much lower than our method.","{'title': '5 Results and Analysis', 'number': '5'}"
"Previous research regarded sentence compression, simplification, and similarity computation as totally different problems and proposed distinct method for each one.","{'title': '5 Results and Analysis', 'number': '5'}"
"Therefore, it is interesting to compare our method to the application-specific methods.","{'title': '5 Results and Analysis', 'number': '5'}"
"However, it is really difficult for us to reimplement the methods purposely designed for these applications.","{'title': '5 Results and Analysis', 'number': '5'}"
Thus here we just conduct an informal comparison with these methods.,"{'title': '5 Results and Analysis', 'number': '5'}"
"Sentence compression: Sentence compression is widely studied, which is mostly reviewed as a word deletion task.","{'title': '5 Results and Analysis', 'number': '5'}"
"Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process.","{'title': '5 Results and Analysis', 'number': '5'}"
"Besides, they also used paraphrase patterns extracted from bilingual parallel corpora (like our PT-4) as a kind of rewriting resource.","{'title': '5 Results and Analysis', 'number': '5'}"
"However, as most other sentence compression methods, their method allows information loss after compression, which means that the generated sentences are not necessarily paraphrases of the source sentences.","{'title': '5 Results and Analysis', 'number': '5'}"
Sentence Simplification: Carroll et al. (1999) has proposed an automatic text simplification method for language-impaired readers.,"{'title': '5 Results and Analysis', 'number': '5'}"
"Their method contains two main parts, namely the lexical simplifier and syntactic simplifier.","{'title': '5 Results and Analysis', 'number': '5'}"
"The former one focuses on replacing words with simpler synonyms, while the latter is designed to transfer complex syntactic structures into easy ones (e.g., replacing passive sentences with active forms).","{'title': '5 Results and Analysis', 'number': '5'}"
"Our method is, to some extent, simpler than Carroll et al.’s, since our method does not contain syntactic simplification strategies.","{'title': '5 Results and Analysis', 'number': '5'}"
We will try to address sentence restructuring in our future work.,"{'title': '5 Results and Analysis', 'number': '5'}"
Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation.,"{'title': '5 Results and Analysis', 'number': '5'}"
"They paraphrase a sentence s by replacing its words with WordNet synonyms, so that s can be more similar in wording to another sentence s'.","{'title': '5 Results and Analysis', 'number': '5'}"
"A similar method has also been proposed in (Zhou et al., 2006), which uses paraphrase phrases like our PT-1 instead of WordNet synonyms.","{'title': '5 Results and Analysis', 'number': '5'}"
"These methods can be roughly viewed as special cases of ours, which only focus on the sentence similarity computation application and only use one kind of paraphrase resource.","{'title': '5 Results and Analysis', 'number': '5'}"
This paper proposes a method for statistical paraphrase generation.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
The contributions are as follows.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
"(1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
"(2) It generates paraphrases for different applications with a uniform model, rather than presenting distinct methods for each application.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
"(3) It uses multiple resources, including paraphrase phrases, patterns, and collocations, to relieve data shortage and generate more varied and interesting paraphrases.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
Our future work will be carried out along two directions.,"{'title': '6 Conclusions and Future Work', 'number': '6'}"
"First, we will improve the components of the method, especially the paraphrase planning algorithm.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
"The algorithm currently used is simple but greedy, which may miss some useful paraphrase units.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
"Second, we will extend the method to other applications, We hope it can serve as a universal framework for most if not all applications.","{'title': '6 Conclusions and Future Work', 'number': '6'}"
"The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).","{'title': 'Acknowledgements', 'number': '7'}"
"Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.","{'title': 'Acknowledgements', 'number': '7'}"
