col1,col2
"A significant portion of the world’s text is tagged by readers on social bookmarkwebsites. attribution an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document.",{}
Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa.,{}
This introduces a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA’s latent topics and user tags.,{}
This allows Labeled LDA to directly learn word-tag correspondences.,{}
We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web from Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets.,{}
"As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.",{}
"From news sources such as Reuters to modern community web portals like del.icio.us, a significant proportion of the world’s textual data is labeled with multiple human-provided tags.","{'title': '1 Introduction', 'number': '1'}"
"These collections reflect the fact that documents are often about more than one thing—for example, a news story about a highway transportation bill might naturally be filed under both transportation and politics, with neither category acting as a clear subset of the other.","{'title': '1 Introduction', 'number': '1'}"
"Similarly, a single web page in del.icio.us might well be annotated with tags as diverse as arts, physics, alaska, and beauty.","{'title': '1 Introduction', 'number': '1'}"
"However, not all tags apply with equal specificity across the whole document, opening up new opportunities for information retrieval and corpus analysis on tagged corpora.","{'title': '1 Introduction', 'number': '1'}"
"For instance, users who browse for documents with a particular tag might prefer to see summaries that focus on the portion of the document most relevant to the tag, a task we call tag-specific snippet extraction.","{'title': '1 Introduction', 'number': '1'}"
"And when a user browses to a particular document, a tag-augmented user interface might provide overview visualization cues highlighting which portions of the document are more or less relevant to the tag, helping the user quickly access the information they seek.","{'title': '1 Introduction', 'number': '1'}"
One simple approach to these challenges can be found in models that explicitly address the credit attribution problem by associating individual words in a document with their most appropriate labels.,"{'title': '1 Introduction', 'number': '1'}"
"For instance, in our news story about the transportation bill, if the model knew that the word “highway” went with transportation and that the word “politicians” went with politics, more relevant passages could be extracted for either label.","{'title': '1 Introduction', 'number': '1'}"
We seek an approach that can automatically learn the posterior distribution of each word in a document conditioned on the document’s label set.,"{'title': '1 Introduction', 'number': '1'}"
"One promising approach to the credit attribution problem lies in the machinery of Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization.","{'title': '1 Introduction', 'number': '1'}"
LDA is a completely unsupervised algorithm that models each document as a mixture of topics.,"{'title': '1 Introduction', 'number': '1'}"
"The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic, and further infers per-document discrete distributions over topics.","{'title': '1 Introduction', 'number': '1'}"
"Most importantly, LDA makes the explicit assumption that each word is generated from one underlying topic.","{'title': '1 Introduction', 'number': '1'}"
"Although LDA is expressive enough to model multiple topics per document, it is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure.","{'title': '1 Introduction', 'number': '1'}"
"In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an end-use application, even when time and resources exist to provide some document labels.","{'title': '1 Introduction', 'number': '1'}"
Several modifications of LDA to incorporate supervision have been proposed in the literature.,"{'title': '1 Introduction', 'number': '1'}"
"Two such models, Supervised LDA (Blei and McAuliffe, 2007) and DiscLDA (Lacoste-Julien et al., 2008) are inappropriate for multiply labeled corpora because they limit a document to being associated with only a single label.","{'title': '1 Introduction', 'number': '1'}"
Supervised LDA posits that a label is generated from each document’s empirical topic mixture distribution.,"{'title': '1 Introduction', 'number': '1'}"
DiscLDA associates a single categorical label variable with each document and associates a topic mixture with each label.,"{'title': '1 Introduction', 'number': '1'}"
"A third model, MM-LDA (Ramage et al., 2009), is not constrained to one label per document because it models each document as a bag of words with a bag of labels, with topics for each observation drawn from a shared topic distribution.","{'title': '1 Introduction', 'number': '1'}"
"But, like the other models, MM-LDA’s learned topics do not correspond directly with the label set.","{'title': '1 Introduction', 'number': '1'}"
"Consequently, these models fall short as a solution to the credit attribution problem.","{'title': '1 Introduction', 'number': '1'}"
"Because labels have meaning to the people that assigned them, a simple solution to the credit attribution problem is to assign a document’s words to its labels rather than to a latent and possibly less interpretable semantic space.","{'title': '1 Introduction', 'number': '1'}"
"This paper presents Labeled LDA (L-LDA), a generative model for multiply labeled corpora that marries the multi-label supervision common to modern text datasets with the word-assignment ambiguity resolution of the LDA family of models.","{'title': '1 Introduction', 'number': '1'}"
"In contrast to standard LDA and its existing supervised variants, our model associates each label with one topic in direct correspondence.","{'title': '1 Introduction', 'number': '1'}"
"In the following section, L-LDA is shown to be a natural extension of both LDA (by incorporating supervision) and Multinomial Naive Bayes (by incorporating a mixture model).","{'title': '1 Introduction', 'number': '1'}"
We demonstrate that L-LDA can go a long way toward solving the credit attribution problem in multiply labeled documents with improved interpretability over LDA (Section 4).,"{'title': '1 Introduction', 'number': '1'}"
"We show that L-LDA’s credit attribution ability enables it to greatly outperform supFigure 1: Graphical model of Labeled LDA: unlike standard LDA, both the label set Λ as well as the topic prior α influence the topic mixture e. port vector machines on a tag-driven snippet extraction task on web pages from del.icio.us (Section 6).","{'title': '1 Introduction', 'number': '1'}"
"And despite its generative semantics, we show that Labeled LDA is competitive with a strong baseline discriminative classifier on two multi-label text classification tasks (Section 7).","{'title': '1 Introduction', 'number': '1'}"
Labeled LDA is a probabilistic graphical model that describes a process for generating a labeled document collection.,"{'title': '2 Labeled LDA', 'number': '2'}"
"Like Latent Dirichlet Allocation, Labeled LDA models each document as a mixture of underlying topics and generates each word from one topic.","{'title': '2 Labeled LDA', 'number': '2'}"
"Unlike LDA, L-LDA incorporates supervision by simply constraining the topic model to use only those topics that correspond to a document’s (observed) label set.","{'title': '2 Labeled LDA', 'number': '2'}"
"The model description that follows assumes the reader is familiar with the basic LDA model (Blei et al., 2003).","{'title': '2 Labeled LDA', 'number': '2'}"
"Let each document d be represented by a tuple consisting of a list of word indices w(d) = (w1, ... , wNd) and a list of binary topic presence/absence indicators Λ(d) = (l1,...,lK) where each wz E {1, ... , V } and each lk E {0,1}.","{'title': '2 Labeled LDA', 'number': '2'}"
"Here Nd is the document length, V is the vocabulary size and K the total number of unique labels in the corpus.","{'title': '2 Labeled LDA', 'number': '2'}"
We set the number of topics in Labeled LDA to be the number of unique labels K in the corpus.,"{'title': '2 Labeled LDA', 'number': '2'}"
The generative process for the algorithm is found in Table 1.,"{'title': '2 Labeled LDA', 'number': '2'}"
"Steps 1 and 2—drawing the multinomial topic distributions over vocabulary )3k for each topic k, from a Dirichlet prior 77—remain the same as for traditional LDA (see (Blei et al., 2003), page 4).","{'title': '2 Labeled LDA', 'number': '2'}"
"The traditional LDA model then draws a multinomial mixture distribution e(d) over all K topics, for each document d, from a Dirichlet prior α.","{'title': '2 Labeled LDA', 'number': '2'}"
"However, we would like to restrict e(d) to be defined only over the topics that correspond to ,C3k is a vector consisting of the parameters of the multinomial distribution corresponding to the kth topic, α are the parameters of the Dirichlet topic prior and 77 are the parameters of the word prior, while Φk is the label prior for topic k. For the meaning of the projection matrix L(d), please refer to Eq 1. its labels A(d).","{'title': '2 Labeled LDA', 'number': '2'}"
"Since the word-topic assignments zi (see step 9 in Table 1) are drawn from this distribution, this restriction ensures that all the topic assignments are limited to the document’s labels.","{'title': '2 Labeled LDA', 'number': '2'}"
"Towards this objective, we first generate the document’s labels A(d) using a Bernoulli coin toss for each topic k, with a labeling prior probability Φk, as shown in step 5.","{'title': '2 Labeled LDA', 'number': '2'}"
"Next, we define the vector of document’s labels to be X(d) = {k|Λ(d) k = 1}.","{'title': '2 Labeled LDA', 'number': '2'}"
"This allows us to define a document-specific label projection matrix L(d) of size Md × K for each document d, where Md = |X(d)|, as follows: For each row i ∈ {1, ... , Md} and column j ∈ {1,...,K} : In other words, the ith row of L(d) has an entry of 1 in column j if and only if the ith document label A(d) i is equal to the topic j, and zero otherwise.","{'title': '2 Labeled LDA', 'number': '2'}"
"As the name indicates, we use the L(d) matrix to project the parameter vector of the Dirichlet topic prior α = (α1, ... , αK)T to a lower dimensional vector α(d) as follows: Clearly, the dimensions of the projected vector correspond to the topics represented by the labels of the document.","{'title': '2 Labeled LDA', 'number': '2'}"
"For example, suppose K = 4 and that a document d has labels given by A(d) = {0,1,1, 0} which implies X(d) = {2, 3}, then L(d) Then, 0(d) is drawn from a Dirichlet distribution with parameters α(d) = L(d) × α = (α2, α3)T (i.e., with the Dirichlet restricted to the topics 2 and 3).","{'title': '2 Labeled LDA', 'number': '2'}"
This fulfills our requirement that the document’s topics are restricted to its own labels.,"{'title': '2 Labeled LDA', 'number': '2'}"
The projection step constitutes the deterministic step 6 in Table 1.,"{'title': '2 Labeled LDA', 'number': '2'}"
The remaining part of the model from steps 7 through 10 are the same as for regular LDA.,"{'title': '2 Labeled LDA', 'number': '2'}"
The dependency of 0 on both α and A is indicated by directed edges from A and α to 0 in the plate notation in Figure 1.,"{'title': '2 Labeled LDA', 'number': '2'}"
"This is the only additional dependency we introduce in LDA’s representation (please compare with Figure 1 in (Blei et al., 2003)).","{'title': '2 Labeled LDA', 'number': '2'}"
"In most applications discussed in this paper, we will assume that the documents are multiply tagged with human labels, both at learning and inference time.","{'title': '2 Labeled LDA', 'number': '2'}"
"When the labels A(d) of the document are observed, the labeling prior Φ is d-separated from the rest of the model given A(d).","{'title': '2 Labeled LDA', 'number': '2'}"
"Hence the model is same as traditional LDA, except the constraint that the topic prior α(d) is now restricted to the set of labeled topics X(d).","{'title': '2 Labeled LDA', 'number': '2'}"
"Therefore, we can use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) for training where the sampling probability for a topic for position i in a document d in Labeled LDA is given by: where nwi −i,j is the count of word wi in topic j, that does not include the current assignment zi, a missing subscript or superscript (e.g. n(·) −i,j)) indicates a summation over that dimension, and 1 is a vector of 1’s of appropriate dimension.","{'title': '2 Labeled LDA', 'number': '2'}"
"Although the equation above looks exactly the same as that of LDA, we have an important distinction in that, the target topic j is restricted to belong to the set of labels, i.e., j ∈ X(d).","{'title': '2 Labeled LDA', 'number': '2'}"
"Once the topic multinomials ,C3 are learned from the training set, one can perform inference on any new labeled test document using Gibbs sampling restricted to its tags, to determine its per-word label assignments z.","{'title': '2 Labeled LDA', 'number': '2'}"
"In addition, one can also compute its posterior distribution θ over topics by appropriately normalizing the topic assignments z.","{'title': '2 Labeled LDA', 'number': '2'}"
It should now be apparent to the reader how the new model addresses some of the problems in multi-labeled corpora that we highlighted in Section 1.,"{'title': '2 Labeled LDA', 'number': '2'}"
"For example, since there is a one-to-one correspondence between the labels and topics, the model can display automatic topical summaries for each label k in terms of the topic-specific distribution βk.","{'title': '2 Labeled LDA', 'number': '2'}"
"Similarly, since the model assigns a label zz to each word wz in the document d automatically, we can now extract portions of the document relevant to each label k (it would be all words wz E w(d) such that zz = k).","{'title': '2 Labeled LDA', 'number': '2'}"
"In addition, we can use the topic distribution θ(d) to rank the user specified labels in the order of their relevance to the document, thereby also eliminating spurious ones if necessary.","{'title': '2 Labeled LDA', 'number': '2'}"
"Finally, we note that other less restrictive variants of the proposed L-LDA model are possible.","{'title': '2 Labeled LDA', 'number': '2'}"
"For example, one could consider a version that allows topics that do not correspond to the label set of a given document with a small probability, or one that allows a common background topic in all documents.","{'title': '2 Labeled LDA', 'number': '2'}"
"We did implement these variants in our preliminary experiments, but they did not yield better performance than L-LDA in the tasks we considered.","{'title': '2 Labeled LDA', 'number': '2'}"
Hence we do not report them in this paper.,"{'title': '2 Labeled LDA', 'number': '2'}"
The derivation of the algorithm so far has focused on its relationship to LDA.,"{'title': '2 Labeled LDA', 'number': '2'}"
"However, Labeled LDA can also be seen as an extension of the event model of a traditional Multinomial Naive Bayes classifier (McCallum and Nigam, 1998) by the introduction of a mixture model.","{'title': '2 Labeled LDA', 'number': '2'}"
"In this section, we develop the analogy as another way to understand L-LDA from a supervised perspective.","{'title': '2 Labeled LDA', 'number': '2'}"
Consider the case where no document in the collection is assigned two or more labels.,"{'title': '2 Labeled LDA', 'number': '2'}"
"Now for a particular document d with label ld, Labeled LDA draws each word’s topic variable zz from a multinomial constrained to the document’s label set, i.e. zz = ld for each word position i in the document.","{'title': '2 Labeled LDA', 'number': '2'}"
"During learning, the Gibbs sampler will assign each zz to ld while incrementing Old(wz), effectively counting the occurences of each word type in documents labeled with ld.","{'title': '2 Labeled LDA', 'number': '2'}"
"Thus in the singly labeled document case, the probability of each document under Labeled LDA is equal to the probability of the document under the Multinomial Naive Bayes event model trained on those same document instances.","{'title': '2 Labeled LDA', 'number': '2'}"
"Unlike the Multinomial Naive Bayes classifier, Labeled LDA does not encode a decision boundary for unlabeled documents by comparing P(w(d)|ld) to P(w(d)|¬ld), although we discuss using Labeled LDA for multilabel classification in Section 7.","{'title': '2 Labeled LDA', 'number': '2'}"
Labeled LDA’s similarity to Naive Bayes ends with the introduction of a second label to any document.,"{'title': '2 Labeled LDA', 'number': '2'}"
"In a traditional one-versus-rest Multinomial Naive Bayes model, a separate classifier for each label would be trained on all documents with that label, so each word can contribute a count of 1 to every observed label’s word distribution.","{'title': '2 Labeled LDA', 'number': '2'}"
"By contrast, Labeled LDA assumes that each document is a mixture of underlying topics, so the count mass of single word instance must instead be distributed over the document’s observed labels.","{'title': '2 Labeled LDA', 'number': '2'}"
Social bookmarking websites contain millions of tags describing many of the web’s most popular and useful pages.,"{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"However, not all tags are uniformly appropriate at all places within a document.","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"In the sections that follow, we examine mechanisms by which Labeled LDA’s credit assignment mechanism can be utilized to help support browsing and summarizing tagged document collections.","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"To create a consistent dataset for experimenting with our model, we selected 20 tags of medium to high frequency from a collection of documents dataset crawled from del.icio.us, a popular social bookmarking website (Heymann et al., 2008).","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"From that larger dataset, we selected uniformly at random four thousand documents that contained at least one of the 20 tags, and then filtered each document’s tag set by removing tags not present in our tag set.","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"After filtering, the resulting corpus averaged 781 non-stop words per document, with each document having 4 distinct tags on average.","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"In contrast to many existing text datasets, our tagged corpus is highly multiply labeled: almost 90% of of the documents have more than one tag.","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
"(For comparison, less than one third of the news documents in the popular RCV1-v2 collection of newswire are multiply labeled).","{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
We will refer to this collection of data as the del.icio.us tag dataset.,"{'title': '3 Credit attribution within tagged documents', 'number': '3'}"
A first question we ask of Labeled LDA is how its topics compare with those learned by traditional LDA on the same collection of documents.,"{'title': '4 Topic Visualization', 'number': '4'}"
We ran our implementations of Labeled LDA and LDA on the del.icio.us corpus described above.,"{'title': '4 Topic Visualization', 'number': '4'}"
"Both are based on the standard collapsed Gibbs sampler, with the constraints for Labeled LDA implemented as in Section 2.","{'title': '4 Topic Visualization', 'number': '4'}"
"Figure 2: Comparison of some of the 20 topics learned on del.icio.us by Labeled LDA (left) and traditional LDA (right), with representative words for each topic shown in the boxes.","{'title': '4 Topic Visualization', 'number': '4'}"
Labeled LDA’s topics are named by their associated tag.,"{'title': '4 Topic Visualization', 'number': '4'}"
Arrows from right-to-left show the mapping of LDA topics to the closest Labeled LDA topic by cosine similarity.,"{'title': '4 Topic Visualization', 'number': '4'}"
"Tags not shown are: design, education, english, grammar, history, internet, language, philosophy, politics, programming, reference, style, writing.","{'title': '4 Topic Visualization', 'number': '4'}"
Figure 2 shows the top words associated with 20 topics learned by Labeled LDA and 20 topics learned by unsupervised LDA on the del.icio.us document collection.,"{'title': '4 Topic Visualization', 'number': '4'}"
"Labeled LDA’s topics are directly named with the tag that corresponds to each topic, an improvement over standard practice of inferring the topic name by inspection (Mei et al., 2007).","{'title': '4 Topic Visualization', 'number': '4'}"
The topics learned by the unsupervised variant were matched to a Labeled LDA topic highest cosine similarity.,"{'title': '4 Topic Visualization', 'number': '4'}"
"The topics selected are representative: compared to Labeled LDA, unmodified LDA allocates many topics for describing the largest parts of the The Elements of Style, William Strunk, Jr.","{'title': '4 Topic Visualization', 'number': '4'}"
"Asserting that one must first know the rules to break them, this classic reference book is a must-have for any student and conscientious writer.","{'title': '4 Topic Visualization', 'number': '4'}"
"Intended for use in which the practice of composition is combined with the study of literature,it gives in brief space the principal requirements of plain English style and concentratesattention on the rules of usage and principles of composition most commonly violated. corpus and under-represents tags that are less uncommon: of the 20 topics learned, LDA learned multiple topics mapping to each of five tags (web, culture, and computer, reference, and politics, all of which were common in the dataset) and learned no topics that aligned with six tags (books, english, science, history, grammar, java, and philosophy, which were rarer).","{'title': '4 Topic Visualization', 'number': '4'}"
"In addition to providing automatic summaries of the words best associated with each tag in the corpus, Labeled LDA’s credit attribution mechanism can be used to augment the view of a single document with rich contextual information about the document’s tags.","{'title': '5 Tagged document visualization', 'number': '5'}"
"Figure 3 shows one web document from the collection, a page describing a guide to writing English prose.","{'title': '5 Tagged document visualization', 'number': '5'}"
"The 10 most common tags for that document are writing, reference, english, grammar, style, language, books, book, strunk, and education, the first eight of which were included in our set of 20 tags.","{'title': '5 Tagged document visualization', 'number': '5'}"
"In the figure, each word that has high posterior probability from one tag has been annotated with that tag.","{'title': '5 Tagged document visualization', 'number': '5'}"
"The red words come from the style tag, green from the grammar tag, blue from the reference tag, and black from the education tag.","{'title': '5 Tagged document visualization', 'number': '5'}"
"In this case, the model does very well at assigning individual words to the tags that, subjectively, seem to strongly imply the presence of that tag on this page.","{'title': '5 Tagged document visualization', 'number': '5'}"
A more polished rendering could add subtle visual cues about which parts of a page are most appropriate for a particular set of tags.,"{'title': '5 Tagged document visualization', 'number': '5'}"
Tag (Labeled LDA) (LDA) Topic ID book image pdf review library posted read copyright books title works water map human life work science time world years sleep windows file version linux computerfree system software mac comment god jesus people gospel bible reply lord religion written applications spring open web java pattern eclipse development ajax people day link posted time comments back music jane permalink comments read nice post great april blog march june wordpress news information service web online project site free search home web images design content java css website articles page learning jun quote pro views added check anonymous card core power ghz life written jesus words made man called mark john person fact name house light radio media photography news music travel cover game review street public art health food city history science books L-LDA this classic reference book is a must-have for any student and conscientious writer.,"{'title': '5 Tagged document visualization', 'number': '5'}"
Intended for SVM the rules of usage and principles of composition most commonly violated.,"{'title': '5 Tagged document visualization', 'number': '5'}"
Search: CONTENTS Bibliographic language L-LDA the beginning of a sentence must refer to the grammatical subject 8.,"{'title': '5 Tagged document visualization', 'number': '5'}"
"Divide words at SVM combined with the study of literature, it gives in brief space the principal requirements of","{'title': '5 Tagged document visualization', 'number': '5'}"
Another natural application of Labeled LDA’s credit assignment mechanism is as a means of selecting snippets of a document that best describe its contents from the perspective of a particular tag.,"{'title': '6 Snippet Extraction', 'number': '6'}"
Consider again the document in Figure 3.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"Intuitively, if this document were shown to a user interested in the tag grammar, the most appropriate snippet of words might prefer to contain the phrase “rules of usage,” whereas a user interested in the term style might prefer the title “Elements of Style.” To quantitatively evaluate Labeled LDA’s performance at this task, we constructed a set of 29 recently tagged documents from del.icio.us that were labeled with two or more tags from the 20 tag subset, resulting in a total of 149 (document,tag) pairs.","{'title': '6 Snippet Extraction', 'number': '6'}"
"For each pair, we extracted a 15-word window with the highest tag-specific score from the document.","{'title': '6 Snippet Extraction', 'number': '6'}"
Two systems were used to score each window: Labeled LDA and a collection of onevs-rest SVMs trained for each tag in the system.,"{'title': '6 Snippet Extraction', 'number': '6'}"
L-LDA scored each window as the expected probability that the tag had generated each word.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"For SVMs, each window was taken as its own document and scored using the tag-specific SVM’s un-thresholded scoring function, taking the window with the most positive score.","{'title': '6 Snippet Extraction', 'number': '6'}"
While a complete solution to the tag-specific snippet extraction quality as extracted by L-LDA and SVM.,"{'title': '6 Snippet Extraction', 'number': '6'}"
The center column is the number of document-tag pairs for which a system’s snippet was judged superior.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"The right column is the number of snippets for which all three annotators were in complete agreement (numerator) in the subset of document scored by all three annotators (denominator). problem might be more informed by better linguistic features (such as phrase boundaries), this experimental setup suffices to evaluate both kinds of models for their ability to appropriately assign words to underlying labels.","{'title': '6 Snippet Extraction', 'number': '6'}"
Figure 3 shows some example snippets output by our system for this document.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"Note that while SVMs did manage to select snippets that were vaguely on topic, Labeled LDA’s outputs are generally of superior subjective quality.","{'title': '6 Snippet Extraction', 'number': '6'}"
"To quantify this intuition, three human annotators rated each pair of snippets.","{'title': '6 Snippet Extraction', 'number': '6'}"
"The outputs were randomly labeled as “System A” or “System B,” and the annotators were asked to judge which system generated a better tag-specific document subset.","{'title': '6 Snippet Extraction', 'number': '6'}"
The judges were also allowed to select neither system if there was no clear winner.,"{'title': '6 Snippet Extraction', 'number': '6'}"
The results are summarized in Table 2.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"L-LDA was judged superior by a wide margin: of the 149 judgments, L-LDA’s output was selected as preferable in 72 cases, whereas SVM’s was selected in only 21.","{'title': '6 Snippet Extraction', 'number': '6'}"
The difference between these scores was highly significant (p < .001) by the sign test.,"{'title': '6 Snippet Extraction', 'number': '6'}"
"To quantify the reliability of the judgments, 51 of the 149 document-tag pairs were labeled by all three annotators.","{'title': '6 Snippet Extraction', 'number': '6'}"
"In this group, the judgments were in substantial agreement,1 with Fleiss’ Kappa at .63.","{'title': '6 Snippet Extraction', 'number': '6'}"
Further analysis of the triply-annotated subset yields further evidence of L-LDA’s advantage over SVM’s: 33 of the 51 were tag-page pairs where L-LDA’s output was picked by at least one annotator as a better snippet (although L-LDA might not have been picked by the other annotators).,"{'title': '6 Snippet Extraction', 'number': '6'}"
"And of those, 24 were unanimous in that all three judges selected L-LDA’s output.","{'title': '6 Snippet Extraction', 'number': '6'}"
"By contrast, only 10 of the 51 were tag-page pairs where SVMs’ output was picked by at least one annotator, and of those, only 2 were selected unanimously.","{'title': '6 Snippet Extraction', 'number': '6'}"
In the preceding section we demonstrated how Labeled LDA’s credit attribution mechanism enabled effective modeling within documents.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"In this section, we consider whether L-LDA can be adapted as an effective multi-label classifier for documents as a whole.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"To answer that question, we applied a modified variant of L-LDA to a multi-label document classification problem: given a training set consisting of documents with multiple labels, predict the set of labels appropriate for each document in a test set.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
Multi-label classification is a well researched problem.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"Many modern approaches incorporate label correlations (e.g., Kazawa et al. (2004), Ji et al.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
(2008)).,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"Others, like our algorithm are based on mixture models (such as Ueda and Saito (2003)).","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"However, we are aware of no methods that trade off label-specific word distributions with document-specific label distributions in quite the same way.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"In Section 2, we discussed learning and inference when labels are observed.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"In the task of multilabel classification, labels are available at training time, so the learning part remains the same as discussed before.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"However, inferring the best set of labels for an unlabeled document at test time is more complex: it involves assessing all label assignments and returning the assignment that has the highest posterior probability.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"However, this is not straight-forward, since there are 2K possible label assignments.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"To make matters worse, the support of α(A(d)) is different for different label assignments.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"Although we are in the process of developing an efficient sampling algorithm for this inference, for the purposes of this paper we make the simplifying assumption that the model reduces to standard LDA at inference, where the document is free to sample from any of the K topics.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
This is a reasonable assumption because allowing the model to explore the whole topic space for each document is similar to exploring all possible label assignments.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
The document’s most likely labels can then be inferred by suitably thresholding its posterior probability over topics.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"As a baseline, we use a set of multiple one-vsrest SVM classifiers which is a popular and extremely competitive baseline used by most previous papers (see (Kazawa et al., 2004; Ueda and Saito, 2003) for instance).","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"We scored each model based on Micro-F1 and Macro-F1 as our evaluation measures (Lewis et al., 2004).","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"While the former allows larger classes to dominate its results, the latter assigns an equal weight to all classes, providing us complementary information.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"We ran experiments on a corpus from the Yahoo directory, modeling our experimental conditions on the ones described in (Ji et al., 2008).2 We considered documents drawn from 8 top level categories in the Yahoo directory, where each document can be placed in any number of subcategories.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"The results were mixed, with SVMs ahead on one measure: Labeled LDA beat SVMs on five out of eight datasets on MacroF1, but didn’t win on any datasets on MicroF1.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
Results are presented in Table 3.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"Because only a processed form of the documents was released, the Yahoo dataset does not lend itself well to error analysis.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"However, only 33% of the documents in each top-level category were applied to more than one sub-category, so the credit assignment machinery of L-LDA was unused for the majority of documents.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
We therefore ran an artificial second set of experiments considering only those documents that had been given more than one label in the training data.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"On these documents, the results were again mixed, but Labeled LDA comes out ahead.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"For MacroF1, L-LDA beat SVMs on four datasets, SVMs beat L-LDA on one dataset, and three were a statistical tie.3 On MicroF1, L-LDA did much better than on the larger subset, outperforming on four datasets with the other four a statistical tie.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"It is worth noting that the Yahoo datasets are skewed by construction to contain many documents with highly overlapping content: because each collection is within the same super-class such as “Arts”, “Business”, etc., each sub-categories’ of the named Yahoo directory categories.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
Numbers in parentheses are standard deviations across runs.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"L-LDA outperforms SVMs on 5 subsets with MacroF1, but on no subsets with MicroF1. vocabularies will naturally overlap a great deal.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"L-LDA’s credit attribution mechanism is most effective at partitioning semantically distinct words into their respective label vocabularies, so we expect that Labeled-LDA’s performance as a text classifier would improve on collections with more semantically diverse labels.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"We also applied our method to text classification on the del.icio.us dataset, where the documents are naturally multiply labeled (more than 89%) and where the tags are less inherently similar than in the Yahoo subcategories.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"Therefore we expect Labeled LDA to do better credit assignment on this subset and consequently to show improved performance as a classifier, and indeed this is the case.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
We evaluated L-LDA and multiple one-vs-rest SVMs on 4000 documents with the 20 tag subset described in Section 3.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"L-LDA and multiple one-vs-rest SVMs were trained on the first 80% of documents and evaluated on the remaining 20%, with results averaged across 10 random permutations of the dataset.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
The results are shown in Table 4.,"{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"We tuned the SVMs’ shared cost parameter C(= 10.0) and selected raw term frequency over tf-idf weighting based on 4-fold cross-validation on 3,000 documents drawn from an independent permutation of the data.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"For L-LDA, we tuned the shared parameters of threshold and proportionality constants in word and topic priors.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"L-LDA and SVM have very similar performance on MacroF1, while L-LDA substantially outperforms on MicroF1.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"In both cases, L-LDA’s improvement is statistically significantly by a 2-tailed paired t-test at 95% confidence. multi-label text classification for predicting 20 tags on del.icio.us data.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
"L-LDA outperforms SVMs significantly on both metrics by a 2-tailed, paired t-test at 95% confidence.","{'title': '7 Multilabeled Text Classification', 'number': '7'}"
One of the main advantages of L-LDA on multiply labeled documents comes from the model’s document-specific topic mixture θ.,"{'title': '8 Discussion', 'number': '8'}"
"By explicitly modeling the importance of each label in the document, Labeled LDA can effective perform some contextual word sense disambiguation, which suggests why L-LDA can outperform SVMs on the del.icio.us dataset.","{'title': '8 Discussion', 'number': '8'}"
"As a concrete example, consider the excerpt of text from the del.icio.us dataset in Figure 5.","{'title': '8 Discussion', 'number': '8'}"
"The document itself has several tags, including design and programming.","{'title': '8 Discussion', 'number': '8'}"
"Initially, many of the likelihood probabilities p(wllabel) for the (content) words in this excerpt are higher for the label programming than design, including “content”, “client”, “CMS” and even “designed”, while design has higher likelihoods for just “website” and “happy”.","{'title': '8 Discussion', 'number': '8'}"
"However, after performing inference on this document using L-LDA, the inferred document probability for design (p(design)) is much higher than it is for programming.","{'title': '8 Discussion', 'number': '8'}"
"In fact, the higher probability for the tag more than makes up the difference in the likelihood for all the words except “CMS” (Content Management System), so underline) words are generated from the design tag; red (dashed underline) from the programming tag.","{'title': '8 Discussion', 'number': '8'}"
"By themselves, most words used here have a higher probability in programming than in design.","{'title': '8 Discussion', 'number': '8'}"
"But because the document as a whole is more about design than programming(incorporating words not shown here), inferring the document’s topic-mixture θ enables L-LDA to correctly re-assign most words. that L-LDA correctly infers that most of the words in this passage have more to do with design than programming.","{'title': '8 Discussion', 'number': '8'}"
"This paper has introduced Labeled LDA, a novel model of multi-labeled corpora that directly addresses the credit assignment problem.","{'title': '9 Conclusion', 'number': '9'}"
The new model improves upon LDA for labeled corpora by gracefully incorporating user supervision in the form of a one-to-one mapping between topics and labels.,"{'title': '9 Conclusion', 'number': '9'}"
"We demonstrate the model’s effectiveness on tasks related to credit attribution within documents, including document visualizations and tagspecific snippet extraction.","{'title': '9 Conclusion', 'number': '9'}"
An approximation to Labeled LDA is also shown to be competitive with a strong baseline (multiple one vs-rest SVMs) for multi-label classification.,"{'title': '9 Conclusion', 'number': '9'}"
"Because Labeled LDA is a graphical model in the LDA family, it enables a range of natural extensions for future investigation.","{'title': '9 Conclusion', 'number': '9'}"
"For example, the current model does not capture correlations between labels, but such correlations might be introduced by composing Labeled LDA with newer state of the art topic models like the Correlated Topic Model (Blei and Lafferty, 2006) or the Pachinko Allocation Model (Li and McCallum, 2006).","{'title': '9 Conclusion', 'number': '9'}"
"And with improved inference for unsupervised Λ, Labeled LDA lends itself naturally to modeling semi-supervised corpora where labels are observed for only some documents.","{'title': '9 Conclusion', 'number': '9'}"
This project was supported in part by the President of Stanford University through the IRiSS Initiatives Assessment project.,"{'title': 'Acknowledgments', 'number': '10'}"
