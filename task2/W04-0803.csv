col1,col2
The SENSEVAL-3 task to perform automatic labeling of semantic roles was designed to encourage research into and use of the FrameNet dataset.,{}
The task was based on the considerable expansion of the FrameNet data since the baseline study of automatic labeling of semantic roles by Gildea and Jurafsky.,{}
"The FrameNet data provide an extensive body of “gold standard” data that can be used in lexical semantics research, as the basis for its further exploitation in NLP applications.",{}
"Eight teams participated in the task, with a total of 20 runs.",{}
Discussions among participants during development of the task and the scoring of their runs contributed to a successful task.,{}
"Participants used a wide variety of techniques, investigating many aspects of the FrameNet data.",{}
They achieved results showing considerable improvements from Gildea and Jurafsky’s baseline study.,{}
"Importantly, their efforts have contributed considerably to making the complex FrameNet dataset more accessible.",{}
They have amply demonstrated that FrameNet is a substantial lexical resource that will permit extensive further research and exploitation in NLP applications in the future.,{}
Word-sense disambiguation has frequently been criticized as a task in search of a reason.,"{'title': 'Introduction', 'number': '1'}"
"Since a considerable portion of a sense inventory has only a single sense, the question has been raised whether the amount of effort required by disambiguation is worthwhile.","{'title': 'Introduction', 'number': '1'}"
"Heretofore, the focus of disambiguation has been on the sense inventory and has not examined the major reason why we would have lexical knowledge bases: how the meanings would be represented and thus, available for use in natural language processing applications.","{'title': 'Introduction', 'number': '1'}"
"At the present time, a major paradigm for representing meaning has emerged in frame semantics, specifically in the FrameNet project.","{'title': 'Introduction', 'number': '1'}"
"A worthy objective for the Senseval community is the development of a wide range of methods for automating frame semantics, specifically identifying and labeling semantic roles in sentences.","{'title': 'Introduction', 'number': '1'}"
"An important baseline study of this process has recently appeared in the literature (Gildea and Jurafsky, 2002).","{'title': 'Introduction', 'number': '1'}"
"The FrameNet project (Johnson et al., 2003) has put together a body of hand-labeled data and the Gildea and Jurafsky study has put together a set of suitable metrics for evaluating the performance of an automatic system.","{'title': 'Introduction', 'number': '1'}"
This Senseval-3 task calls for the development of systems to meet the same objectives as the Gildea and Jurafsky study.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
The data for this task is a sample of the FrameNet hand-annotated data.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
Evaluation of systems is measured using precision and recall of frame elements and overlap of a system’s frame element sentence positions with those identified in the FrameNet data.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"The basic task for Senseval-3 is: Given a sentence, a target word and its frame, identify the frame elements within that sentence and tag them with the appropriate frame element name.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"The FrameNet project has just released a major revision (FrameNet 1.1) to its database, with 487 frames using 696 distinctly-named frame elements (although it is not guaranteed that frame elements with the same name have the same meaning).","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"This release includes 132,968 annotated sentences (mostly taken from the British National Corpus).","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"The Senseval-3 task used 8,002 of these sentences selected randomly from 40 frames (also selected randomly) having at least 370 annotations (out of the 100 frames having the most annotations).1 Participants were provided with a training set that identified, for each of the 40 frames, the lexical unit identification number (which equates to a file name) and a sentence identification name.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"They were also provided with the answers, i.e., the frame element names and their beginning and ending positions.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"Since the training set was much larger than the test set, participants were required to use the FrameNet 1.1 dataset to obtain the full sentence, its target word, and the tagged frame elements.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"For the test data, participants were provided, for each frame, with sentence instances that identified the lexical unit, the lexical unit identification number, the sentence identification number, the full sentence, and a specification of the target along with its start and end positions.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"Participants were required to submit their answers in a text file, with one answer per line.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
Each line was to identify the frame name and the sentence identifier and then all the frame elements with their start and end positions that their systems were able to identify.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"For example, for the sentence However, its task is made much more difficult by the fact that derogations granted to the Welsh water authority allow <Agent>it</> to <Target>pump</> <Fluid>raw sewage</> <Goal>into both those rivers</>. the correct answer would appear as follows: The sentences provided to participants were not presegmented (as defined in the Gildea and Jurafsky 1The test set was generated with the Windows-based program FrameNet Explorer, available at http://www.clres.com/SensSemRoles.html.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"FrameNet Explorer provides several facilities for examining the FrameNet data: by frame, frame element, and lexical units.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"For each unit, a user can explore a frame’s elements, associated lexical units, frame-to-frame relations, frame and frame element definitions, lexical units and their definitions, and all sentences. study); this was left to the participants' systems.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
The FrameNet dataset contains considerable information that was tagged by the FrameNet lexicographers.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
Participants could use (and were strongly encouraged to use) any and all of the FrameNet data in developing and training their systems.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"In the test, participants could use any of this data, but were strongly encouraged to use only data available in the sentence itself and in the frame that is identified.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
(This corresponds to the “more difficult task” identified by Gildea and Jurafsky.),"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"Participants could submit two runs, one with (non-restrictive case) and one without (restrictive case) using the additional data; these were scored separately.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
FrameNet recognizes the permissibility of “conceptually salient” frame elements that have not been instantiated in a sentence; these are called null instantiations (see Johnson et al. for a fuller description).,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"An example occurs in the following sentence (sentID=&quot;1087911&quot;) from the Motion frame: “I went and stood in the sitting room doorway, but I couldn't get any further -- my legs wouldn't move.” In this case, the FrameNet taggers considered the Path frame element to be an indefinite null instantiation (INI).","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"Frame elements that have been so designated for a particular sentence appear to be Core frame elements, but not all core frame elements missing from a sentence have designated as null instantiations.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"The correct answer for this case, based on the tagging, is as follows: Motion.1087911 Theme (82,88) Path (0,0) Participants were instructed to identify null instantiations in submissions by giving a (0,0) value for the frame element’s position.2 Participants were told in the task description that null instantiations would be analyzed separately.3 For this Senseval task, participants were allowed to download the training data at any time; the 21-day restriction on submission of results after downloading the training data was waived since this is a new Senseval task and the dataset is very complex.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
Participants could work with the training data as long as they wished.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
The 7-day restriction of submitting results after downloading the test data still applied.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"In general, FrameNet frames contain many frame elements (perhaps an average of 10), most of which are not instantiated in a given sentence.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
Systems were not penalized if they returned more frame elements than those identified by the FrameNet taggers.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"For the 8002 sentences in the test set, only 16212 frame elements constituted the answer set.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"In scoring the runs, each frame element (not a null instantiation) returned by a system was counted as an item attempted.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"If the frame element was one that had been identified by the FrameNet taggers, the answer was scored as correct.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"In addition, however, the scoring program required that the frame boundaries identified by the system’s answer had to overlap with the boundaries identified by FrameNet.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
An additional measure of system performance was the degree of overlap.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"If a system’s answer coincided exactly to FrameNet’s start and end position, the system received an overlap score of 1.0.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"If not, the overlap score was the number of characters overlapping divided by the length of the FrameNet start and end positions (i.e., end-start+1)4 The number attempted was the number of nonnull frame elements generated by a system.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
Precision was computed as the number of correct answers divided by the number attempted.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
Recall was computed as the number of correct answers divided by the number of frame elements in the test set.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
Overlap was the average overlap of all correct answers.,"{'title': '1 The Senseval-3 Task', 'number': '2'}"
"The percent Attempted was the number of frame elements generated divided by the number of frame elements in the test set, multiplied by 100.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
"If a system returned frame elements not identified in the test set, its precision would be lower.","{'title': '1 The Senseval-3 Task', 'number': '2'}"
Eight teams submitted 20 runs.,"{'title': '2 Results', 'number': '3'}"
Three teams submitted runs only for the restricted case (no prior knowledge about frame boundaries).,"{'title': '2 Results', 'number': '3'}"
"The other five 4Hence the problem with an element having (0,0) as the start and end positions. teams submitted at least two runs, with one team submitting 8 runs and another submitting 4 runs.","{'title': '2 Results', 'number': '3'}"
"Four of these five teams submitted a restricted run and an unrestricted run (frame boundaries were identified, i.e., the task was a classification task of identifying the applicable frame element).","{'title': '2 Results', 'number': '3'}"
The results for the classification task are shown in Table 1.,"{'title': '2 Results', 'number': '3'}"
The average precision over all these runs is 0.803 and the average recall is 0.757.,"{'title': '2 Results', 'number': '3'}"
"The overlap in each run is almost identical to the precision, and differs slightly because there may have been some slight positional errors in either the FrameNet data or the sentence string provided in the test data.","{'title': '2 Results', 'number': '3'}"
The results for the restricted case are shown in Table 2.,"{'title': '2 Results', 'number': '3'}"
The average precision over all these runs is 0.595 and the average recall is 0.481.,"{'title': '2 Results', 'number': '3'}"
"The average overlap is noticeably lower than the precision, indicating the additional difficulty for these runs of identifying the frame element boundaries.","{'title': '2 Results', 'number': '3'}"
"In both cases, the percent attempted is quite high, except for one system in the restricted runs.","{'title': '2 Results', 'number': '3'}"
This indicates that systems were able to identify potential frame elements in quite a large percentage of the cases.,"{'title': '2 Results', 'number': '3'}"
Systems were allowed to return any number of frame elements for a sentence and it is possible for a system to identify more frame elements than were identified by the FrameNet taggers.,"{'title': '2 Results', 'number': '3'}"
"For example, run 08a asserted many more frame elements than were identified in the answer key.","{'title': '2 Results', 'number': '3'}"
"As a result, its percent attempted was much higher than 100 percent.","{'title': '2 Results', 'number': '3'}"
The number of frame elements in other runs not identified in the answer key is unknown.,"{'title': '2 Results', 'number': '3'}"
The effect of a higher number attempted lowers the precision for a run and increases the percent attempted.,"{'title': '2 Results', 'number': '3'}"
"Overall, the results achieved in this SENSEVAL-3 task were quite high.","{'title': '3 Discussion', 'number': '4'}"
Several teams achieved results much better than those obtained by Gildea and Jurafsky.,"{'title': '3 Discussion', 'number': '4'}"
The average precision of 0.80 for all runs in the unrestricted case is only slightly lower than the 82% accuracy achieved in that study when using presegmented constituents.,"{'title': '3 Discussion', 'number': '4'}"
"Many teams achieved precision at or above 0.90, indicating that their routines for classifying constituents is quite good.","{'title': '3 Discussion', 'number': '4'}"
"In view of the fact that the number of frames and frame elements in FrameNet has expanded considerably since the Gildea and Jurafsky study, it appears that the methods employed have become quite accurate in classifying constituents.5 Results for the restricted were also quite good in comparison with the Gildea and Jurafsky study, which achieved 65% precision and 61% recall at the “more difficult task of simultaneously segmenting constituents and identifying their semantic role.” In this task, four teams achieved results between 80 and 90 percent for precision and between 65 and 78 percent for recall.","{'title': '3 Discussion', 'number': '4'}"
The participants in this task used a wide variety of methods and data in their systems.,"{'title': '3 Discussion', 'number': '4'}"
"In addition, they used the FrameNet dataset from a wide diversity of perspectives.","{'title': '3 Discussion', 'number': '4'}"
"In some cases, they developed mechanisms for grouping the FrameNet data by part of speech or making use of the nascent inheritance hierarchy in FrameNet.","{'title': '3 Discussion', 'number': '4'}"
"In some cases, they used all frames as a basis for training and in others, they 5The diversity of frame elements in the test data has not yet been investigated, so the assertion that this task is more difficult is based solely on the general expansion of FrameNet. employed novel grouping methods based on the similarities among different frames.","{'title': '3 Discussion', 'number': '4'}"
The successes of many teams seems to indicate that the FrameNet dataset is an excellent lexical resource and that the resources devoted to its development have been quite valuable.,"{'title': '3 Discussion', 'number': '4'}"
"The collective efforts of the participants have contributed greatly to making this complex database more accessible and more amenable to even further development, not only for research purposes, but also for use in many NLP applications.","{'title': '3 Discussion', 'number': '4'}"
