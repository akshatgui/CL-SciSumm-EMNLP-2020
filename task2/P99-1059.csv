col1,col2
"stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.",{}
"We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.",{}
Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community.,"{'title': '1 Introduction', 'number': '1'}"
"Such formalisms specify syntactic facts about each word of the language—in particular, the type of arguments that the word can or must take.","{'title': '1 Introduction', 'number': '1'}"
"Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965).","{'title': '1 Introduction', 'number': '1'}"
"Other lexicalized formalisms include (Schabes et al., 1988; Mel'euk, 1988; Pollard and Sag, 1994).","{'title': '1 Introduction', 'number': '1'}"
"Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments.","{'title': '1 Introduction', 'number': '1'}"
"&quot;Convene&quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP's head (e.g., &quot;meeting&quot;).","{'title': '1 Introduction', 'number': '1'}"
We use the general term bilexical for a grammar that records such facts.,"{'title': '1 Introduction', 'number': '1'}"
A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles.,"{'title': '1 Introduction', 'number': '1'}"
The acceptability of &quot;Nora convened the * The authors were supported respectively under ARPA Grant N6600194-C-6043 &quot;Human Language Technology&quot; and Ministero dell'Universita e della Ricerca Scientifica e Tecnologica project &quot;Methodologies and Tools of High Performance Systems for Multimedia Applications.&quot; party&quot; then depends on the grammar writer's assessment of whether parties can be convened.,"{'title': '1 Introduction', 'number': '1'}"
"Several recent real-world parsers have improved state-of-the-art parsing accuracy by relying on probabilistic or weighted versions of bilexical grammars (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997).","{'title': '1 Introduction', 'number': '1'}"
"The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice).","{'title': '1 Introduction', 'number': '1'}"
Heavy probabilistic pruning is therefore needed to get acceptable runtimes.,"{'title': '1 Introduction', 'number': '1'}"
"But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.","{'title': '1 Introduction', 'number': '1'}"
Our algorithmic technique throughout is to propose new kinds of subderivations that are not constituents.,"{'title': '1 Introduction', 'number': '1'}"
We use dynamic programming to assemble such subderivations into a full parse.,"{'title': '1 Introduction', 'number': '1'}"
The reader is assumed to be familiar with context-free grammars.,"{'title': '2 Notation for context-free grammars', 'number': '2'}"
"Our notation follows (Harrison, 1978; Hoperoft and Ullman, 1979).","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"A context-free grammar (CFG) is a tuple G = (VN, VT , P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, and S E VN is the start symbol.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"In this paper we adopt the following conventions: a, b, c, d denote symbols in VT, w, X, y denote strings in Vat, and a, 0, denote strings in (VN U VT)*.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"The input to the parser will be a CFG G together with a string of terminal symbols to be parsed, w = d1d2 • • • dn.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"Also h,i, j,k denote positive integers, which are assumed to be < n when we are treating them as indices into w. We write wzo for the input substring di • • • di (and put wj = e for i > j).","{'title': '2 Notation for context-free grammars', 'number': '2'}"
"A &quot;derives&quot; relation, written is associated with a CFG as usual.","{'title': '2 Notation for context-free grammars', 'number': '2'}"
We also use the reflexive and transitive closure of written and define L(G) accordingly.,"{'title': '2 Notation for context-free grammars', 'number': '2'}"
We write a 8 =* a-0 for a derivation in which only /3 is rewritten.,"{'title': '2 Notation for context-free grammars', 'number': '2'}"
We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.,"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
This formalism closely resembles stochastic grammatical formalisms that are used in several existing natural language processing systems (see §1).,"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"We will specify a nonstochastic version, noting that probabilities or other weights may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980).","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
(See §4 for brief discussion.),"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"Suppose G = (VN, VT , P,T[$]) is a CFG in CNF.3 We say that G is bilexical if there exists a set of &quot;delexicalized nonterminals&quot; VD such that VN = {A[a] : A E VD, a E VT} and every production in P has one of the following forms: Thus every nonterminal is lexicalized at some terminal a.","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"A constituent of nonterminal type A[a] is said to have terminal symbol a as its lexical head, &quot;inherited&quot; from the constituent's head child in the parse tree (e.g., C[a]).","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"Notice that the start symbol is necessarily a lexicalized nonterminal, T[$].","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
Hence $ appears in every string of L(G); it is usually convenient to define G so that the language of interest is actually L' (G) = {x : x$ E L (G)} Such a grammar can encode lexically specific preferences.,"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"For example, P might contain the productions in order to allow the derivation VP[solve] solve two puzzles, but meanwhile omit the similar productions since puzzles are not edible, a goat is not solvable, &quot;sleep&quot; is intransitive, and &quot;goat&quot; cannot take plural determiners.","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
(A stochastic version of the grammar could implement &quot;soft preferences&quot; by allowing the rules in the second group but assigning them various low probabilities.),"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
The cost of this expressiveness is a very large grammar.,"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
Standard context-free parsing algorithms are inefficient in such a case.,"{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"The CKY algorithm (Younger, 1967; Aho and Ullman, 1972) is time 0(n3- IPI), where in the worst case I P1 = IVNI3 (one ignores unary productions).","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"For a bilexical grammar, the worst case is IPI = VD I 3 ' I VT12, which is large for a large vocabulary VT. We may improve the analysis somewhat by observing that when parsing d1 • • • dn, the CKY algorithm only considers nonterminals of the form A[di]; by restricting to the relevant productions we obtain 0(n3 • IVDI3 • min(n, IVTI)2)• We observe that in practical applications we always have n < IVTI• Let us then restrict our analysis to the (infinite) set of input instances of the parsing problem that satisfy relation n < WTI.","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"With this assumption, the asymptotic time complexity of the CKY algorithm becomes 0(n5 • IVD13).","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"In other words, it is a factor of n2 slower than a comparable non-lexicalized CFG.","{'title': '3 Bilexical context-free grammars', 'number': '3'}"
"In this section we give a recognition algorithm for bilexical CNF context-free grammars, which runs in time 0(n4 • max(p, IVO)) 0(n4 • VDI).","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"Here p is the maximum number of productions sharing the same pair of terminal symbols (e.g., the pair (b, a) in production (1)).","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The new algorithm is asymptotically more efficient than the CKY algorithm, when restricted to input instances satisfying the relation n < IVTI.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"Where CKY recognizes only constituent substrings of the input, the new algorithm can recognize three types of subderivations, shown and described in Figure 1(a).","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
A declarative specification of the algorithm is given in Figure 1(b).,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The derivability conditions of (a) are guaranteed by (b), by induction, and the correctness of the acceptance condition (see caption) follows.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"This declarative specification, like CKY, may be implemented by bottom-up dynamic programming.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
We sketch one such method.,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"For each possible item, as shown in (a), we maintain a bit (indexed by the parameters of the item) that records whether the item has been derived yet.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
All these bits are initially zero.,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The algorithm makes a single pass through the possible items, setting the bit for each if it can be derived using any rule in (b) from items whose bits are already set.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
At the end of this pass it is straightforward to test whether to accept w (see caption).,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The pass considers the items in increasing order of width, where the width of an item in (a) is defined as max{h, j} — min{h, j}.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"Among items of the same width, those of type .L should be considered last.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The algorithm requires space proportional to the number of possible items, which is at most n3IVD12.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"Each of the five rule templates can instantiate its free variables in at most n4p or (for COMPLETE rules) n41VD12 different ways, each of which is tested once and in constant time; so the runtime is 0(n4 max(P, IVO)).","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"By comparison, the CKY algorithm uses only the first type of item, and relies on rules whose inputs are pairs Such rules can be instantiated in 0(n5) different ways for a fixed grammar, yielding 0(n5) time complexity.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The new algorithm saves a factor of n by combining those two constituents in two steps, one of which is insensitive to k and abstracts over its possible values, the other of which is insensitive to h' and abstracts over its possible values.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
It is straightforward to turn the new 0(n4) recognition algorithm into a parser for stochastic bilexical CFCs (or other weighted bilexical CFGs).,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"In a stochastic CFG, each nonterminal A[a] is accompanied by a probability distribution over productions of the form A[a] —> a.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
A parse is just a derivation (proof tree) of lhn and its probability—like that of any derivation we find—is defined as the product of the probabilities of all productions used to condition inference rules in the proof tree.,"{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
"The highestprobability derivation for any item can be reconstructed recursively at the end of the parse, provided that each item maintains not only a bit indicating whether it can be derived, but also the probability and instantiated root rule of its highest-probability derivation tree.","{'title': '4 Bilexical CFG in time 0(n4)', 'number': '4'}"
We now give a variant of the algorithm of §4; the variant has the same asymptotic complexity but will often be faster in practice.,"{'title': '5 A more efficient variant', 'number': '5'}"
Notice that the ATTACH-LEFT rule of Figure 1(b) tries to combine the nonterminal label B[dhd of a previously derived constituent with every possible nonterminal label of the form C[dh].,"{'title': '5 A more efficient variant', 'number': '5'}"
"The improved version, shown in Figure 2, restricts C[dh] to be the label of a previously derived adjacent constituent.","{'title': '5 A more efficient variant', 'number': '5'}"
This improves speed if there are not many such constituents and we can enumerate them in 0(1) time apiece (using a sparse parse table to store the derived items).,"{'title': '5 A more efficient variant', 'number': '5'}"
"It is necessary to use an agenda data structure (Kay, 1986) when implementing the declarative algorithm of Figure 2.","{'title': '5 A more efficient variant', 'number': '5'}"
Deriving narrower items before wider ones as before will not work here because the rule HALVE derives narrow items from wide ones.,"{'title': '5 A more efficient variant', 'number': '5'}"
"Rather than parsing an input string directly, it is often desirable to parse another string related by a (possibly stochastic) transduction.","{'title': '6 Multiple word senses', 'number': '6'}"
"Let T be a finite-state transducer that maps a morpheme sequence w E Vit to its orthographic realization, a grapheme sequence fo T may realize arbitrary morphological processes, including affixation, local clitic movement, deletion of phonological nulls, forbidden or dispreferred k-grams, typographical errors, and mapping of multiple senses onto the same grapheme.","{'title': '6 Multiple word senses', 'number': '6'}"
"Given grammar G and an input ti), we ask whether E T(L(G)).","{'title': '6 Multiple word senses', 'number': '6'}"
We have extended all the algorithms in this paper to this case: the items simply keep track of the transducer state as well.,"{'title': '6 Multiple word senses', 'number': '6'}"
"Due to space constraints, we sketch only the special case of multiple senses.","{'title': '6 Multiple word senses', 'number': '6'}"
"Suppose that the input is iD= d1 • • • dn, and each d2 has up to g possible senses.","{'title': '6 Multiple word senses', 'number': '6'}"
Each item now needs to track its head's sense along with its head's position in ID.,"{'title': '6 Multiple word senses', 'number': '6'}"
"Wherever an item formerly recorded a head position h (similarly h'), it must now record a pair (h, dh), where dh E VT is a specific sense of dh.","{'title': '6 Multiple word senses', 'number': '6'}"
No rule in Figures 1-2 (or Figure 3 below) will mention more than two such pairs.,"{'title': '6 Multiple word senses', 'number': '6'}"
So the time complexity increases by a factor of 0(g2).,"{'title': '6 Multiple word senses', 'number': '6'}"
"7 Head automaton grammars in time 0(n4) In this section we show that a length-n string generated by a head automaton grammar (Alshawi, 1996) can be parsed in time 0(n4).","{'title': '6 Multiple word senses', 'number': '6'}"
"We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3).","{'title': '6 Multiple word senses', 'number': '6'}"
A head automaton grammar (HAG) is a function H : a 1-4 Ha that defines a head automaton (HA) for each element of its (finite) domain.,"{'title': '6 Multiple word senses', 'number': '6'}"
"Let VT = domain(H) and D = A single head automaton is an acceptor for a language of string pairs (zi, zr) E V x V. Informally, if b is the leftmost symbol of Zr and q' E a(q, b, -4), then Ha can move from state q to state q', matching symbol b and removing it from the left end of Zr.","{'title': '6 Multiple word senses', 'number': '6'}"
"Symmetrically, if b is the rightmost symbol of zi and q' E Sa(q,b,<---) then from q Ha can move to q', matching symbol b and removing it from the right end of z1.5 More formally, we associate with the head automaton Ha a &quot;derives&quot; relation ha, defined as a binary relation on Qa X 1 /4 X V. .","{'title': '6 Multiple word senses', 'number': '6'}"
"For every q E Q, x,y E V, b E VT, de D, and q' E &(q, b, d), we specify that The reflexive and transitive closure of ha is written Ha*.","{'title': '6 Multiple word senses', 'number': '6'}"
"The language generated by Ha is the set We may now define the language generated by the entire grammar H. To generate, we expand the start word $ E VT into x$y for some (x, y) E L (Hs), and then recursively expand the words in strings x and y.","{'title': '6 Multiple word senses', 'number': '6'}"
"More formally, given H, we simultaneously define La for all a E VT to be minimal such that if (x, y) E L(H a), x' e Lx, y' E Ly, then x'ayi E La, where stands for the concatenation language Lai • • Lai,.","{'title': '6 Multiple word senses', 'number': '6'}"
Then H generates language L. We next present a simple construction that transforms a HAG H into a bilexical CFG G generating the same language.,"{'title': '6 Multiple word senses', 'number': '6'}"
The construction also preserves derivation ambiguity.,"{'title': '6 Multiple word senses', 'number': '6'}"
"This means that for each string w, there is a lineartime 1-to-1 mapping between (appropriately de5 Alshawi (1996) describes HAs as accepting (or equivalently, generating) zi and z from the outside in.","{'title': '6 Multiple word senses', 'number': '6'}"
"To make Figure 3 easier to follow, we have defined HAs as accepting symbols in the opposite order, from the inside out.","{'title': '6 Multiple word senses', 'number': '6'}"
"This amounts to the same thing if transitions are reversed, I. is exchanged with F., and any transition probabilities are replaced by those of the reversed Markov chain. fined) canonical derivations of w by H and canonical derivations of w by G. We adopt the notation above for H and the components of its head automata.","{'title': '6 Multiple word senses', 'number': '6'}"
"Let VD be an arbitrary set of size t = max{IQa I : a E VT}, and for each a, define an arbitrary injection fa : Q.","{'title': '6 Multiple word senses', 'number': '6'}"
-4 VD.,"{'title': '6 Multiple word senses', 'number': '6'}"
"We define G = (VN, P,T[$]), where (iii) T fs(q), where we assume WLOG that Is is a singleton set {q}.","{'title': '6 Multiple word senses', 'number': '6'}"
"We omit the formal proof that G and H admit isomorphic derivations and hence generate the same languages, observing only that if (x, y) = (bib2 • • • bi,b3+1- • • bk) E L(Ha)— a condition used in defining La above—then A[a] [bi] • • • B3[MaB3+1[bi+11 • • • Bk[bk], for any A, B1, .","{'title': '6 Multiple word senses', 'number': '6'}"
"Bk that map to initial states in Ha, Hbl, Hb, respectively.","{'title': '6 Multiple word senses', 'number': '6'}"
"In general, G has p = 0(IVD13) = 0(t3).","{'title': '6 Multiple word senses', 'number': '6'}"
The construction therefore implies that we can parse a length-n sentence under H in time 0(n4t3).,"{'title': '6 Multiple word senses', 'number': '6'}"
"If the HAs in H happen to be deterministic, then in each binary production given by (ii) above, symbol A is fully determined by a, b, and C. In this case p = 0(t2), so the parser will operate in time 0(n4t2).","{'title': '6 Multiple word senses', 'number': '6'}"
"We note that this construction can be straightforwardly extended to convert stochastic HAGs as in (Alshawi, 1996) into stochastic CFGs.","{'title': '6 Multiple word senses', 'number': '6'}"
"Probabilities that Ha assigns to state q's various transition and halt actions are copied onto the corresponding productions A[a] a of G, where A = fa(q).","{'title': '6 Multiple word senses', 'number': '6'}"
"8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).","{'title': '6 Multiple word senses', 'number': '6'}"
"In this section we describe and discuss this special case, and give a new 0(n3) algorithm that has a smaller grammar constant than previously reported.","{'title': '6 Multiple word senses', 'number': '6'}"
A head automaton Ha is called split if it has no states that can be entered on a ÷- transition and exited on a ---> transition.,"{'title': '6 Multiple word senses', 'number': '6'}"
"Such an automaton can accept (x, y) only by reading all of y—immediately after which it is said to be in a flip state—and then reading all of x.","{'title': '6 Multiple word senses', 'number': '6'}"
"Formally, a flip state is one that allows entry on a —> transition and that either allows exit on a transition or is a final state.","{'title': '6 Multiple word senses', 'number': '6'}"
We are concerned here with head automaton grammars H such that every Ha is split.,"{'title': '6 Multiple word senses', 'number': '6'}"
These correspond to bilexical CFGs in which any derivation A[a] = xay has the form A[a] = xB[a] xay.,"{'title': '6 Multiple word senses', 'number': '6'}"
"That is, a word's left dependents are more oblique than its right dependents and c-command them.","{'title': '6 Multiple word senses', 'number': '6'}"
Such grammars are broadly applicable.,"{'title': '6 Multiple word senses', 'number': '6'}"
"Even if Ha is not split, there usually exists a split head automaton H&quot;, recognizing the same language.","{'title': '6 Multiple word senses', 'number': '6'}"
"H la exists if {x#y : (x, y) E L(Ha)} is regular (where # VT).","{'title': '6 Multiple word senses', 'number': '6'}"
"In particular, lei', must exist unless Ha has a cycle that includes both ÷- and -4 transitions.","{'title': '6 Multiple word senses', 'number': '6'}"
"Such cycles would be necessary for Ha itself to accept a formal language such as {(bn, cn) : n > 0}, where word a takes 2n dependents, but we know of no natural-language motivation for ever using them in a HAG.","{'title': '6 Multiple word senses', 'number': '6'}"
One more definition will help us bound the complexity.,"{'title': '6 Multiple word senses', 'number': '6'}"
"A split head automaton Ha is said to be g-split if its set of flip states, denoted C Qa, has size < g. The languages that can be recognized by g-split HAs are those that can be written as 1..g 1 Li x R, where the Li and Ri are regular languages over VT. Eisner (1997) actually defined (g-split) bilexical grammars in terms of the latter property.6 We now present our result: Figure 3 specifies an 0(n3g2t2) recognition algorithm for a head automaton grammar H in which every H, is g-split.","{'title': '6 Multiple word senses', 'number': '6'}"
"For deterministic automata, the runtime is 0(n3g2t)—a considerable improvement on the 0(n3g3t2) result of (Eisner, 1997), which also assumes deterministic automata.","{'title': '6 Multiple word senses', 'number': '6'}"
"As in §4, a simple bottom-up implementation will suffice.","{'title': '6 Multiple word senses', 'number': '6'}"
"For a practical speedup, add h\j as an antecedent to the MID rule (and fill in the parse table from right to left).","{'title': '6 Multiple word senses', 'number': '6'}"
"Like our previous algorithms, this one takes two steps (ATTACH, COMPLETE) to attach a child constituent to a parent constituent.","{'title': '6 Multiple word senses', 'number': '6'}"
"But instead of full constituents—strings xd,y e d, —it uses only half-constituents like xdi and The other halves of these constituents can be attached later, because to find an accepting path for (zi, Zr) in a split head automaton, one can separately find the half-path before the flip state (which accepts zr) and the half-path after the flip state (which accepts zi).","{'title': '6 Multiple word senses', 'number': '6'}"
"These two halfpaths can subsequently be joined into an accepting path if they have the same flip state s, i.e., one path starts where the other ends.","{'title': '6 Multiple word senses', 'number': '6'}"
Annotating our left half-constituents with s makes this check possible.,"{'title': '6 Multiple word senses', 'number': '6'}"
"We have formally described, and given faster parsing algorithms for, three practical grammatical rewriting systems that capture dependencies between pairs of words.","{'title': '9 Final remarks', 'number': '7'}"
All three systems admit naive 0(n5) algorithms.,"{'title': '9 Final remarks', 'number': '7'}"
"We give the first 0(n4) results for the natural formalism of bilexical context-free grammar, and for Alshawi's (1996) head automaton grammars.","{'title': '9 Final remarks', 'number': '7'}"
"For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.","{'title': '9 Final remarks', 'number': '7'}"
"Note that, e.g., all senses would restore the g 2 factor.","{'title': '9 Final remarks', 'number': '7'}"
"Indeed, this approach gives added flexibility: a word's sense, unlike its choice of flip state, is visible to the HA that reads it. three models in (Collins, 1997) are susceptible to the 0(n3) method (cf.","{'title': '9 Final remarks', 'number': '7'}"
Collins's 0(n5)).,"{'title': '9 Final remarks', 'number': '7'}"
Our dynamic programming techniques for cheaply attaching head information to derivations can also be exploited in parsing formalisms other than rewriting systems.,"{'title': '9 Final remarks', 'number': '7'}"
"The authors have developed an 0(n7)-time parsing algorithm for bilexicalized tree adjoining grammars (Schabes, 1992), improving the naive 0(n8) method.","{'title': '9 Final remarks', 'number': '7'}"
"The results mentioned in §6 are related to the closure property of CFGs under generalized sequential machine mapping (Hoperoft and Ullman, 1979).","{'title': '9 Final remarks', 'number': '7'}"
This property also holds for our class of bilexical CFGs.,"{'title': '9 Final remarks', 'number': '7'}"
