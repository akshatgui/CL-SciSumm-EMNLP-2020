col1,col2
"In this paper, we propose a new statistical Japanese dependency parser using a cascaded chunking model.",{}
"Conventional Japanese statistical dependency parsers are mainly based on a probabilistic model, which is not always efficient or scalable.",{}
"We propose a new method that is simple and efficient, since it parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.",{}
Experiments using the Kyoto University Corpus show that the method outperforms previous systems as well as improves the parsing and training efficiency.,{}
"Dependency analysis has been recognized as a basic process in Japanese sentence analysis, and a number of studies have been proposed.","{'title': '1 Introduction', 'number': '1'}"
Japanese dependency structure is usually defined in terms of the relationship between phrasal units called bunsetsu segments (hereafter “segments”).,"{'title': '1 Introduction', 'number': '1'}"
"Most of the previous statistical approaches for Japanese dependency analysis (Fujio and Matsumoto, 1998; Haruno et al., 1999; Uchimoto et al., 1999; Kanayama et al., 2000; Uchimoto et al., 2000; Kudo and Matsumoto, 2000) are based on a probabilistic model consisting of the following two steps.","{'title': '1 Introduction', 'number': '1'}"
"First, they estimate modification probabilities, in other words, how probable one segment tends to modify another.","{'title': '1 Introduction', 'number': '1'}"
Second the optimal combination of dependencies is searched from the all candidates dependencies.,"{'title': '1 Introduction', 'number': '1'}"
Such a probabilistic model is not always efficient since it needs to calculate the probabilities for all possible dependencies and creates n˙(n−1)/2 (where n is the number of segments in a sentence) training examples per sentence.,"{'title': '1 Introduction', 'number': '1'}"
"In addition, the probabilistic model assumes that each pairs of dependency structure is independent.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we propose a new Japanese dependency parser which is more efficient and simpler than the probabilistic model, yet performs better in training and testing on the Kyoto University Corpus.","{'title': '1 Introduction', 'number': '1'}"
The method parses a sentence deterministically only deciding whether the current segment modifies segment on its immediate right hand side.,"{'title': '1 Introduction', 'number': '1'}"
"Moreover, it does not assume the independence constraint between dependencies","{'title': '1 Introduction', 'number': '1'}"
This section describes the general formulation of the probabilistic model for parsing which has been applied to Japanese statistical dependency analysis.,"{'title': '2 A Probabilistic Model', 'number': '2'}"
"First of all, we define a sentence as a sequence of segments B = (b1, b2 ..., bm) and its syntactic structure as a sequence of dependency patterns D = (Dep(1), Dep(2), ... , Dep(m−1)) , where Dep(i) = j means that the segment bi depends on (modifies) segment bj.","{'title': '2 A Probabilistic Model', 'number': '2'}"
"In this framework, we assume that the dependency sequence D satisfies the following two constraints.","{'title': '2 A Probabilistic Model', 'number': '2'}"
Statistical dependency analysis is defined as a searching problem for the dependency pattern D that maximizes the conditional probability P(D|B) of the input sequence under the above-mentioned constraints.,"{'title': '2 A Probabilistic Model', 'number': '2'}"
"If we assume that the dependency probabilities are mutually independent, P(D|B) can be rewritten as: modifies bj. fzj is an n dimensional feature vector that represents various kinds of linguistic features related to the segments bz and bj.","{'title': '2 A Probabilistic Model', 'number': '2'}"
We obtain Dbest = argmaxD P(D|B) taking into all the combination of these probabilities.,"{'title': '2 A Probabilistic Model', 'number': '2'}"
"Generally, the optimal solution Dbest can be identified by using bottom-up parsing algorithm such as CYK algorithm.","{'title': '2 A Probabilistic Model', 'number': '2'}"
The problem in the dependency structure analysis is how to estimate the dependency probabilities accurately.,"{'title': '2 A Probabilistic Model', 'number': '2'}"
"A number of statistical and machine learning approaches, such as Maximum Likelihood estimation (Fujio and Matsumoto, 1998), Decision Trees (Haruno et al., 1999), Maximum Entropy models (Uchimoto et al., 1999; Uchimoto et al., 2000; Kanayama et al., 2000), and Support Vector Machines (Kudo and Matsumoto, 2000), have been applied to estimate these probabilities.","{'title': '2 A Probabilistic Model', 'number': '2'}"
"In order to apply a machine learning algorithm to dependency analysis, we have to prepare the positive and negative examples.","{'title': '2 A Probabilistic Model', 'number': '2'}"
"Usually, in a probabilistic model, all possible pairs of segments that are in a dependency relation are used as positive examples, and two segments that appear in a sentence but are not in a dependency relation are used as negative examples.","{'title': '2 A Probabilistic Model', 'number': '2'}"
"Thus, a total of n˙(n − 1)/2 training examples (where n is the number of segments in a sentence) must be produced per sentence.","{'title': '2 A Probabilistic Model', 'number': '2'}"
"In the probabilistic model, we have to estimate the probabilities of each dependency relation.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"However, some machine learning algorithms, such as SVMs, cannot estimate these probabilities directly.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
Kudo and Matsumoto (2000) used the sigmoid function to obtain pseudo probabilities in SVMs.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"However, there is no theoretical endorsement for this heuristics.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Moreover, the probabilistic model is not good in its scalability since it usually requires a total of n˙(n − 1)/2 training examples per sentence.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"It will be hard to combine the probabilistic model with some machine learning algorithms, such as SVMs, which require a polynomial computational cost on the number of given training examples.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"In this paper, we introduce a new method for Japanese dependency analysis, which does not require the probabilities of dependencies and parses a sentence deterministically.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The proposed method can be combined with any type of machine learning algorithm that has classification ability.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"The original idea of our method stems from the cascaded chucking method which has been applied in English parsing (Abney, 1991).","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
Let us introduce the basic framework of the cascaded chunking parsing method: We apply this cascaded chunking parsing technique to Japanese dependency analysis.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Since Japanese is a head-final language, and the chunking can be regarded as the creation of a dependency between two segments, we can simplify the process of Japanese dependency analysis as follows: Figure 1 shows an example of the parsing process with the cascaded chunking model.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"The input for the model is the linguistic features related to the modifier and modifiee, and the output from the model is either of the tags (D or O).","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"In training, the model simulates the parsing algorithm by consulting the correct answer from the training annotated corpus.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"During the training, positive (D) and negative (O) examples are collected.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"In testing, the model consults the trained system and parses the input with the cascaded chunking algorithm.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
We think this proposed cascaded chunking model has the following advantages compared with the traditional probabilistic models.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"If we use the CYK algorithm, the probabilistic model requires O(n3) parsing time, (where n is the number of segments in a sentence.).","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"On the other hand, the cascaded chunking model requires O(n2) in the worst case when all segments modify the rightmost segment.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"The actual parsing time is usually lower than O(n2), since most of segments modify segment on its immediate right hand side.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Furthermore, in the cascaded chunking model, the training examples are extracted using the parsing algorithm itself.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The training examples required for the cascaded chunking model is much smaller than that for the probabilistic model.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The model reduces the training cost significantly and enables training using larger amounts of annotated corpus.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
• No assumption on the independence between dependency relations The probabilistic model assumes that dependency relations are independent.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"However, there are some cases in which one cannot parse a sentence correctly with this assumption.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"For example, coordinate structures cannot be always parsed with the independence constraint.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The cascaded chunking model parses and estimates relations simultaneously.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"This means that one can use all dependency relations, which have narrower scope than that of the current focusing relation being considered, as feature sets.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
We describe the details in the next section.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"The cascaded chunking model can be combined with any machine learning algorithm that works as a binary classifier, since the cascaded chunking model parses a sentence deterministically only deciding whether or not the current segment modifies the segment on its immediate right hand side.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
Probabilities of dependencies are not always necessary for the cascaded chunking model.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Linguistic features that are supposed to be effective in Japanese dependency analysis are: head words and their parts-of-speech tags, functional words and inflection forms of the words that appear at the end of segments, distance between two segments, existence of punctuation marks.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"As those are solely defined by the pair of segments, we refer to them as the static features.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
Japanese dependency relations are heavily constrained by such static features since the inflection forms and postpositional particles constrain the dependency relation.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"However, when a sentence is long and there are more than one possible dependency, static features, by themselves cannot determine the correct dependency.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"To cope with this problem, Kudo and Matsumoto (2000) introduced a new type of features called dynamic features, which are created dynamically during the parsing process.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"For example, if some relation is determined, this modification relation may have some influence on other dependency relation.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Therefore, once a segment has been determined to modify another segment, such information is kept in both of the segments and is added to them as a new feature.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Specifically, we take the following three types of dynamic features in our experiments.","{'title': '3 Cascaded Chunking Model', 'number': '3'}"
He her warm heart be moved A.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The segments which modify the current candidate modifiee.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
(boxes marked with A in Figure 2) B.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
The segments which modify the current candidate modifier.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
(boxes marked with B in Figure 2) C. The segment which is modified by the current candidate modifiee.,"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
(boxes marked with C in Figure 2),"{'title': '3 Cascaded Chunking Model', 'number': '3'}"
"Although any kind of machine learning algorithm can be applied to the cascaded chunking model, we use Support Vector Machines (Vapnik,1998) for our experiments because of their state-of-the-art performance and generalization ability.","{'title': '4 Support Vector Machines', 'number': '4'}"
"SVM is a binary linear classifier trained from the samples, each of which belongs either to positive or negative class as follows: (x1, y1), ... , (xl, yl) (xi E Rn, yi E {+1, −1}), where xi is a feature vector of the i-th sample represented by an n dimensional vector, and yi is the class (positive(+1) or negative(−1) class) label of the i-th sample.","{'title': '4 Support Vector Machines', 'number': '4'}"
SVMs find the optimal separating hyperplane (w • x + b) based on the maximal margin strategy.,"{'title': '4 Support Vector Machines', 'number': '4'}"
The margin can be seen as the distance between the critical examples and the separating hyperplane.,"{'title': '4 Support Vector Machines', 'number': '4'}"
"We omit the details here, the maximal margin strategy can be realized by the following optimization problem:","{'title': '4 Support Vector Machines', 'number': '4'}"
"Furthermore, SVMs have the potential to carry out non-linear classifications.","{'title': 'Minimize: L(w) = 12IlwIl2', 'number': '5'}"
"Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature vectors appear as their dot products.","{'title': 'Minimize: L(w) = 12IlwIl2', 'number': '5'}"
"By simply substituting every dot product of xi and xj in dual form with a Kernel function K(xi, xj), SVMs can handle non-linear hypotheses.","{'title': 'Minimize: L(w) = 12IlwIl2', 'number': '5'}"
"Among many kinds of Kernel functions available, we will focus on the dth polynomial kernel: K(xi, xj) = (xi • xj + 1)d. Use of d-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to d.","{'title': 'Minimize: L(w) = 12IlwIl2', 'number': '5'}"
We used the following two annotated corpora for our experiments.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"This data set consists of the Kyoto University text corpus Version 2.0 (Kurohashi and Nagao, 1997).","{'title': '5 Experiments and Discussion', 'number': '6'}"
"We used 7,958 sentences from the articles on January 1st to January 7th as training examples, and 1,246 sentences from the articles on January 9th as the test data.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"This data set was used in (Uchimoto et al., 1999; Uchimoto et al., 2000) and (Kudo and Matsumoto, 2000).","{'title': '5 Experiments and Discussion', 'number': '6'}"
"In order to investigate the scalability of the cascaded chunking model, we prepared larger data set.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"We used all 38,383 sentences of the Kyoto University text corpus Version 3.0.","{'title': '5 Experiments and Discussion', 'number': '6'}"
The training and test data were generated by a two-fold cross validation.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
The feature sets used in our experiments are shown in Table 1.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"The static features are basically taken from Uchimoto’s list (Uchimoto et al., 1999).","{'title': '5 Experiments and Discussion', 'number': '6'}"
Head Word (HW) is the rightmost content word in the segment.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"Functional Word (FW) is set as follows: - FW = the rightmost functional word, if there is a functional word in the segment - FW = the rightmost inflection form, if there is a predicate in the segment - FW = same as the HW, otherwise.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"The static features include the information on existence of brackets, question marks and punctuation marks, etc.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Besides, there are features that show the relative relation of two segments, such as distance, and existence of brackets, quotation marks and punctuation marks between them.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"For a segment X and its dynamic feature Y (where Y is of type A or B), we set the Functional Representation (FR) feature of X based on the FW of X (X-FW) as follows: - FR = lexical form of X-FW if POS of X-FW is particle, adverb, adnominal or conjunction - FR = inflectional form ofX-FW ifX-FW has an inflectional form.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"- FR = the POS tag ofX-FW, otherwise.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"For a segment X and its dynamic feature C, we set POS tag and POS-subcategory of the HW of X.","{'title': '5 Experiments and Discussion', 'number': '6'}"
All our experiments are carried out on AlphaSever 8400 (21164A 500Mhz) for training and Linux (PentiumIII 1GHz) for testing.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"We used a third degree polynomial kernel function, which is exactly the same setting in (Kudo and Matsumoto, 2000).","{'title': '5 Experiments and Discussion', 'number': '6'}"
Performance on the test data is measured using dependency accuracy and sentence accuracy.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
Dependency accuracy is the percentage of correct dependencies out of all dependency relations.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
Sentence accuracy is the percentage of sentences in which all dependencies are determined correctly.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"The results for the new cascaded chunking model as well as for the previous probabilistic model based on SVMs (Kudo and Matsumoto, 2000) are summarized in Table 2.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"We cannot employ the experiments for the probabilistic model using large dataset, since the data size is too large for our current SVMs learning program to terminate in a realistic time period.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Even though the number of training examples used for the cascaded chunking model is less than a quarter of that for the probabilistic model, and the used feature set is the same, dependency accuracy and sentence accuracy are improved using the cascaded chunking model (89.09% → 89.29%, 46.17% → 47.53%).","{'title': '5 Experiments and Discussion', 'number': '6'}"
"The time required for training and parsing are significantly reduced by applying the cascaded chunking model (336h.→8h, 2.1sec.→ 0.5sec.).","{'title': '5 Experiments and Discussion', 'number': '6'}"
"As can be seen Table 2, the cascaded chunking model is more accurate, efficient and scalable than the probabilistic model.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"It is difficult to apply the probabilistic model to the large data set, since it takes no less than 336 hours (2 weeks) to carry out the experiments even with the standard data set, and SVMs require quadratic or more computational cost on the number of training examples.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"For the first impression, it may seems natural that higher accuracy is achieved with the probabilistic model, since all candidate dependency relations are used as training examples.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"However, the experimental results show that the cascaded chunking model performs better.","{'title': '5 Experiments and Discussion', 'number': '6'}"
Here we list what the most significant contributions are and how well the cascaded chunking model behaves compared with the probabilistic model.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
The probabilistic model is trained with all candidate pairs of segments in the training corpus.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
The problem of this training is that exceptional dependency relations may be used as training examples.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"For example, suppose a segment which appears to right hand side of the correct modifiee and has a similar content word, the pair with this segment becomes a negative example.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"However, this is negative because there is a better and correct candidate at a different point in the sentence.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Therefore, this may not be a true negative example, meaning that this can be positive in other sentences.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"In addition, if a segment is not modified by a modifier because of cross dependency constraints but has a similar content word with correct modifiee, this relation also becomes an exception.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Actually, we cannot ignore these exceptions, since most segments modify a segment on its immediate right hand side.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"By using all candidates of dependency relation as the training examples, we have committed to a number of exceptions which are hard to be trained upon.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Looking in particular on a powerful heuristics for dependency structure analysis: “A segment tends to modify a nearer segment if possible,” it will be most important to train whether the current segment modifies the segment on its immediate right hand side.","{'title': '5 Experiments and Discussion', 'number': '6'}"
The cascaded chunking model is designed along with this heuristics and can remove the exceptional relations which has less potential to improve performance.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
Figure 3 shows the relationship between the size of the training data and the parsing accuracy.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
This figure also shows the accuracy with and without the dynamic features.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"Generally, the results with the dynamic feature set is better than the results without it.","{'title': '5 Experiments and Discussion', 'number': '6'}"
The dynamic features constantly outperform static features when the size of the training data is large.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"In most cases, the improvements is considerable.","{'title': '5 Experiments and Discussion', 'number': '6'}"
Table 3 summarizes the performance without some dynamic features.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"From these results, we can conclude that all dynamic features are effective in improving the performance.","{'title': '5 Experiments and Discussion', 'number': '6'}"
Table 4 summarizes recent results on Japanese dependency analysis.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"Uchimoto et al. (2000) report that using the Kyoto University Corpus for their training and testing, they achieve around 87.93% accuracy by building statistical model based on the Maximum Entropy framework.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"They extend the original probabilistic model, which learns only two class; ‘modify‘ and ‘not modify‘, to the one that learns three classes; ‘between‘, ‘modify‘ and ‘beyond‘.","{'title': '5 Experiments and Discussion', 'number': '6'}"
Their model can also avoid the influence of the exceptional dependency relations.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"Using same training and test data, we can achieve accuracy of 89.29%.","{'title': '5 Experiments and Discussion', 'number': '6'}"
The difference is considerable.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
Kanayama et al. (2000) use an HPSG-based Japanese grammar to restrict the candidate dependencies.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
"Their model uses at most three candidates restricted by the grammar as features; the nearest, the second nearest, and the farthest from the modifier.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Thus, their model can take longer context into account, and disambiguate complex dependency relations.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"However, the features are still static, and dynamic features are not used in their model.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"We cannot directly compare their model with ours because they use a different corpus, EDR corpus, which is ten times as large as the corpus we used.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Nevertheless, they reported an accuracy 88.55%, which is worse than our model.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Haruno et al. (99) report that using the EDR Corpus for their training and testing, they achieve around 85.03% accuracy with Decision Tree and Boosting.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Although Decision Tree can take combinations of features as SVMs, it easily overfits on its own.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"To avoid overfitting, Decision Tree is usually used as an weak learner for Boosting.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"Combining Boosting technique with Decision Tree, the performance may be improved.","{'title': '5 Experiments and Discussion', 'number': '6'}"
"However, Haruno et al. (99) report that the performance with Decision Tree falls down when they added lexical entries with lower frequencies as features even using Boosting.","{'title': '5 Experiments and Discussion', 'number': '6'}"
We think that Decision Tree requires a careful feature selection for achieving higher accuracy.,"{'title': '5 Experiments and Discussion', 'number': '6'}"
We presented a new Japanese dependency parser using a cascaded chunking model which achieves 90.46% accuracy using the Kyoto University Corpus.,"{'title': '6 Conclusion', 'number': '7'}"
Our model parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.,"{'title': '6 Conclusion', 'number': '7'}"
Our model outperforms the previous probabilistic model with respect to accuracy and efficiency.,"{'title': '6 Conclusion', 'number': '7'}"
"In addition, we showed that dynamic features significantly contribute to improve the performance.","{'title': '6 Conclusion', 'number': '7'}"
