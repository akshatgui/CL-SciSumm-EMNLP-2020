col1,col2
"In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.",{}
Our word-character hybrid model offers high performance since it can handle both known and unknown words.,{}
We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.,{}
"We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.",{}
"In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks.","{'title': '1 Introduction', 'number': '1'}"
"Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation.","{'title': '1 Introduction', 'number': '1'}"
"Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).","{'title': '1 Introduction', 'number': '1'}"
"In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a system’s word dictionary1.","{'title': '1 Introduction', 'number': '1'}"
"The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors.","{'title': '1 Introduction', 'number': '1'}"
"The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem.","{'title': '1 Introduction', 'number': '1'}"
"However, it suffers from structural complexity.","{'title': '1 Introduction', 'number': '1'}"
Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time.,"{'title': '1 Introduction', 'number': '1'}"
"However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we overcome such limitations concerning both efficiency and effectiveness.","{'title': '1 Introduction', 'number': '1'}"
"We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed Algorithm (MIRA) (Crammer, 2004; Crammer et al., 2005; McDonald, 2006).","{'title': '1 Introduction', 'number': '1'}"
We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task.,"{'title': '1 Introduction', 'number': '1'}"
"In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance.","{'title': '1 Introduction', 'number': '1'}"
"Here, we describe our strategies that yield good balance for learning these two characteristics.","{'title': '1 Introduction', 'number': '1'}"
We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus.,"{'title': '1 Introduction', 'number': '1'}"
"We conducted our experiments on Penn Chinese Treebank (Xia et al., 2000) and compared our approach with the best previous approaches reported in the literature.","{'title': '1 Introduction', 'number': '1'}"
Experimental results indicate that our approach can achieve state-of-the-art performance.,"{'title': '1 Introduction', 'number': '1'}"
"The paper proceeds as follows: Section 2 gives background on the word-character hybrid model, Section 3 describes our policies for correct path selection, Section 4 presents our training method based on MIRA, Section 5 shows our experimental results, Section 6 discusses related work, and Section 7 concludes the paper.","{'title': '1 Introduction', 'number': '1'}"
"In joint word segmentation and the POS tagging process, the task is to predict a path of word hypotheses y = (y1, ... , y#y) = ((w1, p1), ... , (w#y, p#y)) for a given character sequence x = (c1, ... , c#x), where w is a word, p is its POS tag, and a “#” symbol denotes the number of elements in each variable.","{'title': '2 Background', 'number': '2'}"
"The goal of our learning algorithm is to learn a mapping from inputs (unsegmented sentences) x E X to outputs (segmented paths) y E Y based on training samples of input-output pairs S = {(xt, yt)}t1.","{'title': '2 Background', 'number': '2'}"
"We represent the search space with a lattice based on the word-character hybrid model (Nakagawa and Uchimoto, 2007).","{'title': '2 Background', 'number': '2'}"
"In the hybrid model, given an input sentence, a lattice that consists of word-level and character-level nodes is constructed.","{'title': '2 Background', 'number': '2'}"
"Word-level nodes, which correspond to words found in the system’s word dictionary, have regular POS tags.","{'title': '2 Background', 'number': '2'}"
"Character-level nodes have special tags where position-of-character (POC) and POS tags are combined (Asahara, 2003; Nakagawa, 2004).","{'title': '2 Background', 'number': '2'}"
"POC tags indicate the word-internal positions of the characters, as described in Table 1.","{'title': '2 Background', 'number': '2'}"
Figure 1 shows an example of a lattice for a Chinese sentence: “ ” (Chongming is China’s third largest island).,"{'title': '2 Background', 'number': '2'}"
Note that some nodes and state transitions are not allowed.,"{'title': '2 Background', 'number': '2'}"
"For example, T and E nodes cannot occur at the beginning of the lattice (marked with dashed boxes), and the transitions from T to B nodes are also forbidden.","{'title': '2 Background', 'number': '2'}"
These nodes and transitions are ignored during the lattice construction processing.,"{'title': '2 Background', 'number': '2'}"
"In the training phase, since several paths (marked in bold) can correspond to the correct analysis in the annotated corpus, we need to select one correct path yt as a reference for training.2 The next section describes our strategies for dealing with this issue.","{'title': '2 Background', 'number': '2'}"
"With this search space representation, we can consistently handle unknown words with character-level nodes.","{'title': '2 Background', 'number': '2'}"
"In other words, we use word-level nodes to identify known words and character-level nodes to identify unknown words.","{'title': '2 Background', 'number': '2'}"
"In the testing phase, we can use a dynamic programming algorithm to search for the most likely path out of all candidate paths.","{'title': '2 Background', 'number': '2'}"
"In this section, we describe our strategies for selecting the correct path yt in the training phase.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"As shown in Figure 1, the paths marked in bold can represent the correct annotation of the segmented sentence.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"Ideally, we need to build a wordcharacter hybrid model that effectively learns the characteristics of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes).","{'title': '3 Policies for correct path selection', 'number': '3'}"
We can directly estimate the statistics of known words from an annotated corpus where a sentence is already segmented into words and assigned POS tags.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
"If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words.","{'title': '3 Policies for correct path selection', 'number': '3'}"
We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
We expect that those words could reflect unknown words in the future.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
"Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004).","{'title': '3 Policies for correct path selection', 'number': '3'}"
We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
"In the baseline policy, we first count the frequencies of words3 in the training corpus.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"As a result, the correct path yt can contain both wordlevel and character-level nodes (marked with asterisks (*)).","{'title': '3 Policies for correct path selection', 'number': '3'}"
"To discover more statistics of unknown words, one might consider just increasing the threshold value r to obtain more artificial unknown words.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"However, our experimental results indicate that our word-character hybrid model requires an appropriate balance between known and artificial unknown words to achieve optimal performance.","{'title': '3 Policies for correct path selection', 'number': '3'}"
We now describe our new approach to leverage additional examples of unknown words.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
"Intuition suggests that even though the system can handle some unknown words, many unidentified unknown words remain that cannot be recovered by the system; we wish to learn the characteristics of such unidentified unknown words.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"We propose the following simple scheme: Several types of errors are produced by the baseline model, but we only focus on those caused by unidentified unknown words, which can be easily collected in the evaluation process.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"As described later in Section 5.2, we measure the recall on out-of-vocabulary (OOV) words.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"Here, we define unidentified unknown words as OOV words in each validation set that cannot be recovered by the system.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"After ten cross validation runs, we get a list of the unidentified unknown words derived from the whole training corpus.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"Note that the unidentified unknown words in the cross validation are not necessary to be infrequent words, but some overlap may exist.","{'title': '3 Policies for correct path selection', 'number': '3'}"
"Finally, we obtain the artificial unknown words that combine the unidentified unknown words in cross validation and infrequent words for learning unknown words.","{'title': '3 Policies for correct path selection', 'number': '3'}"
We refer to this approach as the error-driven policy.,"{'title': '3 Policies for correct path selection', 'number': '3'}"
"Let Yt = {yt , ... , yKt � be a lattice consisting of candidate paths for a given sentence xt.","{'title': '4 Training method', 'number': '4'}"
"In the word-character hybrid model, the lattice Yt can contain more than 1000 nodes, depending on the length of the sentence xt and the number of POS tags in the corpus.","{'title': '4 Training method', 'number': '4'}"
"Therefore, we require a learning algorithm that can efficiently handle large and complex lattice structures.","{'title': '4 Training method', 'number': '4'}"
"Online learning is an attractive method for the hybrid model since it quickly converges Algorithm 1 Generic Online Learning Algorithm Input: Training set S = {(xt, yt)}Tt�1 Output: Model weight vector w within a few iterations (McDonald, 2006).","{'title': '4 Training method', 'number': '4'}"
"Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework.","{'title': '4 Training method', 'number': '4'}"
"We focus on an online learning algorithm called MIRA (Crammer, 2004), which has the desired accuracy and scalability properties.","{'title': '4 Training method', 'number': '4'}"
MIRA combines the advantages of margin-based and perceptron-style learning with an optimization scheme.,"{'title': '4 Training method', 'number': '4'}"
"In particular, we use a generalized version of MIRA (Crammer et al., 2005; McDonald, 2006) that can incorporate k-best decoding in the update procedure.","{'title': '4 Training method', 'number': '4'}"
"To understand the concept of kbest MIRA, we begin with a linear score function: where w is a weight vector and f is a feature representation of an input x and an output y.","{'title': '4 Training method', 'number': '4'}"
Learning a mapping between an input-output pair corresponds to finding a weight vector w such that the best scoring path of a given sentence is the same as (or close to) the correct path.,"{'title': '4 Training method', 'number': '4'}"
"Given a training example (xt, yt), MIRA tries to establish a margin between the score of the correct path s(xt,yt; w) and the score of the best candidate path s(xt, y; w) based on the current weight vector w that is proportional to a loss function L(yt, y).","{'title': '4 Training method', 'number': '4'}"
"In each iteration, MIRA updates the weight vector w by keeping the norm of the change in the weight vector as small as possible.","{'title': '4 Training method', 'number': '4'}"
"With this framework, we can formulate the optimization problem as follows (McDonald, 2006): where bestk(xt; w(i)) E Yt represents a set of top k-best paths given the weight vector w(i).","{'title': '4 Training method', 'number': '4'}"
"The above quadratic programming (QP) problem can be solved using Hildreth’s algorithm (Yair Censor, 1997).","{'title': '4 Training method', 'number': '4'}"
Replacing Eq.,"{'title': '4 Training method', 'number': '4'}"
"(2) into line 4 of Algorithm 1, we obtain k-best MIRA.","{'title': '4 Training method', 'number': '4'}"
The next question is how to efficiently generate bestk(xt; w(i)).,"{'title': '4 Training method', 'number': '4'}"
"In this paper, we apply a dynamic programming search (Nagata, 1994) to kbest MIRA.","{'title': '4 Training method', 'number': '4'}"
The algorithm has two main search steps: forward and backward.,"{'title': '4 Training method', 'number': '4'}"
"For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice.","{'title': '4 Training method', 'number': '4'}"
"For the backward search, we use A*style decoding to generate the top k-best paths.","{'title': '4 Training method', 'number': '4'}"
"A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i).","{'title': '4 Training method', 'number': '4'}"
The final weight vector w is the average of the weight vectors after each iteration.,"{'title': '4 Training method', 'number': '4'}"
"As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting.","{'title': '4 Training method', 'number': '4'}"
"For inference, we can use Viterbi-style decoding to search for the most likely path y* for a given sentence x where: In conventional sequence labeling where the observation sequence (word) boundaries are fixed, one can use the 0/1 loss to measure the errors of a predicted path with respect to the correct path.","{'title': '4 Training method', 'number': '4'}"
"However, in our model, word boundaries vary based on the considered path, resulting in a different numbers of output tokens.","{'title': '4 Training method', 'number': '4'}"
"As a result, we cannot directly use the 0/1 loss.","{'title': '4 Training method', 'number': '4'}"
We instead compute the loss function through false positives (FP) and false negatives (FN).,"{'title': '4 Training method', 'number': '4'}"
"Here, FP means the number of output nodes that are not in the correct path, and FN means the number of nodes in the correct path that cannot be recognized by the system.","{'title': '4 Training method', 'number': '4'}"
We define the loss function by: This loss function can reflect how bad the predicted path y� is compared to the correct path yt.,"{'title': '4 Training method', 'number': '4'}"
"A weighted loss function based on FP and FN can be found in (Ganchev et al., 2007).","{'title': '4 Training method', 'number': '4'}"
"This section discusses the structure of f(x,y).","{'title': '4 Training method', 'number': '4'}"
We broadly classify features into two categories: unigram and bigram features.,"{'title': '4 Training method', 'number': '4'}"
We design our feature templates to capture various levels of information about words and POS tags.,"{'title': '4 Training method', 'number': '4'}"
Let us introduce some notation.,"{'title': '4 Training method', 'number': '4'}"
"We write w−1 and w0 for the surface forms of words, where subscripts −1 and 0 indicate the previous and current positions, respectively.","{'title': '4 Training method', 'number': '4'}"
POS tags p−1 and p0 can be interpreted in the same way.,"{'title': '4 Training method', 'number': '4'}"
We denote the characters by cj.,"{'title': '4 Training method', 'number': '4'}"
Unigram features: Table 2 shows our unigram features.,"{'title': '4 Training method', 'number': '4'}"
"Templates W0–W3 are basic word-level unigram features, where Length(w0) denotes the length of the word w0.","{'title': '4 Training method', 'number': '4'}"
Using just the surface forms can overfit the training data and lead to poor predictions on the test data.,"{'title': '4 Training method', 'number': '4'}"
"To alleviate this problem, we use two generalized features of the surface forms.","{'title': '4 Training method', 'number': '4'}"
The first is the beginning and end characters of the surface (A0–A7).,"{'title': '4 Training method', 'number': '4'}"
"For example, (AB(w0)) denotes the beginning character of the current word w0, and (AB(w0), AE(w0)) denotes the beginning and end characters in the word.","{'title': '4 Training method', 'number': '4'}"
The second is the types of beginning and end characters of the surface (T0–T7).,"{'title': '4 Training method', 'number': '4'}"
"We define a set of general character types, as shown in Table 4.","{'title': '4 Training method', 'number': '4'}"
"Templates C0–C6 are basic character-level unigram features taken from (Nakagawa, 2004).","{'title': '4 Training method', 'number': '4'}"
These templates operate over a window of f2 characters.,"{'title': '4 Training method', 'number': '4'}"
"The features include characters (C0), pairs of characters (C1–C2), character types (C3), and pairs of character types (C4–C5).","{'title': '4 Training method', 'number': '4'}"
"In addition, we add pairs of characters and character types (C6).","{'title': '4 Training method', 'number': '4'}"
Bigram features: Table 3 shows our bigram features.,"{'title': '4 Training method', 'number': '4'}"
Templates B0-B9 are basic wordlevel bigram features.,"{'title': '4 Training method', 'number': '4'}"
These features aim to capture all the possible combinations of word and POS bigrams.,"{'title': '4 Training method', 'number': '4'}"
Templates TB0-TB6 are the types of characters for bigrams.,"{'title': '4 Training method', 'number': '4'}"
"For example, (TE(w−1), TB(w0)) captures the change of character types from the end character in the previous word to the beginning character in the current word.","{'title': '4 Training method', 'number': '4'}"
"Note that if one of the adjacent nodes is a character-level node, we use the template CB0 that represents POS bigrams.","{'title': '4 Training method', 'number': '4'}"
"In our preliminary experiments, we found that if we add more features to non-word-level bigrams, the number of features grows rapidly due to the dense connections between non-word-level nodes.","{'title': '4 Training method', 'number': '4'}"
"However, these features only slightly improve performance over using simple POS bigrams.","{'title': '4 Training method', 'number': '4'}"
"For Seg, a token is considered to be a correct one if the word boundary is correctly identified.","{'title': '4 Training method', 'number': '4'}"
"For Seg & Tag, both the word boundary and its POS tag have to be correctly identified to be counted as a correct token.","{'title': '4 Training method', 'number': '4'}"
F1 =,"{'title': '4 Training method', 'number': '4'}"
"Previous studies on joint Chinese word segmentation and POS tagging have used Penn Chinese Treebank (CTB) (Xia et al., 2000) in experiments.","{'title': '5 Experiments', 'number': '5'}"
"However, versions of CTB and experimental settings vary across different studies.","{'title': '5 Experiments', 'number': '5'}"
"In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach.","{'title': '5 Experiments', 'number': '5'}"
Table 5 provides the statistics of our experimental settings on the small and large training data.,"{'title': '5 Experiments', 'number': '5'}"
"The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003).","{'title': '5 Experiments', 'number': '5'}"
Note that the development set was only used for evaluating the trained model to obtain the optimal values of tunable parameters.,"{'title': '5 Experiments', 'number': '5'}"
We evaluated both word segmentation (Seg) and joint word segmentation and POS tagging (Seg & Tag).,"{'title': '5 Experiments', 'number': '5'}"
"We used recall (R), precision (P), and F1 as evaluation metrics.","{'title': '5 Experiments', 'number': '5'}"
"Following (Sproat and Emerson, 2003), we also measured the recall on OOV (ROOV) tokens and in-vocabulary (RIV) tokens.","{'title': '5 Experiments', 'number': '5'}"
These performance measures can be calculated as follows: # of correct tokens # of tokens in test data Our model has three tunable parameters: the number of training iterations N; the number of top k-best paths; and the threshold r for infrequent words.,"{'title': '5 Experiments', 'number': '5'}"
"Since we were interested in finding an optimal combination of word-level and characterlevel nodes for training, we focused on tuning r. We fixed N = 10 and k = 5 for all experiments.","{'title': '5 Experiments', 'number': '5'}"
"For the baseline policy, we varied r in the range of [1, 5] and found that setting r = 3 yielded the best performance on the development set for both the small and large training corpus experiments.","{'title': '5 Experiments', 'number': '5'}"
"For the error-driven policy, we collected unidentified unknown words using 10-fold cross validation on the training set, as previously described in Section 3.","{'title': '5 Experiments', 'number': '5'}"
Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies.,"{'title': '5 Experiments', 'number': '5'}"
The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase.,"{'title': '5 Experiments', 'number': '5'}"
"The total number of words is the same, but the different policies yield different balances between the known and artificial unknown words for learning the hybrid model.","{'title': '5 Experiments', 'number': '5'}"
Optimal balances were selected using the development set.,"{'title': '5 Experiments', 'number': '5'}"
The error-driven policy provides additional artificial unknown words in the training set.,"{'title': '5 Experiments', 'number': '5'}"
"The error-driven policy can improve ROOV as well as maintain good RIV, resulting in overall F1 improvements.","{'title': '5 Experiments', 'number': '5'}"
"In this section, we attempt to make meaningful comparison with the best prior approaches reported in the literature.","{'title': '5 Experiments', 'number': '5'}"
"Although most previous studies used CTB, their versions of CTB and experimental settings are different, which complicates comparison.","{'title': '5 Experiments', 'number': '5'}"
Ng and Low (2004) (N&L04) used CTB 3.0.,"{'title': '5 Experiments', 'number': '5'}"
"However, they just showed POS tagging results on a per character basis, not on a per word basis.","{'title': '5 Experiments', 'number': '5'}"
Zhang and Clark (2008) (Z&C08) generated CTB 3.0 from CTB 4.0.,"{'title': '5 Experiments', 'number': '5'}"
"Jiang et al. (2008a; 2008b) (Jiang08a, Jiang08b) used CTB 5.0.","{'title': '5 Experiments', 'number': '5'}"
Shi and Wang (2007) used CTB that was distributed in the SIGHAN Bakeoff.,"{'title': '5 Experiments', 'number': '5'}"
"Besides CTB, they also used HowNet (Dong and Dong, 2006) to obtain semantic class features.","{'title': '5 Experiments', 'number': '5'}"
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings.,"{'title': '5 Experiments', 'number': '5'}"
We decided to follow the experimental settings of Jiang et al. (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora.,"{'title': '5 Experiments', 'number': '5'}"
The performance scores of previous studies are directly taken from their papers.,"{'title': '5 Experiments', 'number': '5'}"
We also conducted experiments using the system implemented by Nakagawa and Uchimoto (2007) (N&U07) for comparison.,"{'title': '5 Experiments', 'number': '5'}"
"Our experiment on the large training corpus is identical to that of Jiang et al. (Jiang et al., 2008a; Jiang et al., 2008b).","{'title': '5 Experiments', 'number': '5'}"
Table 7 compares the F1 results with previous studies on CTB 5.0.,"{'title': '5 Experiments', 'number': '5'}"
"The result of our error-driven model is superior to previous reported results for both Seg and Seg & Tag, and the result of our baseline model compares favorably to the others.","{'title': '5 Experiments', 'number': '5'}"
"Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs 1–10364.","{'title': '5 Experiments', 'number': '5'}"
We then divided CTB 3.0 into ten equal sets and conducted 10-fold cross validation.,"{'title': '5 Experiments', 'number': '5'}"
"Unfortunately, Zhang and Clark’s experimental setting did not allow us to use our errordriven policy since performing 10-fold cross validation again on each main cross validation trial is computationally too expensive.","{'title': '5 Experiments', 'number': '5'}"
"Therefore, we used our baseline policy in this setting and fixed r = 3 for all cross validation runs.","{'title': '5 Experiments', 'number': '5'}"
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0.,"{'title': '5 Experiments', 'number': '5'}"
Table 9 shows a summary of averaged F1 results on CTB 3.0.,"{'title': '5 Experiments', 'number': '5'}"
"Our baseline model outperforms all prior approaches for both Seg and Seg & Tag, and we hope that our error-driven model can further improve performance.","{'title': '5 Experiments', 'number': '5'}"
"In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods.","{'title': '6 Related work', 'number': '6'}"
"Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance.","{'title': '6 Related work', 'number': '6'}"
"Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems.","{'title': '6 Related work', 'number': '6'}"
"However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model.","{'title': '6 Related work', 'number': '6'}"
"Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006).","{'title': '6 Related work', 'number': '6'}"
"For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).","{'title': '6 Related work', 'number': '6'}"
"Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004).","{'title': '6 Related work', 'number': '6'}"
"In this paper, we first introduce MIRA to joint word segmentation and POS tagging and show very encouraging results.","{'title': '6 Related work', 'number': '6'}"
"With regard to error-driven learning, Brill (1995) proposed a transformation-based approach that acquires a set of error-correcting rules by comparing the outputs of an initial tagger with the correct annotations on a training corpus.","{'title': '6 Related work', 'number': '6'}"
Our approach does not learn the error-correcting rules.,"{'title': '6 Related work', 'number': '6'}"
We only aim to capture the characteristics of unknown words and augment their representatives.,"{'title': '6 Related work', 'number': '6'}"
"As for search space representation, Ng and Low (2004) found that for Chinese, the characterbased model yields better results than the wordbased model.","{'title': '6 Related work', 'number': '6'}"
Nakagawa and Uchimoto (2007) provided empirical evidence that the characterbased model is not always better than the wordbased model.,"{'title': '6 Related work', 'number': '6'}"
They proposed a hybrid approach that exploits both the word-based and characterbased models.,"{'title': '6 Related work', 'number': '6'}"
Our approach overcomes the limitation of the original hybrid model by a discriminative online learning algorithm for training.,"{'title': '6 Related work', 'number': '6'}"
"In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word segmentation and POS tagging.","{'title': '7 Conclusion', 'number': '7'}"
Our approach has two important advantages.,"{'title': '7 Conclusion', 'number': '7'}"
"The first is robust search space representation based on a hybrid model in which word-level and characterlevel nodes are used to identify known and unknown words, respectively.","{'title': '7 Conclusion', 'number': '7'}"
We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus.,"{'title': '7 Conclusion', 'number': '7'}"
The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model.,"{'title': '7 Conclusion', 'number': '7'}"
"Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature.","{'title': '7 Conclusion', 'number': '7'}"
"In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese.","{'title': '7 Conclusion', 'number': '7'}"
"We would like to thank Tetsuji Nakagawa for his helpful suggestions about the word-character hybrid model, Chen Wenliang for his technical assistance with the Chinese processing, and the anonymous reviewers for their insightful comments.","{'title': 'Acknowledgments', 'number': '8'}"
