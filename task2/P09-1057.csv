col1,col2
"We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.",Introduction
"We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.",Introduction
"In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging.",Experiment/Discussion
"The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods.",Experiment/Discussion
"In this paper, we develop new methods for unsupervised part-of-speech tagging.",Experiment/Discussion
"We adopt the problem formulation of Merialdo (1994), in which we are given a raw word sequence and a dictionary of legal tags for each word type.",Experiment/Discussion
The goal is to tag each word token so as to maximize accuracy against a gold tag sequence.,Experiment/Discussion
"Whether this is a realistic problem set-up is arguable, but an interesting collection of methods and results has accumulated around it, and these can be clearly compared with one another.",Experiment/Discussion
"We use the standard test set for this task, a 24,115-word subset of the Penn Treebank, for which a gold tag sequence is available.",Experiment/Discussion
"There are 5,878 word types in this test set.",Experiment/Discussion
"We use the standard tag dictionary, consisting of 57,388 word/tag pairs derived from the entire Penn Treebank.1 8,910 dictionary entries are relevant to the 5,878 word types in the test set.",Experiment/Discussion
"Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data.",Experiment/Discussion
There are 45 distinct grammatical tags.,Experiment/Discussion
"In this set-up, there are no unknown words.",Experiment/Discussion
Figure 1 shows prior results for this problem.,Experiment/Discussion
"While the methods are quite different, they all make use of two common model elements.",Experiment/Discussion
"One is a probabilistic n-gram tag model P(ti|ti−n+1...ti−1), which we call the grammar.",Experiment/Discussion
"The other is a probabilistic word-given-tag model P(wi|ti), which we call the dictionary.",Experiment/Discussion
"The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication.",Experiment/Discussion
"They improve this to 83.9% by employing a fully Bayesian approach which integrates over all possible parameter values, rather than estimating a single distribution.",Experiment/Discussion
They further improve this to 86.8% by using priors that favor sparse distributions.,Experiment/Discussion
"Smith and Eisner (2005) employ a contrastive estimation technique, in which they automatically generate negative examples and use CRF training.",Experiment/Discussion
"In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.",Experiment/Discussion
They show considerable improvements in tagging accuracy when using a coarser-grained version (with 17-tags) of the tag set from the Penn Treebank.,Experiment/Discussion
"Goldberg et al. (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions.",Experiment/Discussion
"They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed).",Experiment/Discussion
"Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one).",Experiment/Discussion
"In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments.",Experiment/Discussion
"The literature omits one other baseline, which is EM with a 2-gram tag model.",Experiment/Discussion
"Here we obtain 81.7% accuracy, which is better than the 3-gram model.",Experiment/Discussion
It seems that EM with a 3-gram tag model runs amok with its freedom.,Experiment/Discussion
"For the rest of this paper, we will limit ourselves to a 2-gram tag model.",Experiment/Discussion
We analyze the tag sequence output produced by EM and try to see where EM goes wrong.,Experiment/Discussion
"The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.",Experiment/Discussion
The Bayesian methods overcome this effect by using priors which favor sparser distributions.,Experiment/Discussion
But it is not easy to model such priors into EM learning.,Experiment/Discussion
"As a result, EM exploits a lot of rare tags (like FW = foreign word, or SYM = symbol) and assigns them to common word types (in, of, etc.).",Experiment/Discussion
We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence).,Experiment/Discussion
The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus.,Experiment/Discussion
"We see how the rare tag labels (like FW, SYM, etc.) are abused by EM.",Experiment/Discussion
"As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output.",Experiment/Discussion
We also look at things more globally.,Experiment/Discussion
We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence.,Experiment/Discussion
"We call this the observed grammar size, and it is 915.",Experiment/Discussion
"That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence.",Experiment/Discussion
"Here, we show a sample word sequence and the corresponding IP network generated for that sequence. can compare it with the gold tagging’s observed grammar size, which is 760.",Experiment/Discussion
"So we can safely say that EM is learning a grammar that is too big, still abusing its freedom.",Experiment/Discussion
Bayesian sparse priors aim to create small models.,Experiment/Discussion
We take a different tack in the paper and directly ask: What is the smallest model that explains the text?,Experiment/Discussion
Our approach is related to minimum description length (MDL).,Experiment/Discussion
We formulate our question precisely by asking which tag sequence (of the 106425 available) has the smallest observed grammar size.,Experiment/Discussion
The answer is 459.,Experiment/Discussion
"That is, there exists a tag sequence that contains 459 distinct tag bigrams, and no other tag sequence contains fewer.",Experiment/Discussion
We obtain this answer by formulating the problem in an integer programming (IP) framework.,Experiment/Discussion
Figure 2 illustrates this with a small sample word sequence.,Experiment/Discussion
"We create a network of possible taggings, and we assign a binary variable to each link in the network.",Experiment/Discussion
"We create constraints to ensure that those link variables receiving a value of 1 form a left-to-right path through the tagging network, and that all other link variables receive a value of 0.",Experiment/Discussion
We accomplish this by requiring the sum of the links entering each node to equal to the sum of the links leaving each node.,Experiment/Discussion
We also create variables for every possible tag bigram and word/tag dictionary entry.,Experiment/Discussion
We constrain link variable assignments to respect those grammar and dictionary variables.,Experiment/Discussion
"For example, we do not allow a link variable to “activate” unless the corresponding grammar variable is also “activated”.",Experiment/Discussion
"Finally, we add an objective function that minimizes the number of grammar variables that are assigned a value of 1.",Experiment/Discussion
Figure 3 shows the IP solution for the example word sequence from Figure 2.,Experiment/Discussion
"Of course, a small grammar size does not necessarily correlate with higher tagging accuracy.",Experiment/Discussion
"For the small toy example shown in Figure 3, the correct tagging is “PRO AUX V .",Experiment/Discussion
"PRO V” (with 5 tag pairs), whereas the IP tries to minimize the grammar size and picks another solution instead.",Experiment/Discussion
"For solving the integer program, we use CPLEX software (a commercial IP solver package).",Experiment/Discussion
"Alternatively, there are other programs such as lp solve, which are free and publicly available for use.",Experiment/Discussion
"Once we create an integer program for the full test corpus, and pass it to CPLEX, the solver returns an sponding grammar sizes for the sample word sequence from Figure 2 using the given dictionary and grammar.",Experiment/Discussion
The IP solver finds the smallest grammar set that can explain the given word sequence.,Experiment/Discussion
"In this example, there exist two solutions that each contain only 4 tag pair entries, and IP returns one of them. objective function value of 459.3 CPLEX also returns a tag sequence via assignments to the link variables.",Experiment/Discussion
"However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random.",Experiment/Discussion
"We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%.",Experiment/Discussion
"We also note that CPLEX takes 320 seconds to return the optimal solution for the integer program corresponding to this particular test data (24,115 tokens with the 45-tag set).",Experiment/Discussion
It might be interesting to see how the performance of the IP method (in terms of time complexity) is affected when scaling up to larger data and bigger tagsets.,Experiment/Discussion
We leave this as part of future work.,Experiment/Discussion
But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver.,Experiment/Discussion
"Our IP formulation can find us a small model, but it does not attempt to fit the model to the data.",Experiment/Discussion
"Fortunately, we can use EM for that.",Experiment/Discussion
"We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP.",Experiment/Discussion
"Starting with uniform probabilities, EM finds a tagging that is 84.5% accurate, substantially better than the 81.7% originally obtained with the fully-connected grammar.",Experiment/Discussion
So we see a benefit to our explicit small-model approach.,Experiment/Discussion
While EM does not find the most accurate 3Note that the grammar identified by IP is not uniquely minimal.,Experiment/Discussion
"For the same word sequence, there exist other minimal grammars having the same size (459 entries).",Experiment/Discussion
"In our experiments, we choose the first solution returned by CPLEX. sequence consistent with the IP grammar (90.3%), it finds a relatively good one.",Experiment/Discussion
The IP+EM tagging (with 84.5% accuracy) has some interesting properties.,Experiment/Discussion
"First, the dictionary we observe from the tagging is of higher quality (with fewer spurious tagging assignments) than the one we observe from the original EM tagging.",Experiment/Discussion
Figure 4 shows some examples.,Experiment/Discussion
We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM).,Experiment/Discussion
"In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57).",Experiment/Discussion
"In the case of the observed dictionary, using a smaller grammar model does not affect the precision (EM = 0.91, IP+EM = 0.89) or recall (EM = 0.89, IP+EM = 0.89).",Experiment/Discussion
"During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier.",Experiment/Discussion
"Here are a few examples of bad dictionary entries that get removed when we use the minimized grammar for EM training: in FW a SYM of RP In RBR During EM training, the minimized grammar helps to eliminate many incorrect entries (i.e., zero out model parameters) from the dictionary, thereby yielding an improved dictionary model.",Experiment/Discussion
So using the minimized grammar (which has higher precision) helps to improve the quality of the chosen dictionary (examples shown in Figure 4).,Experiment/Discussion
This in turn helps improve the tagging accuracy from 81.7% to 84.5%.,Experiment/Discussion
It is clear that the IP-constrained grammar is a better choice to run EM on than the full grammar.,Experiment/Discussion
Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training.,Experiment/Discussion
"In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar).",Experiment/Discussion
"Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method.",Experiment/Discussion
We now run EM again on the full grammar (all possible tag bigrams) in combination with this good dictionary (containing fewer entries than the full dictionary).,Experiment/Discussion
"Unlike the original training with full grammar, where EM could choose any tag bigram, now the choice of grammar entries is constrained by the good dictionary model that we provide EM with.",Experiment/Discussion
"This allows EM to recover some of the good tag pairs, and results in a good grammardictionary combination that yields better tagging performance.",Experiment/Discussion
"With these improvements in mind, we embark on an alternating scheme to find better models and taggings.",Experiment/Discussion
"We run EM for multiple passes, and in each pass we alternately constrain either the grammar model or the dictionary model.",Experiment/Discussion
The procedure is simple and proceeds as follows: We notice significant gains in tagging performance when applying this technique.,Experiment/Discussion
"The tagging accuracy increases at each step and finally settles at a high of 91.6%, which outperforms the existing state-of-the-art systems for the 45-tag set.",Experiment/Discussion
"The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al. (2008) without using any additional linguistic constraints or manual cleaning of the dictionary.",Experiment/Discussion
Figure 5 shows the tagging performance achieved at each step.,Experiment/Discussion
"We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements.",Experiment/Discussion
Initializing the last trained dictionary or grammar at each step with uniform weights also yields the same tagging improvements as shown in Figure 5.,Experiment/Discussion
"We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76.",Experiment/Discussion
The figure also shows the model’s internal grammar and dictionary sizes.,Experiment/Discussion
Figure 6 and 7 show how the precision/recall of the observed grammar and dictionary varies for different models from Figure 5.,Experiment/Discussion
"In the case of the observed grammar (Figure 6), precision increases at each step, whereas recall drops initially (owing to the grammar minimization) but then picks up again.",Experiment/Discussion
"The precision/recall of the observed dictionary on the other hand, is not affected by much.",Experiment/Discussion
"Multiple random restarts for EM, while not often emphasized in the literature, are key in this domain.",Experiment/Discussion
Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate.,Experiment/Discussion
"When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy.",Experiment/Discussion
"Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8).",Experiment/Discussion
"As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models.",Experiment/Discussion
"Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al. (2008).",Experiment/Discussion
But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data.,Experiment/Discussion
"Because EM is efficient, we can extend our word-sequence trainModel 1 Model 2 Model 3 Model 4 Model 5 ing data from the 24,115-token set to the entire Penn Treebank (973k tokens).",Experiment/Discussion
"We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%.",Experiment/Discussion
"This is our final result on the 45-tagset, and we note that it is higher than previously reported results.",Experiment/Discussion
"Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008).",Experiment/Discussion
Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset.,Experiment/Discussion
"When tagging the same standard test corpus with the smaller 17-tagset, our method is able to achieve a substantially high accuracy of 96.8%, which is the best result reported so far on this task.",Experiment/Discussion
"The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008).",Experiment/Discussion
"The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types).",Experiment/Discussion
"The InitEM-HMM system from Goldberg et al. (2008) reports an accuracy of 93.8%, followed by the LDA+AC model (Latent Dirichlet Allocation model with a strong Ambiguity Class component) from Toutanova and Johnson (2008).",Experiment/Discussion
"In comparison, the Bayesian HMM (BHMM) model from Goldwater et al. (2007) and the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively).",Experiment/Discussion
"Our system (IP+EM) which uses integer programming and EM, gets the highest accuracy (96.8%).",Experiment/Discussion
The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank.,Experiment/Discussion
"The IP+EM models used in the 17-tagset experiments reported here were not trained on the entire Penn Treebank, but instead used a smaller section containing 77,963 tokens for estimating model parameters.",Experiment/Discussion
"We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9).",Experiment/Discussion
"We find that our performance does not degrade when the parameter estimation is done using less data, and our model still achieves a high accuracy of 96.8%.",Experiment/Discussion
The literature also includes results reported in a different setting for the tagging problem.,Experiment/Discussion
"In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types.",Experiment/Discussion
"In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities).",Experiment/Discussion
"Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries.",Experiment/Discussion
"We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word.",Experiment/Discussion
We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary.,Experiment/Discussion
"Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the tag given a particular suffix (e.g., P(VBG I ing) = 0.97, P(N I ing) = 0.0001, ...).",Experiment/Discussion
"Next, for every unknown word “w”, the trained P(tag I suffix) model is used to predict the top 3 tag possibilities for “w” (using only its suffix information), and subsequently this word along with its 3 tags are added as a new entry to the lexicon.",Experiment/Discussion
"We do this for every unknown word, and eventually we have a dictionary containing entries for all the words.",Experiment/Discussion
"Once the completed lexicon (containing both correct entries for words in the lexicon and the predicted entries for unknown words) is available, we follow the same methodology from Sections 3 and 4 using integer programming to minimize the size of the grammar and then applying EM to estimate parameter values.",Experiment/Discussion
Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete.,Experiment/Discussion
"The second and third rows in the table shows tagging accuracies for different systems when a cutoff of 2 (i.e., all word types that occur with frequency counts < 2 in the test corpus are removed) and a cutoff of 3 (i.e., all word types occurring with frequency counts < 3 in the test corpus are removed) is applied to the dictionary.",Experiment/Discussion
"This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary.",Experiment/Discussion
"As the results in Figure 9 illustrate, the IP+EM method clearly does better than all the other systems except for the LDA+AC model.",Experiment/Discussion
"The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model.",Experiment/Discussion
"The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution.",Results/Conclusion
"In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task.",Results/Conclusion
"While some previous methods introduced for the same task have achieved big tagging improvements using additional linguistic knowledge or manual supervision, our models are not provided with any additional information.",Results/Conclusion
Figure 10 illustrates for the 45-tag set some of the common mistakes that our best tagging model (92.3%) makes.,Results/Conclusion
"In some cases, the model actually gets a reasonable tagging but is penalized perhaps unfairly.",Results/Conclusion
"For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO.",Results/Conclusion
"The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”.",Results/Conclusion
"In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS).",Results/Conclusion
"Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998).",Results/Conclusion
"In MDL, there is a single objective function to (1) maximize the likelihood of observing the data, and at the same time (2) minimize the length of the model description (which depends on the model size).",Results/Conclusion
"However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results.",Results/Conclusion
"In the past, only a few approaches utilizing MDL have been shown to work for natural language applications.",Results/Conclusion
"These approaches employ heuristic search methods with MDL for the task of unsupervised learning of morphology of natural languages (Goldsmith, 2001; Creutz and Lagus, 2002; Creutz and Lagus, 2005).",Results/Conclusion
"The method proposed in this paper is the first application of the MDL idea to POS tagging, and the first to use an integer programming formulation rather than heuristic search techniques.",Results/Conclusion
"We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007).",Results/Conclusion
We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging.,Results/Conclusion
Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.,Results/Conclusion
"The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values.",Results/Conclusion
"The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as “V V”, etc.).",Results/Conclusion
"However, it is easy to model some of these linguistic constraints (both at the local and global levels) directly using integer programming, and this may result in further improvements and lead to new possibilities for future research.",Results/Conclusion
"For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.",Results/Conclusion
