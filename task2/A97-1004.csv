col1,col2
We present a trainable model for identifying sentence boundaries in raw text.,{}
"Given a corpus annotated with sentence boundaries, our model learns to classify each occurrence of., ?, and !as either a valid or invalid sentence boundary.",{}
"The training procedure requires no hand-crafted rules, lexica, part-of-speech tags, or domain-specific information.",{}
"The model can therefore be trained easily on any genre of English, and should be trainable on any other Romanalphabet language.",{}
"Performance is comparable to or better than the performance of similar systems, but we emphasize the simplicity of retraining for new domains.",{}
The task of identifying sentence boundaries in text has not received as much attention as it deserves.,"{'title': '1 Introduction', 'number': '1'}"
"Many freely available natural language processing tools require their input to be divided into sentences, but make no mention of how to accomplish this (e.g.","{'title': '1 Introduction', 'number': '1'}"
"(Brill, 1994; Collins, 1996)).","{'title': '1 Introduction', 'number': '1'}"
Others perform the division implicitly without discussing performance (e.g.,"{'title': '1 Introduction', 'number': '1'}"
"(Cutting et al., 1992)).","{'title': '1 Introduction', 'number': '1'}"
"On first glance, it may appear that using a short list, of sentence-final punctuation marks, such as ., ?, and !, is sufficient.","{'title': '1 Introduction', 'number': '1'}"
"However, these punctuation marks are not used exclusively to mark sentence breaks.","{'title': '1 Introduction', 'number': '1'}"
"For example, embedded quotations may contain any of the sentence-ending punctuation marks and . is used as a decimal point, in email addresses, to indicate ellipsis and in abbreviations.","{'title': '1 Introduction', 'number': '1'}"
"Both ! and ? are somewhat less ambiguous *The authors would like to acknowledge the support of ARPA grant N66001-94-C-6043, ARO grant DAAH0494-G-0426 and NSF grant SBR89-20230. but appear in proper names and may be used multiple times for emphasis to mark a single sentence boundary.","{'title': '1 Introduction', 'number': '1'}"
Lexically-based rules could be written and exception lists used to disambiguate the difficult cases described above.,"{'title': '1 Introduction', 'number': '1'}"
"However, the lists will never be exhaustive, and multiple rules may interact badly since punctuation marks exhibit absorption properties.","{'title': '1 Introduction', 'number': '1'}"
"Sites which logically should be marked with multiple punctuation marks will often only have one ((Nunberg, 1990) as summarized in (White, 1995)).","{'title': '1 Introduction', 'number': '1'}"
"For example, a sentence-ending abbreviation will most likely not be followed by an additional period if the abbreviation already contains one (e.g. note that D.0 is followed by only a single . in The president lives in Washington, D.C.).","{'title': '1 Introduction', 'number': '1'}"
"As a result, we believe that manually writing rules is not a good approach.","{'title': '1 Introduction', 'number': '1'}"
"Instead, we present a solution based on a maximum entropy model which requires a few hints about what. information to use and a corpus annotated with sentence boundaries.","{'title': '1 Introduction', 'number': '1'}"
The model trains easily and performs comparably to systems that require vastly more information.,"{'title': '1 Introduction', 'number': '1'}"
Training on 39441 sentences takes 18 minutes on a Sun Ultra Sparc and disambiguating the boundaries in a single Wall Street Journal article requires only 1.4 seconds.,"{'title': '1 Introduction', 'number': '1'}"
"To our knowledge, there have been few papers about identifying sentence boundaries.","{'title': '2 Previous Work', 'number': '2'}"
"The most recent work will be described in (Palmer and Hearst, To appear).","{'title': '2 Previous Work', 'number': '2'}"
"There is also a less detailed description of Palmer and Hearst's system, SATZ, in (Palmer and Hearst, 1994).'","{'title': '2 Previous Work', 'number': '2'}"
The SATZ architecture uses either a decision tree or a neural network to disambiguate sentence boundaries.,"{'title': '2 Previous Work', 'number': '2'}"
The neural network achieves 98.5% accuracy on a corpus of Wall Street Journal We recommend these articles for a more comprehensive review of sentence-boundary identification work than we will be able to provide here. articles using a lexicon which includes part-of-speech (POS) tag information.,"{'title': '2 Previous Work', 'number': '2'}"
"By increasing the quantity ol training data and decreasing the size of their test corpus, Palmer and Hearst achieved performance of !","{'title': '2 Previous Work', 'number': '2'}"
)8.9% with the neural network.,"{'title': '2 Previous Work', 'number': '2'}"
They obtained similar results using the decision tree.,"{'title': '2 Previous Work', 'number': '2'}"
"All the results we will present for our algorithms are on their initial, larger test corpus.","{'title': '2 Previous Work', 'number': '2'}"
"In (Riley, 1989), Riley describes a decision-tree based approach to the problem.","{'title': '2 Previous Work', 'number': '2'}"
"His performance on I he Brown corpus is 99.8%, using a model learned from a. corpus of 25 million words.","{'title': '2 Previous Work', 'number': '2'}"
"Liberman and Church suggest in (Liberma.n and Church, 1992) that a. system could be quickly built to divide newswire text into sentences with a nearly negligible error rate, but do not actually build such a system.","{'title': '2 Previous Work', 'number': '2'}"
We present two systems for identifying sentence boundaries.,"{'title': '3 Our Approach', 'number': '3'}"
One is targeted at high performance and uses some knowledge about the structure of English financial newspaper text which may not be applicable to text from other genres or in other languages.,"{'title': '3 Our Approach', 'number': '3'}"
The other system uses no domain-specific knowledge and is aimed at being portable across English text genres and Roman alphabet languages.,"{'title': '3 Our Approach', 'number': '3'}"
"Potential sentence boundaries are identified by scanning the text for sequences of characters separated by whitespace (tokens) containing one of the symbols !, . or ?.","{'title': '3 Our Approach', 'number': '3'}"
"We use information about the token containing the potential sentence boundary, as well as contextual information about the tokens immediately to the left and to the right.","{'title': '3 Our Approach', 'number': '3'}"
"We also conducted tests using wider contexts, but performance did not improve.","{'title': '3 Our Approach', 'number': '3'}"
We call the token containing the symbol which marks a putative sentence boundary the Candidate.,"{'title': '3 Our Approach', 'number': '3'}"
'Hie portion of the Candidate preceding the potential sentence boundary is called the Prefix and the portion following it is called the Suffix.,"{'title': '3 Our Approach', 'number': '3'}"
"The system that focused on maximizing performance used the following hints, or contextual &quot;templates&quot;: The templates specify only the form of the information.","{'title': '3 Our Approach', 'number': '3'}"
"The exact information used by the maximum entropy model for the potential sentence boundary marked by . in Corp. in Example 1 would be: PreviousWordIsCapitalized, Prefix= Corp, Suffix=NULL, PrefixFeature=CorporateDesignator.","{'title': '3 Our Approach', 'number': '3'}"
"The highly portable system uses only the identity of the Candidate and its neighboring words, and a list of abbreviations induced from the training data.2 Specifically, the &quot;templates&quot; used are: The information this model would use for Example 1 would be: PreviousWord=ANLP, FollowingWord=chairmon, Prefix=Corp, Suffix=NULL, PrefixFeature=InducedAbbreviation.","{'title': '3 Our Approach', 'number': '3'}"
"The abbreviation list is automatically produced from the training data, and the contextual questions are also automatically generated by scanning the training data. with question templates.","{'title': '3 Our Approach', 'number': '3'}"
"As a. result, no hand-crafted rules or lists are required by the highly portable system and it can be easily retrained for other languages or text genres.","{'title': '3 Our Approach', 'number': '3'}"
"The model used here for sentence-boundary detection is based on the maximum entropy model used for POS tagging in (Ratna.parkhi, 1996).","{'title': '4 Maximum Entropy', 'number': '4'}"
"For each potential sentence boundary token (., ?, and !","{'title': '4 Maximum Entropy', 'number': '4'}"
"), we estimate a. joint, probability distribution p of the token and its surrounding context, both of which are denoted by c, occurring as an actual sentence boundary.","{'title': '4 Maximum Entropy', 'number': '4'}"
"The distribution is given by: p(b, c) = Ir „,,,.f-(b„c), where b e no, yes}, where the cri's are the unknown parameters of the model, and where each aj corresponds to a fi, or a feature.","{'title': '4 Maximum Entropy', 'number': '4'}"
"Thus the probability of seeing an actual sentence boundary in the context c is given by p(yes, c).","{'title': '4 Maximum Entropy', 'number': '4'}"
"The contextual information deemed useful for sentence-boundary detection, which. we described earlier, must be encoded using features.","{'title': '4 Maximum Entropy', 'number': '4'}"
"For example, a useful feature might be: This feature will allow the model to discover that the period at the end of the word Mr. seldom occurs as a sentence boundary.","{'title': '4 Maximum Entropy', 'number': '4'}"
"Therefore the parameter corresponding to this feature will hopefully boost the probability p(no, c) if the Prefix is Mr.","{'title': '4 Maximum Entropy', 'number': '4'}"
"The parameters are chosen to maximize the likelihood of the training data, using the Generalized Iterative Scaling (Darroch and Ratcliff, 1972) algorithm.","{'title': '4 Maximum Entropy', 'number': '4'}"
"The model also can be viewed under the Maximum Entropy framework, in which we choose a distribution p that maximizes the entropy H (p) where /:5(b, c) is the observed distribution of sentenceboundaries and contexts in the training data.","{'title': '4 Maximum Entropy', 'number': '4'}"
"As a result, the model in practice tends not to commit towards a particular outcome (yes or no) unless it has seen sufficient evidence for that outcome; it is maximally uncertain beyond meeting the evidence.","{'title': '4 Maximum Entropy', 'number': '4'}"
"All experiments use a simple decision rule to classify each potential sentence boundary: a potential sentence boundary is an actual sentence boundary if and only if p(yesic) > .5, where and where c is the context including the potential sentence boundary.","{'title': '4 Maximum Entropy', 'number': '4'}"
"We trained our system on 39441 sentences (898737 words) of Wall Street Journal text from sections 00 through 24 of the second release of the Penn Treebank3 (Marcus, Santorini, and Marcinkiewicz, 1993).","{'title': '5 System Performance', 'number': '5'}"
We corrected punctuation mistakes and erroneous sentence boundaries in the training data.,"{'title': '5 System Performance', 'number': '5'}"
"Performance figures for our best performing system, which used a hand-crafted list of honorifics and corporate designators, are shown in Table 1.","{'title': '5 System Performance', 'number': '5'}"
"The first test set, WSJ, is Palmer and Hearst's initial test data and the second is the entire Brown corpus.","{'title': '5 System Performance', 'number': '5'}"
We present the Brown corpus performance to show the importance of training on the genre of text. on which testing will be performed.,"{'title': '5 System Performance', 'number': '5'}"
"Table 1 also shows the number of sentences in each corpus, the number of candidate punctuation marks, the accuracy over potential sentence boundaries, the number of false positives and the number of false negatives.","{'title': '5 System Performance', 'number': '5'}"
"Performance on the WSJ corpus was, as we expected, higher than performance on the Brown corpus since we trained the model on financial newspaper text.","{'title': '5 System Performance', 'number': '5'}"
Possibly more significant. than the system's performance is its portability to new domains and languages.,"{'title': '5 System Performance', 'number': '5'}"
"A trimmed down system which used no information except that derived from the training corpus performs nearly as well, and requires no resources other than a training corpus.","{'title': '5 System Performance', 'number': '5'}"
Its performance on the same two corpora is shown in Table 2.,"{'title': '5 System Performance', 'number': '5'}"
"Since 39441 training sentences is considerably more than might exist in a new domain or a language other than English, we experimented with the quantity of training data required to maintain performance.","{'title': '5 System Performance', 'number': '5'}"
Table 3 shows performance on the WSJ corpus as a. function of training set size using the best performing system and the more portable system.,"{'title': '5 System Performance', 'number': '5'}"
"As can seen from the table, performance degrades a.s the quantity of training data decreases, but even with only 500 example sentences performance is beter than the baselines of 64.00/0 if a. sentence boundary is guessed at every potential site and 78.4%, if only token-final instances of sentence-ending punctuation are assumed to be boundaries.","{'title': '5 System Performance', 'number': '5'}"
We have described an approach to identifying sentence boundaries which performs comparably to other state-of-the-art systems that require vastly more resources.,"{'title': '6 Conclusions', 'number': '6'}"
"For example, Riley's performance on the Brown corpus is higher than ours, but his system is trained on the Brown corpus and uses thirty times as much data as our system.","{'title': '6 Conclusions', 'number': '6'}"
"Also, Palmer L Hearst's system requires POS tag information, which limits its use to those genres or languages for which there are either POS tag lexica or POS tag annotated corpora that could be used to train automatic taggers.","{'title': '6 Conclusions', 'number': '6'}"
"In comparison, our system does not require POS ta.gs or any supporting resources beyond the sentence-boundary annotated corpus.","{'title': '6 Conclusions', 'number': '6'}"
It is therefore easy and inexpensive to retrain this system for different genres of text in English and text in other Roman-alphabet languages.,"{'title': '6 Conclusions', 'number': '6'}"
"Furthermore, we showed tha.t a small training corpus is sufficient for good performance, and we estimate that annotating enough data to achieve good performance would require only several hours of work, in comparison to the many hours required to generate POS tag and lexical probabilities.","{'title': '6 Conclusions', 'number': '6'}"
We would like to thank David Palmer for giving us the test data he and Marti Hearst used for their sentence detection experiments.,"{'title': '7 Acknowledgments', 'number': '7'}"
We would also like to thank the anonymous reviewers for their helpful insights.,"{'title': '7 Acknowledgments', 'number': '7'}"
