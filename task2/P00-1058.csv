col1,col2
"We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.",{}
"We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.",{}
Why use tree-adjoining grammar for statistical parsing?,"{'title': '1 Introduction', 'number': '1'}"
"Given that statistical natural language processing is concerned with the probable rather than the possible, it is not because TAG can describe constructions like arbitrarily large Dutch verb clusters.","{'title': '1 Introduction', 'number': '1'}"
"Rather, what makes TAG useful for statistical parsing are the structural descriptions it assigns to breadand-butter sentences.","{'title': '1 Introduction', 'number': '1'}"
"The approach of Chelba and Jelinek (1998) to language modeling is illustrative: even though the probability estimate of w appearing as the lith word can be conditioned on the entire history w1, ... , wk-1, the quantity of available training data limits the usable context to about two words but which two?","{'title': '1 Introduction', 'number': '1'}"
A trigram model chooses wk-1 and wk-2 and works quite well; a model which chose wk-7 and wk-11 would probably work less well.,"{'title': '1 Introduction', 'number': '1'}"
"But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model.","{'title': '1 Introduction', 'number': '1'}"
"Thus the (virtual) grammar serves to structure the history so that the two most useful words can be chosen, even though the structure of the problem itself is entirely linear.","{'title': '1 Introduction', 'number': '1'}"
"Similarly, nothing about the parsing problem requires that we construct any structure other than phrase structure.","{'title': '1 Introduction', 'number': '1'}"
"But beginning with (Magerman, 1995) statistical parsers have used bilexical dependencies with great success.","{'title': '1 Introduction', 'number': '1'}"
"Since these dependencies are not encoded in plain phrase-structure trees, the standard approach has been to let the lexical heads percolate up the tree, so that when one lexical head is immediately dominated by another, it is understood to be dependent on it.","{'title': '1 Introduction', 'number': '1'}"
"Effectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model.","{'title': '1 Introduction', 'number': '1'}"
"However, this solution is not ideal.","{'title': '1 Introduction', 'number': '1'}"
"Aside from cases where context-free derivations are incapable of encoding both constituency and dependency (which are somewhat isolated and not of great interest for statistical parsing) there are common cases where percolation of single heads is not sufficient to encode dependencies correctly for example, relative clause attachment or raising/auxiliary verbs (see Section 3).","{'title': '1 Introduction', 'number': '1'}"
More complicated grammar transformations are necessary.,"{'title': '1 Introduction', 'number': '1'}"
A more suitable approach is to employ a grammar formalism which produces structural descriptions that can encode both constituency and dependency.,"{'title': '1 Introduction', 'number': '1'}"
"Lexicalized TAG is such a formalism, because it assigns to each sentence not only a parse tree, which is built out of elementary trees and is interpreted as encoding constituency, but a derivation tree, which records how the various elementary trees were combined together and is commonly intepreted as encoding dependency.","{'title': '1 Introduction', 'number': '1'}"
"The ability of probabilistic LTAG to model bilexical dependencies was noted early on by (Resnik, 1992).","{'title': '1 Introduction', 'number': '1'}"
It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG.,"{'title': '1 Introduction', 'number': '1'}"
We discuss a few such cases in Section 3.,"{'title': '1 Introduction', 'number': '1'}"
In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank.,"{'title': '1 Introduction', 'number': '1'}"
"We find that the automatically-extracted grammar gives an improvement over the EM-based induction method of (Hwa, 1998), and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement.","{'title': '1 Introduction', 'number': '1'}"
"We emphasize that TAG is attractive not because it can do things that CFG cannot, but because it does everything that CFG can, only more cleanly.","{'title': '1 Introduction', 'number': '1'}"
"(This is where the analogy with (Chelba and Jelinek, 1998) breaks down.)","{'title': '1 Introduction', 'number': '1'}"
Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by offering two such possibilities.,"{'title': '1 Introduction', 'number': '1'}"
"The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG (Schabes and Waters, 1995).","{'title': '2 The formalism', 'number': '2'}"
"In this variant there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction.","{'title': '2 The formalism', 'number': '2'}"
"Auxiliary trees and adjunction are restricted as in TIG: essentially, no wrapping adjunction or anything equivalent to wrapping adjunction is allowed.","{'title': '2 The formalism', 'number': '2'}"
"Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).","{'title': '2 The formalism', 'number': '2'}"
In sisteradjunction the root of a modifier tree is added as a new daughter to any other node.,"{'title': '2 The formalism', 'number': '2'}"
(Note that as it stands sister-adjunction is completely unconstrained; it will be constrained by the probability model.),"{'title': '2 The formalism', 'number': '2'}"
We introduce this operation simply so we can derive the flat structures found in the Penn Treebank.,"{'title': '2 The formalism', 'number': '2'}"
"Following (Schabes and Shieber, 1994), multiple modifier trees can be sister-adjoined at a single site, but only one auxiliary tree may be adjoined at a single node.","{'title': '2 The formalism', 'number': '2'}"
"Figure 1 shows an example grammar and the derivation of the sentence &quot;John should leave tomorrow.&quot; The derivation tree encodes this process, with each arc corresponding to a composition operation.","{'title': '2 The formalism', 'number': '2'}"
Arcs corresponding to substitution and adjunction are labeled with the Gorn address1 of the substitution or adjunction site.,"{'title': '2 The formalism', 'number': '2'}"
An arc corresponding to the sister-adjunction of a tree between the ith and i + 1th children of rl (allowing for two imaginary children beyond the leftmost and rightmost children) is labeled rl; i.,"{'title': '2 The formalism', 'number': '2'}"
"This grammar, as well as the grammar used by the parser, is lexicalized in the sense that every elementary tree has exactly one terminal node, its lexical anchor.","{'title': '2 The formalism', 'number': '2'}"
"Since sister-adjunction can be simulated by ordinary adjunction, this variant is, like TIG (and CFG), weakly context-free and O(n3)-time parsable.","{'title': '2 The formalism', 'number': '2'}"
"Rather than coin a new acronym for this particular variant, we will simply refer to it as &quot;TAG&quot; and trust that no confusion will arise.","{'title': '2 The formalism', 'number': '2'}"
"The parameters of a probabilistic TAG (Resnik, 1992; Schabes, 1992) are: where a ranges over initial trees, ,3 over auxiliary trees, y over modifier trees, and rl over nodes.","{'title': '2 The formalism', 'number': '2'}"
"Pi(a) is the probability of beginning a derivation with a; Ps(a j TI) is the probability of substituting a at TI; Pa(,3 j TI) is the probability of adjoining ,3 at TI; finally, Pa(NONE j TI) is the probability of nothing adjoining at rl.","{'title': '2 The formalism', 'number': '2'}"
"(Carroll and Weir, 1997) suggest other parameterizations worth exploring as well.","{'title': '2 The formalism', 'number': '2'}"
"Our variant adds another set of parameters: This is the probability of sister-adjoining y between the ith and i + 1th children of rl (as before, allowing for two imaginary children beyond the leftmost and rightmost children).","{'title': '2 The formalism', 'number': '2'}"
"Since multiple modifier trees can adjoin at the same location, Psa(-Y) is also conditioned on a flag f which indicates whether y is the first modifier tree (i.e., the one closest to the head) to adjoin at that location.","{'title': '2 The formalism', 'number': '2'}"
The probability of a derivation can then be expressed as a product of the probabilities of address i has address i â€¢ j. the individual operations of the derivation.,"{'title': '2 The formalism', 'number': '2'}"
Thus the probability of the example derivation of Figure 1 would be where a(i) is the node of a with address i.,"{'title': '2 The formalism', 'number': '2'}"
"We want to obtain a maximum-likelihood estimate of these parameters, but cannot estimate them directly from the Treebank, because the sample space of PTAG is the space of TAG derivations, not the derived trees that are found in the Treebank.","{'title': '2 The formalism', 'number': '2'}"
"One approach, taken in (Hwa, 1998), is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihood estimate by EM.","{'title': '2 The formalism', 'number': '2'}"
"Another approach, taken in (Magerman, 1995) and others for lexicalized PCFGs and (Neumann, 1998; Xia, 1999; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations.","{'title': '2 The formalism', 'number': '2'}"
We take this approach as well.,"{'title': '2 The formalism', 'number': '2'}"
"(One could imagine combining the two approaches, using heuristics to extract a grammar but EM to estimate its parameters.)","{'title': '2 The formalism', 'number': '2'}"
"In a lexicalized TAG, because each composition brings together two lexical items, every composition probability involves a bilexical dependency.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"Given a CFG and headpercolation scheme, an equivalent TAG can be constructed whose derivations mirror the dependency analysis implicit in the headpercolation scheme.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"Furthermore, there are some dependency analyses encodable by TAGs that are not encodable by a simple head-percolation scheme.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"For example, for the sentence &quot;John should have left,&quot; Magerman's rules make should and have the heads of their respective VPs, so that there is no dependency between left and its subject John (see Figure 2a).","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"Since nearly a quarter of nonempty subjects appear in such a configuration, this is not a small problem.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"(We could make VP the head of VP instead, but this would generate auxiliaries independently of each other, so that, for example, P(John leave) > 0.)","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"TAG can produce the desired dependencies (b) easily, using the grammar of Figure 1.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
Bilexical dependencies are not the only nonlocal dependencies that can be used to improve parsing accuracy.,"{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"For example, the attachment of an S depends on the presence or absence of the embedded subject (Collins, 1999); Treebank-style two-level NPs are mismodeled by PCFG (Collins, 1999; Johnson, 1998); the generation of a node depends on the label of its grandparent (Charniak, 2000; Johnson, 1998).","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
Such changes are not always obvious a priori and often must be devised anew for each language or each corpus.,"{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"But none of these cases really requires special treatment in a PTAG model, because each composition probability involves not only a bilexical dependency but a &quot;biarboreal&quot; (tree-tree) dependency.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"That is, PTAG generates an entire elementary tree at once, conditioned on the entire elementary tree being modified.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
Thus dependencies that have to be stipulated in a PCFG by tree transformations or parser modifications are captured for free in a PTAG model.,"{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"Of course, the price that the PTAG model pays is sparser data; the backoff model must therefore be chosen carefully.","{'title': '3 Some properties of probabilistic TAG', 'number': '3'}"
"We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997).","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"For each node T1, these rules classify exactly one child of T1 as a head and the rest as either arguments or adjuncts.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
Using this classification we can construct a TAG derivation (including elementary trees) from a derived tree as follows: Rules (1) and (2) produce the desired result; rule (3) changes the analysis somewhat by making subtrees with recursive arguments into predicative auxiliary trees.,"{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"It produces, among other things, the analysis of auxiliary verbs described in the previous section.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"It is applied in a greedy fashion, with potential rjs considered top-down and potential Bs bottomup.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
The complicated restrictions on 0 are simply to ensure that a well-formed TIG derivation is produced.,"{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"Now that we have augmented the training data to include TAG derivations, we could try to directly estimate the parameters of the model from Section 2.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"But since the number of (tree, site) pairs is very high, the data would be too sparse.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"We therefore generate an elementary tree in two steps: first the tree template (that is, the elementary tree minus its Frequency anchor), then the anchor.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"The probabilities are decomposed as follows: where Ta is the tree template of a, to is the part-of-speech tag of the anchor, and wa is the anchor itself.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"The generation of the tree template has two backoff levels: at the first level, the anchor of il is ignored, and at the second level, the POS tag of the anchor as well as the flag f are ignored.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"The generation of the anchor has three backoff levels: the first two are as before, and the third just conditions the anchor on its POS tag.","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
"The backed-off models are combined by linear interpolation, with the weights chosen as in (Bikel et al., 1997).","{'title': '4 Inducing a stochastic grammar from the Treebank', 'number': '4'}"
We ran the algorithm given in Section 4.1 on sections 02{21 of the Penn Treebank.,"{'title': '5 The experiment', 'number': '5'}"
"The extracted grammar is large (about 73,000 trees, with words seen fewer than four times replaced with the symbol *UNKNOWN*), but if we consider elementary tree templates, the grammar is quite manageable: 3626 tree templates, of which 2039 occur more than once (see Figure 4).","{'title': '5 The experiment', 'number': '5'}"
The 616 most frequent tree-template types account for 99% of tree-template tokens in the training data.,"{'title': '5 The experiment', 'number': '5'}"
Removing all but these trees from the grammar increased the error rate by about 5% (testing on a subset of section 00).,"{'title': '5 The experiment', 'number': '5'}"
A few of the most frequent tree-templates are shown in Figure 3.,"{'title': '5 The experiment', 'number': '5'}"
"So the extracted grammar is fairly compact, but how complete is it?","{'title': '5 The experiment', 'number': '5'}"
"If we plot the growth of the grammar during training (Figure 5), it's not clear the grammar will ever converge, even though the very idea of a grammar requires it.","{'title': '5 The experiment', 'number': '5'}"
"Three possible explanations are: In a random sample of 100 once-seen elementary tree templates, we found (by casual inspection) that 34 resulted from annotation errors, 50 from deficiencies in the heuristics, and four apparently from performance errors.","{'title': '5 The experiment', 'number': '5'}"
Only twelve appeared to be genuine.,"{'title': '5 The experiment', 'number': '5'}"
Therefore the continued growth of the grammar is not as rapid as Figure 5 might indicate.,"{'title': '5 The experiment', 'number': '5'}"
"Moreover, our extraction heuristics evidently have room to improve.","{'title': '5 The experiment', 'number': '5'}"
"The majority of trees resulting from deficiencies in the heuristics involved complicated coordination structures, which is not surprising, since coordination has always been problematic for TAG.","{'title': '5 The experiment', 'number': '5'}"
"To see what the impact of this failure to converge is, we ran the grammar extractor on some held-out data (section 00).","{'title': '5 The experiment', 'number': '5'}"
"Out of 45082 tree tokens, 107 tree templates, or 0.2%, had not been seen in training.","{'title': '5 The experiment', 'number': '5'}"
This amounts to about one unseen tree template every 20 sentences.,"{'title': '5 The experiment', 'number': '5'}"
"When we consider lexicalized trees, this figure of course rises: out of the same 45082 tree tokens, 1828 lexicalized trees, or 4%, had not been seen in training.","{'title': '5 The experiment', 'number': '5'}"
So the coverage of the grammar is quite good.,"{'title': '5 The experiment', 'number': '5'}"
"Note that even in cases where the parser encounters a sentence for which the (fallible) extraction heuristics would have produced an unseen tree template, it is possible that the parser will use other trees to produce the correct bracketing.","{'title': '5 The experiment', 'number': '5'}"
"We used a CKY-style parser similar to the one described in (Schabes and Waters, 1996), with a modification to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions.","{'title': '5 The experiment', 'number': '5'}"
We also extended the parser to simulate sister-adjunction as regular adjunction and compute the flag f which distinguishes the first modifier from subsequent modifiers.,"{'title': '5 The experiment', 'number': '5'}"
"We use a beam search, computing the score of an item [TI, i, j] by multiplying it by the prior probability P(TI) (Goodman, 1997); any item with score less than 10-5 times that of the best item in a cell is pruned.","{'title': '5 The experiment', 'number': '5'}"
"Following (Collins, 1997), words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in (Ratnaparkhi, 1996).","{'title': '5 The experiment', 'number': '5'}"
Tree templates occurring only once in training were ignored entirely.,"{'title': '5 The experiment', 'number': '5'}"
"We first compared the parser with (Hwa, 1998): we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses.","{'title': '5 The experiment', 'number': '5'}"
The metric used was the percentage of guessed brackets which did not cross any correct brackets.,"{'title': '5 The experiment', 'number': '5'}"
"Our parser scored 84.4% compared with 82.4% for (Hwa, 1998), an error reduction of 11%.","{'title': '5 The experiment', 'number': '5'}"
"Next we compared our parser against lexicalized PCFG parsers, training on sections 02{21 and testing on section 23.","{'title': '5 The experiment', 'number': '5'}"
The results are shown in Figure 6.,"{'title': '5 The experiment', 'number': '5'}"
These results place our parser roughly in the middle of the lexicalized PCFG parsers.,"{'title': '5 The experiment', 'number': '5'}"
"While the results are not state-of-the-art, they do demonstrate the viability of TAG as a framework for statistical parsing.","{'title': '5 The experiment', 'number': '5'}"
"With improvements in smoothing and cleaner handling of punctuation and coordination, perhaps these results can be brought more upto-date.","{'title': '5 The experiment', 'number': '5'}"
"(Neumann, 1998) describes an experiment similar to ours, although the grammar he extracts only arrives at a complete parse for 10% of unseen sentences.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"(Xia, 1999) describes a grammar extraction process similar to ours, and describes some techniques for automatically filtering out invalid elementary trees.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
Our work has a great deal in common with independent work by Chen and VijayShanker (2000).,"{'title': '6 Conclusion: related and future work', 'number': '6'}"
"They present a more detailed discussion of various grammar extraction processes and the performance of supertagging models (B. Srinivas, 1997) based on the extracted grammars.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"They do not report parsing results, though their intention is to evaluate how the various grammars affect parsing accuracy and how k-best supertagging afffects parsing speed.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"Srinivas's work on supertags (B. Srinivas, 1997) also uses TAG for statistical parsing, but with a rather different strategy: tree templates are thought of as extended parts-ofspeech, and these are assigned to words based on local (e.g., n-gram) context.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"As for future work, there are still possibilities made available by TAG which remain to be explored.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"One, also suggested by (Chen and Vijay-Shanker, 2000), is to group elementary trees into families and relate the trees of a family by transformations.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"For example, one would imagine that the distribution of active verbs and their subjects would be similar to the distribution of passive verbs and their notional subjects, yet they are treated as independent in the current model.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"If the two configurations could be related, then the sparseness of verb-argument dependencies would be reduced.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
Another possibility is the use of multiplyanchored trees.,"{'title': '6 Conclusion: related and future work', 'number': '6'}"
"Nothing about PTAG requires that elementary trees have only a single anchor (or any anchor at all), so multiplyanchored trees could be used to make, for example, the attachment of a PP dependent not only on the preposition (as in the current model) but the lexical head of the prepositional object as well, or the attachment of a relative clause dependent on the embedded verb as well as the relative pronoun.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
The smoothing method described above would have to be modified to account for multiple anchors.,"{'title': '6 Conclusion: related and future work', 'number': '6'}"
"In summary, we have argued that TAG provides a cleaner way of looking at statistical parsing than lexicalized PCFG does, and demonstrated that in practice it performs in the same range.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
"Moreover, the greater flexibility of TAG suggests some potential improvements which would be cumbersome to implement using a lexicalized CFG.","{'title': '6 Conclusion: related and future work', 'number': '6'}"
Further research will show whether these advantages turn out to be significant in practice.,"{'title': '6 Conclusion: related and future work', 'number': '6'}"
This research is supported in part by ARO grant DAAG55971-0228 and NSF grant SBR89-20230-15.,"{'title': 'Acknowledgements', 'number': '7'}"
"Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.","{'title': 'Acknowledgements', 'number': '7'}"
S. D. G.,"{'title': 'Acknowledgements', 'number': '7'}"
