col1,col2
"This paper considers statistical parsing of Czech, which differs radically from English in at least two (1) it is a inflected and it has relatively word order. differences are likely to pose new problems for techniques that have been developed on English.",{}
We describe our experience in building on the parsing model of (Collins 97).,{}
Our final results — 80% dependency accuracy — represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.,{}
Much of the recent research on statistical parsing has focused on English; languages other than English are likely to pose new problems for statistical methods.,"{'title': '1 Introduction', 'number': '1'}"
"This paper considers statistical parsing of Czech, using the Prague Dependency Treebank (PDT) (Haji, 1998) as a source of training and test data (the PDT contains around 480,000 words of general news, business news, and science articles Other Slavic languages (such as Polish, Russian, Slovak, Slovene, Serbo-croatian, Ukrainian) also show these characteristics.","{'title': '1 Introduction', 'number': '1'}"
Many European languages exhibit FWO and HI phenomena to a lesser extent.,"{'title': '1 Introduction', 'number': '1'}"
Thus the techniques and results found for Czech should be relevant to parsing several other languages.,"{'title': '1 Introduction', 'number': '1'}"
"This paper first describes a baseline approach, based on the parsing model of (Collins 97), which recovers dependencies with 72% accuracy.","{'title': '1 Introduction', 'number': '1'}"
"We then describe a series of refinements to the model, giving an improvement to 80% accuracy, with around 82% accuracy on newspaper/business articles.","{'title': '1 Introduction', 'number': '1'}"
"(As a point of comparison, the parser achieves 91% dependency accuracy on English (Wall Street Journal) text.)","{'title': '1 Introduction', 'number': '1'}"
"The Prague Dependency Treebank PDT (Hap, 1998) has been modeled after the Penn Treebank (Marcus et al. 93), with one important exception: following the Praguian linguistic tradition, the syntactic annotation is based on dependencies rather than phrase structures.","{'title': '2 Data and Evaluation', 'number': '2'}"
"Thus instead of &quot;nonterminal&quot; symbols used at the non-leaves of the tree, the PDT uses so-called analytical functions capturing the type of relation between a dependent and its governing node.","{'title': '2 Data and Evaluation', 'number': '2'}"
Thus the number of nodes is equal to the number of tokens (words + punctuation) plus one (an artificial root node with rather technical function is added to each sentence).,"{'title': '2 Data and Evaluation', 'number': '2'}"
"The PDT contains also a traditional morpho-syntactic annotation (tags) at each word position (together with a lemma, uniquely representing the underlying lexical unit).","{'title': '2 Data and Evaluation', 'number': '2'}"
"As Czech is a HI language, the size of the set of possible tags is unusually high: more than 3,000 tags may be assigned by the Czech morphological analyzer.","{'title': '2 Data and Evaluation', 'number': '2'}"
"The PDT also contains machine-assigned tags and lemmas for each word (using a tagger described in (Haji 6 and Hladka, 1998)).","{'title': '2 Data and Evaluation', 'number': '2'}"
"For evaluation purposes, the PDT has been divided into a training set (19k sentences) and a development/evaluation test set pair (about 3,500 sentences each).","{'title': '2 Data and Evaluation', 'number': '2'}"
"Parsing accuracy is defined as the ratio of correct dependency links vs. the total number of dependency links in a sentence (which equals, with the one artificial root node added, to the number of tokens in a sentence).","{'title': '2 Data and Evaluation', 'number': '2'}"
"As usual, with the development test set being available during the development phase, all final results has been obtained on the evaluation test set, which nobody could see beforehand.","{'title': '2 Data and Evaluation', 'number': '2'}"
The parsing model builds on Model 1 of (Collins 97); this section briefly describes the model.,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
The parser uses a lexicalized grammar — each nonterminal has an associated head-word and part-ofspeech (POS).,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"We write non-terminals as X (x): X is the non-terminal label, and x is a (w, t) pair where w is the associated head-word, and t as the POS tag.","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"See figure 1 for an example lexicalized tree, and a list of the lexicalized rules that it contains.","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"Each rule has the fonnl : 'With the exception of the top rule in the tree, which has the form TOP H (h) .","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"H is the head-child of the phrase, which inherits the head-word h from its parent P. L1...Ln and RI are left and right modifiers of H. Either n or m may be zero, and n = The model can be considered to be a variant of Probabilistic Context-Free Grammar (PCFG).","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
In PCFGs each rule a —> in the CFG underlying the PCFG has an associated probability P(/31a).,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"In (Collins 97), P (01a) is defined as a product of terms, by assuming that the right-hand-side of the rule is generated in three steps: probability I P, h, H), where Ln+i (in+1) = STOP.","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"The STOP symbol is added to the vocabulary of nonterminals, and the model stops generating left modifiers when it is generated.","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
Other rules in the tree contribute similar sets of probabilities.,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
The probability for the entire tree is calculated as the product of all these terms.,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"(Collins 97) describes a series of refinements to this basic model: the addition of &quot;distance&quot; (a conditioning feature indicating whether or not a modifier is adjacent to the head); the addition of subcategorization parameters (Model 2), and parameters that model wh-movement (Model 3); estimation techniques that smooth various levels of back-off (in particular using POS tags as word-classes, allowing the model to learn generalizations about POS classes of words).","{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
Search for the highest probability tree for a sentence is achieved using a CKY-style parsing algorithm.,"{'title': '3 A Sketch of the Parsing Model', 'number': '3'}"
"Many statistical parsing methods developed for English use lexicalized trees as a representation (e.g., (Jelinek et al. 94; Magerman 95; Ratnaparkhi 97; Charniak 97; Collins 96; Collins 97)); several (e.g., (Eisner 96; Collins 96; Collins 97; Charniak 97)) emphasize the use of parameters associated with dependencies between pairs of words.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"The Czech PDT contains dependency annotations, but no tree structures.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"For parsing Czech we considered a strategy of converting dependency structures in training data to lexicalized trees, then running the parsing algorithms originally developed for English.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
A key point is that the mapping from lexicalized trees to dependency structures is many-to-one.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"As an example, figure 2 shows an input dependency structure, and three different lexicalized trees with this dependency structure.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
The choice of tree structure is crucial in determining the independence assumptions that the parsing model makes.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
There are at least 3 degrees of freedom when deciding on the tree structures: To provide a baseline result we implemented what is probably the simplest possible conversion scheme: The baseline approach gave a result of 71.9% accuracy on the development test set.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"While the baseline approach is reasonably successful, there are some linguistic phenomena that lead to clear problems.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"This section describes some tree transformations that are linguistically motivated, and lead to improvements in parsing accuracy.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
In the PDT the verb is taken to be the head of both sentences and relative clauses.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
Figure 4 illustrates how the baseline transformation method can lead to parsing errors in relative clause cases.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"Figure 4(c) shows the solution to the problem: the label of the relative clause is changed to SBAR, and an additional VP level is added to the right of the relative pronoun.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"Similar transformations were applied for relative clauses involving Wh-PPs (e.g., &quot;the man to whom I gave a book&quot;), Wh-NPs (e.g., &quot;the man whose book I read&quot;) and Wh-Adverbials (e.g., &quot;the place where I live&quot;).","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"The PDT takes the conjunct to be the head of coordination structures (for example, and would be the head of the NP dogs and cats).","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
In these cases the baseline approach gives tree structures such as that in figure 5(a).,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"The non-terminal label for the phrase is JP (because the head of the phrase, the conjunct and, is tagged as J).","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"This choice of non-terminal is problematic for two reasons: (1) the JP label is assigned to all coordinated phrases, for example hiding the fact that the constituent in figure 5(a) is an NP; (2) the model assumes that left and right modifiers are generated independently of each other, and as it stands will give unreasonably high probability to two unlike phrases being coordinated.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"To fix these problems, the non-terminal label in coordination cases was altered to be the same as that of the second conjunct (the phrase directly to the right of the head of the phrase).","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
See figure 5.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
A similar transformation was made for cases where a comma was the head of a phrase.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
Figure 6 shows an additional change concerning commas.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
This change increases the sensitivity of the model to punctuation.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"This section describes some modifications to the parameterization of the model. guish main clauses from relative clauses: both have a verb as the head, so both are labeled VP.","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
(b) A typical parsing error due to relative and main clauses not being distinguished.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
"(note that two main clauses can be coordinated by a comma, as in John likes Mary, Mary likes Tim).","{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
(c) The solution to the problem: a modification to relative clause structures in training data.,"{'title': '4 ParsingtheCzechPDT', 'number': '4'}"
The model of (Collins 97) had conditioning variables that allowed the model to learn a preference for dependencies which do not cross verbs.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"From the results in table 3, adding this condition improved accuracy by about 0.9% on the development set.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
The parser of (Collins 96) used punctuation as an indication of phrasal boundaries.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"It was found that if a constituent Z (...XY...) has two children X and Y separated by a punctuation mark, then Y is generally followed by a punctuation mark or the end of sentence marker.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"The parsers of (Collins 96,97) encoded this as a hard constraint.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
In the Czech parser we added a cost of -2.5 (log probability)2 to structures that violated this constraint.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
The model of section 3 made the assumption that modifiers are generated independently of each other.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"This section describes a hi gram model, where the context is increased to consider the previously generated modifier ((Eisner 96) also describes use of bigram statistics).","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
The right-hand-side of a rule is now assumed to be generated in the following three step process: where Lo is defined as a special NULL symbol.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Thus the previous modifier, Li_1, is added to the conditioning context (in the previous model the left modifiers had probability Introducing bigram-dependencies into the parsing model improved parsing accuracy by about 0.9 % (as shown in Table 3).","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Part of speech (POS) tags serve an important role in statistical parsing by providing the model with a level of generalization as to how classes of words tend to behave, what roles they play in sentences, and what other classes they tend to combine with.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Statistical parsers of English typically make use of the roughly 50 POS tags used in the Penn Treebank corpus, but the Czech PDT corpus provides a much richer set of POS tags, with over 3000 possible tags defined by the tagging system and over 1000 tags actually found in the corpus.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Using that large a tagset with a training corpus of only 19,000 sentences would lead to serious sparse data problems.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
It is also clear that some of the distinctions being made by the tags are more important than others for parsing.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
We therefore explored different ways of extracting smaller but still maximally informative POS tagsets.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"The POS tags in the Czech PDT corpus (Haji 6 and Hladka, 1997) are encoded in 13-character strings.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
Table 1 shows the role of each character.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"For example, the tag NNMP1 A would be used for a word that had &quot;noun&quot; as both its main and detailed part of speech, that was masculine, plural, nominative (case 1), and whose negativeness value was &quot;affirmative&quot;.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Within the corpus, each word was annotated with all of the POS tags that would be possible given its spelling, using the output of a morphological analysis program, and also with the single one of those tags that a statistical POS tagging program had predicted to be the correct tag (Hake and Hladka, 1998).","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Table 2 shows a phrase from the corpus, with the alternative possible tags and machine-selected tag for each word.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"In the training portion of the corpus, the correct tag as judged by human annotators was also provided.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"In the baseline approach, the first letter, or &quot;main part of speech&quot;, of the full POS strings was used as the tag.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
This resulted in a tagset with 13 possible values.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"A number of alternative, richer tagsets were explored, using various combinations of character positions from the tag string.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"The most successful alternative was a two-letter tag whose first letter was always the main POS, and whose second letter was the case field if the main POS was one that displays case, while otherwise the second letter was the detailed POS.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"(The detailed POS was used for the main POS values D, J, V, and X; the case field was used for the other possible main POS values.)","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"This two-letter scheme resulted in 58 tags, and provided about a 1.1% parsing improvement over the baseline on the development set.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Even richer tagsets that also included the person, gender, and number values were tested without yielding any further improvement, presumably because the damage from sparse data outweighed the value of the additional information present.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"An entirely different approach, rather than searching by hand for effective tagsets, would be to use clustering to derive them automatically.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"We explored two different methods, bottom-up and topdown, for automatically deriving POS tag sets based on counts of governing and dependent tags extracted from the parse trees that the parser constructs from the training data.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Neither tested approach resulted in any improvement in parsing performance compared to the hand-designed &quot;two letter&quot; tagset, but the implementations of each were still only preliminary, and a clustered tagset more adroitly derived might do better.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"One final issue regarding POS tags was how to deal with the ambiguity between possible tags, both in training and test.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"In the training data, there was a choice between using the output of the POS tagger or the human annotator's judgment as to the correct tag.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"In test data, the correct answer was not available, but the POS tagger output could be used if desired.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"This turns out to matter only for unknown words, as the parser is designed to do its own tagging, for words that it has seen in training at least 5 times, ignoring any tag supplied with the input.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"For &quot;unknown&quot; words (seen less than 5 times), the parser can be set either to believe the tag supplied by the POS tagger or to allow equally any of the dictionary-derived possible tags for the word, effectively allowing the parse context to make the choice.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"(Note that the rich inflectional morphology of Czech leads to a higher rate of &quot;unknown&quot; word forms than would be true in English; in one test, 29.5% of the words in test data were &quot;unknown&quot;) Our tests indicated that if unknown words are treated by believing the POS tagger's suggestion, then scores are better if the parser is also trained on the POS tagger's suggestions, rather than on the human annotator's correct tags.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
Training on the correct tags results in 1% worse performance.,"{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"Even though the POS tagger's tags are less accurate, they are more like what the parser will be using in the test data, and that turns out to be the key point.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"On the other hand, if the parser allows all possible dictionary tags for unknown words in test material, then it pays to train on the actual correct tags.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"In initial tests, this combination of training on the correct tags and allowing all dictionary tags for unknown test words somewhat outperformed the alternative of using the POS tagger's predictions both for training and for unknown test words.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"When tested with the final version of the parser on the full development set, those two strategies performed at the same level.","{'title': '4.3.1 Preferences for dependencies that do not cross verbs', 'number': '5'}"
"We ran three versions of the parser over the final test set: the baseline version, the full model with all additions, and the full model with everything but the bigram model.","{'title': '5 Results', 'number': '6'}"
"The baseline system on the fithat although the Science section only contributes 25% of the sentences in test data, it contains much longer sentences than the other sections and therefore accounts for 38% of the dependencies in test data. nal test set achieved 72.3% accuracy.","{'title': '5 Results', 'number': '6'}"
The final system achieved 80.0% accuracy3: a 7.7% absolute improvement and a 27.8% relative improvement.,"{'title': '5 Results', 'number': '6'}"
The development set showed very similar results: a baseline accuracy of 71.9% and a final accuracy of 79.3%.,"{'title': '5 Results', 'number': '6'}"
Table 3 shows the relative improvement of each component of the mode14.,"{'title': '5 Results', 'number': '6'}"
Table 4 shows the results on the development set by genre.,"{'title': '5 Results', 'number': '6'}"
It is interesting to see that the performance on newswire text is over 2% better than the averaged performance.,"{'title': '5 Results', 'number': '6'}"
The Science section of the development set is considerably harder to parse (presumably because of longer sentences and more open vocabulary).,"{'title': '5 Results', 'number': '6'}"
The main piece of previous work on parsing Czech that we are aware of is described in (Kuboli 99).,"{'title': '5 Results', 'number': '6'}"
This is a rule-based system which is based on a manually designed set of rules.,"{'title': '5 Results', 'number': '6'}"
"The system's accuracy is not evaluated on a test corpus, so it is difficult to compare our results to theirs.","{'title': '5 Results', 'number': '6'}"
"We can, however, make some comparison of the results in this paper to those on parsing English.","{'title': '5 Results', 'number': '6'}"
"(Collins 99) describes results of 91% accuracy in recovering dependencies on section 0 of the Penn Wall Street Journal Treebank, using Model 2 of (Collins 97).","{'title': '5 Results', 'number': '6'}"
"This task is almost certainly easier for a number of reasons: there was more training data (40,000 sentences as opposed to 19,000); Wall Street Journal may be an easier domain than the PDT, as a reasonable proportion of sentences come from a sub-domain, financial news, which is relatively restricted.","{'title': '5 Results', 'number': '6'}"
"Unlike model 1, model 2 of the parser takes subcategorization information into account, which gives some improvement on English and might well also improve results on Czech.","{'title': '5 Results', 'number': '6'}"
"Given these differences, it is difficult to make a direct comparison, but the overall conclusion seems to be that the Czech accuracy is approaching results on English, although it is still somewhat behind.","{'title': '5 Results', 'number': '6'}"
The 80% dependency accuracy of the parser represents good progress towards English parsing performance.,"{'title': '6 Conclusions', 'number': '7'}"
A major area for future work is likely to be an improved treatment of morphology; a natural approach to this problem is to consider more carefully how POS tags are used as word classes by the model.,"{'title': '6 Conclusions', 'number': '7'}"
"We have begun to investigate this issue, through the automatic derivation of POS tags through clustering or &quot;splitting&quot; approaches.","{'title': '6 Conclusions', 'number': '7'}"
"It might also be possible to exploit the internal structure of the POS tags, for example through incremental prediction of the POS tag being generated; or to exploit the use of word lemmas, effectively splitting word—word relations into syntactic dependencies (POS tag—POS tag relations) and more semantic (lemma—lemma) dependencies.","{'title': '6 Conclusions', 'number': '7'}"
