col1,col2
"In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.",Introduction
Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.,Introduction
The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag.,Introduction
"Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear.",Introduction
"This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser.",Introduction
But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses.,Introduction
We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.,Introduction
The supertags in LTAG combine both phrase structure information and dependency information in a single representation.,Introduction
"Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need &quot;only&quot; combine the individual supertags.",Introduction
This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.,Introduction
"In this paper, we have proposed novel methods for robust parsing that integrate the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.",Experiment/Discussion
Our thesis is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.,Experiment/Discussion
The supertags are designed such that only those elements on which the lexical item imposes constraints appear within a given supertag.,Experiment/Discussion
"Further, each lexical item is associated with as many supertags as the number of different syntactic contexts in which the lexical item can appear.",Experiment/Discussion
"This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser.",Experiment/Discussion
But this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses.,Experiment/Discussion
We have explored these ideas in the context of the Lexicalized Tree-Adjoining Grammar (LTAG) framework.,Experiment/Discussion
The supertags in LTAG combine both phrase structure information and dependency information in a single representation.,Experiment/Discussion
"Supertag disambiguation results in a representation that is effectively a parse (an almost parse), and the parser need &quot;only&quot; combine the individual supertags.",Experiment/Discussion
This method of parsing can also be used to parse sentence fragments such as in spoken utterances where the disambiguated supertag sequence may not combine into a single structure.,Experiment/Discussion
"In this paper, we present a robust parsing approach called supertagging that integrates the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.",Experiment/Discussion
The idea underlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (supertags) that impose complex constraints in a local context.,Experiment/Discussion
"This makes the number of different descriptions for each lexical item much larger than when the descriptions are less complex, thus increasing the local ambiguity for a parser.",Experiment/Discussion
"However, this local ambiguity can be resolved by using statistical distributions of supertag co-occurrences collected from a corpus of parses.",Experiment/Discussion
Supertag disambiguation results in a representation that is effectively a parse (an almost parse).,Experiment/Discussion
"In the linguistic context, there can be many ways of increasing the complexity of descriptions of lexical items.",Experiment/Discussion
The idea is to associate lexical items with descriptions that allow for all and only those elements on which the lexical item imposes constraints to be within the same description.,Experiment/Discussion
"Further, it is necessary to associate each lexical item with as many descriptions as the number of different syntactic contexts in which the lexical item can appear.",Experiment/Discussion
"This, of course, increases the local ambiguity for the parser.",Experiment/Discussion
"The parser has to decide which complex description out of the set of descriptions associated with each lexical item is to be used for a given reading of a sentence, even before combining the descriptions together.",Experiment/Discussion
The obvious solution is to put the burden of this job entirely on the parser.,Experiment/Discussion
"The parser will eventually disambiguate all the descriptions and pick one per lexical item, for a given reading of the sentence.",Experiment/Discussion
"However, there is an alternate method of parsing that reduces the amount of disambiguation done by the parser.",Experiment/Discussion
"The idea is to locally check the constraints that are associated with the descriptions of lexical items to filter out incompatible descriptions.1 During this disambiguation, the system can also exploit statistical information that can be associated with the descriptions based on their distribution in a corpus of parses.",Experiment/Discussion
We first employed these ideas in the context of Lexicalized Tree Adjoining grammars (LTAG) in Joshi and Srinivas (1994).,Experiment/Discussion
"Although presented with respect to LTAG, these techniques are applicable to other lexicalized grammars as well.",Experiment/Discussion
"In this paper, we present vastly improved supertag disambiguation results—from previously published 68% accuracy to 92% accuracy using a larger training corpus and better smoothing techniques.",Experiment/Discussion
"The layout of the paper is as follows: In Section 2, we present an overview of the robust parsing approaches.",Experiment/Discussion
A brief introduction to Lexicalized Tree Adjoining grammars is presented in Section 3.,Experiment/Discussion
Section 4 illustrates the goal of supertag disambiguation through an example.,Experiment/Discussion
Various methods and their performance results for supertag disambiguation are discussed in detail in Section 5 and Section 6.,Experiment/Discussion
"In Section 7, we discuss the efficiency gained in performing supertag disambiguation before parsing.",Experiment/Discussion
A robust and lightweight dependency analyzer that uses the supertag output is briefly presented in Section 8.,Experiment/Discussion
"In Section 9, we will discuss the applicability of supertag disambiguation to other lexicalized grammars.",Experiment/Discussion
"In recent years, there have been a number of attempts at robust parsing of natural language.",Experiment/Discussion
They can be broadly categorized under two paradigms—finite-state-grammarbased parsers and statistical parsers.,Experiment/Discussion
We briefly present these two paradigms and situate our approach to robust parsing relative to these paradigms.,Experiment/Discussion
"Finite-state-grammar-based approaches to parsing are exemplified by the parsing systems in Joshi, (1960), Abney (1990), Appelt et al. (1993), Roche (1993), Grishman (1995), Hobbs et al.",Experiment/Discussion
"(1997), Joshi and Hopely (1997), and Karttunen et al. (1997).",Experiment/Discussion
These systems use grammars that are represented as cascaded finite-state regular expression recognizers.,Experiment/Discussion
The regular expressions are usually hand-crafted.,Experiment/Discussion
Each recognizer in the cascade provides a locally optimal output.,Experiment/Discussion
"The output of these systems is mostly in the form of noun groups and verb groups rather than constituent structure, often called a shallow parse.",Experiment/Discussion
There are no clause-level attachments or modifier attachments in the shallow parse.,Experiment/Discussion
"These parsers always produce one output, since they use the longestmatch heuristic to resolve cases of ambiguity when more than one regular expression matches the input string at a given position.",Experiment/Discussion
At present none of these systems use any statistical information to resolve ambiguity.,Experiment/Discussion
"The grammar itself can be partitioned into domain-independent and domain-specific regular expressions, which implies that porting to a new domain would involve rewriting the domain-dependent expressions.",Experiment/Discussion
This approach has proved to be quite successful as a preprocessor in information extraction systems (Hobbs et al. 1995; Grishman 1995).,Experiment/Discussion
"Pioneered by the IBM natural language group (Fujisaki et al. 1989) and later pursued by, for example, Schabes, Roth, and Osborne (1993), Jelinek et al.",Experiment/Discussion
"(1994), Magerman (1995), Collins (1996), and Charniak (1997), this approach decouples the issue of wellformedness of an input string from the problem of assigning a structure to it.",Experiment/Discussion
These systems attempt to assign some structure to every input string.,Experiment/Discussion
"The rules to assign a structure to an input are extracted automatically from hand-annotated parses of large corpora, which are then subjected to smoothing to obtain reasonable coverage of the language.",Experiment/Discussion
The resultant set of rules are not linguistically transparent and are not easily modifiable.,Experiment/Discussion
Lexical and structural ambiguity is resolved using probability information that is encoded in the rules.,Experiment/Discussion
This allows the system to assign the most-likely structure to each input.,Experiment/Discussion
"The output of these systems consists of constituent analysis, the degree of detail of which is dependent on the detail of annotation present in the treebank that is used to train the system.",Experiment/Discussion
"There are also parsers that use probabilistic (weighting) information in conjunction with hand-crafted grammars, for example, Black et al. (1993), Nagao (1994), Alshawi and Carter (1994), and Srinivas, Doran, and Kulick (1995).",Experiment/Discussion
In these cases the probabilistic information is primarily used to rank the parses produced by the parser and not so much for the purpose of robustness of the system.,Experiment/Discussion
Lexicalized grammars are particularly well-suited for the specification of natural language grammars.,Experiment/Discussion
"The lexicon plays a central role in linguistic formalisms such as LFG (Kaplan and Bresnan 1983), GPSG (Gazdar et al. 1985), HPSG (Pollard and Sag 1987), CCG (Steedman 1987), Lexicon Grammar (Gross 1984), LTAG (Schabes and Joshi 1991), Link Grammar (Sleator and Temperley 1991), and some version of GB (Chomsky 1992).",Experiment/Discussion
"Parsing, lexical semantics, and machine translation, to name a few areas, have all benefited from lexicalizatiort.",Experiment/Discussion
Lexicalizatiort provides a clean interface for combining the syntactic and semantic information in the lexicon.,Experiment/Discussion
We discuss the merits of lexicalization and other related issues in the context of partial parsing and briefly discuss Feature-based Lexicalized Tree Adjoining Grammars (LTAGs) as a representative of the class of lexicalized grammars.,Experiment/Discussion
"Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) (Joshi, Levy, and Takahashi 1975; Vijay-Shanker 1987; Schabes, Abeille, and Joshi 1988; Vijay-Shanker and Joshi 1991; Joshi and Schabes 1996) is a tree-rewriting grammar formalism unlike context-free grammars and head grammars, which are string-rewriting formalisms.",Experiment/Discussion
The primitive elements of FB-LTAGs are called elementary trees.,Experiment/Discussion
Each elementary tree is associated with at least one lexical item on its frontier.,Experiment/Discussion
The lexical item associated with an elementary tree is called the anchor of that tree.,Experiment/Discussion
An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints.,Experiment/Discussion
Elementary trees are of two kinds: (a) initial trees and (b) auxiliary trees.,Experiment/Discussion
"In an FB-LTAG grammar for natural language, initial trees are phrase structure trees of simple sentences containing no recursion, while recursive structures are represented by auxiliary trees.",Experiment/Discussion
Elementary trees are combined by substitution and adjunction operations.,Experiment/Discussion
The result of combining the elementary trees is the derived tree and the process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree.,Experiment/Discussion
The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence.,Experiment/Discussion
A more detailed discussion of LTAGs with an example and some of the key properties of elementary trees is presented in Appendix A.,Experiment/Discussion
Part-of-speech disambiguation techniques (POS taggers) (Church 1988; Weischedel et al. 1993; Brill 1993) are often used prior to parsing to eliminate (or substantially reduce) the part-of-speech ambiguity The POS taggers are all local in the sense that they use information from a limited context in deciding which tag(s) to choose for each word.,Experiment/Discussion
"As is well known, these taggers are quite successful.",Experiment/Discussion
"In a lexicalized grammar such as the Lexicalized Tree Adjoining Grammar (LTAG), each lexical item is associated with at least one elementary structure (tree).",Experiment/Discussion
"The elementary structures of LTAG localize dependencies, including long-distance dependencies, by requiring that all and only the dependent elements be present within the same structure.",Experiment/Discussion
"As a result of this localization, a lexical item may be (and, in general, almost always is) associated with more than one elementary structure.",Experiment/Discussion
"We will call these elementary structures supertags, in order to distinguish them from the standard partof-speech tags.",Experiment/Discussion
"Note that even when a word has a unique standard part of speech, say a verb (V), there will usually be more than one supertag associated with this word.",Experiment/Discussion
"Since there is only one supertag for each word (assuming there is no global ambiguity) when the parse is complete, an LTAG parser (Schabes, Abeille, and Joshi 1988) needs to search a large space of supertags to select the right one for each word before combining them for the parse of a sentence.",Experiment/Discussion
It is this problem of supertag disambiguation that we address in this paper.,Experiment/Discussion
"Since LTAGs are lexicalized, we are presented with a novel opportunity to eliminate or substantially reduce the supertag assignment ambiguity by using local information, such as local lexical dependencies, prior to parsing.",Experiment/Discussion
"As in standard part-of-speech disambiguation, we can use local statistical information in the form of n-gram models based on the distribution of supertags in an LTAG parsed corpus.",Experiment/Discussion
"Moreover, since the supertags encode dependency information, we can also use information about the distribution of distances between a given supertag and its dependent supertags.",Experiment/Discussion
"Note that as in standard part-of-speech disambiguation, supertag disambiguation could have been done by a parser.",Experiment/Discussion
"However, carrying out part-of-speech disambiguation prior to parsing makes the job of the parser much easier and therefore speeds it up.",Experiment/Discussion
Supertag disambiguation reduces the work of the parser even further.,Experiment/Discussion
"After supertag disambiguation, we would have effectively completed the parse and the parser need &quot;only&quot; combine the individual structures; hence the term &quot;almost parsing.&quot; This method can also be used to associate a structure to sentence fragments and in cases where the supertag sequence after disambiguation may not combine into a single structure.",Experiment/Discussion
"LTAGs, by virtue of possessing the Extended Domain of Locality (EDL) property,' associate with each lexical item, one elementary tree for each syntactic environment that an Noun Phrase companies have not been profitable the lexical item may appear in.",Experiment/Discussion
"As a result, each lexical item is invariably associated with more than one elementary tree.",Experiment/Discussion
We call the elementary structures associated with each lexical item super parts-of-speech (super POS) or supertags.3 Figure 1 illustrates a few elementary trees associated with each word of the sentence: the purchase price includes two ancillary companies.,Experiment/Discussion
Table 1 provides an example context in which each supertag shown in Figure 1 would be used.,Experiment/Discussion
The example in Figure 2 illustrates the initial set of supertags assigned to each word of the sentence: the purchase price includes two ancillary companies.,Experiment/Discussion
The order of the supertags for each lexical item in the example is not relevant.,Experiment/Discussion
"Figure 2 also shows the final supertag sequence assigned by the supertagger, which picks the best supertag sequence using statistical information (described in Section 6) about individual supertags and their dependencies on other supertags.",Experiment/Discussion
The chosen supertags are combined to derive a parse.,Experiment/Discussion
"Without the supertagger, the parser would have to process combinations of the entire set of trees (at least the 17 trees shown); with it the parser need only process combinations of 7 trees.",Experiment/Discussion
The structure of the supertag can be best seen as providing admissibility constraints on syntactic environments in which it may be used.,Experiment/Discussion
Some of these constraints can be checked locally.,Experiment/Discussion
The following are a few constraints that can be used to determine the admissibility of a syntactic environment for a supertag:4 A selection of the supertags associated with each word of the sentence: the purchase price includes two ancillary companies.,Experiment/Discussion
"Supertags with the built-in lexical item by, that represent passive constructions are typically eliminated from being considered during the parse of an active sentence.",Experiment/Discussion
"More generally, these constraints can be used to eliminate supertags that cannot have their features satisfied in the context of the input string.",Experiment/Discussion
An example of this is the elimination of supertag that requires a wh+ NP when the input string does not contain wh-words.,Experiment/Discussion
"Table 2 indicates the decrease in supertag ambiguity for 2,012 WSJ sentences (48,763 words) by using the structural constraints relative to the supertag ambiguity without the structural constraints.5 These filters prove to be very effective in reducing supertag ambiguity.",Experiment/Discussion
The graph in Figure 3 plots the number of supertags at the sentence level for sentences of length 2 to 50 words with and without the filters.,Experiment/Discussion
"As can be seen from the graph, the supertag ambiguity is significantly lower when the filters are used.",Experiment/Discussion
The graph in Figure 4 shows the percentage drop in supertag ambiguity due to filtering for sentences of length 2 to 50 words.,Experiment/Discussion
"As can be seen, the average reduction in supertag ambiguity is about 50%.",Experiment/Discussion
"This means that given a sentence, close to 50% of the supertags can be eliminated even before parsing begins by just using structural constraints of the supertags.",Experiment/Discussion
This reduction in supertag ambiguity speeds up the parser significantly.,Experiment/Discussion
"In fact, the supertag Comparison of number of supertags with and without filtering for sentences of length 2 to 50 words. ambiguity in XTAG system is so large that the parser is prohibitively slow without the use of these filters.",Experiment/Discussion
Table 3 tabulates the reduction of supertag ambiguity due to the filters against various parts of speech.6 Verbs in all their forms contribute most to the problem of supertag ambiguity and most of the supertag ambiguity for verbs is due to light verbs and verb particles.,Experiment/Discussion
The filters are very effective in eliminating over 50% of the verb anchored supertags.,Experiment/Discussion
"Even though structural constraints are effective in reducing supertag ambiguity, the search space for the parser is still sufficiently large.",Experiment/Discussion
"In the next few sections, we present stochastic and rule-based approaches to supertag disambiguation.",Experiment/Discussion
Percentage drop in the number of supertags with and without filtering for sentences of length 2 to 50 words.,Experiment/Discussion
"Before proceeding to discuss the various models for supertag disambiguation, we would like to trace the time course of development of this work.",Experiment/Discussion
"We do this not only to show the improvements made to the early work reported in our 1994 paper (Joshi and Srinivas 1994), but also to explain the rationale for choosing certain models of supertag disambiguation over others.",Experiment/Discussion
We summarize the early work in the following subsection.,Experiment/Discussion
"As reported in Joshi and Srinivas (1994), we experimented with a trigram model as well as the dependency model for supertag disambiguation.",Experiment/Discussion
"The trigram model that was trained on (part-of-speech, supertag) pairs, instead of (words, supertag) pairs, collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ sentences produced a correct supertag for 68% of the words in the test set.",Experiment/Discussion
We have since significantly improved the performance of the trigram model by using a larger training set and incorporating smoothing techniques.,Experiment/Discussion
We present a detailed discussion of the model and its performance on a range of corpora in Section 6.5.,Experiment/Discussion
"In Section 6.2, we briefly mention the dependency model of supertagging that was reported in the earlier work.",Experiment/Discussion
"In an n-gram model for disambiguating supertags, dependencies between supertags that appear beyond the n-word window cannot be incorporated.",Experiment/Discussion
This limitation can be overcome if no a priori bound is set on the size of the window but instead a Bangalore and Joshi Supertagging probability distribution of the distances of the dependent supertags for each supertag is maintained.,Experiment/Discussion
We define dependency between supertags in the obvious way: A supertag is dependent on another supertag if the former substitutes or adjoins into the latter.,Experiment/Discussion
"Thus, the substitution and the foot nodes of a supertag can be seen as specifying dependency requirements of the supertag.",Experiment/Discussion
The probability with which a supertag depends on another supertag is collected from a corpus of sentences annotated with derivation structures.,Experiment/Discussion
"Given a set of supertags for each word and the dependency information between pairs of supertags, the objective of the dependency model is to compute the most likely dependency linkage that spans the entire string.",Experiment/Discussion
"The result of producing the dependency linkage is a sequence of supertags, one for each word of the sentence along with the dependency information.",Experiment/Discussion
"Since first reported in Joshi and Srinivas (1994), we have not continued experiments using this model of supertagging, primarily for two reasons.",Experiment/Discussion
We are restrained by the lack of a large corpus of LTAG parsed derivation structures that is needed to reliably estimate the various parameters of this model.,Experiment/Discussion
"We are currently in the process of collecting a large LTAG parsed WSJ corpus, with each sentence annotated with the correct derivation.",Experiment/Discussion
A second reason for the disuse of the dependency model for supertagging is that the objective of supertagging is to see how far local techniques can be used to disambiguate supertags even before parsing begins.,Experiment/Discussion
"The dependency model, in contrast, is too much like full parsing and is contrary to the spirit of supertagging.",Experiment/Discussion
We have improved the performance of the trigram model by incorporating smoothing techniques into the model and training the model on a larger training corpus.,Experiment/Discussion
We have also proposed some new models for supertag disambiguation.,Experiment/Discussion
"In this section, we discuss these developments in detail.",Experiment/Discussion
Two sets of data are used for training and testing the models for supertag disambiguation.,Experiment/Discussion
"The first set has been collected by parsing the Wall Street Journal', IBM Manual, and ATIS corpora using the wide-coverage English grammar being developed as part of the XTAG system (Doran et al. 1994).",Experiment/Discussion
The correct derivation from all the derivations produced by the XTAG system was picked for each sentence from these corpora.,Experiment/Discussion
The second and larger data set was collected by converting the Penn Treebank parses of the Wall Street Journal sentences.,Experiment/Discussion
"The objective was to associate each lexical item of a sentence with a supertag, given the phrase structure parse of the sentence.",Experiment/Discussion
This process involved a number of heuristics based on local tree contexts.,Experiment/Discussion
"The heuristics made use of information about the labels of a word's dominating nodes (parent, grandparent, and great-grandparent), labels of its siblings (left and right) and siblings of its parent.",Experiment/Discussion
An example of the result of this conversion is shown in Figure 5.,Experiment/Discussion
"It must be noted that this conversion is not perfect and is correct only to a first order of approximation owing mostly to errors in conversion and lack of certain kinds of information such as distinction between adjunct and argument preposition phrases, in the Penn Treebank parses.",Experiment/Discussion
"Even though the converted supertag corpus can be refined further, the corpus in its present form has proved to be an invaluable resource in improving the performance of the supertag models as is discussed in the following sections.",Experiment/Discussion
Using structural information to filter out supertags that cannot be used in any parse of the input string reduces the supertag ambiguity but obviously does not eliminate it completely.,Experiment/Discussion
One method of disambiguating the supertags assigned to each word is to order the supertags by the lexical preference that the word has for them.,Experiment/Discussion
The frequency with which a certain supertag is associated with a word is a direct measure of its lexical preference for that supertag.,Experiment/Discussion
Associating frequencies with the supertags and using them to associate a particular supertag with a word is clearly the simplest means of disambiguating supertags.,Experiment/Discussion
"Therefore a unigram model is given by: where Thus, the most frequent supertag that a word is associated with in a training corpus is selected as the supertag for the word according to the unigram model.",Experiment/Discussion
For the words that do not appear in the training corpus we back off to the part of speech of the word and use the most frequent supertag associated with that part of speech as the supertag for the word. the previously discussed two sets of data.,Experiment/Discussion
The words are first assigned standard parts of speech using a conventional tagger (Church 1988) and then are assigned supertags according to the unigram model.,Experiment/Discussion
A word in a sentence is considered correctly supertagged if it is assigned the same supertag as it is associated with in the correct parse of the sentence.,Experiment/Discussion
The results of these experiments are tabulated in Table 4.,Experiment/Discussion
"Although the performance of the unigram model for supertagging is significantly lower than the performance of the unigram model for part-of-speech tagging (91% accuracy), it performed much better than expected considering the size of the supertag set is much larger than the size of part-of-speech tag set.",Experiment/Discussion
"One of the reasons for this high performance is that the most frequent supertag for the most frequent words— determiners, nouns, and auxiliary verbs—is the correct supertag most of the time.",Experiment/Discussion
"Also, backing off to the part of speech helps in supertagging unknown words, which most often are nouns.",Experiment/Discussion
"The bulk of the errors committed by the unigram model is incorrectly tagged verbs (subcategorization and transformation), prepositions (noun attached vs. verb attached) and nouns (head vs. modifier noun).",Experiment/Discussion
We first explored the use of trigram model of supertag disambiguation in Joshi and Srinivas (1994).,Experiment/Discussion
"The trigram model was trained on (part-of-speech, supertag) pairs collected from the LTAG derivations of 5,000 WSJ sentences and tested on 100 WSJ sentences.",Experiment/Discussion
It produced a correct supertag for 68% of the words in the test set.,Experiment/Discussion
"A major drawback of this early work was that it used no lexical information in the supertagging process as the training material consisted of (part-of-speech, supertag) pairs.",Experiment/Discussion
"Since that early work, we have improved the performance of the model by incorporating lexical information and sophisticated smoothing techniques, as well as training on larger training sets.",Experiment/Discussion
"In this section, we present the details and the performance evaluation of this model.",Experiment/Discussion
"In a unigram model, a word is always associated with the supertag that is most preferred by the word, irrespective of the context in which the word appears.",Experiment/Discussion
An alternate method that is sensitive to context is the n-gram model.,Experiment/Discussion
The n-gram model takes into account the contextual dependency probabilities between supertags within a window of n words in associating supertags to words.,Experiment/Discussion
"Thus, the most probable supertag sequence for an n-word sentence is given by: = argmaxTPr(Ti, T2, .",Experiment/Discussion
.,Experiment/Discussion
"• , TN) * Pr(Wi, W2/ • / WN T1, T2/ • .",Experiment/Discussion
"• TN) (3) where Ti is the supertag for word K. To compute this using only local information, we approximate, assuming that the probability of a word depends only on its supertag and also use an n-gram (trigram, in this case) approximation The term Pr(T, ITi_2, Ti_i) is known as the contextual probability since it indicates the size of the context used in the model and the term Pr(Wj T,) is called the word emit probability since it is the probability of emitting the word W, given the tag Ti.",Experiment/Discussion
These probabilities are estimated using a corpus where each word is tagged with its correct supertag.,Experiment/Discussion
The contextual probabilities were estimated using the relative frequency estimates of the contexts in the training corpus.,Experiment/Discussion
"To estimate the probabilities for contexts that do not appear in the training corpus, we used the Good-Turing discounting technique (Good 1953) combined with Katz's back off model (Katz 1987).",Experiment/Discussion
The idea here is to discount the frequencies of events that occur in the corpus by an amount related to their frequencies and utilize this discounted probability mass in the back off model to distribute to unseen events.,Experiment/Discussion
"Thus, the Good-Turing discounting technique estimates the frequency of unseen events based on the distribution of the frequency of the counts of observed events in the corpus.",Experiment/Discussion
"If r is the observed frequency of an event, and N,. is the number of events with the observed frequency r, and N is the total number of events, then the probability of an unseen event is given by N1/ N. Furthermore, the frequencies of the observed events are adjusted so that the total probability of all events sums to one.",Experiment/Discussion
"The adjusted frequency for observed events, r*, is computed as Once the frequencies of the observed events are discounted and the frequencies for unseen events are estimated, Katz's back off model is used.",Experiment/Discussion
"In this technique, if the observed frequency of an <n-gram, supertag> sequence is zero then its probability is computed based on the observed frequency of an (n - 1)-gram sequence.",Experiment/Discussion
"Thus, where a(Ti,T)) and 13(Tk) are constants to ensure that the probabilities sum to one.",Experiment/Discussion
"The word emit probability for the (word, supertag) pairs that appear in the training corpus is computed using the relative frequency estimates as shown in Equation 7.",Experiment/Discussion
"For the (word, supertag) pairs that do not appear in the corpus, the word emit probability is estimated as shown in Equation 8.",Experiment/Discussion
"Some of the word features used in our impleBangalore and Joshi Supertagging mentation include prefixes and suffixes of length less than or equal to three characters, capitalization, and digit features.",Experiment/Discussion
"The counts for the (word, supertag) pairs for the words that do not appear in the corpus is estimated using the leaving-one-out technique (Niesler and Woodland 1996; Ney, Essen, and Kneser 1995).",Experiment/Discussion
A token UNK is associated with each supertag and its count NuNK is estimated by: where N1(TI) is the number of words that are associated with the supertag Tj that appear in the corpus exactly once.,Experiment/Discussion
N(T1) is the frequency of the supertag Tj and NuNK(TI) is the estimated count of UNK in 7.1.,Experiment/Discussion
"The constant n is introduced so as to ensure that the probability is not greater than one, especially for supertags that are sparsely represented in the corpus.",Experiment/Discussion
"We use word features similar to the ones used in Weischedel et al. (1993), such as capitalization, hyphenation, and endings of words, for estimating the word emit probability of unknown words.",Experiment/Discussion
6.5.1 Experiments and Results.,Experiment/Discussion
"We tested the performance of the trigram model on various domains such as the Wall Street Journal (WSJ), the IBM Manual corpus and the ATIS corpus.",Experiment/Discussion
"For the IBM Manual corpus and the ATIS domains, a supertag annotated corpus was collected using the parses of the XTAG system (Doran et al. 1994) and selecting the correct analysis for each sentence.",Experiment/Discussion
The corpus was then randomly split into training and test material.,Experiment/Discussion
"Supertag performance is measured as the percentage of words that are correctly supertagged by a model when compared with the key for the words in the test corpus. data, from the XTAG parses and from the conversion of the Penn Treebank parses to evaluate the performance of the trigram model.",Experiment/Discussion
Table 5 shows the performance on the two sets of data.,Experiment/Discussion
"The first data set, data collected from the XTAG parses, was split into 8,000 words of training and 3,000 words of test material.",Experiment/Discussion
"The data collected from converting the Penn Treebank was used in two experiments differing in the size of the training corpus-200,000 words' and 1,000,000 words9—and tested on 47,000 words'''.",Experiment/Discussion
"A total of 300 different supertags were used in these experiments. mance of the trigram supertagger on the IBM Manual corpus, a set of 14,000 words correctly supertagged was used as the training corpus and a set of 1,000 words was used as a test corpus.",Experiment/Discussion
The performance of the supertagger on this corpus is shown in Table 6.,Experiment/Discussion
"Performance on the ATIS corpus was evaluated using a set of 1,500 words correctly supertagged as the training corpus and a set of 400 words as a test corpus.",Experiment/Discussion
The performance of the supertagger on the ATIS corpus is also shown in Table 6.,Experiment/Discussion
"As expected, the performance on the ATIS corpus is higher than that of the WSJ and the IBM Manual corpus despite the extremely small training corpus.",Experiment/Discussion
"Also, the performance of the IBM Manual corpus is better than the WSJ corpus when the size of the training corpus is taken into account.",Experiment/Discussion
The baseline for the ATIS domain is remarkably high due to the repetitive constructions and limited vocabulary in that domain.,Experiment/Discussion
"This is also true for the IBM Manual corpus, although to a lesser extent.",Experiment/Discussion
The trigram model of supertagging is attractive for limited domains since it performs quite well with relatively insignificant amounts of training material.,Experiment/Discussion
"The performance of the supertagger can be improved in an iterative fashion by using the supertagger to supertag larger amounts of training material, which can be quickly hand-corrected and used to train a better-performing supertagger. most to the performance of a POS tagger, since the baseline performance of assigning the most likely POS for each word produces 91% accuracy (Brill 1993).",Experiment/Discussion
"Contextual information contributes relatively a small amount towards the performance, improving it from 91% to 96-97%, a 5.5% improvement.",Experiment/Discussion
"In contrast, contextual information has greater effect on the performance of the supertagger.",Experiment/Discussion
"As can be seen, from the above experiments, the baseline performance of the supertagger is about 77% and the performance improves to about 92% with the inclusion of contextual information, an Bangalore and joshi Supertagging improvement of 19.5%.",Experiment/Discussion
The relatively low baseline performance for the supertagger is a direct consequence of the fact that there are many more supertags per word than there are POS tags.,Experiment/Discussion
"Further, since many combinations of supertags are not possible, contextual information has a larger effect on the performance of the supertagger.",Experiment/Discussion
"In an error-driven transformation-based (EDTB) tagger (Brill 1993), a set of patternaction templates that include predicates that test for features of words appearing in the context of interest are defined.",Experiment/Discussion
These templates are then instantiated with the appropriate features to obtain transformation rules.,Experiment/Discussion
The effectiveness of a transformation rule to correct an error and the relative order of application of the rules are learned using a corpus.,Experiment/Discussion
The learning procedure takes a gold corpus in which the words have been correctly annotated and a training corpus that is derived from the gold corpus by removing the annotations.,Experiment/Discussion
The objective in the learning phase is to learn the optimum ordering of rule applications so as to minimize the number of tag mismatches between the training and the reference corpus.,Experiment/Discussion
6.6.1 Experiments and Results.,Experiment/Discussion
A EDTB model has been trained using templates defined on a three-word window.,Experiment/Discussion
"We trained the templates on 200,000 words' and tested on 47,000 words' of the WSJ corpus.",Experiment/Discussion
The model performed at an accuracy of 90%.,Experiment/Discussion
The EDTB model provides a great deal of flexibility to integrate domain-specific and linguistic information into the model.,Experiment/Discussion
"However, a major drawback of this approach is that the training procedure is extremely slow, which prevented us from training on the 1,000,000 word corpus.",Experiment/Discussion
7.,Experiment/Discussion
"Supertagging before Parsing The output of the supertagger, an almost parse, has been used in a variety of applications including information retrieval (Chandrasekar and Srinivas 1997b, 1997c, 1997d) and information extraction (Doran et al. 1997), text simplification (Chandrasekar, Doran, and Srinivas 1996, Chandrasekar and Srinivas 1997a), and language modeling (Srinivas 1996) to illustrate that supertags provide an appropriate level of lexical description needed for most applications.",Experiment/Discussion
The output of the supertagger has also been used as a front end to a lexicalized grammar parser.,Experiment/Discussion
"As mentioned earlier, a lexicalized grammar parser can be conceptualized to consist of two stages (Schabes, Abeille, and Joshi 1988).",Experiment/Discussion
"In the first stage, the parser looks up the lexicon and selects all the supertags associated with each word of the sentence to be parsed.",Experiment/Discussion
"In the second stage, the parser searches the lattice of selected supertags in an attempt to combine them using substitution and adjunction operations so as to yield a derivation that spans the input string.",Experiment/Discussion
"At the end of the second stage, the parser would not only have parsed the input, but would have associated a small set of (usually one) supertags with each word.",Experiment/Discussion
The supertagger can be used as a front end to a lexicalized grammar parser so as to prune the search-space of the parser even before parsing begins.,Experiment/Discussion
"It should be clear that by reducing the number of supertags that are selected in the first stage, the search-space for the second stage can be reduced significantly and hence the parser can be made more efficient.",Experiment/Discussion
"Supertag disambiguation techniques, as discussed in the previous sections, attempt to disambiguate the supertags selected in the first pass, based on lexical preferences and local lexical dependencies, so as to ideally select one supertag for each word.",Experiment/Discussion
"Once the supertagger selects the appropriate supertag for each word, the second stage of the parser is needed only to combine the individual supertags to arrive at the parse of the input.",Experiment/Discussion
"Tested on about 1,300 WSJ sentences with each word in the sentence correctly supertagged, the LTAG parser took approximately 4 seconds per sentence to yield a parse (combine the supertags and perform feature unification).",Experiment/Discussion
"In contrast, the same 1,300 WSJ sentences without the supertag annotation took nearly 120 seconds per sentence to yield a parse.",Experiment/Discussion
Thus the parsing speedup gained by this integration is a factor of about 30.,Experiment/Discussion
"In the XTAG system, we have integrated the trigram supertagger as a front end to an LTAG parser to pick the appropriate supertag for each word even before parsing begins.",Experiment/Discussion
"However, a drawback of this approach is that the parser would fail completely if any word of the input is incorrectly tagged by the supertagger.",Experiment/Discussion
This problem could be circumvented to an extent by extending the supertagger to produce n-best supertags for each word.,Experiment/Discussion
"Although this extension would increase the load on the parser, it would certainly improve the chances of arriving at a parse for a sentence.",Experiment/Discussion
"In fact, Table 7 presents the performance of the supertagger that selects, at most, the top three supertags for each word.",Experiment/Discussion
The optimum number of supertags to output to balance the success rate of the parser against the efficiency of the parser must be determined empirically.,Experiment/Discussion
A more serious limitation of this approach is that it fails to parse ill-formed and extragrammatical strings such as those encountered in spoken utterances and unrestricted texts.,Experiment/Discussion
This is due to the fact that the Earley-style LTAG parser attempts to combine the supertags to construct a parse that spans the entire string.,Experiment/Discussion
"In cases where the supertag sequence for a string cannot be combined into a unified structure, the parser fails completely.",Experiment/Discussion
One possible extension to account for ill-formed and extragrammatical strings is to extend the Earley parser to produce partial parses for the fragments whose supertags can be combined.,Experiment/Discussion
An alternate method of computing dependency linkages robustly is presented in the next section.,Experiment/Discussion
Supertagging associates each word with a unique supertag.,Experiment/Discussion
"To establish the dependency links among the words of the sentence, we exploit the dependency requirements Bangalore and Joshi Supertagging encoded in the supertags.",Experiment/Discussion
Substitution nodes and foot nodes in supertags serve as slots that must be filled by the arguments of the anchor of the supertag.,Experiment/Discussion
A substitution slot of a supertag is filled by the complements of the anchor while the foot node of a supertag is filled by a word that is being modified by the supertag.,Experiment/Discussion
These argument slots have a polarity value reflecting their orientation with respect to the anchor of the supertag.,Experiment/Discussion
Also associated with a supertag is a list of internal nodes (including the root node) that appear in the supertag.,Experiment/Discussion
"Using the structural information coupled with the argument requirements of a supertag, a simple heuristic-based, linear time, deterministic algorithm (which we call a lightweight dependency analyzer (LDA)) produces dependency linkages not necessarily spanning the entire sentence.",Experiment/Discussion
"The LDA can produce a number of partial linkages, since it is driven primarily by the need to satisfy local constraints without being driven to construct a single dependency linkage that spans the entire input.",Experiment/Discussion
"This, in fact, contributes to the robustness of LDA and promises to be a useful tool for parsing sentence fragments that are rampant in speech utterances, as exemplified by the Switchboard corpus.",Experiment/Discussion
"Tested on section 20 of the Wall Street Journal corpus, which contained 47,333 dependency links in the gold standard, the LDA, trained on 200,000 words, produced 38,480 dependency links correctly, resulting in a recall score of 82.3%.",Experiment/Discussion
"Also, a total of 41,009 dependency links were produced by the LDA, resulting in a precision score of 93.8%.",Experiment/Discussion
A detailed evaluation of the LDA is presented in Srinivas (199M).,Experiment/Discussion
"Although we have presented supertagging in the context of LTAG, it is applicable to other lexicalized grammar formalisms such as CCG (Steedman 1997), HPSG (Pollard and Sag 1987), and LFG (Kaplan and Bresnan 1983).",Experiment/Discussion
We have implemented a broad coverage CCG grammar (Doran and Srinivas 1994) containing about 80 categories based on the XTAG English grammar.,Experiment/Discussion
These categories have been used to tag the same training and test corpora used in the supertagging experiments discussed in this paper and a supertagger to disambiguate the CCG categories has been developed.,Experiment/Discussion
We are presently analyzing the performance of the supertagger using the LTAG trees and the CCG categories.,Experiment/Discussion
"The idea of supertagging can also be applied to a grammar in HPSG formalism indirectly, by compiling the HPSG grammar into an LTAG grammar (Kasper et al. 1995).",Experiment/Discussion
A more direct approach would be to tag words with feature structures that represent supertags (Kempe 1994).,Experiment/Discussion
"For LFG, the lexicalized subset of fragments used in the LFG-DOP model (Bod and Kaplan 1998) can be seen as supertags.",Experiment/Discussion
An approach that is closely related to supertagging is the reductionist approach to parsing that is being carried out under the Constraint Grammar framework (Karlsson et al. 1994; Voutilainen 1994; Tapanainen and Jarvinen 1994).,Experiment/Discussion
"In this framework, each word is associated with the set of possible functional tags that it may be assigned in the language.",Experiment/Discussion
This constitutes the lexicon.,Experiment/Discussion
The grammar consists of a set of rules that eliminate functional tags for words based on the context of a sentence.,Experiment/Discussion
"Parsing a sentence in this framework amounts to eliminating as many implausible functional tags as possible for each word, given the context of the sentence.",Experiment/Discussion
"The resultant output structure might contain significant syntactic ambiguity, which may not have been eliminated by the rule applications, thus producing almost parses.",Experiment/Discussion
"Thus, the reductionist approach to parsing is similar to supertagging in that both view parsing as tagging with rich descriptions.",Experiment/Discussion
"However, the key difference is that the tagging is done in a probabilistic setting in the supertagging approach while it is rule based in the constraint grammar approach.",Experiment/Discussion
We are currently developing supertaggers for other languages.,Experiment/Discussion
"In collaboration with Anne Abeille and Marie-Helene Candito of the University of Paris, using their French TAG grammar, we have developed a supertagger for French.",Experiment/Discussion
We are currently working on evaluating the performance of this supertagger.,Experiment/Discussion
"Also, the annotated corpora necessary for training supertaggers for Korean and Chinese are under development at the University of Pennsylvania.",Experiment/Discussion
A version of the supertagger trained on the WSJ corpus is available under GNU Public License from http: / / www.cis.upenn.edu / —xtag / swrelease.html.,Experiment/Discussion
"In this paper, we have presented a novel approach to robust parsing distinguished from the previous approaches to robust parsing by integrating the flexibility of linguistically motivated lexical descriptions with the robustness of statistical techniques.",Experiment/Discussion
"By associating rich descriptions (supertags) that impose complex constraints in a local context, we have been able to use local computational models for effective supertag disambiguation.",Experiment/Discussion
"A trigram supertag disambiguation model, trained on 1,000,000 (word, supertag) pairs of the Wall Street Journal corpus, performs at an accuracy level of 92.2%.",Experiment/Discussion
"After disambiguation, we have effectively completed the parse of the sentence, creating an almost parse, in that the parser need only combine the selected structures to arrive at a parse for the sentence.",Experiment/Discussion
We have presented a lightweight dependency analyzer (LDA) that takes the output of the supertagger and uses the dependency requirements of the supertags to produce a dependency linkage for a sentence.,Experiment/Discussion
This method can also serve to parse sentence fragments in cases where the supertag sequence after disambiguation may not combine to form a single structure.,Experiment/Discussion
This approach is applicable to all lexicalized grammar parsers.,Experiment/Discussion
"Feature-based Lexicalized Tree Adjoining Grammar (FB-LTAG) is a tree-rewriting grammar formalism, unlike context-free Grammars and head grammars, which are stringrewriting formalisms.",Experiment/Discussion
"FB-LTAGs trace their lineage to Tree Adjunct Grammars (TAGs), which were first developed in Joshi, Levy, and Takahashi (1975) and later extended to include unification-based feature structures (Vijay-Shanker 1987; Vijay-Shanker and Joshi 1991) and lexicalization (Schabes, Abeille, and Joshi 1988).",Experiment/Discussion
"For a more recent and comprehensive reference, see Joshi and Schabes (1996).",Experiment/Discussion
The primitive elements of FB-LTAGs are called elementary trees.,Experiment/Discussion
Each elementary tree is associated with at least one lexical item on its frontier.,Experiment/Discussion
The lexical item associated with an elementary tree is called the anchor of that tree.,Experiment/Discussion
An elementary tree serves as a complex description of the anchor and provides a domain of locality over which the anchor can specify syntactic and semantic (predicate argument) constraints.,Experiment/Discussion
Elementary trees are of two kinds: (a) Initial Trees and (b) Auxiliary Trees.,Experiment/Discussion
"In an FB-LTAG grammar for natural language, initial trees are phrase structure trees of simple sentences containing no recursion, while recursive structures are represented by auxiliary trees.",Experiment/Discussion
Examples of initial trees (as) and auxiliary trees (i3s) are shown in Figure 6.,Experiment/Discussion
"Nodes on the frontier of initial trees are marked as substitution sites by a &quot;i&quot;, while exactly one node on the frontier of an auxiliary tree, whose label matches the label of the root of the tree, is marked as a foot node by a &quot;*&quot;.",Experiment/Discussion
The other nodes on the frontier of an auxiliary tree are marked as substitution sites.,Experiment/Discussion
"Each node of an elementary tree is associated with two feature structures (FS), Elementary trees for the sentence: the company is being acquired. the top and the bottom.",Experiment/Discussion
"The bottom FS contains information relating to the subtree rooted at the node, and the top FS contains information relating to the supertree at that node.13 Features may get their values from three different sources: • The derivation process: from unification with features from trees that adjoin or substitute.",Experiment/Discussion
Elementary trees are combined by substitution and adjunction operations.,Experiment/Discussion
Substitution inserts elementary trees at the substitution nodes of other elementary trees.,Experiment/Discussion
Figure 7(a) shows two elementary trees and the tree resulting from the substitution of one tree into the other.,Experiment/Discussion
"In this operation, a node marked for substitution in an elementary tree is replaced by another elementary tree whose root label matches the label of the node.",Experiment/Discussion
"The top FS of the resulting node is the result of unification of the top features of the two original nodes, while the bottom FS of the resulting node is simply the bottom features of the root node of the substituting tree.",Experiment/Discussion
"In an adjunction operation, an auxiliary tree is inserted into an elementary tree.",Experiment/Discussion
Figure 7(b) shows an auxiliary tree adjoining into an elementary tree and the result of the adjunction.,Experiment/Discussion
The root and foot nodes of the auxiliary tree must match the node label at which the auxiliary tree adjoins.,Experiment/Discussion
"The node being adjoined to splits, and its top FS unifies with the top FS of the root node of the auxiliary tree, while its bottom FS unifies with the bottom FS of the foot node of the auxiliary tree.",Experiment/Discussion
"Figure 7(b) shows an auxiliary tree and an elementary tree, and the tree resulting from an adjunction operation.",Experiment/Discussion
"For a parse to be well-formed, the top and bottom FS at each node should be unified at the end of a parse.",Experiment/Discussion
"The result of combining the elementary trees shown in Figure 6 is the derived tree, shown in Figure 8(a).",Experiment/Discussion
"The process of combining the elementary trees to yield a parse of the sentence is represented by the derivation tree, shown in Figure 8(b).",Experiment/Discussion
The nodes of the derivation tree are the tree names that are anchored by the appropriate lexical items.,Experiment/Discussion
The combining operation is indicated by the type of the arcs (a broken line indicates substitution and a bold line indicates adjunction) while the address of the operation is indicated as part of the node label.,Experiment/Discussion
"The derivation tree can also be interpreted as a dependency tree with unlabeled arcs between words of the sentence, as shown in Figure 8(c).",Experiment/Discussion
"A broad-coverage grammar system, XTAG, has been implemented in the LTAG formalism.",Experiment/Discussion
"In this section, we briefly discuss some aspects related to XTAG for the sake of completeness.",Experiment/Discussion
A more detailed report on XTAG can be found in XTAG-Group (1995).,Experiment/Discussion
"The XTAG system consists of a morphological analyzer, a part-of-speech tagger, a wide-coverage LTAG English grammar, a predictive left-to-right Earley-style parser for LTAG (Schabes 1990), and an X-windows interface for grammar development (Doran et al. 1994).",Experiment/Discussion
The input sentence is subjected to morphological analysis and is tagged with parts of speech before being sent to the parser.,Experiment/Discussion
The parser retrieves the elementary trees that the words of the sentence anchor and combines them by adjunction and substitution operations to derive a parse of the sentence.,Experiment/Discussion
"The grammar of XTAG has been used to parse sentences from ATIS, IBM Manual and WSJ corpora (TAG-Group 1995).",Experiment/Discussion
The resulting XTAG corpus contains sentences from these domains along with all the derivations for each sentence.,Experiment/Discussion
"The derivations provide In this section, we define the key properties of LTAGs: lexicalization, Extended Domain of Locality (EDL), and factoring of recursion from the domain of dependency (FRD), and discuss how these properties are realized in natural language grammars written in LTAGs.",Experiment/Discussion
"A more detailed discussion about these properties is presented in Joshi (1985, 1987), Kroch and Joshi (1985), Schabes, Abeille, and Joshi (1988), and Joshi and Schabes (1996).",Experiment/Discussion
A grammar is lexicalized if it consists of: This property proves to be linguistically crucial since it establishes a direct link between the lexicon and the syntactic structures defined in the grammar.,Experiment/Discussion
"In fact, in lexicalized grammars all we have is the lexicon, which projects the elementary structures of each lexical item; there is no independent grammar.",Experiment/Discussion
The Extended Domain of Locality (EDL) property has two parts: Part (1) of EDL allows the anchor to impose syntactic and semantic constraints on its arguments directly since they appear in the same elementary structure that it anchors.,Experiment/Discussion
"Hence, all elements that appear within one elementary structure are considered to be local.",Experiment/Discussion
This property also defines how large an elementary structure in a grammar can be.,Experiment/Discussion
Figure 9 shows trees for the following example sentences: Figure 9(a) shows the elementary tree anchored by seem that is used to derive a raising analysis for sentence 1.,Experiment/Discussion
Notice that the elements appearing in the tree are only those that serve as arguments to the anchor and nothing else.,Experiment/Discussion
"In particular, the subject NP (John in sentence 1) does not appear in the elementary tree for seem since it does not serve as an argument for seem.",Experiment/Discussion
Figure 9(b) shows the elementary tree anchored by the transitive verb hit in which both the subject NP and object NP are realized within the same elementary tree.,Experiment/Discussion
LTAG is distinguished from other grammar formalisms by possessing part (2) of the EDL property.,Experiment/Discussion
"In LTAGs, there is one elementary tree for every syntactic environment that the anchor may appear in.",Experiment/Discussion
Each elementary tree encodes the linear order of the arguments of the anchor in a particular syntactic environment.,Experiment/Discussion
"For example, a transitive verb such as hit is associated with both the elementary tree shown in Figure 9(b) for a declarative transitive sentence such as sentence 2, and the elementary tree shown in Figure 9(c) for an object extracted transitive sentence such as sentence 3.",Experiment/Discussion
Notice that the object noun phrase is realized to the left of the subject noun phrase in the object extraction tree.,Experiment/Discussion
"As a consequence of the fact that LTAGs possess the part (2) of the EDL property, the derivation structures in LTAGs contain the information of a dependency structure.",Experiment/Discussion
Another aspect of EDL is that the arguments of the anchor can be filled in any order.,Experiment/Discussion
This is possible because the elementary structures allocate a slot for each argument of the anchor in each syntactic environment that the anchor appears in.,Experiment/Discussion
There can be many ways of constructing the elementary structures of a grammar so as to possess the EDL property.,Experiment/Discussion
"However, by requiring that the constructed elementary structures be &quot;minimal,&quot; the third property of LTAGs namely, factoring of recursion from the domain of dependencies, follows as a corollary of EDL.",Experiment/Discussion
Factoring of recursion from the domain of dependencies (FRD): Recursion is factored away from the domain for the statement of dependencies.,Results/Conclusion
"In LTAGs, recursive constructs are represented as auxiliary trees.",Results/Conclusion
They combine with elementary trees by the operation of adjunction.,Results/Conclusion
"Elementary trees define the domain for stating dependencies such as agreement, subcategorization, and filler-gap dependencies.",Results/Conclusion
"Auxiliary trees, by adjunction to elementary trees, account for the longdistance behavior of these dependencies.",Results/Conclusion
An additional advantage of a grammar possessing FRD and EDL properties is that feature structures in these grammars are extremely simple.,Results/Conclusion
"Since the recursion has been factored out of the domain of dependency, and since the domain is large enough for agreement, subcategorization, and filler-gap dependencies, feature structures in such systems do not involve any recursion.",Results/Conclusion
In fact they reduce to typed terms that can be combined by simple term-like unification.,Results/Conclusion
This work was done when the first author was at the University of Pennsylvania.,Results/Conclusion
"It was partially supported by NSF grant NSF-STC SBR 8920230, ARPA grant N00014-94 and ARO grant DAAH04-94-G0426.",Results/Conclusion
"We would like to thank Steven Abney, Raman Chandrasekar, Christine Doran, Beth Ann Hockey, Mark Liberman, Mitch Marcus, and Mark Steedman for useful comments and discussions which have helped shape this work.",Results/Conclusion
We also thank the reviewers for their insightful comments and suggestions to improve an earlier version of this paper.,Results/Conclusion
