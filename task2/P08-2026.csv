col1,col2
"Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data.",{}
Here we apply this technique to parser adaptation.,{}
"In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts.",{}
This an of 84.3% on a standard test set of biomedical abstracts from the Genia corpus.,{}
This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set).,{}
"Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data.","{'title': '1 Introduction', 'number': '1'}"
"While for many years it was thought not to help state-of-the art parsers, more recent work has shown otherwise.","{'title': '1 Introduction', 'number': '1'}"
In this paper we apply this technique to parser adaptation.,"{'title': '1 Introduction', 'number': '1'}"
In particular we self-train the standard Charniak/Johnson Penn-Treebank (C/J) parser using unannotated biomedical data.,"{'title': '1 Introduction', 'number': '1'}"
"As is well known, biomedical data is hard on parsers because it is so far from more “standard” English.","{'title': '1 Introduction', 'number': '1'}"
To our knowledge this is the first application of self-training where the gap between the training and self-training data is so large.,"{'title': '1 Introduction', 'number': '1'}"
"In section two, we look at previous work.","{'title': '1 Introduction', 'number': '1'}"
"In particular we note that there is, in fact, very little data on self-training when the corpora for self-training is so different from the original labeled data.","{'title': '1 Introduction', 'number': '1'}"
"Section three describes our main experiment on standard test data (Clegg and Shepherd, 2005).","{'title': '1 Introduction', 'number': '1'}"
Section four looks at some preliminary results we obtained on development data that show in slightly more detail how selftraining improved the parser.,"{'title': '1 Introduction', 'number': '1'}"
We conclude in section five.,"{'title': '1 Introduction', 'number': '1'}"
"While self-training has worked in several domains, the early results on self-training for parsing were negative (Steedman et al., 2003; Charniak, 1997).","{'title': '2 Previous Work', 'number': '2'}"
"However more recent results have shown that it can indeed improve parser performance (Bacchiani et al., 2006; McClosky et al., 2006a; McClosky et al., 2006b).","{'title': '2 Previous Work', 'number': '2'}"
"One possible use for this technique is for parser adaptation — initially training the parser on one type of data for which hand-labeled trees are available (e.g., Wall Street Journal (M. Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain.","{'title': '2 Previous Work', 'number': '2'}"
"Interestingly, there is little to no data showing that this actually works.","{'title': '2 Previous Work', 'number': '2'}"
Two previous papers would seem to address this issue: the work by Bacchiani et al. (2006) and McClosky et al.,"{'title': '2 Previous Work', 'number': '2'}"
(2006b).,"{'title': '2 Previous Work', 'number': '2'}"
"However, in both cases the evidence is equivocal.","{'title': '2 Previous Work', 'number': '2'}"
"Bacchiani and Roark train the Roark parser (Roark, 2001) on trees from the Brown treebank and then self-train and test on data from Wall Street Journal.","{'title': '2 Previous Work', 'number': '2'}"
While they show some improvement (from 75.7% to 80.5% f-score) there are several aspects of this work which leave its results less than convincing as to the utility of selftraining for adaptation.,"{'title': '2 Previous Work', 'number': '2'}"
"The first is the parsing results are quite poor by modern standards.1 Steedman et al. (2003) generally found that selftraining does not work, but found that it does help if the baseline results were sufficiently bad.","{'title': '2 Previous Work', 'number': '2'}"
"Secondly, the difference between the Brown corpus treebank and the Wall Street Journal corpus is not that great.","{'title': '2 Previous Work', 'number': '2'}"
One way to see this is to look at out-of-vocabulary statistics.,"{'title': '2 Previous Work', 'number': '2'}"
The Brown corpus has an out-of-vocabulary rate of approximately 6% when given WSJ training as the lexicon.,"{'title': '2 Previous Work', 'number': '2'}"
"In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% (Lease and Charniak, 2005).","{'title': '2 Previous Work', 'number': '2'}"
Thus the bridge the selftrained parser is asked to build is quite short.,"{'title': '2 Previous Work', 'number': '2'}"
"This second point is emphasized by the second paper on self-training for adaptation (McClosky et al., 2006b).","{'title': '2 Previous Work', 'number': '2'}"
This paper is based on the C/J parser and thus its results are much more in line with modern expectations.,"{'title': '2 Previous Work', 'number': '2'}"
"In particular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data.","{'title': '2 Previous Work', 'number': '2'}"
Note this last point.,"{'title': '2 Previous Work', 'number': '2'}"
It was not the case that it used the self-training to bridge the corpora difference.,"{'title': '2 Previous Work', 'number': '2'}"
"It self-trained on NANC, not Brown.","{'title': '2 Previous Work', 'number': '2'}"
"NANC is a news corpus, quite like WSJ data.","{'title': '2 Previous Work', 'number': '2'}"
"Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular.","{'title': '2 Previous Work', 'number': '2'}"
It said nothing about the task we address here.,"{'title': '2 Previous Work', 'number': '2'}"
Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation.,"{'title': '2 Previous Work', 'number': '2'}"
"Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers.","{'title': '2 Previous Work', 'number': '2'}"
Their best result was an f-score of 80.2%.,"{'title': '2 Previous Work', 'number': '2'}"
"This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 'This is not a criticism of the work.","{'title': '2 Previous Work', 'number': '2'}"
The results are completely in line with what one would expect given the base parser and the relatively small size of the Brown treebank. the parser of Bikel (2004).,"{'title': '2 Previous Work', 'number': '2'}"
The other parsers were not close.,"{'title': '2 Previous Work', 'number': '2'}"
"However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al., 2006)).","{'title': '2 Previous Work', 'number': '2'}"
"However, since the newer parsers do not perform quite as well as the C/J parser on WSJ data, it is probably the case that they would not significantly alter the landscape.","{'title': '2 Previous Work', 'number': '2'}"
We used as the base parser the standardly available C/J parser.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"We then self-trained the parser on approximately 270,000 sentences — a random selection of abstracts from Medline.3 Medline is a large database of abstracts and citations from a wide variety of biomedical literature.","{'title': '3 Central Experimental Result', 'number': '3'}"
"As we note in the next section, the number 270,000 was selected by observing performance on a development set.","{'title': '3 Central Experimental Result', 'number': '3'}"
We weighted the original WSJ hand annotated sentences equally with self-trained Medline data.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"So, for example, McClosky et al. (2006a) found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level.","{'title': '3 Central Experimental Result', 'number': '3'}"
We did no tuning to find out if there is some better weighting for our domain than one-to-one.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"The resulting parser was tested on a test corpus of hand-parsed sentences from the Genia Treebank (Tateisi et al., 2005).","{'title': '3 Central Experimental Result', 'number': '3'}"
These are exactly the same sentences as used in the comparisons of the last section.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"Genia is a corpus of abstracts from the Medline database selected from a search with the keywords Human, Blood Cells, and Transcription Factors.","{'title': '3 Central Experimental Result', 'number': '3'}"
Thus the Genia treebank data are all from a small domain within Biology.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"As already noted, the Medline abstracts used for self-training were chosen randomly and thus span a large number of biomedical sub-domains.","{'title': '3 Central Experimental Result', 'number': '3'}"
"The results, the central results of this paper, are shown in Figure 1.","{'title': '3 Central Experimental Result', 'number': '3'}"
Clegg and Shepherd (2005) do not provide separate precision and recall numbers.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"However we can see that the Medline self-trained parser achieves an f-score of 84.3%, which is an absolute reduction in error of 4.1%.","{'title': '3 Central Experimental Result', 'number': '3'}"
This corresponds to an error rate reduction of 20% over the L/C baseline.,"{'title': '3 Central Experimental Result', 'number': '3'}"
"Prior to the above experiment on the test data, we did several preliminary experiments on development data from the Genia Treebank.","{'title': '4 Discussion', 'number': '4'}"
These results are summarized in Figure 2.,"{'title': '4 Discussion', 'number': '4'}"
Here we show the f-score for four versions of the parser as a function of number of self-training sentences.,"{'title': '4 Discussion', 'number': '4'}"
The dashed line on the bottom is the raw C/J parser with no self-training.,"{'title': '4 Discussion', 'number': '4'}"
"At 80.4, it is clearly the worst of the lot.","{'title': '4 Discussion', 'number': '4'}"
"On the other hand, it is already better than the 80.2% best previous result for biomedical data.","{'title': '4 Discussion', 'number': '4'}"
This is solely due to the introduction of the 50-best reranker which distinguishes the C/J parser from the preceding Charniak parser.,"{'title': '4 Discussion', 'number': '4'}"
The almost flat line above it is the C/J parser with NANC self-training data.,"{'title': '4 Discussion', 'number': '4'}"
"As mentioned previously, NANC is a news corpus, quite like the original WSJ data.","{'title': '4 Discussion', 'number': '4'}"
At 81.4% it gives us a one percent improvement over the original WSJ parser.,"{'title': '4 Discussion', 'number': '4'}"
"The topmost line, is the C/J parser trained on Medline data.","{'title': '4 Discussion', 'number': '4'}"
"As can be seen, even just a thousand lines of Medline is already enough to drive our results to a new level and it continues to improve until about 150,000 sentences at which point performance is nearly flat.","{'title': '4 Discussion', 'number': '4'}"
"However, as 270,000 sentences is fractionally better than 150,000 sentences that is the number of self-training sentences we used for our results on the test set.","{'title': '4 Discussion', 'number': '4'}"
"Lastly, the middle jagged line is for an interesting idea that failed to work.","{'title': '4 Discussion', 'number': '4'}"
We mention it in the hope that others might be able to succeed where we have failed.,"{'title': '4 Discussion', 'number': '4'}"
We reasoned that textbooks would be a particularly good bridging corpus.,"{'title': '4 Discussion', 'number': '4'}"
"After all, they are written to introduce someone ignorant of a field to the ideas and terminology within it.","{'title': '4 Discussion', 'number': '4'}"
Thus one might expect that the English of a Biology textbook would be intermediate between the more typical English of a news article and the specialized English native to the domain.,"{'title': '4 Discussion', 'number': '4'}"
To test this we created a corpus of seven texts (`BioBooks&quot;) on various areas of biology that were available on the web.,"{'title': '4 Discussion', 'number': '4'}"
We observe in Figure 2 that for all quantities of self-training data one does better with Medline than BioBooks.,"{'title': '4 Discussion', 'number': '4'}"
"For example, at 37,000 sentences the BioBook corpus is only able to achieve and an f-measure of 82.8% while the Medline corpus is at 83.4%.","{'title': '4 Discussion', 'number': '4'}"
"Furthermore, BioBooks levels off in performance while Medline has significant improvement left in it.","{'title': '4 Discussion', 'number': '4'}"
"Thus, while the hypothesis seems reasonable, we were unable to make it work.","{'title': '4 Discussion', 'number': '4'}"
"We self-trained the standard C/J parser on 270,000 sentences of Medline abstracts.","{'title': '5 Conclusion', 'number': '5'}"
By doing so we achieved a 20% error reduction over the best previous result for biomedical parsing.,"{'title': '5 Conclusion', 'number': '5'}"
"In terms of the gap between the supervised data and the self-trained data, this is the largest that has been attempted.","{'title': '5 Conclusion', 'number': '5'}"
"Furthermore, the resulting parser is of interest in its own right, being as it is the most accurate biomedical parser yet developed.","{'title': '5 Conclusion', 'number': '5'}"
"This parser is available on the web.4 Finally, there is no reason to believe that 84.3% is an upper bound on what can be achieved with current techniques.","{'title': '5 Conclusion', 'number': '5'}"
Lease and Charniak (2005) achieve their results using small amounts of hand-annotated biomedical part-ofspeech-tagged data and also explore other possible sources or information.,"{'title': '5 Conclusion', 'number': '5'}"
It is reasonable to assume that its use would result in further improvement.,"{'title': '5 Conclusion', 'number': '5'}"
This work was supported by DARPA GALE contract HR0011-06-2-0001.,"{'title': 'Acknowledgments', 'number': '6'}"
We would like to thank the BLLIP team for their comments.,"{'title': 'Acknowledgments', 'number': '6'}"
