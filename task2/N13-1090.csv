col1,col2
Continuous space language models have recently demonstrated outstanding results across a variety of tasks.,{}
"In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.",{}
"We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.",{}
This allows vector-oriented reasoning based on the offsets between words.,{}
"For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.",{}
We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.,{}
"Remarkably, this method outperforms the best previous systems.",{}
A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.,"{'title': '1 Introduction', 'number': '1'}"
"In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.","{'title': '1 Introduction', 'number': '1'}"
"As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.","{'title': '1 Introduction', 'number': '1'}"
"Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.","{'title': '1 Introduction', 'number': '1'}"
"By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks.","{'title': '1 Introduction', 'number': '1'}"
"This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.","{'title': '1 Introduction', 'number': '1'}"
"In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.","{'title': '1 Introduction', 'number': '1'}"
"Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.","{'title': '1 Introduction', 'number': '1'}"
"For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on.","{'title': '1 Introduction', 'number': '1'}"
"Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.","{'title': '1 Introduction', 'number': '1'}"
"Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows.","{'title': '1 Introduction', 'number': '1'}"
"In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.","{'title': '1 Introduction', 'number': '1'}"
"Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990).","{'title': '2 Related Work', 'number': '2'}"
"More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words.","{'title': '2 Related Work', 'number': '2'}"
"These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).","{'title': '2 Related Work', 'number': '2'}"
"This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models.","{'title': '2 Related Work', 'number': '2'}"
"This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).","{'title': '2 Related Work', 'number': '2'}"
"Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.","{'title': '2 Related Work', 'number': '2'}"
"The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1.","{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
"This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices.","{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
"The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words.","{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
The hidden layer s(t) maintains a representation of the sentence history.,"{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary.,"{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
"The values in the hidden and output layers are computed as follows: where In this framework, the word representations are found in the columns of U, with each column representing a word.","{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
The RNN is trained with backpropagation to maximize the data log-likelihood under the model.,"{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
The model itself has no knowledge of syntax or morphology or semantics.,"{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
"Remarkably, training such a purely lexical model to maximize likelihood will induce word representations with striking syntactic and semantic properties.","{'title': '3 Recurrent Neural Network Model', 'number': '3'}"
"To understand better the syntactic regularities which are inherent in the learned representation, we created a test set of analogy questions of the form “a is to b as c is to ” testing base/comparative/superlative forms of adjectives; singular/plural forms of common nouns; possessive/non-possessive forms of common nouns; and base, past and 3rd person present tense forms of verbs.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"More precisely, we tagged 267M words of newspaper text with Penn Treebank POS tags (Marcus et al., 1993).","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB).,"{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
The total test set size is 8000.,"{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
The test set is available online.,"{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"1 In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
Each relation is exemplified by 3 or 4 gold word pairs.,"{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
This can be viewed as another analogy problem.,"{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"For example, take the ClassInclusion:Singular Collective relation with the prototypical word pair clothing:shirt.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy “clothing is to shirt as dish is to bowl,” and ask how valid it is.","{'title': '4 Measuring Linguistic Regularity', 'number': '4'}"
"As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.","{'title': '5 The Vector Offset Method', 'number': '5'}"
We have found that a simple vector offset method based on cosine distance is remarkably effective in solving these questions.,"{'title': '5 The Vector Offset Method', 'number': '5'}"
"In this method, we assume relationships are present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset.","{'title': '5 The Vector Offset Method', 'number': '5'}"
This is illustrated in Figure 2.,"{'title': '5 The Vector Offset Method', 'number': '5'}"
"In this model, to answer the analogy question a:b c:d where d is unknown, we find the embedding vectors xa, xb, xc (all normalized to unit norm), and compute y = xb − xa + xc. y is the continuous space representation of the word we expect to be the best answer.","{'title': '5 The Vector Offset Method', 'number': '5'}"
"Of course, no word might exist at that exact position, so we then search for the word whose embedding vector has the greatest cosine similarity to y and output it: provided.","{'title': '5 The Vector Offset Method', 'number': '5'}"
We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations.,"{'title': '5 The Vector Offset Method', 'number': '5'}"
"We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd).","{'title': '5 The Vector Offset Method', 'number': '5'}"
"To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).","{'title': '6 Experimental Results', 'number': '6'}"
"Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.","{'title': '6 Experimental Results', 'number': '6'}"
"The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.","{'title': '6 Experimental Results', 'number': '6'}"
Table 2 shows results for both RNNLM and LSA vectors on the syntactic task.,"{'title': '6 Experimental Results', 'number': '6'}"
LSA was trained on the same data as the RNN.,"{'title': '6 Experimental Results', 'number': '6'}"
"We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.","{'title': '6 Experimental Results', 'number': '6'}"
"2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.","{'title': '6 Experimental Results', 'number': '6'}"
"This resulted in a 36k word vocabulary, and a test set with 6632 questions.","{'title': '6 Experimental Results', 'number': '6'}"
"Turian’s Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors.","{'title': '6 Experimental Results', 'number': '6'}"
These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.,"{'title': '6 Experimental Results', 'number': '6'}"
We conducted similar experiments with the semantic test set.,"{'title': '6 Experimental Results', 'number': '6'}"
"For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score.","{'title': '6 Experimental Results', 'number': '6'}"
"The results are evaluated using the two standard metrics defined in the task, Spearman’s rank correlation coefficient p and MaxDiff accuracy.","{'title': '6 Experimental Results', 'number': '6'}"
"In both cases, larger values are better.","{'title': '6 Experimental Results', 'number': '6'}"
"To compare to previous systems, we report the average over all 69 relations in the test set.","{'title': '6 Experimental Results', 'number': '6'}"
"From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best.","{'title': '6 Experimental Results', 'number': '6'}"
"In this case, however, Turian’s CW vectors are comparable in performance to the HLBL vectors.","{'title': '6 Experimental Results', 'number': '6'}"
"With the RNN vectors, the performance improves as the number of dimensions increases.","{'title': '6 Experimental Results', 'number': '6'}"
"Surprisingly, we found that even though the RNN vectors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 & RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).","{'title': '6 Experimental Results', 'number': '6'}"
We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations.,"{'title': '7 Conclusion', 'number': '7'}"
We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities.,"{'title': '7 Conclusion', 'number': '7'}"
"We present a new dataset for measuring syntactic performance, and achieve almost 40% correct.","{'title': '7 Conclusion', 'number': '7'}"
"We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.","{'title': '7 Conclusion', 'number': '7'}"
"Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.","{'title': '7 Conclusion', 'number': '7'}"
