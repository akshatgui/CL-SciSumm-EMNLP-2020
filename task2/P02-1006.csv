col1,col2
In this paper we explore the power of surface text patterns for open-domain question answering systems.,{}
"In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.",{}
A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.,{}
Patterns are then automatically extracted from the returned documents and standardized.,{}
"We calculate the precision of each pattern, and the average precision for each question type.",{}
These patterns are then applied to find answers to new questions.,{}
"Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.",{}
Most of the recent open domain questionanswering systems use external knowledge and tools for answer pinpointing.,"{'title': '1 Introduction', 'number': '1'}"
"These may include named entity taggers, WordNet, parsers, hand-tagged corpora, and ontology lists (Srihari and Li, 00; Harabagiu et al., 01; Hovy et al., 01; Prager et al., 01).","{'title': '1 Introduction', 'number': '1'}"
"However, at the recent TREC-10 QA evaluation (Voorhees, 01), the winning system used just one resource: a fairly extensive list of surface patterns (Soubbotin and Soubbotin, 01).","{'title': '1 Introduction', 'number': '1'}"
The apparent power of such patterns surprised many.,"{'title': '1 Introduction', 'number': '1'}"
We therefore decided to investigate their potential by acquiring patterns automatically and to measure their accuracy.,"{'title': '1 Introduction', 'number': '1'}"
"It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases (Lee et al., 01; Wang et al., 01).","{'title': '1 Introduction', 'number': '1'}"
"For example, for BIRTHDATEs (with questions like “When was X born?”), typical answers are “Mozart was born in 1756.” “Gandhi (1869–1948)...” These examples suggest that phrases like “<NAME> was born in <BIRTHDATE>” “<NAME> (<BIRTHDATE>–” when formulated as regular expressions, can be used to locate the correct answer.","{'title': '1 Introduction', 'number': '1'}"
"In this paper we present an approach for automatically learning such regular expressions (along with determining their precision) from the web, for given types of questions.","{'title': '1 Introduction', 'number': '1'}"
Our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of QA pairs.,"{'title': '1 Introduction', 'number': '1'}"
"Similar techniques have been investigated extensively in the field of information extraction (Riloff, 96).","{'title': '1 Introduction', 'number': '1'}"
"These techniques are greatly aided by the fact that there is no need to hand-tag a corpus, while the abundance of data on the web makes it easier to determine reliable statistical estimates.","{'title': '1 Introduction', 'number': '1'}"
Our system assumes each sentence to be a simple sequence of words and searches for repeated word orderings as evidence for useful answer phrases.,"{'title': '1 Introduction', 'number': '1'}"
We use suffix trees for extracting substrings of optimal length.,"{'title': '1 Introduction', 'number': '1'}"
"We borrow the idea of suffix trees from computational biology (Gusfield, 97) where it is primarily used for detecting DNA sequences.","{'title': '1 Introduction', 'number': '1'}"
"Suffix trees can be processed in time linear on the size of the corpus and, more importantly, they do not restrict the length of substrings.","{'title': '1 Introduction', 'number': '1'}"
We then test the patterns learned by our system on new unseen questions from the TREC-10 set and evaluate their results to determine the precision of the patterns.,"{'title': '1 Introduction', 'number': '1'}"
We describe the pattern-learning algorithm with an example.,"{'title': '2 Learning of Patterns', 'number': '2'}"
A table of patterns is constructed for each individual question type by the following procedure (Algorithm 1).,"{'title': '2 Learning of Patterns', 'number': '2'}"
"(1756–1791)”, which the suffix tree would extract as one of the outputs along with the score of 3.","{'title': '2 Learning of Patterns', 'number': '2'}"
7.,"{'title': '2 Learning of Patterns', 'number': '2'}"
Pass each phrase in the suffix tree through a filter to retain only those phrases that contain both the question and the answer term.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"For the example, we extract only those phrases from the suffix tree that contain the words “Mozart” and “1756”.","{'title': '2 Learning of Patterns', 'number': '2'}"
8.,"{'title': '2 Learning of Patterns', 'number': '2'}"
Replace the word for the question term by the tag “<NAME>” and the word for the answer term by the term “<ANSWER>”.,"{'title': '2 Learning of Patterns', 'number': '2'}"
This procedure is repeated for different examples of the same question type.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"For BIRTHDATE we also use “Gandhi 1869”, “Newton 1642”, etc.","{'title': '2 Learning of Patterns', 'number': '2'}"
"For BIRTHDATE, the above steps produce the following output: ...","{'title': '2 Learning of Patterns', 'number': '2'}"
These are some of the most common substrings of the extracted sentences that contain both <NAME> and <ANSWER>.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"Since the suffix tree records all substrings, partly overlapping strings such as c and d are separately saved, which allows us to obtain separate counts of their occurrence frequencies.","{'title': '2 Learning of Patterns', 'number': '2'}"
"As will be seen later, this allows us to differentiate patterns such as d (which records a still living person, and is quite precise) from its more general substring c (which is less precise).","{'title': '2 Learning of Patterns', 'number': '2'}"
"In our example, for the pattern “<NAME> was born in <ANSWER>” we check the presence of the following strings in the answer sentence i) Mozart was born in <ANY_WORD> ii) Mozart was born in 1756 Calculate the precision of each pattern by the formula P = Ca / Co where Ca = total number of patterns with the answer term present Co = total number of patterns present with answer term replaced by any word 6.","{'title': '2 Learning of Patterns', 'number': '2'}"
Retain only the patterns matching a sufficient number of examples (we choose the number of examples > 5).,"{'title': '2 Learning of Patterns', 'number': '2'}"
"We obtain a table of regular expression patterns for a given question type, along with the precision of each pattern.","{'title': '2 Learning of Patterns', 'number': '2'}"
This precision is the probability of each pattern containing the answer and follows directly from the principle of maximum likelihood estimation.,"{'title': '2 Learning of Patterns', 'number': '2'}"
For BIRTHDATE the following table is obtained: For a given question type a good range of patterns was obtained by giving the system as few as 10 examples.,"{'title': '2 Learning of Patterns', 'number': '2'}"
The rather long list of patterns obtained would have been very difficult for any human to come up with manually.,"{'title': '2 Learning of Patterns', 'number': '2'}"
The question term could appear in the documents obtained from the web in various ways.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"Thus “Mozart” could be written as “Wolfgang Amadeus Mozart”, “Mozart, Wolfgang Amadeus”, “Amadeus Mozart” or “Mozart”.","{'title': '2 Learning of Patterns', 'number': '2'}"
"To learn from such variations, in step 1 of Algorithm 1 we specify the various ways in which the question term could be specified in the text.","{'title': '2 Learning of Patterns', 'number': '2'}"
The presence of any of these names would cause it to be tagged as the original question term “Mozart”.,"{'title': '2 Learning of Patterns', 'number': '2'}"
The same arrangement is also done for the answer term so that presence of any variant of the answer term would cause it to be treated exactly like the original answer term.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"While easy to do for BIRTHDATE, this step can be problematic for question types such as DEFINITION, which may contain various acceptable answers.","{'title': '2 Learning of Patterns', 'number': '2'}"
"In general the input example terms have to be carefully selected so that the questions they represent do not have a long list of possible answers, as this would affect the confidence of the precision scores for each pattern.","{'title': '2 Learning of Patterns', 'number': '2'}"
"All the answers need to be enlisted to ensure a high confidence in the precision score of each pattern, in the present framework.","{'title': '2 Learning of Patterns', 'number': '2'}"
The precision of the patterns obtained from one QA-pair example in algorithm 1 is calculated from the documents obtained in algorithm 2 for other examples of the same question type.,"{'title': '2 Learning of Patterns', 'number': '2'}"
"In other words, the precision scores are calculated by cross-checking the patterns across various examples of the same type.","{'title': '2 Learning of Patterns', 'number': '2'}"
"This step proves to be very significant as it helps to eliminate dubious patterns, which may appear because the contents of two or more websites may be the same, or the same web document reappears in the search engine output for algorithms 1 and 2.","{'title': '2 Learning of Patterns', 'number': '2'}"
Algorithm 1 does not explicitly specify any particular question type.,"{'title': '2 Learning of Patterns', 'number': '2'}"
Judicious choice of the QA example pair therefore allows it to be used for many question types without change.,"{'title': '2 Learning of Patterns', 'number': '2'}"
Using the patterns to answer a new question we employ the following algorithm:,"{'title': '3 Finding Answers', 'number': '3'}"
"From our Webclopedia QA Typology (Hovy et al., 2002a) we selected 6 different question types: BIRTHDATE, LOCATION, INVENTOR, DISCOVERER, DEFINITION, WHY-FAMOUS.","{'title': '4 Experiments', 'number': '4'}"
The pattern table for each of these question types was constructed using Algorithm 1.,"{'title': '4 Experiments', 'number': '4'}"
"Some of the patterns obtained along with their precision are as follows For each question type, we extracted the corresponding questions from the TREC-10 set.","{'title': '4 Experiments', 'number': '4'}"
These questions were run through the testing phase of the algorithm.,"{'title': '4 Experiments', 'number': '4'}"
Two sets of experiments were performed.,"{'title': '4 Experiments', 'number': '4'}"
"In the first case, the TREC corpus was used as the input source and IR was performed by the IR component of our QA system (Lin, 2002).","{'title': '4 Experiments', 'number': '4'}"
"In the second case, the web was the input source and the IR was performed by the AltaVista search engine.","{'title': '4 Experiments', 'number': '4'}"
"Results of the experiments, measured by Mean Reciprocal Rank (MRR) score (Voorhees, 01), are:","{'title': '4 Experiments', 'number': '4'}"
The results indicate that the system performs better on the Web data than on the TREC corpus.,"{'title': 'TREC Corpus', 'number': '5'}"
The abundance of data on the web makes it easier for the system to locate answers with high precision scores (the system finds many examples of correct answers among the top 20 when using the Web as the input source).,"{'title': 'TREC Corpus', 'number': '5'}"
A similar result for QA was obtained by Brill et al. (2001).,"{'title': 'TREC Corpus', 'number': '5'}"
The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.,"{'title': 'TREC Corpus', 'number': '5'}"
The WHY-FAMOUS question type is an exception and may be due to the fact that the system was tested on a small number of questions.,"{'title': 'TREC Corpus', 'number': '5'}"
No external knowledge has been added to these patterns.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"We frequently observe the need for matching part of speech and/or semantic types, however.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"For example, the question: “Where are the Rocky Mountains located?” is answered by “Denver’s new airport, topped with white fiberglass cones in imitation of the Rocky Mountains in the background, continues to lie empty”, because the system picked the answer “the background” using the pattern “the <NAME> in <ANSWER>,”.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
Using a named entity tagger and/or an ontology would enable the system to use the knowledge that “background” is not a location.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
DEFINITION questions pose a related problem.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"Frequently the system’s patterns match a term that is too general, though correct technically.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"For “what is nepotism?” the pattern “<ANSWER>, <NAME>” matches “...in the form of widespread bureaucratic abuses: graft, nepotism...”; for “what is sonar?” the pattern “<NAME> and related <ANSWER>s” matches “...while its sonar and related underseas systems are built...”.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The patterns cannot handle long-distance dependencies.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"For example, for “Where is London?” the system cannot locate the answer in “London, which has one of the most busiest airports in the world, lies on the banks of the river Thames” due to the explosive danger of unrestricted wildcard matching, as would be required in the pattern “<QUESTION>, (<any_word>)*, lies on <ANSWER>”.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
This is one of the reasons why the system performs very well on certain types of questions from the web but performs poorly with documents obtained from the TREC corpus.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The abundance and variation of data on the Internet allows the system to find an instance of its patterns without losing answers to longterm dependencies.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"The TREC corpus, on the other hand, typically contains fewer candidate answers for a given question and many of the answers present may match only long-term dependency patterns.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
More information needs to be added to the text patterns regarding the length of the answer phrase to be expected.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The system searches in the range of 50 bytes of the answer phrase to capture the pattern.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
It fails to perform under certain conditions as exemplified by the question “When was Lyndon B. Johnson born?”.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"The system selects the sentence “Tower gained national attention in 1960 when he lost to democratic Sen. Lyndon B. Johnson, who ran for both reelection and the vice presidency” using the pattern “<NAME> <ANSWER> –“.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The system lacks the information that the <ANSWER> tag should be replaced exactly by one word.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"Simple extensions could be made to the system so that instead of searching in the range of 50 bytes for the answer phrase it could search for the answer in the range of 1–2 chunks (basic phrases in English such as simple NP, VP, PP, etc.).","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
A more serious limitation is that the present framework can handle only one anchor point (the question term) in the candidate answer sentence.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"For example, in “Which county does the city of Long Beach lie?”, the answer “Long Beach is situated in Los Angeles County” requires the pattern.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"“<QUESTION_TERM_1> situated in <ANSWER> <QUESTION_TERM_2>”, where <QUESTION_TERM_1> and <QUESTION_TERM_2> represent the terms “Long Beach” and “county” respectively.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"The performance of the system depends significantly on there being only one anchor word, which allows a single word match between the question and the candidate answer sentence.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The presence of multiple anchor words would help to eliminate many of the candidate answers by simply using the condition that all the anchor words from the question must be present in the candidate answer sentence.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The system does not classify or make any distinction between upper and lower case letters.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"For example, “What is micron?” is answered by “In Boise, Idaho, a spokesman for Micron, a maker of semiconductors, said Simms are ‘ a very high volume product for us ...’ ”.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The answer returned by the system would have been perfect if the word “micron” had been capitalized in the question.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
Canonicalization of words is also an issue.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"While giving examples in the bootstrapping procedure, say, for BIRTHDATE questions, the answer term could be written in many ways (for example, Gandhi’s birth date can be written as “1869”, “Oct.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"2, 1869”, “2nd October 1869”, “October 2 1869”, and so on).","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
Instead of enlisting all the possibilities a date tagger could be used to cluster all the variations and tag them with the same term.,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
"The same idea could also be extended for smoothing out the variations in the question term for names of persons (Gandhi could be written as “Mahatma Gandhi”, “Mohandas Karamchand Gandhi”, etc.","{'title': '5 Shortcoming and Extensions', 'number': '6'}"
).,"{'title': '5 Shortcoming and Extensions', 'number': '6'}"
The web results easily outperform the TREC results.,"{'title': '6 Conclusion', 'number': '7'}"
This suggests that there is a need to integrate the outputs of the Web and the TREC corpus.,"{'title': '6 Conclusion', 'number': '7'}"
"Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.","{'title': '6 Conclusion', 'number': '7'}"
This would work well for question types like BIRTHDATE or LOCATION but is not clear for question types like DEFINITION.,"{'title': '6 Conclusion', 'number': '7'}"
The simplicity of this method makes it perfect for multilingual QA.,"{'title': '6 Conclusion', 'number': '7'}"
"Many tools required by sophisticated QA systems (named entity taggers, parsers, ontologies, etc.) are language specific and require significant effort to adapt to a new language.","{'title': '6 Conclusion', 'number': '7'}"
"Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched.","{'title': '6 Conclusion', 'number': '7'}"
This work was supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007.,"{'title': 'Acknowledgements', 'number': '8'}"
