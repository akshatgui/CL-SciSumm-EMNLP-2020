col1,col2
Unsupervised learning of linguistic structure is a difficult problem.,{}
A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.,{}
"Typically, this is done using maximum-likelihood estimation (MLE) of the model parameters.",{}
We show using part-of-speech tagging that a fully Bayesian approach can greatly improve performance.,{}
"Rather than estimating a single set of parameters, the Bayesian approach integrates over all possible parameter values.",{}
"This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.",{}
"Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.",{}
"We find improvements both when training from data alone, and using a tagging dictionary.",{}
Unsupervised learning of linguistic structure is a difficult problem.,"{'title': '1 Introduction', 'number': '1'}"
"Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and Eisner, 2005).","{'title': '1 Introduction', 'number': '1'}"
Nearly all of these approaches have one aspect in common: the goal of learning is to identify the set of model parameters that maximizes some objective function.,"{'title': '1 Introduction', 'number': '1'}"
Values for the hidden variables in the model are then chosen based on the learned parameterization.,"{'title': '1 Introduction', 'number': '1'}"
"Here, we propose a different approach based on Bayesian statistical principles: rather than searching for an optimal set of parameter values, we seek to directly maximize the probability of the hidden variables given the observed data, integrating over all possible parameter values.","{'title': '1 Introduction', 'number': '1'}"
"Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements over maximum-likelihood estimation (MLE) for the same model structure.","{'title': '1 Introduction', 'number': '1'}"
Two factors can explain the improvement.,"{'title': '1 Introduction', 'number': '1'}"
"First, integrating over parameter values leads to greater robustness in the choice of tag sequence, since it must have high probability over a range of parameters.","{'title': '1 Introduction', 'number': '1'}"
"Second, integration permits the use of priors favoring sparse distributions, which are typical of natural language.","{'title': '1 Introduction', 'number': '1'}"
These kinds of priors can lead to degenerate solutions if the parameters are estimated directly.,"{'title': '1 Introduction', 'number': '1'}"
"Before describing our approach in more detail, we briefly review previous work on unsupervised POS tagging.","{'title': '1 Introduction', 'number': '1'}"
"Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).","{'title': '1 Introduction', 'number': '1'}"
"More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM (Smith and Eisner, 2005).","{'title': '1 Introduction', 'number': '1'}"
Non-model-based approaches have also been proposed (Brill (1995); see also discussion in Banko and Moore (2004)).,"{'title': '1 Introduction', 'number': '1'}"
All of this work is really POS disambiguation: learning is strongly constrained by a dictionary listing the allowable tags for each word in the text.,"{'title': '1 Introduction', 'number': '1'}"
"Smith and Eisner (2005) also present results using a diluted dictionary, where infrequent words may have any tag.","{'title': '1 Introduction', 'number': '1'}"
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary.,"{'title': '1 Introduction', 'number': '1'}"
A different tradition treats the identification of syntactic classes as a knowledge-free clustering problem.,"{'title': '1 Introduction', 'number': '1'}"
"Distributional clustering and dimensionality reduction techniques are typically applied when linguistically meaningful classes are desired (Sch¨utze, 1995; Clark, 2000; Finch et al., 1995); probabilistic models have been used to find classes that can improve smoothing and reduce perplexity (Brown et al., 1992; Saul and Pereira, 1997).","{'title': '1 Introduction', 'number': '1'}"
"Unfortunately, due to a lack of standard and informative evaluation techniques, it is difficult to compare the effectiveness of different clustering methods.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all.","{'title': '1 Introduction', 'number': '1'}"
"We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels.","{'title': '1 Introduction', 'number': '1'}"
We also evaluate using tag accuracy when possible.,"{'title': '1 Introduction', 'number': '1'}"
"Our system outperforms an HMM trained with MLE on both metrics in all circumstances tested, often by a wide margin.","{'title': '1 Introduction', 'number': '1'}"
Its accuracy in some cases is close to that of Smith and Eisner’s (2005) discriminative model.,"{'title': '1 Introduction', 'number': '1'}"
"Our results show that the Bayesian approach is particularly useful when learning is less constrained, either because less evidence is available (corpus size is small) or because the dictionary contains less information.","{'title': '1 Introduction', 'number': '1'}"
"In the following section, we discuss the motivation for a Bayesian approach and present our model and search procedure.","{'title': '1 Introduction', 'number': '1'}"
"Section 3 gives results illustrating how the parameters of the prior affect results, and Section 4 describes how to infer a good choice of parameters from unlabeled data.","{'title': '1 Introduction', 'number': '1'}"
"Section 5 presents results for a range of corpus sizes and dictionary information, and Section 6 concludes.","{'title': '1 Introduction', 'number': '1'}"
"In model-based approaches to unsupervised language learning, the problem is formulated in terms of identifying latent structure from data.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"We define a model with parameters 0, some observed variables w (the linguistic input), and some latent variables t (the hidden structure).","{'title': '2 A Bayesian HMM', 'number': '2'}"
The goal is to assign appropriate values to the latent variables.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"Standard approaches do so by selecting values for the model parameters, and then choosing the most probable variable assignment based on those parameters.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"For example, maximum-likelihood estimation (MLE) seeks parameters 0� such that where P(w|0) = & P(w, t|0).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Sometimes, a non-uniform prior distribution over 0 is introduced, in which case 0� is the maximum a posteriori (MAP) solution for 0: The values of the latent variables are then taken to be those that maximize P(t|w, �0).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"In contrast, the Bayesian approach we advocate in this paper seeks to identify a distribution over latent variables directly, without ever fixing particular values for the model parameters.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The distribution over latent variables given the observed data is obtained by integrating over all possible values of 0: This distribution can be used in various ways, including choosing the MAP assignment to the latent variables, or estimating expected values for them.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"To see why integrating over possible parameter values can be useful when inducing latent structure, consider the following example.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"We are given a coin, which may be biased (t = 1) or fair (t = 0), each with probability .5.","{'title': '2 A Bayesian HMM', 'number': '2'}"
Let 0 be the probability of heads.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"If the coin is biased, we assume a uniform distribution over 0, otherwise 0 = .5.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"We observe w, the outcomes of 10 coin flips, and we wish to determine whether the coin is biased (i.e. the value of t).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Assume that we have a uniform prior on B, with p(B) = 1 for all B ∈ [0, 1].","{'title': '2 A Bayesian HMM', 'number': '2'}"
"First, we apply the standard methodology of finding the MAP estimate for B and then selecting the value of t that maximizes P(t|w, B).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"In this case, an elementary calculation shows that the MAP estimate is B = nH/10, where nH is the number of heads in w (likewise, nT is the number of tails).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Consequently, P(t|w, �B) favors t = 1 for any sequence that does not contain exactly five heads, and assigns equal probability tot = 1 and t = 0 for any sequence that does contain exactly five heads — a counterintuitive result.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"In contrast, using some standard results in Bayesian analysis we can show that applying Equation 3 yields approach is sensitive to the robustness of a choice of t to the value of B, as illustrated in Figure 1.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Even though a sequence (Figure 1 (a)), P(t = B) is only greater than 0.5 for a small range of B around B (Figure 1 (b)), meaning that the choice oft = 1 is not very robust to variation in B.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"In contrast, a sequence with nH = 8 favors t = 1 for a wide range of B around B.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"By integrating over B, Equation 3 takes into account the consequences of possible variation in B.","{'title': '2 A Bayesian HMM', 'number': '2'}"
Another advantage of integrating over B is that it permits the use of linguistically appropriate priors.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"In many linguistic models, including HMMs, the distributions over variables are multinomial.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The value of Q determines which parameters B will have high probability: when Q = 1, all parameter values are equally likely; when Q > 1, multinomials that are closer to uniform are prior is conjugate to a distribution if the posterior has the same form as the pri d B as a function of B. mation.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"For a sequence of draws x = ... , xn) from a multinomial distribution B with observed counts ... , nK, a symmetric prior over B yields the MAP estimate Bk = When Q 1, standard MLE techniques such as EM can be used to find the MAP estimate simply by adding of size Q 1 to each of the expected counts nk at each iteration.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"However, when Q < 1, the values of B that set one or more of the Bk equal to 0 can have infinitely high posterior probability, meaning that MAP estimation can yield degenerate solutions.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"If, instead of estimating B, we integrate over all possible values, we no longer encounter such difficulties.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Instead, the probability that outcome xi value of a latent variable, t, from observed data, w, chooses a value of t robust to uncertainty in B.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"(a) Posterior distribution on B given w. (b) Probability preferred; and when Q < 1, high probability is assigned to sparse multinomials, where one or more parameters are at or near 0.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Consequently, it makes sense to use a Dirichlet prior with Q < 1.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"However, as noted by Johnson et al. (2007), this choice of Q leads to difficulties with MAP estiwhere nk is the number of times k occurred in x−i.","{'title': '2 A Bayesian HMM', 'number': '2'}"
2.3 Inference See MacKay and Peto (1995) for a derivation.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"We ors over the transition and output distributions: initialize the tags at random, then iteratively resamti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all other tags.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Exchangeτ(t,t′)|α — Dirichlet(α) ability allows us to treat the current counts of the ω(t)|β — Dirichlet(β) other tag trigrams and outputs as “previous” obserwhere ti and wi are the ith tag and word.","{'title': '2 A Bayesian HMM', 'number': '2'}"
We assume vations.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"The only complication is that resampling that sentence boundaries are marked with a distin- a tag changes the identity of three trigrams at once, guished tag.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"For a model with T possible tags, each and we must account for this in computing its condiof the transition distributions τ(t,t′) has T compo- tional distribution.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The sampling distribution for ti nents, and each of the output distributions ω(t) has is given in Figure 2.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Wt components, where Wt is the number of word In Bayesian statistical inference, multiple samples types that are permissible outputs for tag t. We will from the posterior are often used in order to obtain use τ and ω to refer to the entire transition and out- statistics such as the expected values of model variput parameter sets.","{'title': '2 A Bayesian HMM', 'number': '2'}"
This model assumes that the ables.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"For POS tagging, estimates based on multiprior over state transitions is the same for all his- ple samples might be useful if we were interested in, tories, and the prior over output distributions is the for example, the probability that two words have the same for all states.","{'title': '2 A Bayesian HMM', 'number': '2'}"
We relax the latter assumption in same tag.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"However, computing such probabilities Section 4. across all pairs of words does not necessarily lead to Under this model, Equation 5 gives us a consistent clustering, and the result would be diffin(ti−2,ti−1,ti) + α cult to evaluate.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Using a single sample makes stanP(ti|t−i, α) = (6) dard evaluation methods possible, but yields subn(ti−2,ti−1) + Tα optimal results because the value for each tag is samn(ti,wi) + β pled from a distribution, and some tags will be asP(wi|ti, t−i,w−i,β) = (7) signed low-probability values.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Our solution is to n(ti) + Wtiβ treat the Gibbs sampler as a stochastic search prowhere n(ti−2,ti−1,ti) and n(ti,wi) are the number of cedure with the goal of identifying the MAP tag seoccurrences of the trigram (ti−2,ti−1,ti) and the quence.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"This can be done using tempering (annealtag-word pair (ti, wi) in the i — 1 previously gener- ing), where a temperature of φ is equivalent to raisated tags and words.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Note that, by integrating out ing the probabilities in the sampling distribution to the parameters τ and ω, we induce dependencies the power of 1 φ.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"As φ approaches 0, even a single between the variables in the model.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The probabil- sample will provide a good MAP estimate. ity of generating a particular trigram tag sequence 3 Fixed Hyperparameter Experiments (likewise, output) depends on the number of times 3.1 Method that sequence (output) has been generated previ- Our initial experiments follow in the tradition begun ously.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Importantly, trigrams (and outputs) remain by Merialdo (1994), using a tag dictionary to conexchangeable: the probability of a set of trigrams strain the possible parts of speech allowed for each (outputs) is the same regardless of the order in which word.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"(This also fixes Wt, the number of possible it was generated.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The property of exchangeability is words for tag t.) The dictionary was constructed by crucial to the inference algorithm we describe next. listing, for each word, all tags found for that word in 747 the entire WSJ treebank.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"For the experiments in this section, we used a 24,000-word subset of the treebank as our unlabeled training corpus.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"54.5% of the tokens in this corpus have at least two possible tags, with the average number of tags per token being 2.3.","{'title': '2 A Bayesian HMM', 'number': '2'}"
We varied the values of the hyperparameters α and Q and evaluated overall tagging accuracy.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Where direct comparison is possible, we list the scores reported by Smith and Eisner (2005) for their conditional random field model trained using contrastive estimation (CRF/CE).2 For all experiments, we ran our Gibbs sampling algorithm for 20,000 iterations over the entire data set.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Since our inference procedure is stochastic, our reported results are an average over 5 independent runs.","{'title': '2 A Bayesian HMM', 'number': '2'}"
Results from our model for a range of hyperparameters are presented in Table 1.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"With the best choice of hyperparameters (α = .003, Q = 1), we achieve average tagging accuracy of 86.8%.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"This far surpasses the MLHMM performance of 74.5%, and is closer to the 90.1% accuracy of CRF/CE on the same data set using oracle parameter selection.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"The effects of α, which determines the probabil2Results of CRF/CE depend on the set of features used and the contrast neighborhood.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"In all cases, we list the best score reported for any contrast neighborhood using trigram (but no spelling) features.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"To ensure proper comparison, all corpora used in our experiments consist of the same randomized sets of sentences used by Smith and Eisner.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Note that training on sets of contiguous sentences from the beginning of the treebank consistently improves our results, often by 1-2 percentage points or more.","{'title': '2 A Bayesian HMM', 'number': '2'}"
MLHMM scores show less difference between randomized and contiguous corpora.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
BHMM as a function of the hyperparameters α and Q.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
Results are averaged over 5 runs on the 24k corpus with full tag dictionary.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"Standard deviations in most cases are less than .5. ity of the transition distributions, are stronger than the effects of Q, which determines the probability of the output distributions.","{'title': '2 A Bayesian HMM', 'number': '2'}"
The optimal value of .003 for α reflects the fact that the true transition probability matrix for this corpus is indeed sparse.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"As α grows larger, the model prefers more uniform transition probabilities, which causes it to perform worse.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Although the true output distributions tend to be sparse as well, the level of sparseness depends on the tag (consider function words vs. content words in particular).","{'title': '2 A Bayesian HMM', 'number': '2'}"
"Therefore, a value of Q that accurately reflects the most probable output distributions for some tags may be a poor choice for other tags.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"This leads to the smaller effect of Q, and suggests that performance might be improved by selecting a different Q for each tag, as we do in the next section.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"A final point worth noting is that even when α = Q = 1 (i.e., the Dirichlet priors exert no influence) the BHMM still performs much better than the MLHMM.","{'title': '2 A Bayesian HMM', 'number': '2'}"
"This result underscores the importance of integrating over model parameters: the BHMM identifies a sequence of tags that have high probability over a range of parameter values, rather than choosing tags based on the single best set of parameters.","{'title': '2 A Bayesian HMM', 'number': '2'}"
The improved results of the BHMM demonstrate that selecting a sequence that is robust to variations in the parameters leads to better performance.,"{'title': '2 A Bayesian HMM', 'number': '2'}"
"In our initial experiments, we experimented with different fixed values of the hyperparameters and reported results based on their optimal values.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
"However, choosing hyperparameters in this way is timeconsuming at best and impossible at worst, if there is no gold standard available.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
"Luckily, the Bayesian approach allows us to automatically select values for the hyperparameters by treating them as additional variables in the model.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
"We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
"Informally, to update the value of hyperparameter α, we sample a proposed new value α′ from a normal distribution with p = α and a = .1α.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
"The probability of accepting the new value depends on the ratio between P(t|w, α) and P(t|w, α′) and a term correcting for the asymmetric proposal distribution.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution.,"{'title': '4 Hyperparameter Inference', 'number': '3'}"
"In the experiments reported in the following section, we used two different versions of our model.","{'title': '4 Hyperparameter Inference', 'number': '3'}"
The first version (BHMM1) uses a single value of Q for all word classes (as above); the second version (BHMM2) uses a separate Qj for each tag class j.,"{'title': '4 Hyperparameter Inference', 'number': '3'}"
"In this set of experiments, we used the full tag dictionary (as above), but performed inference on the hyperparameters.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Following Smith and Eisner (2005), we trained on four different corpora, consisting of the first 12k, 24k, 48k, and 96k words of the WSJ corpus.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"For all corpora, the percentage of ambiguous tokens is 54%-55% and the average number of tags per token is 2.3.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
Table 2 shows results for the various models and a random baseline (averaged by the various models on different sized corpora.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
BHMM1 and BHMM2 use hyperparameter inference; CRF/CE uses parameter selection based on an unlabeled development set.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
Standard deviations (a) for the BHMM results fell below those shown for each corpus size. over 5 random tag assignments).,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Hyperparameter inference leads to slightly lower scores than are obtained by oracle hyperparameter selection, but both versions of BHMM are still far superior to MLHMM for all corpus sizes.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Not surprisingly, the advantages of BHMM are most pronounced on the smallest corpus: the effects of parameter integration and sensible priors are stronger when less evidence is available from the input.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"In the limit as corpus size goes to infinity, the BHMM and MLHMM will make identical predictions.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"In unsupervised learning, it is not always reasonable to assume that a large tag dictionary is available.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"To determine the effects of reduced or absent dictionary information, we ran a set of experiments inspired by those of Smith and Eisner (2005).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"First, we collapsed the set of 45 treebank tags onto a smaller set of 17 (the same set used by Smith and Eisner).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"We created a full tag dictionary for this set of tags from the entire treebank, and also created several reduced dictionaries.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Each reduced dictionary contains the tag information only for words that appear at least d times in the training corpus (the 24k corpus, for these experiments).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
All other words are fully ambiguous between all 17 classes.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"We ran tests with d = 1, 2, 3, 5, 10, and oc (i.e., knowledge-free syntactic clustering).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"With standard accuracy measures, it is difficult to variation of information between clusterings induced by the assigned and gold standard tags as the amount of information in the dictionary is varied.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
Standard deviations (Q) for the BHMM results fell below those shown in each column.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"The percentage of ambiguous tokens and average number of tags per token for each value of d is also shown. evaluate the quality of a syntactic clustering when no dictionary is used, since cluster names are interchangeable.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"We therefore introduce another evaluation measure for these experiments, a distance metric on clusterings known as variation of information (Meilˇa, 2002).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"The variation of information (VI) between two clusterings C (the gold standard) and C′ (the found clustering) of a set of data points is a sum of the amount of information lost in moving from C to C′, and the amount that must be gained.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"It is defined in terms of entropy H and mutual information I: V I(C, C′) = H(C) + H(C′) − 2I(C, C′).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Even when accuracy can be measured, VI may be more informative: two different tag assignments may have the same accuracy but different VI with respect to the gold standard if the errors in one assignment are less consistent than those in the other.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
Table 3 gives the results for this set of experiments.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"One or both versions of BHMM outperform MLHMM in terms of tag accuracy for all values of d, although the differences are not as great as in earlier experiments.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"The differences in VI are more striking, particularly as the amount of dictionary information is reduced.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"When ambiguity is greater, both versions of BHMM show less confusion with respect to the true tags than does MLHMM, and BHMM2 performs the best in all circumstances.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
The confusion matrices in Figure 3 provide a more intuitive picture of the very different sorts of clusterings produced by MLHMM and BHMM2 when no tag dictionary is available.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
Similar differences hold to a lesser degree when a partial dictionary is provided.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"With MLHMM, different tokens of the same word type are usually assigned to the same cluster, but types are assigned to clusters more or less at random, and all clusters have approximately the same number of types (542 on average, with a standard deviation of 174).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"The clusters found by BHMM2 tend to be more coherent and more variable in size: in the 5 runs of BHMM2, the average number of types per cluster ranged from 436 to 465 (i.e., tokens of the same word are spread over fewer clusters than in MLHMM), with a standard deviation between 460 and 674.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Determiners, prepositions, the possessive marker, and various kinds of punctuation are mostly clustered coherently.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Nouns are spread over a few clusters, partly due to a distinction found between common and proper nouns.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Likewise, modal verbs and the copula are mostly separated from other verbs.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Errors are often sensible: adjectives and nouns are frequently confused, as are verbs and adverbs.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"The kinds of results produced by BHMM1 and BHMM2 are more similar to each other than to the results of MLHMM, but the differences are still informative.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"Recall that BHMM1 learns a single value for Q that is used for all output distributions, while BHMM2 learns separate hyperparameters for each cluster.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
This leads to different treatments of difficult-to-classify low-frequency items.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"In BHMM1, these items tend to be spread evenly among all clusters, so that all clusters have similarly sparse output distributions.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"In BHMM2, the system creates one or two clusters consisting entirely of very infrequent items, where the priors on these clusters strongly prefer uniform outputs, and all other clusters prefer extremely sparse outputs (and are more coherent than in BHMM1).","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"This explains the difference in VI between the two systems, as well as the higher accuracy of BHMM1 for d > 3: the single Q discourages placing lowfrequency items in their own cluster, so they are more likely to be clustered with items that have similar transition probabilities.","{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
The problem of junk clusters in BHMM2 might be alleviated by using a non-uniform prior over the hyperparameters to encourage some degree of sparsity in all clusters.,"{'title': '5 Inferred Hyperparameter Experiments', 'number': '4'}"
"In this paper, we have demonstrated that, for a standard trigram HMM, taking a Bayesian approach to POS tagging dramatically improves performance over maximum-likelihood estimation.","{'title': '6 Conclusion', 'number': '5'}"
Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.,"{'title': '6 Conclusion', 'number': '5'}"
"The Bayesian approach is particularly helpful when learning is less constrained, either because less data is available or because dictionary information is limited or absent.","{'title': '6 Conclusion', 'number': '5'}"
"For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.","{'title': '6 Conclusion', 'number': '5'}"
We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.,"{'title': '6 Conclusion', 'number': '5'}"
