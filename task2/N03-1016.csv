col1,col2
We present an extension of the classic A* search procedure to tabular PCFG parsing.,{}
The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.,{}
We discuss various estimates and give efficient algorithms for computing them.,{}
"On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.",{}
"Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.",{}
"Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.",{}
PCFG parsing algorithms with worst-case cubic-time bounds are well-known.,"{'title': '1 Introduction', 'number': '1'}"
"However, when dealing with wide-coverage grammars and long sentences, even cubic algorithms can be far too expensive in practice.","{'title': '1 Introduction', 'number': '1'}"
Two primary types of methods for accelerating parse selection have been proposed.,"{'title': '1 Introduction', 'number': '1'}"
"Roark (2001) and Ratnaparkhi (1999) use a beam-search strategy, in which only the best n parses are tracked at any moment.","{'title': '1 Introduction', 'number': '1'}"
"Parsing time is linear and can be made arbitrarily fast by reducing n. This is a greedy strategy, and the actual Viterbi (highestprobability) parse can be pruned from the beam because, while it is globally optimal, it may not be locally optimal at every parse stage.","{'title': '1 Introduction', 'number': '1'}"
"Chitrao and Grishman (1990), Caraballo and Charniak (1998), Charniak et al. (1998), and Collins (1999) describe best-first parsing, which is intended for a tabular item-based framework.","{'title': '1 Introduction', 'number': '1'}"
"In best-first parsing, one builds a figure-of-merit (FOM) over parser items, and uses the FOM to decide the order in which agenda items should be processed.","{'title': '1 Introduction', 'number': '1'}"
"This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound).","{'title': '1 Introduction', 'number': '1'}"
We discuss best-first parsing further in section 3.3.,"{'title': '1 Introduction', 'number': '1'}"
Both of these speed-up techniques are based on greedy models of parser actions.,"{'title': '1 Introduction', 'number': '1'}"
"The beam search greedily prunes partial parses at each beam stage, and a best-first FOM greedily orders parse item exploration.","{'title': '1 Introduction', 'number': '1'}"
"If we wish to maintain optimality in a search procedure, the obvious thing to try is A* methods (see for example Russell and Norvig, 1995).","{'title': '1 Introduction', 'number': '1'}"
"We apply A* search to a tabular itembased parser, ordering the parse items based on a combination of their known internal cost of construction and a conservative estimate of their cost of completion (see figure 1).","{'title': '1 Introduction', 'number': '1'}"
"A* search has been proposed and used for speech applications (Goel and Byrne, 1999, Corazza et al., 1994); however, it has been little used, certainly in the recent statistical parsing literature, apparently because of difficulty in conceptualizing and computing effective admissible estimates.","{'title': '1 Introduction', 'number': '1'}"
"The contribution of this paper is to demonstrate effective ways of doing this, by precomputing grammar statistics which can be used as effective A* estimates.","{'title': '1 Introduction', 'number': '1'}"
The A* formulation provides three benefits.,"{'title': '1 Introduction', 'number': '1'}"
"First, it substantially reduces the work required to parse a sentence, without sacrificing either the optimality of the answer or the worst-case cubic time bounds on the parser.","{'title': '1 Introduction', 'number': '1'}"
"Second, the resulting parser is structurally simpler than a FOM-driven best-first parser.","{'title': '1 Introduction', 'number': '1'}"
"Finally, it allows us to easily prove the correctness of our algorithm, over a broad range of control strategies and grammar encodings.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we describe two methods of constructing A* bounds for PCFGs.","{'title': '1 Introduction', 'number': '1'}"
"One involves context summarization, which uses estimates of the sort proposed in Corazza et al. (1994), but considering richer summaries.","{'title': '1 Introduction', 'number': '1'}"
"The other involves grammar summarization, which, to our knowledge, is entirely novel.","{'title': '1 Introduction', 'number': '1'}"
"We present the estimates that we use, along with algorithms to efficiently calculate them, and illustrate their effectiveness in a tabular PCFG parsing algorithm, applied to Penn Treebank sentences.","{'title': '1 Introduction', 'number': '1'}"
"An agenda-based PCFG parser operates on parse items called edges, such as NP:[0,2], which denote a grammar symbol over a span.","{'title': '2 An A* Algorithm', 'number': '2'}"
"The parser maintains two data structures: a chart or table, which records edges for which (best) parses have already been found, and an agenda of newly-formed edges waiting to be processed.","{'title': '2 An A* Algorithm', 'number': '2'}"
The core loop involves removing an edge from the agenda and combining that edge with edges already in the chart to create new edges.,"{'title': '2 An A* Algorithm', 'number': '2'}"
"For example, NP:[0,2] might be removed from the agenda, and, if there were a rule S → NP VP and VP:[2,8] was already entered into the chart, the edge S:[0,8] would be formed, and added to the agenda if it were not in the chart already.","{'title': '2 An A* Algorithm', 'number': '2'}"
"The way an A* parser differs from a classic chart parser is that, like a best-first parser, agenda edges are processed according to a priority.","{'title': '2 An A* Algorithm', 'number': '2'}"
"In best-first parsing, this priority is called a figure-of-merit (FOM), and is based on various approximations to P(e|s), the fraction of parses of a sentence s which include an edge e (though see Goodman (1997) for an alternative notion of FOM).","{'title': '2 An A* Algorithm', 'number': '2'}"
Edges which seem promising are explored first; others can wait on the agenda indefinitely.,"{'title': '2 An A* Algorithm', 'number': '2'}"
"Note that even if we did know P(e|s) exactly, we still would not know whether e occurs in any best parse of s. Nonetheless, good FOMs empirically lead quickly to good parses.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Best-first parsing aims to find a (hopefully good) parse quickly, but gives no guarantee that the first parse discovered is the Viterbi parse, nor does it allow one to recognize the Viterbi parse when it is found.","{'title': '2 An A* Algorithm', 'number': '2'}"
"In A* parsing, we wish to construct priorities which will speed up parsing, yet still guarantee optimality (that the first parse returned is indeed a best parse).","{'title': '2 An A* Algorithm', 'number': '2'}"
"With a categorical CFG chart parser run to exhaustion, it does not matter in what order one removes edges from the agenda; all edges involved in full parses of the sentence will be constructed at some point.","{'title': '2 An A* Algorithm', 'number': '2'}"
"A cubic time bound follows straightforwardly by simply testing for edge existence, ensuring that we never process an edge twice.","{'title': '2 An A* Algorithm', 'number': '2'}"
"With PCFG parsing, there is a subtlety involved.","{'title': '2 An A* Algorithm', 'number': '2'}"
"In addition to knowing whether edges can be constructed, we also want to know the scores of edges’ best parses.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Therefore, we record estimates of best-parse scores, updating them as better parses are found.","{'title': '2 An A* Algorithm', 'number': '2'}"
"If, during parsing, we find a new, better way to construct some edge e that has previously been entered into the chart, we may also have found a better way to construct any edges which have already been built using e. Best-first parsers deal with this by allowing an upward propagation, which updates such edges’ scores (Caraballo and Charniak, 1998).","{'title': '2 An A* Algorithm', 'number': '2'}"
"If run to exhaustion, all edges’ Viterbi scores will be correct, but the propagation destroys the cubic time bound of the parser, since in effect each edge can be processed many times.","{'title': '2 An A* Algorithm', 'number': '2'}"
"In order to ensure optimality, it is sufficient that, for any edge e, all edges f which are contained in a best parse of e get removed from the agenda before e itself does.","{'title': '2 An A* Algorithm', 'number': '2'}"
"If we have an edge priority which ensures this ordering, we can avoid upward propagation entirely (and omit the data structures involved in it) and still be sure that each edge leaves the agenda scored correctly.","{'title': '2 An A* Algorithm', 'number': '2'}"
"If the grammar happens to be in CNF, one way to do this is to give shorter spans higher priority than longer ones; this priority essentially gives the CKY algorithm.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Formally, assume we have a PCFG G and a sentence s = 0wn (we place indices as fenceposts between words).","{'title': '2 An A* Algorithm', 'number': '2'}"
"An inside parse of an edge e = X:[i, j] is a derivation in G from X to iwj.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Let OG(e, s) denote the log-probability of a best inside parse of e (its Viterbi inside score).1 We will drop the G, s, and even e when context permits.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Our parser, like a best-first parser, maintains estimates b(e, s) of O(e, s) which begin at −oo, only increase over time, and always represent the score of the best parses of their edges e discovered so far.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Optimality means that for any e, b(e, s) will equal OG(e, s) when e is removed from the agenda.","{'title': '2 An A* Algorithm', 'number': '2'}"
"If one uses b(e, s) to prioritize edges, we show in Klein and Manning (2001a), that the parser is optimal over arbitrary PCFGs, and a wide range of control strategies.","{'title': '2 An A* Algorithm', 'number': '2'}"
"This is proved using an extension of Dijkstra’s algorithm to a certain kind of hypergraph associated with parsing, shown in figure 1(b): parse items are nodes in the hypergraph, hyperarcs take sets of parse items to their result item, and hyperpaths map to parses.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Reachability from start corresponds to parseability, and shortest paths to Viterbi parses.","{'title': '2 An A* Algorithm', 'number': '2'}"
"The hypergraph shown in figure 1(b) shows a parse of the goal S:[0,3] which includes NP:[0,2].2 This parse can be split into an inside portion (solid lines) and an outside portion (dashed lines), as indicated in figure 1(a).","{'title': '2 An A* Algorithm', 'number': '2'}"
"The outside portion is an outside parse: formally, an outside parse of an edge X:[i, j] in sentence s = 0wn is a derivation from G’s root symbol to w0iXwjn.","{'title': '2 An A* Algorithm', 'number': '2'}"
"We use αG(e, s) to denote the score of a best outside parse of e. Using b(e, s) as the edge priority corresponds to a generalization of uniform cost search on graphs (Russell and Norvig, 1995).","{'title': '2 An A* Algorithm', 'number': '2'}"
"In the analogous generalization of A* search, we add to b(e, s) an estimate a(e, s) of the competion cost αG(e, s) (the cost of the dashed outside parse) to focus exploration on regions of the graph which appear to have good total cost.","{'title': '2 An A* Algorithm', 'number': '2'}"
A* search is correct as long as the estimate a satisfies two conditions.,"{'title': '2 An A* Algorithm', 'number': '2'}"
"First, it must be admissible, meaning that it must not underestimate the actual log-probability required to complete the parse.","{'title': '2 An A* Algorithm', 'number': '2'}"
"Second, it must be monotonic, meaning that as one builds up a tree, the combined log-probability β + a never increases.","{'title': '2 An A* Algorithm', 'number': '2'}"
"The proof of this is very similar to the proof of the uniform-cost case in Klein and Manning (2001a), and so we omit it for space reasons (it can be found in Klein and Manning, 2002).","{'title': '2 An A* Algorithm', 'number': '2'}"
"Concretely, we can use b + a as the edge priority, provided a is an admissible, monotonic estimate of α.","{'title': '2 An A* Algorithm', 'number': '2'}"
"We will still have a correct algorithm, and even rough heuristics can dramatically cut down the number of edges processed (and therefore total work).","{'title': '2 An A* Algorithm', 'number': '2'}"
"We next discuss several estimates, describe how to compute them efficiently, and show the edge savings when parsing Penn treebank WSJ sentences.","{'title': '2 An A* Algorithm', 'number': '2'}"
"When parsing with a PCFG G, each edge e = X:[i, j] spans some interval [i, j] of the sentence and is labeled 2The example here shows a bottom-up construction of a parse tree.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"However, the present algorithm and estimates work just as well for top-down chart parsing, given suitable active items as nodes; see (Klein and Manning, 2001a). by some grammar symbol (or state) X.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Our presentation assumes that G is a binarized grammar, and so in general X may be either a complete state like NP that was in an original n-ary grammar, or an intermediate state, like an Earley dotted rule, that is the result of implicit or explicit grammar binarization.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For the edge e, its yield in s = 0wn is the sequence of terminals that it spans (iwj).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Its context is its state X along with the rest of the terminals of sentence (0wiXjwn).,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Scores are logprobabilities; lower cost is higher log-probability.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"So, ‘>’ or ‘better’ will mean higher log-probability.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"One way to construct an admissible estimate is to summarize the context in some way, and to find the score of the best parse of any context that fits that summary.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Let c(e, s) be the context of e in s. Let u be a summary function of contexts.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"We can then use the context summary estimate: That is, we return the exact Viterbi outside score for some context, generally not the actual context, whose summary matches the actual one’s summary.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If the number of summaries is reasonable, we can precompute and store the estimate for each summary once and for all, then retrieve them in constant time per edge at parse time.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If we give no information in the summary, the estimate will be constantly 0.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This is the trivial estimate NULL, and corresponds to simply using inside estimates b alone as priorities.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"On the other extreme, if each context had a unique summary, then a(e, s) would be αG(e, s) itself.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This is the ideal estimate, which we call TRUE.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In practice, of course, precomputing TRUE would not be feasible.3 3Note that our ideal estimate is not P(els) like the ideal FOM, rather it is P(T9,e)/P(Te) (where T9,e is a best parse of the goal g among those which contain e, and Te is a best parse of e over the yield of e).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"That is, we are not estimating parser choice probabilities, but parse tree probabilities. αG(e', s') ≥ αG(e, s) We used various intermediate summaries, some illustrated in figure 2.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"S1 specifies only the total number of words outside e, while S specifies separately the number to the left and right.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
SX also specifies e’s label.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
SXL and SXR add the tags adjacent to e on the left and right respectively.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"S1XLR includes both the left and right tags, but merges the number of words to the left and right.4 As the summaries become richer, the estimates become sharper.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"As an example, consider an NP in the context “VBZ NP , PRP VBZ DT NN .” shown in figure 2.5 The summary SX reveals only that there is an NP with 1 word to the left and 6 the right, and gives an estimate of −11.3.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
This score is backed by the concrete parse shown in figure 2(a).,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This is a best parse of a context compatible with what little we specified, but very optimistic.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
It assumes very common tags in very common patterns.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"SXL adds that the tag to the left is VBZ, and the hypothesis that the NP is part of a sentence-initial PP must be abandoned; the best score drops to −13.9, backed by the parse in figure 2(b).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Specifying the right tag to be “,” drops the score further to −15.1, given by figure 2(c).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The actual best parse is figure 2(d), with a score of −18.1.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"These estimates are similar to quantities calculated in Corazza et al. (1994); in that work, they are interested in the related problem of finding best completions for strings which contain gaps.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For the SX estimate, for example, the string would be the edge’s label and two (fixed-length) gaps.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"They introduce quantities essentially the same as our SX estimate to fill gaps, and their oneword update algorithms are similarly related to those we use here.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The primary difference here is in the application of these quantities, not their calculation.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The context summary estimates described above use local information, combined with span sizes.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This gives the effect that, for larger contexts, the best parses which back the estimates will have less and less to do with the actual contexts (and hence will become increasingly optimistic).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Context summary estimates do not pin down the exact context, but do use the original grammar G. For grammar projection estimates, we use the exact context, but project the grammar to some G' which is so much simpler that it is feasible to first exhaustively parse with G' and then use the result to guide the search in the full grammar G. Formally, we have a projection π which maps grammar states of G (that is, the dotted rules of an Earley-style parser) to some reduced set.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
This projection of states induces a projection of rules.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If a set R = {r} of rules in G collide as the rule r' in G', we give r' the probability P(r') = maxIER P(r).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Note that the resulting grammar G' will not generally be a proper PCFG; it may assign more than probability 1 to the set of trees it generates.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In fact, it will usually assign infinite mass.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"However, all that matters for our purposes is that every tree in G projects under π to a tree in G' with the same or higher probability, which is true because every rule in G does.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Therefore, we know that αG(e, s) < αG1(e, s).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If G' is much more compact than G, for each new sentence s, we can first rapidly calculate aπ = αG1 for all edges, then parse with G. The identity projection t returns G and therefore aι is TRUE.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"On the other extreme, a constant projection gives NULL (if any rewrite has probability 1).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In between, we tried three other grammar projection estimates (examples in figure 3).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"First, consider mapping all terminal states to a single terminal token, but not altering the grammar in any other way.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If we do this projection, then we get the SX estimate from the last section (collapsing the terminals together effectively hides which terminals are in the context, but not their number).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"However, the resulting grammar is nearly as large as G, and therefore it is much more efficient to use the precomputed context summary formulation.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Second, for the projection XBAR, we tried collapsing all the incomplete states of each complete state to a single state (so NP→ · CC NP and NP→ · PP would both become NP').","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This turned out to be ineffective, since most productions then had merged probability 1.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For our current grammar, the best estimate of this type was one we called F, for filter, which collapsed all complete (passive) symbols together, but did not collapse any terminal symbols.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"So, for example, a state like NP→ · CC NP CC NP would become X→ · CC X CC X (see section 3.3 for a description of our grammar encodings).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
This estimate has an interesting behavior which is complementary to the context summary estimates.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"It does not indicate well when an edge would be moderately expensive to integrate into a sentence, but it is able to completely eliminate certain edges which are impossible to integrate into a full parse (for example in this case maybe the two CC tags required to complete the NP are not present in the future context).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
A close approximation to the F estimate can also be computed online especially quickly during parsing.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Since we are parsing with the Penn treebank covering grammar, almost any (phrasal) non-terminal can be built over almost any span.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"As discussed in Klein and Manning (2001b), the only source of constraint on what edges can be built where is the tags in the rules.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Therefore, an edge with a label like NP— · CC NP CC NP can essentially be built whenever (and only whenever) two CC tags are in the edge’s right context, one of them being immediately to the right.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"To the extent that this is true, F can be approximated by simply scanning for the tag configuration required by a state’s local rule, and returning 0 if it is present and —oo otherwise.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
This is the method we used to implement F; exactly parsing with the projected grammar was much slower and did not result in substantial improvement.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"It is worth explicitly discussing how the F estimate differs from top-down grammar-driven filtering standardly used by top-down chart parsers; in the treebank grammar, there is virtually no top-down filtering to be exploited (again, see Klein and Manning (2001b)).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In a left-to-right parse, top-down filtering is a prefix licensing condition; F is more of a sophisticated lookahead condition on suffixes.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
The relationships between all of these estimates are shown in figure 4.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
The estimates form a join lattice (figure 4(a)): adding context information to a merged context estimate can only sharpen the individual outside estimates.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In this sense, for example S � SX.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
The lattice top is TRUE and the bottom is NULL.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In addition, the minimum (U) of a set of admissible estimates is still an admissible estimate.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"We can use this to combine our basic estimates into composite estimates: SXMLR = U (SXL, SXR) will be valid, and a better estimate than either SXL or SXR individually.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Similarly, B is U (SXMLR, SIXLR).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"There are other useful grammar projections, which are beyond the scope of this paper.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"First, much recent statistical parsing work has gotten value from splitting grammar states, such as by annotating nodes with their parent and even grandparent categories (Johnson, 1998).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This annotation multiplies out the state space, giving a much larger grammar, and projecting back to the unannotated state set can be used as an outside estimate.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Second, and perhaps more importantly, this technique can be applied to lexical parsing, where the state projections are onto the delexicalized PCFG symbols and/or onto the word-word dependency structures.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
This is particularly effective when the tree model takes a certain factored form; see Klein and Manning (2003) for details.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Following (Charniak et al., 1998), we parsed unseen sentences of length 18–26 from the Penn Treebank, using the grammar induced from the remainder of the treebank.6 We tried all estimates described above.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Rules were encoded as both inside (I) and outside (O) tries, shown in figure 5.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Such an encoding binarizes the grammar, and compacts it.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"I-tries are as in Charniak et al. (1998), where NP— DT JJ NN becomes NP — XDT JJ NN and XDT JJ — DT JJ, and correspond to dropping the portion of an Earley dotted rule after the dot.7 O-tries, as in Leermakers (1992), turn NP— DT JJ NN into NP — XNP→ · NN NN and XNP→ · NN — DT JJ, and correspond to dropping the portion which precedes the dot.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Figure 6 shows the overall savings for several estimates of each type.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The I-tries were superior for the coarser estimates, while O-tries were superior for the finer estimates.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In addition, only O-tries permit the accelerated version of F, since they explicitly declare their right requirements.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Additionally, with I-tries, only the top-level intermediate rules have probability less than 1, while for O-tries, one can back-weight probability as in (Mohri, 1997), also shown in figure 5, enabling sub-parts of rare rules to be penalized even before they are completed.8 For all subsequent results, we discuss only the O-trie numbers.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Figure 8 lists the overall savings for each context summary estimate, with and without F joined in.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"We see that the NULL estimate (i.e., uniform cost search) is not very effective – alone it only blocks 11% of the edges.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"But it is still better than exhaustive parsing: with it, one stops parsing when the best parse is found, while in exhaustive parsing one continues until no edges remain.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Even the simplest non-trivial estimate, S, blocks 40% of the edges, and the best estimate BF blocks over 97% of the edges, a speed-up of over 35 times, without sacrificing optimality or algorithmic complexity.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For comparison to previous FOM work, figure 7 shows, for an edge count and an estimate, the proportion of sentences for which a first parse was found using at most that many edges.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"To situate our results, the FOMs used by (Caraballo and Charniak, 1998) require 10K edges to parse 96% of these sentences, while BF requires only 6K edges.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"On the other hand, the more complex, tuned FOM in (Charniak et al., 1998) is able to parse all of these sentences using around 2K edges, while BF requires 7K edges.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Our estimates do not reduce the total edge count quite as much as the best FOMs can, but they are in the same range.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This is as much as one could possibly expect, since, crucially, our first parses are al8However, context summary estimates which include the state compensate for this automatically. ways optimal, while the FOM parses need not be (and indeed sometimes are not).9 Also, our parser never needs to propagate score changes upwards, and so may be expected to do less work overall per edge, all else being equal.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This savings is substantial, even if no propagation is done, because no data structure needs to be created to track the edges which are supported by each given edge (for us, this represents a factor of approximately 2 in memory savings).","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Moreover, the context summary estimates require only a single table lookup per edge, while the accelerated version of F requires only a rapid quadratic scan of the input per sentence (less than 1% of parse time per sentence), followed by a table lookup per edge.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The complex FOMs in (Charniak et al., 1998) require somewhat more online computation to assemble.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
It is interesting that SXR is so much more effective than SXL; this is primarily because of the way that the rules have been encoded.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"If we factor the rules in the other direction, we get the opposite effect.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Also, when combined with F, the difference in their performance drops from 10.3% to 0.8%; F is a right-filter and is partially redundant when added to SXR, but is orthogonal to SXL.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"A disadvantage of admissibility for the context summary estimates is that, necessarily, they are overly optimistic as to the contents of the outside context.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The larger the outside context, the farther the gap between the true cost and the estimate.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Figure 9 shows average outside estimates for Viterbi edges as span size increases.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For small outside spans, all estimates are fairly good approximations of TRUE.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"As the span increases, the approximations fall behind.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Beyond the smallest outside spans, all of the curves are approximately linear, but the actual value’s slope is roughly twice that of the estimates.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The gap between our empirical methods and the true cost grows fairly steadily, but the differences between the empirical methods themselves stay relatively constant.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This reflects the nature of these estimates: they have differing local information in their summaries, but all are equally ignorant about the more distant context elements.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The various local environments can be more or less costly to integrate into a parse, but, within a few words, the local restrictions have been incorporated one way or another, and the estimates are all free to be equally optimistic about the remainder of the context.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The cost to “package up” the local restrictions creates their constant differences, and the shared ignorance about the wider context causes their same-slope linear drop-off.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This suggests that it would be interesting to explore other, more global, notions of context.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"We do not claim that our context estimates are the best possible – one could hope to find features of the context, such as number of verbs to the right or number of unusual tags in the context, which would partition the contexts more effectively than adjacent tags, especially as the outside context grows in size.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
The amount of work required to (pre)calculate context summary estimates depends on how easy it is to efficiently take the max over all parses compatible with each context summary.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
The benefit provided by an estimate will depend on how well the restrictions in that summary nail down the important features of the full context.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Figure 10 shows recursive pseudocode for the SX estimate; the others are similar.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"To precalculate our A* estimates efficiently, we used a memoization approach rather than a dynamic programming approach.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"This resulted in code comparable in efficiency, but which was simpler to reason about, and, more importantly, allowed us to exploit sparseness when present.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"For example with left-factored trie encodings, 76% of (state, right tag) combinations are simply impossible.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
Tables which mapped arguments to returned results were used to memoize each procedure.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In our experiments, we forced these tables to be filled in a precomputation step, but depending on the situation it might be advantageous to allow them to fill as needed, with early parses proceeding slowly while the tables populate.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"With the optimal forward estimate TRUE, the actual distance to the closest goal, we would never expand edges other than those in best parses, but computing TRUE is as hard as parsing the sentence in the first place.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"On the other hand, no precomputation is needed for NULL.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
In between is a trade off of space/time requirements for precomputation and the online savings during the parsing of new sentences.,"{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Figure 8 shows the average savings versus the precomputation time.10 Where on this curve one chooses to be depends on many factors; 9 hours may be too much to spend computing B, but an hour for SXMLR gives nearly the same performance, and the one minute required for SX is comparable to the I/O time to read the Penn treebank in our system.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"The grammar projection estimate F had to be recomputed for each sentence parsed, but took less than 1% of the total parse time.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"Although this method alone was less effective than SX (only 58.3% edge savings), it was extremely effective in combination with the context summary methods.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"In practice, the combination of F and SX is easy to implement, fast to initialize, and very effective: one cuts out 95% of the work in parsing at the cost of one minute of precomputation and 5 Mb of storage for outside estimates for our grammar.","{'title': '3 A* Estimates for Parsing', 'number': '3'}"
"While the A* estimates given here can be used to accelerate PCFG parsing, most high-performance parsing has utilized models over lexicalized trees.","{'title': '4 Extension to Other Models', 'number': '4'}"
These A* methods can be adapted to the lexicalized case.,"{'title': '4 Extension to Other Models', 'number': '4'}"
"In Klein and Manning (2003), we apply a pair of grammar projection estimates to a lexicalized parsing model of a certain factored form.","{'title': '4 Extension to Other Models', 'number': '4'}"
"In that model, the score of a lexicalized tree is the product of the scores of two projections of that tree, one onto unlexicalized phrase structure, and one onto phrasalcategory-free word-to-word dependency structure.","{'title': '4 Extension to Other Models', 'number': '4'}"
"Since this model has a projection-based form, grammar projection methods are easy to apply and especially effective, giving over three orders of magnitude in edge savings.","{'title': '4 Extension to Other Models', 'number': '4'}"
"The total cost per sentence includes the time required for two exhaustive PCFG parses, after which the A* search takes only seconds, even for very long sentences.","{'title': '4 Extension to Other Models', 'number': '4'}"
"Even when a lexicalized model is not in this factored form, it still admits factored grammar projection bounds; we are currently investigating this case.","{'title': '4 Extension to Other Models', 'number': '4'}"
"An A* parser is simpler to build than a best-first parser, does less work per edge, and provides both an optimality guarantee and a worst-case cubic time bound.","{'title': '5 Conclusions', 'number': '5'}"
We have described two general ways of constructing admissible A* estimates for PCFG parsing and given several specific estimates.,"{'title': '5 Conclusions', 'number': '5'}"
"Using these estimates, our parser is capable of finding the Viterbi parse of an average-length Penn treebank sentence in a few seconds, processing less than 3% of the edges which would be constructed by an exhaustive parser.","{'title': '5 Conclusions', 'number': '5'}"
We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.,"{'title': 'Acknowledgements.', 'number': '6'}"
This paper is based on work supported by the National Science Foundation (NSF) under Grant No.,"{'title': 'Acknowledgements.', 'number': '6'}"
"IIS-0085896, by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program, by an NSF Graduate Fellowship to the first author, and by an IBM Faculty Partnership Award to the second author.","{'title': 'Acknowledgements.', 'number': '6'}"
