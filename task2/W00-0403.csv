col1,col2
"We present a multi-document summarizer, called MEAD, which generates summaries using cluster centroids produced by a topic detection and tracking system.",Introduction
"We also describe two new techniques, based on sentence utility and subsumption, which we have applied to the evaluation of both single and multiple document summaries.",Introduction
"Finally, we describe two user studies that test our models of multi-document summarization.",Introduction
"On October 12, 1999, a relatively small number of news sources mentioned in passing that Pakistani Defense Minister Gen. Pervaiz Musharraf was away visiting Sri Lanka.",Experiment/Discussion
"However, all world agencies would be actively reporting on the major events that were to happen in Pakistan in the following days: Prime Minister Nawaz Sharif announced that in Gen. Musharrafs absence, the Defense Minister had been -sacked and replaced by General Zia Addin.",Experiment/Discussion
"Large numbers of messages from various sources started to inundate the newswire: about the army's occupation of the capital, the Prime Minister's ouster and his subsequent placement under house arrest, Gen. Musharraf s return to his country, his ascendancy to power, and the imposition of military control over Pakistan.",Experiment/Discussion
The paragraph above summarizes a large amount of news from different sources.,Experiment/Discussion
"While it was not automatically generated, one can imagine the use of such automatically generated summaries.",Experiment/Discussion
In this paper we will describe how multi-document summaries are built and evaluated.,Experiment/Discussion
The process of identifying all articles on an emerging event is called Topic Detection and Tracking (TDT).,Experiment/Discussion
"A large body of research in TDT has been created over the past two years [Allan et al., 98].",Experiment/Discussion
"We will present an extension of our own research on TDT [Radev et al., 1999] to cover summarization of multidocument clusters.",Experiment/Discussion
"Our entry in the official TDT evaluation, called CIDR [Radev et al., 1999], uses modified TF*IDF to produce clusters of news articles on the same event.",Experiment/Discussion
"We developed a new technique for multi-document summarization (or MDS), called centroid-based summarization (CBS) which uses as input the centroids of the clusters produced by C1DR to identify which sentences are central to the topic of the cluster, rather than the individual articles.",Experiment/Discussion
"We have implemented CBS in a system, named MEAD.",Experiment/Discussion
"The main contributions of this paper are: the development of a centroid-based multi-document summarizer, the use of cluster-based sentence utility (CBSU) and cross-sentence informational subsumption (CSIS) for evaluation of single and multi-document summaries, two user studies that support our findings, and an evaluation of MEAD.",Experiment/Discussion
"An event cluster, produced by a TDT system, consists of chronologically ordered news articles from multiple sources, which describe an event as it develops over time.",Experiment/Discussion
Event clusters range from2 to 10 documents from which MEAD produces summaries in the form of sentence extracts.,Experiment/Discussion
"A key feature of MEAD is its use of cluster centroids, which consist of words which are central not only to one article in a cluster, but to all the articles.",Experiment/Discussion
"MEAD is significantly different from previous work on multi-document summarization [Radev & McKeown, 1998; Carbonell and Goldstein, 1998; Mani and Bloedorn, 1999; McKeown et al., 1999], which use techniques such as graph matching, maximal marginal relevance, or language generation.",Experiment/Discussion
"Finally, evaluation of multi-document summaries is a difficult problem.",Experiment/Discussion
There is not yet a widely accepted evaluation scheme.,Experiment/Discussion
"We propose a utility-based evaluation scheme, which can be used to evaluate both single-document and multi-document summaries.",Experiment/Discussion
"Cluster-based sentence utility (CBSU, or utility) refers to the degree of relevance (from 0 to 10) of a ' particular sentence to the general topic of the entire cluster (for a dis cussion of what is a topic, see [Allan et al. 1998]).",Experiment/Discussion
A utility of 0 means that the sentence is not relevant to the cluster and a 10 marks an essential sentence.,Experiment/Discussion
"A related notion to CBSU is cross-sentence informational subsumption (CSIS, or subsumption), which reflects that certain sentences repeat some of the information present in other sentences and may, therefore, be omitted during summarization.",Experiment/Discussion
"If the information content of sentence a (denoted as i(a)) is contained within sentence b, then a becomes informationally redundant and the content of b is said to subsume that of a: In the example below, (2) subsumes (1) because the crucial information in (1) is also included in (2) which presents additional content: &quot;the court&quot;, &quot;last August&quot;, and &quot;sentenced him to life&quot;.",Experiment/Discussion
The cluster shown in Figure I shows subsumption links across two articles about recent terrorist activities in Algeria (ALG 18853 and ALG 18854).,Experiment/Discussion
"An arrow from sentence A to sentence B indicates that the information content of A is subsumed by the information content of B. Sentences 2, 4, and 5 from the first article repeat the information from sentence The full text of these articles is shown in the Appendix.",Experiment/Discussion
"2 in the second article, while sentence 9 from the former article is later repeated in sentences 3 and 4 of the latter article.",Experiment/Discussion
Sentences subsuming each other are said to belong to the same equivalence class.,Experiment/Discussion
An equivalence class may contain more than two sentences within the same or different articles.,Experiment/Discussion
"In the following example, although sentences (3) and (4) are not exact paraphrases of each other, they can be substituted for each other without crucial loss of information and therefore belong to the same equivalence class, i.e. i(3) c 1(4) and i(4) c i(3).",Experiment/Discussion
In the user study section we will take a look at the way humans perceive CSIS and equivalence class.,Experiment/Discussion
Thursday that 18 decapitated bodies have been found by the authorities.,Experiment/Discussion
"Maximal marginal relevance (or MMR) is a technique similar to CSIS and was introduced in [Carbonell and Goldstein, 1998].",Experiment/Discussion
"In that paper, MMR is used to produce summaries of single documents that avoid redundancy.",Experiment/Discussion
The authors mention that their preliminary results indicate that multiple documents on the same topic also contain redundancy but they fall short of using MMR for multi-document summarization.,Experiment/Discussion
"Their metric is used as an enhancement to a query-based summary whereas CSIS is designed for query-independent (a.k.a., generic) summaries.",Experiment/Discussion
"We now describe the corpus used for the evaluation of MEAD, and later in this section we present MEAD's algorithm.",Experiment/Discussion
"AFP, UPI AFP, UPI AP, AFP AP, AFP, UPI AP, PRI, VOA AP, NYT Algerian terrorists threaten Belgium The FBI puts Osama bin Laden on the most wanted list Explosion in a Moscow apartment building (September 9, 1999) Explosion in a Moscow apartment building (September 13, 1999) General strike in Denmark Toxic spill in Spain For our experiments, we prepared a snail corpus consisting of a total of 558 sentences in 27 documents, organized in 6 clusters (Table 1), all extracted by CIDR.",Experiment/Discussion
Four of the clusters are from Usenet newsgroups.,Experiment/Discussion
The remaining two clusters are from the official TDT corpus2.,Experiment/Discussion
"Among the factors for our selection of clusters are: coverage of as many news sources as possible, coverage of both TDT and non-TDT data, coverage of different types of news (e.g., terrorism, internal affairs, and environment), and diversity in cluster sizes (in our case, from 2 to 10 articles).",Experiment/Discussion
"The test corpus is used in the evaluation in such a way that each cluster is summarized at 9 different compression rates, thus giving nine times as many sample points as one would expect from the size of the corpus.",Experiment/Discussion
"Table 2 shows a sample centroid, produced by CIDR [Radev et al., 1999] from cluster A.",Experiment/Discussion
The &quot;count&quot; column indicates the average number of occurrences of a word *across the entire cluster.,Experiment/Discussion
The IDF values were computed from the TDT corpus.,Experiment/Discussion
"A centroid, in this context, is a pseudo-document which consists of words which have Count*IDF scores above a predefined threshold in the documents that constitute the cluster.",Experiment/Discussion
"CIDR computes Count*IDF in an iterative fashion, updating its values as more articles are inserted in a given cluster.",Experiment/Discussion
We hypothesize that sentences that contain the words from the centroid are more indicative of the topic of the cluster.,Experiment/Discussion
"2 The selection of Cluster E is due to an idea by the participants in the Novelty Detection Workshop, led by James Allan.",Experiment/Discussion
MEAD decides which sentences to include in the extract by ranking them according to a set of parameters.,Experiment/Discussion
"The input to MEAD is a cluster of articles (e.g., extracted by CIDR) and a value for the compression rate r. For example, if the cluster contains a total of 50 sentences (n = 50) and the value of r is 20%, the output of MEAD will contain 10 sentences.",Experiment/Discussion
Sentences are laid in the same order as they appear in the original documents with documents ordered chronologically.,Experiment/Discussion
We benefit here from the time stamps associated with each document. where i (/ n) is the sentence number within the cluster.,Experiment/Discussion
"INPUT: Cluster of d documents 3 with n sentences (compression rate = r) 3 Note that currently, MEAD requires that sentence boundaries be marked.",Experiment/Discussion
The system performance S is one of the numbers6 described in the previous subsection.,Experiment/Discussion
"For {13), the value of S is 0.627 (which is lower than random).",Experiment/Discussion
"For {14}, S is 0.833, which is between R and J.",Experiment/Discussion
"In the example, only two of the six possible sentence selections, {14) and {24} are between R and J.",Experiment/Discussion
"Three others, {13}, (231, and {34) are below R. while {12} is better than J.",Experiment/Discussion
"To restrict system performance (mostly) between 0 and I, we use a mapping between R and J in such a way that when S = R, the normalized system performance, D, is equal to 0 and when S = J, D becomes I.",Experiment/Discussion
The corresponding linear function7 is: Figure 2 shows the mapping •between system performance S on the left (a) and normalized system performance D on the right (b).,Experiment/Discussion
"A small part of the 0I segment is mapped to the entire 0-1 segment; therefore the difference between two systems, performing at e.g., 0.785 and 0.812 can be significant!",Experiment/Discussion
Example: the normalized system performance for the {14) system then becomes (0.833 - 0.732) / (0.841 — 0.732) or 0.927.,Experiment/Discussion
"Since the score is close to 1, the {14) system is almost as good as the interjudge agreement.",Experiment/Discussion
"The normalized system performance for the {24} system is similarly (0.837 — 0.732) / (0.841 7 The formula is valid when J > R (that is, the judges agree among each other better than randomly).",Experiment/Discussion
— 0.732) or 0.963.,Experiment/Discussion
"Of the two systems, {24} outperforms {14).",Experiment/Discussion
"To use CSIS in the evaluation, we introduce a new parameter, E, which tells us how much to penalize a system that includes redundant information.",Experiment/Discussion
"In the example from Table 7 (arrows indicate subsumption), a summarizer with r = 20% needs to pick 2 out of 12 sentences.",Experiment/Discussion
Suppose that it picks 1/1 and 2/1 (in bold).,Experiment/Discussion
"If E = 1, it should get full credit of 20 utility points.",Experiment/Discussion
"If E = 0, it should get no credit for the second sentence as it is subsumed by the first sentence.",Experiment/Discussion
"By varying E between 0 and I, the evaluation may favor or ignore subsumption.",Experiment/Discussion
We ran two user experiments.,Experiment/Discussion
"First, six judges were each given six clusters and asked to ascribe an importance score from 0 to 10 to each sentence within a particular cluster.",Experiment/Discussion
"Next, five judges had to indicate for each sentence which other sentence(s), if any, it subsumes 8.",Experiment/Discussion
"Using the techniques described in Section 0, we computed the cross-judge agreement (J) for the 6 clusters for various r (Figure 3).",Experiment/Discussion
"Overall, interjudge agreement was quite high.",Experiment/Discussion
An interesting drop in interjudge agreement occurs for 20-30% summaries.,Experiment/Discussion
The drop most likely results from the fact that 10% summaries are typically easier to produce because the few most important sentences in a cluster are easier to identify.,Experiment/Discussion
8 We should note that both annotation tasks were quite time consuming and frustrating for the users who took anywhere from 6 to 10 hours each to complete their part.,Experiment/Discussion
"In the second experiment, we asked users to indicate all cases when within a cluster, a sentence is subsumed by another.",Experiment/Discussion
The judges' data on the first seven sentences of cluster A are shown in Table 8.,Experiment/Discussion
The &quot;-F score&quot; indicates the number of judges who agree on the most frequent subsumption.,Experiment/Discussion
The t score&quot; indicates that the consensus was no subsumption.,Experiment/Discussion
We found relatively low interjudge agreement on the cases in which at least one judge indicated evidence of subsumption.,Experiment/Discussion
"Overall, out of 558 sentences, there was full agreement (5 judges) on 292 sentences (Table 9).",Experiment/Discussion
"Unfortunately, h 291 of these 292 sentences the agreement was that there is no subsumption.",Experiment/Discussion
"When the bar of agreement was lowered to four judges, 23 out of 406 agreements are on sentences with subsumption.",Experiment/Discussion
"Overall, out of 80 In conclusion, we found very high interjudge agreement in the first experiment and moderately low agreement in the second experiment.",Experiment/Discussion
We concede that the time necessary to do a proper job at the second task is partly to blame.,Experiment/Discussion
"Since the baseline of random sentence selection is already included in the evaluation formulae, we used the Lead-based method (selecting the positionally first (n*r/c) sentences from each cluster where c = number of clusters) as the baseline to evaluate our system.",Experiment/Discussion
"In Table 10 we show the normalized performance (D) of MEAD, for the six clusters at nine compression rates.",Experiment/Discussion
MEAD performed better than Lead in 29 (in bold) out of 54 cases.,Experiment/Discussion
"Note that for the largest cluster, Cluster D, MEAD outperformed Lead at all compression rates. showed how MEAD's sentence scoring weights can - be modified to produce summaries significantly better than the alternatives.",Experiment/Discussion
"We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evaluating multidocument summaries.",Experiment/Discussion
All our findings are backed by the analysis of two experiments that we performed with human subjects.,Experiment/Discussion
"We found that the interjudge agreement on sentence utility is very high while the agreement on crosssentence subsumption is moderately low, ahhough promising.",Experiment/Discussion
"In the future, we would like to test our multidocument summarizer on a larger corpus and improve the summarization algorithm.",Experiment/Discussion
We would also like to explore how the techniques we proposed here can be used for multiligual multidocument summarization.,Experiment/Discussion
We then modified the MEAD algorithm to include lead information as well as centroids (see Section 0).,Experiment/Discussion
"In this case, MEAD+Lead performed better than the Lead baseline in 41 cases.",Experiment/Discussion
We are in the process of running experiments with other SCORE formulas.,Experiment/Discussion
It may seem that utility-based evaluation requires too much effort and is prone to low interjudge agreement.,Experiment/Discussion
We believe that our results show that interjudge agreement is quite high.,Experiment/Discussion
"As far as the amount of effort required, we believe that the larger effort on the part of the judges is more or less compensated with the ability to evaluate summaries off-line and at variable compression rates.",Experiment/Discussion
Alternative evaluations don't make such evaluations possible.,Experiment/Discussion
We should concede that a utility-based approach is probably not feasible for query-based summaries as these are typically done only on-line.,Experiment/Discussion
We discussed the possibility of a sentence contributing negatively to the utility of another sentence due to redundancy.,Experiment/Discussion
We should also point out that sentences can also reinforce one another positively.,Experiment/Discussion
"For example, if a sentence mentioning a new entity is included in a summary, one might also want to include a sentence that puts the entity in the context of the reit of the article or cluster.",Experiment/Discussion
"We presented a new multi-document summarizer, MEAD.",Results/Conclusion
It summarizes clusters of news articles automatically grouped by a topic detection system.,Results/Conclusion
MEAD uses information from the centroids of the clusters to select sentences that are most likely to be relevant to the cluster topic.,Results/Conclusion
"We used a new utility-based technique, CBSU, for the evaluation of MEAD and of summarizers in general.",Results/Conclusion
We found that MEAD produces summaries that are similar in quality to the ones produced by humans.,Results/Conclusion
"We also compared MEAD's performance to an alternative method, multi-document lead, and",Results/Conclusion
"We would like to thank Inderjeet Mani, Wlodek Zadrozny, Rie Kubota Ando, Joyce Chai, and Nanda Kambhatla for their valuable feedback.",Results/Conclusion
"We would also like to thank Carl Sable, Min-Yen Kan, Dave Evans, Adam Budzikowski, and Veronika Horvath for their help with the evaluation.",Results/Conclusion
