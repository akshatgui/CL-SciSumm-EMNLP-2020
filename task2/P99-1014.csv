col1,col2
"We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation.",{}
The models are empirically evalutated by a general decision test.,{}
"Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora.",{}
We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.,{}
An important challenge in computational linguistics concerns the construction of large-scale computational lexicons for the numerous natural languages where very large samples of language use are now available.,"{'title': '1 Introduction', 'number': '1'}"
Resnik (1993) initiated research into the automatic acquisition of semantic selectional restrictions.,"{'title': '1 Introduction', 'number': '1'}"
Ribas (1994) presented an approach which takes into account the syntactic position of the elements whose semantic relation is to be acquired.,"{'title': '1 Introduction', 'number': '1'}"
"However, those and most of the following approaches require as a prerequisite a fixed taxonomy of semantic relations.","{'title': '1 Introduction', 'number': '1'}"
"This is a problem because (i) entailment hierarchies are presently available for few languages, and (ii) we regard it as an open question whether and to what degree existing designs for lexical hierarchies are appropriate for representing lexical meaning.","{'title': '1 Introduction', 'number': '1'}"
Both of these considerations suggest the relevance of inductive and experimental approaches to the construction of lexicons with semantic information.,"{'title': '1 Introduction', 'number': '1'}"
This paper presents a method for automatic induction of semantically annotated subcategorization frames from unannotated corpora.,"{'title': '1 Introduction', 'number': '1'}"
"We use a statistical subcat-induction system which estimates probability distributions and corpus frequencies for pairs of a head and a subcat frame (Carroll and Rooth, 1998).","{'title': '1 Introduction', 'number': '1'}"
The statistical parser can also collect frequencies for the nominal fillers of slots in a subcat frame.,"{'title': '1 Introduction', 'number': '1'}"
"The induction of labels for slots in a frame is based upon estimation of a probability distribution over tuples consisting of a class label, a selecting head, a grammatical relation, and a filler head.","{'title': '1 Introduction', 'number': '1'}"
The class label is treated as hidden data in the EMframework for statistical estimation.,"{'title': '1 Introduction', 'number': '1'}"
"In our clustering approach, classes are derived directly from distributional dataâ€”a sample of pairs of verbs and nouns, gathered by parsing an unannotated corpus and extracting the fillers of grammatical relations.","{'title': '2 EM-Based Clustering', 'number': '2'}"
Semantic classes corresponding to such pairs are viewed as hidden variables or unobserved data in the context of maximum likelihood estimation from incomplete data via the EM algorithm.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"This approach allows us to work in a mathematically welldefined framework of statistical inference, i.e., standard monotonicity and convergence results for the EM algorithm extend to our method.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"The two main tasks of EM-based clustering are i) the induction of a smooth probability model on the data, and ii) the automatic discovery of class-structure in the data.","{'title': '2 EM-Based Clustering', 'number': '2'}"
Both of these aspects are respected in our application of lexicon induction.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
The basic ideas of our EM-based clustering approach were presented in Rooth (Ms).,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"Our approach constrasts with the merely heuristic and empirical justification of similarity-based approaches to clustering (Dagan et al., to appear) for which so far no clear probabilistic interpretation has been given.","{'title': '2 EM-Based Clustering', 'number': '2'}"
The probability model we use can be found earlier in Pereira et al. (1993).,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"However, in contrast to this approach, our statistical inference method for clustering is formalized clearly as an EM-algorithm.","{'title': '2 EM-Based Clustering', 'number': '2'}"
Approaches to probabilistic clustering similar to ours were presented recently in Saul and Pereira (1997) and Hofmann and Puzicha (1998).,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"There also EM-algorithms for similar probability models have been derived, but applied only to simpler tasks not involving a combination of EMbased clustering models as in our lexicon induction experiment.","{'title': '2 EM-Based Clustering', 'number': '2'}"
For further applications of our clustering model see Rooth et al. (1998).,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"We seek to derive a joint distribution of verbnoun pairs from a large sample of pairs of verbs v E V and nouns n E N. The key idea is to view v and n as conditioned on a hidden class c e C, where the classes are given no prior interpretation.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"The semantically smoothed probability of a pair (v, n) is defined to be: The joint distribution p(c, v, n) is defined by p(c, v , n) = p(c)p(vic)p(nic).","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Note that by construction, conditioning of v and n on each other is solely made through the classes c. In the framework of the EM algorithm (Dempster et al., 1977), we can formalize clustering as an estimation problem for a latent class (LC) model as follows.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"We are given: (i) a sample space)) of observed, incomplete data, corresponding to pairs from V x N, (ii) a sample space X of unobserved, complete data, corresponding to triples from Cx V x N, (iii) a set X(y) = {x E X I x = (c, y), c E CI of complete data related to the observation y, (iv) a complete-data specification po(x), corresponding to the joint probability p(c, v, n) over Cx V x N, with parametervector 0 = (Oc,Ov,OncIc E C, v E V, n E N), (v) an incomplete data specification P0(Y) which is related to the complete-data specification as the marginal probability P0 (Y) = Ex(y)po(x). '","{'title': '2 EM-Based Clustering', 'number': '2'}"
"The EM algorithm is directed at finding a value a of 0 that maximizes the incompletedata log-likelihood function L as a function of 0 for a given sample )), i.e., a = arg max L(0) where L(0) = ln Fly po (y).","{'title': '2 EM-Based Clustering', 'number': '2'}"
"0 As prescribed by the EM algorithm, the parameters of L(0) are estimated indirectly by proceeding iteratively in terms of complete-data estimation for the auxiliary function Q(0; 9(0), which is the conditional expectation of the complete-data log-likelihood lnpo(x) given the observed data y and the current fit of the parameter values 0(0 (E-step).","{'title': '2 EM-Based Clustering', 'number': '2'}"
"This auxiliary function is iteratively maximized as a function of 0 (M-step), where each iteration is defined by Note that our application is an instance of the EM-algorithm for context-free models (Baum et al., 1970), from which the following particularily simple reestimation formulae can be derived.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Let x = (c, y) for fixed c and y.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Then probabilistic context-free grammar of (Carroll and Rooth, 1998) gave for the British National Corpus (117 million words).","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Intuitively, the conditional expectation of the number of times a particular v, n, or c choice is made during the derivation is prorated by the conditionally expected total number of times a choice of the same kind is made.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"As shown by Baum et al. (1970), these expectations can be calculated efficiently using dynamic programming techniques.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Every such maximization step increases the log-likelihood function L, and a sequence of re-estimates eventually converges to a (local) maximum of L. In the following, we will present some examples of induced clusters.","{'title': '2 EM-Based Clustering', 'number': '2'}"
Input to the clustering algorithm was a training corpus of 1280715 tokens (608850 types) of verb-noun pairs participating in the grammatical relations of intransitive and transitive verbs and their subject- and object-fillers.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
The data were gathered from the maximal-probability parses the head-lexicalized Fig.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
2 shows an induced semantic class out of a model with 35 classes.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"At the top are listed the 20 most probable nouns in the p(n5) distribution and their probabilities, and at left are the 30 most probable verbs in the p(v15) distribution.","{'title': '2 EM-Based Clustering', 'number': '2'}"
5 is the class index.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
Those verb-noun pairs which were seen in the training data appear with a dot in the class matrix.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
Verbs with suffix .as : s indicate the subject slot of an active intransitive.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"Similarily .aso: s denotes the subject slot of an active transitive, and .aso: o denotes the object slot of an active transitive.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Thus v in the above discussion actually consists of a combination of a verb with a subcat frame slot as : s, aso : s, or aso : o.","{'title': '2 EM-Based Clustering', 'number': '2'}"
"Induced classes often have a basis in lexical semantics; class 5 can be interpreted as clustering agents, denoted by proper names, &quot;man&quot;, and &quot;woman&quot;, together with verbs denoting communicative action.","{'title': '2 EM-Based Clustering', 'number': '2'}"
Fig.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
1 shows a cluster involving verbs of scalar change and things which can move along scales.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
Fig.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
5 can be interpreted as involving different dispositions and modes of their execution.,"{'title': '2 EM-Based Clustering', 'number': '2'}"
60 100 160 200 260 300 nix*. el cleeeeâ€¢,"{'title': '2 EM-Based Clustering', 'number': '2'}"
"We evaluated our clustering models on a pseudodisambiguation task similar to that performed in Pereira et al. (1993), but differing in detail.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"The task is to judge which of two verbs v and v' is more likely to take a given noun n as its argument where the pair (v, n) has been cut out of the original corpus and the pair (v', n) is constructed by pairing 71 with a randomly chosen verb v' such that the combination (v', n) is completely unseen.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Thus this test evaluates how well the models generalize over unseen verbs.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
The data for this test were built as follows.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"We constructed an evaluation corpus of (v, n, v') triples by randomly cutting a test corpus of 3000 (v, n) pairs out of the original corpus of 1280712 tokens, leaving a training corpus of 1178698 tokens.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"Each noun n in the test corpus was combined with a verb v' which was randomly chosen according to its frequency such that the pair (v', n) did appear neither in the training nor in the test corpus.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"However, the elements v, v', and n were required to be part of the training corpus.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"Furthermore, we restricted the verbs and nouns in the evalutation corpus to the ones which occured at least 30 times and at most 3000 times with some verb-functor v in the training corpus.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
The resulting 1337 evaluation triples were used to evaluate a sequence of clustering models trained from the training corpus.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"The clustering models we evaluated were parametrized in starting values of the training algorithm, in the number of classes of the model, and in the number of iteration steps, resulting in a sequence of 3 x 10 x 6 models.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"Starting from a lower bound of 50 % random choice, accuracy was calculated as the number of times the model decided for p(nlv) > p(n1v1) out of all choices made.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Fig.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"3 shows the evaluation results for models trained with 50 iterations, averaged over starting values, and plotted against class cardinality.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Different starting values had an effect of 2 % on the performance of the test.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
We obtained a value of about 80 % accuracy for models between 25 and 100 classes.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Models with more than 100 classes show a small but stable overfitting effect.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"A second experiment addressed the smoothing power of the model by counting the number of (v, n) pairs in the set V xN of all possible combinations of verbs and nouns which received a positive joint probability by the model.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"The V x Nspace for the above clustering models included about 425 million (v, n) combinations; we approximated the smoothing size of a model by randomly sampling 1000 pairs from V x N and returning the percentage of positively assigned pairs in the random sample.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Fig.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
4 plots the smoothing results for the above models against the number of classes.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
Starting values had an influence of Â± 1 % on performance.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"Given the proportion of the number of types in the training corpus to the V x N-space, without clustering we have a smoothing power of 0.14 % whereas for example a model with 50 classes and 50 iterations has a smoothing power of about 93 %.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
"Corresponding to the maximum likelihood paradigm, the number of training iterations had a decreasing effect on the smoothing performance whereas the accuracy of the pseudodisambiguation was increasing in the number of iterations.","{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
We found a number of 50 iterations to be a good compromise in this trade-off.,"{'title': '3 Evaluation of Clustering Models', 'number': '3'}"
The goal of the following experiment was to derive a lexicon of several hundred intransitive and transitive verbs with subcat slots labeled with latent classes.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
To induce latent classes for the subject slot of a fixed intransitive verb the following statistical inference step was performed.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"Given a latent class model pLc(.) for verb-noun pairs, and a sample n1, , nm of subjects for a fixed intransitive verb, we calculate the probability of an arbitrary subject n E N by: The estimation of the parameter-vector 0 = (Ocic e C) can be formalized in the EM framework by viewing p(n) or p(c, n) as a function of 0 for fixed pLc(.).","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"The re-estimation formulae resulting from the incomplete data estimation for these probability functions have the following form (1(n) is the frequency of n in the sample of subjects of the fixed verb): A similar EM induction process can be applied also to pairs of nouns, thus enabling induction of latent semantic annotations for transitive verb frames.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"Given a LC model NA.) for verb-noun pairs, and a sample (ni, n2)1, , (n, n) of noun arguments (ni subjects, and n2 direct objects) for a fixed transitive verb, we calculate the probability of its noun argument pairs by: in an EM framework by viewing p(ni, n2) or p(ci, c2, ni, n2) as a function of 0 for fixed pLc(.).","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"The re-estimation formulae resulting from this incomplete data estimation problem have the following simple form (f(ni, n2) is the frequency of (ni, n2) in the sample of noun argument pairs of the fixed verb): Note that the class distributions p(c) and p(ci, c2) for intransitive and transitive models can be computed also for verbs unseen in the LC model. blush 5 0.982975 snarl 5 0.962094 constance 3 mandeyille 2 christina 3 jinkwa 2 willie 2.99737 man 1.99859 ronni 2 scott 1.99761 claudia 2 omalley 1.99755 gabriel 2 shamlou 1 maggie 2 angalo 1 bathsheba 2 corbett 1 sarah 2 southgate 1 girl 1.9977 ace 1 Experiments used a model with 35 classes.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"From maximal probability parses for the British National Corpus derived with a statistical parser (Carroll and Rooth, 1998), we extracted frequency tables for intransitve verb/subject pairs and transitive verb/subject/object triples.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
The 500 most frequent verbs were selected for slot labeling.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Fig.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"6 shows two verbs v for which the most probable class label is 5, a class which we earlier described as communicative action, together with the estimated frequencies of f (n)pe(cln) for those ten nouns n for which this estimated frequency is highest.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Fig.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
7 shows corresponding data for an intransitive scalar motion sense of increase.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Fig.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
8 shows the intransitive verbs which take 17 as the most probable label.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"Intuitively, the verbs are semantically coherent.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"When compared to Levin (1993)'s 48 top-level verb classes, we found an agreement of our classification with her class of &quot;verbs of changes of state&quot; except for the last three verbs in the list in Fig.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
8 which is sorted by probability of the class label.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Similar results for German intransitive scalar motion verbs are shown in Fig.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
9.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"The data for these experiments were extracted from the maximal-probability parses of a 4.1 million word corpus of German subordinate clauses, yielding 418290 tokens (318086 types) of pairs of verbs or adjectives and nouns.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
The lexicalized probabilistic grammar for German used is described in Beil et al. (1999).,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
We compared the German example of scalar motion verbs to the linguistic classification of verbs given by Schuhmacher (1986) and found an agreement of our classification with the class of &quot;einfache Anderungsverben&quot; (simple verbs of change) except for the verbs anwachsen (increase) and stagnieren(stagnate) which were not classified there at all.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Fig.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"10 shows the most probable pair of classes for increase as a transitive verb, together with estimated frequencies for the head filler pair.","{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
Note that the object label 17 is the class found with intransitive scalar motion verbs; this correspondence is exploited in the next section.,"{'title': '4 Lexicon Induction Based on Latent Classes', 'number': '4'}"
"In some linguistic accounts, multi-place verbs are decomposed into representations involving (at least) one predicate or relation per argument.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
"For instance, the transitive causative/inchoative verb increase, is composed of an actor/causative verb combining with a one-place predicate in the structure on the left in Fig.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
11.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"Linguistically, such representations are motivated by argument alternations (diathesis), case linking and deep word order, language acquistion, scope ambiguity, by the desire to represent aspects of lexical meaning, and by the fact that in some languages, the postulated decomposed representations are overt, with each primitive predicate corresponding to a morpheme.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
For references and recent discussion of this kind of theory see Hale and Keyser (1993) and Kural (1996).,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"We will sketch an understanding of the lexical representations induced by latent-class labeling in terms of the linguistic theories mentioned above, aiming at an interpretation which combines computational learnability, linguistic motivation, and denotational-semantic adequacy.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
The basic idea is that latent classes are computational models of the atomic relation symbols occurring in lexical-semantic representations.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"As a first implementation, consider replacing the relation symbols in the first tree in Fig.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
11 with relation symbols derived from the latent class labeling.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"In the second tree in Fig 11, R17 and R8 are relation symbols with indices derived from the labeling procedure of Sect.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
4.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"Such representations can be semantically interpreted in standard ways, for instance by interpreting relation symbols as denoting relations between events and individuals.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
Such representations are semantically inadequate for reasons given in philosophical critiques of decomposed linguistic representations; see Fodor (1998) for recent discussion.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
A lexicon estimated in the above way has as many primitive relations as there are latent classes.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
We guess there should be a few hundred classes in an approximately complete lexicon (which would have to be estimated from a corpus of hundreds of millions of words or more).,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"Fodor's arguments, which are based on the very limited degree of genuine interdefinability of lexical items and on Putnam's arguments for contextual determination of lexical meaning, indicate that the number of basic concepts has the order of magnitude of the lexicon itself.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
"More concretely, a lexicon constructed along the above principles would identify verbs which are labelled with the same latent classes; for instance it might identify the representations of grab and touch.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
"For these reasons, a semantically adequate lexicon must include additional relational constants.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
"We meet this requirement in a simple way, by including as a conjunct a unique constant derived from the open-class root, as in the third tree in Fig.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
11.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"We introduce indexing of the open class root (copied from the class index) in order that homophony of open class roots not result in common conjuncts in semantic representationsâ€”for instance, we don't want the two senses of decline exemplified in decline the proposal and decline five percent to have an common entailment represented by a common conjunct.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
This indexing method works as long as the labeling process produces different latent class labels for the different senses.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
The last tree in Fig.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
11 is the learned representation for the scalar motion sense of the intransitive verb increase.,"{'title': '5 Linguistic Interpretation', 'number': '5'}"
"In our approach, learning the argument alternation (diathesis) relating the transitive increase (in its scalar motion sense) to the intransitive increase (in its scalar motion sense) amounts to learning representations with a common component Ri7 A increaser,.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
"In this case, this is achieved.","{'title': '5 Linguistic Interpretation', 'number': '5'}"
We have proposed a procedure which maps observations of subcategorization frames with their complement fillers to structured lexical entries.,"{'title': '6 Conclusion', 'number': '6'}"
"We believe the method is scientifically interesting, practically useful, and flexible because: 1.","{'title': '6 Conclusion', 'number': '6'}"
The algorithms and implementation are efficient enough to map a corpus of a hundred million words to a lexicon.,"{'title': '6 Conclusion', 'number': '6'}"
2.,"{'title': '6 Conclusion', 'number': '6'}"
The model and induction algorithm have foundations in the theory of parameterized families of probability distributions and statistical estimation.,"{'title': '6 Conclusion', 'number': '6'}"
"As exemplified in the paper, learning, disambiguation, and evaluation can be given simple, motivated formulations.","{'title': '6 Conclusion', 'number': '6'}"
3.,"{'title': '6 Conclusion', 'number': '6'}"
The derived lexical representations are linguistically interpretable.,"{'title': '6 Conclusion', 'number': '6'}"
This suggests the possibility of large-scale modeling and observational experiments bearing on questions arising in linguistic theories of the lexicon.,"{'title': '6 Conclusion', 'number': '6'}"
4.,"{'title': '6 Conclusion', 'number': '6'}"
"Because a simple probabilistic model is used, the induced lexical entries could be incorporated in lexicalized syntax-based probabilistic language models, in particular in head-lexicalized models.","{'title': '6 Conclusion', 'number': '6'}"
This provides for potential application in many areas.,"{'title': '6 Conclusion', 'number': '6'}"
5.,"{'title': '6 Conclusion', 'number': '6'}"
"The method is applicable to any natural language where text samples of sufficient size, computational morphology, and a robust parser capable of extracting subcategorization frames with their fillers are available.","{'title': '6 Conclusion', 'number': '6'}"
