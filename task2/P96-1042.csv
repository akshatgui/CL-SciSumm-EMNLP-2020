col1,col2
"Corpus-based methods for natural language processing often use supervised training, requiring expensive manual annotation of training corpora.",{}
"This paper investigates methods for reducing annotacost by selection. this approach, during training the learning program examines many unlabeled examples and selects for labeling (annotation) only those that are most informative at each stage.",{}
This avoids redundantly annotating examples that contribute little new information.,{}
This paper extends our previous on sample selection for probabilistic classifiers.,{}
"We describe a family of methods for committee-based sample selection, and report experimental results for the task of stochastic part-ofspeech tagging.",{}
"We find that all variants achieve a significant reduction in annotation cost, though their computational efficiency differs.",{}
"In particular, the simplest method, which has no parameters to tune, gives excellent results.",{}
We also show that sample selection yields a significant reduction in the size of the model used by the tagger.,{}
Many corpus-based methods for natural language processing (NLP) are based on supervised training— acquiring information from a manually annotated corpus.,"{'title': '1 Introduction', 'number': '1'}"
"Therefore, reducing annotation cost is an important research goal for statistical NLP.","{'title': '1 Introduction', 'number': '1'}"
"The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994).","{'title': '1 Introduction', 'number': '1'}"
"It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial.","{'title': '1 Introduction', 'number': '1'}"
"Indeed, fully unsupervised training may not be feasible for certain tasks.","{'title': '1 Introduction', 'number': '1'}"
"This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we investigate and extend the committee-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995).","{'title': '1 Introduction', 'number': '1'}"
"When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994).","{'title': '1 Introduction', 'number': '1'}"
This avoids redundantly annotating many examples that contribute roughly the same information to the learner.,"{'title': '1 Introduction', 'number': '1'}"
Our work focuses on sample selection for training probabilistic classifiers.,"{'title': '1 Introduction', 'number': '1'}"
"In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)).","{'title': '1 Introduction', 'number': '1'}"
"As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well-understood method of stochastic part-of-speech tagging using Hidden Markov Models.","{'title': '1 Introduction', 'number': '1'}"
We first review the basic approach of committeebased sample selection and its application to partof-speech tagging.,"{'title': '1 Introduction', 'number': '1'}"
"This basic approach gives rise to a family of algorithms (including the original algorithm described in (Dagan and Engelson, 1995)) which we then describe.","{'title': '1 Introduction', 'number': '1'}"
"First, we describe the 'simplest' committee-based selection algorithm, which has no parameters to tune.","{'title': '1 Introduction', 'number': '1'}"
"We then generalize the selection scheme, allowing more options to adapt and tune the approach for specific tasks.","{'title': '1 Introduction', 'number': '1'}"
"The paper compares the performance of several instantiations of the general scheme, including a batch selection method similar to that of Lewis and Gale (1994).","{'title': '1 Introduction', 'number': '1'}"
"In particular, we found that the simplest version of the method achieves a significant reduction in annotation cost, comparable to that of other versions.","{'title': '1 Introduction', 'number': '1'}"
"We also evaluate the computational efficiency of the different variants, and the number of unlabeled examples they consume.","{'title': '1 Introduction', 'number': '1'}"
"Finally, we study the effect of sample selection on the size of the model acquired by the learner.","{'title': '1 Introduction', 'number': '1'}"
"This section presents the framework and terminology assumed for probabilistic classification, as well as its instantiation for stochastic bigram part-ofspeech tagging.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"A probabilistic classifier classifies input examples e by classes c E C, where C is a known set of possible classes.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"Classification is based on a score function, Fm (c, e), which assigns a score to each possible class of an example.","{'title': '2 Probabilistic Classification', 'number': '2'}"
The classifier then assigns the example to the class with the highest score.,"{'title': '2 Probabilistic Classification', 'number': '2'}"
"Fm is determined by a probabilistic model M. In many applications, Fm is the conditional probability function, Pm (cle), specifying the probability of each class given the example, but other score functions that correlate with the likelihood of the class are often used.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"In stochastic part-of-speech tagging, the model assumed is a Hidden Markov Model (HMM), and input examples are sentences.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"The class c, to which a sentence is assigned is a sequence of the parts of speech (tags) for the words in the sentence.","{'title': '2 Probabilistic Classification', 'number': '2'}"
The score function is typically the joint (or conditional) probability of the sentence and the tag sequence'.,"{'title': '2 Probabilistic Classification', 'number': '2'}"
The tagger then assigns the sentence to the tag sequence which is most probable according to the HMM.,"{'title': '2 Probabilistic Classification', 'number': '2'}"
"The probabilistic model M, and thus the score function Fm, are defined by a set of parameters, {a}.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"During training, the values of the parameters are estimated from a set of statistics, S, extracted from a training set of annotated examples.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"We denote a particular model by M = {ai}, where each ai is a specific value for the corresponding ai.","{'title': '2 Probabilistic Classification', 'number': '2'}"
"In bigram part-of-speech tagging the HMM model M contains three types of parameters: transition probabilities P(ti—t) giving the probability of tag tj occuring after tag ti, lexical probabilities P(t1w) giving the probability of tag t labeling word w, and tag probabilities P(t) giving the marginal probability of a tag occurring.2 The values of these parameters are estimated from a tagged corpus which provides a training set of labeled examples (see Section 4.1).","{'title': '2 Probabilistic Classification', 'number': '2'}"
"A sample selection method needs to evaluate the expected usefulness, or information gain, of learning from a given example.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"The methods we investigate approach this evaluation implicitly, measuring an example's informativeness as the uncertainty in its classification given the current training data (Seung, Opper, and Sompolinsky, 1992; Lewis and Gale, 1994; MacKay, 1992).","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
The reasoning is that if an example's classification is uncertain given current training data then the example is likely to contain unknown information useful for classifying similar examples in the future.,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"We investigate the committee-based method, where the learning algorithm evaluates an example by giving it to a committee containing several variant models, all 'consistent' with the training data seen so far.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"The more the committee members agree on the classification of the example, the greater our certainty in its classification.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"This is because when the training data entails a specific classification with high certainty, most (in a probabilistic sense) classifiers consistent with the data will produce that classification.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"The committee-based approach was first proposed in a theoretical context for learning binary nonprobabilistic classifiers (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993).","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"Taking a Bayesian perspective, the posterior probability of a model, P(MIS), is determined given statistics S from the training set (and some prior distribution for the models).","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
Committee members are then generated by drawing models randomly from POI IS).,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
An example is selected for labeling if the committee members largely disagree on its classification.,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"This procedure assumes that one can sample from the models' posterior distribution, at least approximately.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"To illustrate the generation of committeemembers, consider a model containing a single binomial parameter a (the probability of a success), with estimated value a.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"The statistics S for such a model are given by N, the number of trials, and x, the number of successes in those trials.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"Given N and x, the 'best' parameter value may be estimated by one of several estimation methods.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"For example, the maximum likelihood estimate for a is a = k, giving the model M = {a} = {kJ.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"When generating a committee of models, however, we are not interested in the 'best' model, but rather in sampling the distribution of models given the statistics.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"For our example, we need to sample the posterior density of estimates for a, namely P(a = a I S)• Sampling this distribution yields a set of estimates scattered around ff (assuming a uniform prior), whose variance decreases as N increases.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"In other words, the more statistics there are for estimating the parameter, the more similar are the parameter values used by different committee members.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"For models with multiple parameters, parameter estimates for different committee members differ more when they are based on low training counts, and they agree more when based on high counts.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
Selecting examples on which the committee members disagree contributes statistics to currently uncertain parameters whose uncertainty also affects classification.,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
It may sometimes be difficult to sample P(MIS) due to parameter interdependence.,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"Fortunately, models used in natural language processing often assume independence between most model parameters.","{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
In such cases it is possible to generate committee members by sampling the posterior distribution for each independent group of parameters separately.,"{'title': '3 Evaluating Example Uncertainty', 'number': '3'}"
"In order to generate committee members for bigram tagging, we sample the posterior distributions for transition probabilities, P(t2-4i), and for lexical probabilities, P(tlw) (as described in Section 2).","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
Both types of the parameters we sample have the form of multinomial distributions.,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
Each multinomial random variable corresponds to a conditioning event and its values are given by the corresponding set of conditioned events.,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"For example, a transition probability parameter P(ti—+t3) has conditioning event ti and conditioned event ti.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"Let { ui} denote the set of possible values of a given multinomial variable, and let S = Ind denote a set of statistics extracted from the training set for that variable, where ni is the number of times that the value ui appears in the training set for the variable, defining N = Ei ni.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
The parameters whose posterior distributions we wish to estimate are ai = P(ui).,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"The maximum likelihood estimate for each of the multinomial's distribution parameters, ai, is & = .","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"In practice, this estimator is usually smoothed in some way to compensate for data sparseness.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
Such smoothing typically reduces slightly the estimates for values with positive counts and gives small positive estimates for values with a zero count.,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"For simplicity, we describe here the approximation of P(ai = aiIS) for the unsmoothed estimator3.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"We approximate the posterior P(ai = ai IS) by first assuming that the multinomial is a collection of independent binomials, each of which corresponds to a single value ui of the multinomial; we then normalize the values so that they sum to 1.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"For each such binomial, we approximate P(ai = ai IS) as a truncated normal distribution (restricted to [0,1]), with estimated mean /./.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"=1*- and variance o-2 = To generate a particular multinomial distribution, we randomly choose values for the binomial parameters ai from their approximated posterior distributions (using the simple sampling method given in (Press et al., 1988, p. 214)), and renormalize them so that they sum to 1.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"Finally, to generate a random HMM given statistics S, we choose values independently for the parameters of each multinomial, since all the different multinomials in an HMM are independent.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"Typically, concept learning problems are formulated such that there is a set of training examples that are independent of each other.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"When training a bigram model (indeed, any HMM), this is not true, as each word is dependent on that before it.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
This problem is solved by considering each sentence as an individual example.,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"More generally, it is possible to break the text at any point where tagging is unambiguous.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
We thus use unambiguous words (those with only one possible part of speech) as example boundaries in bigram tagging.,"{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
"This allows us to train on smaller examples, focusing training more on the truly informative parts of the corpus.","{'title': '4 Bigram Part-Of-Speech Tagging', 'number': '4'}"
Within the committee-based paradigm there exist different methods for selecting informative examples.,"{'title': '5 Selection Algorithms', 'number': '5'}"
"Previous research in sample selection has used either sequential selection (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Dagan and Engelson, 1995), or batch selection (Lewis and Catlett, 1994; Lewis and Gale, 1994).","{'title': '5 Selection Algorithms', 'number': '5'}"
We describe here general algorithms for both sequential and batch selection.,"{'title': '5 Selection Algorithms', 'number': '5'}"
"Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee.","{'title': '5 Selection Algorithms', 'number': '5'}"
Those examples determined to be sufficiently informative are selected for training.,"{'title': '5 Selection Algorithms', 'number': '5'}"
"Most simply, we can use a committee of size two and select an example when the two models disagree on its classification.","{'title': '5 Selection Algorithms', 'number': '5'}"
"This gives the following, parameter-free, two member sequential selection algorithm, executed for each unlabeled input example e: 'The normal approximation, while easy to implement, can be avoided.","{'title': '5 Selection Algorithms', 'number': '5'}"
"The posterior probability P(a, = adS) for the multinomial is given exactly by the Dirichlet distribution (Johnson, 1972) (which reduces to the Beta distribution in the binomial case).","{'title': '5 Selection Algorithms', 'number': '5'}"
In this work we assumed a uniform prior distribution for each model parameter; we have not addressed the question of how to best choose a prior for this problem.,"{'title': '5 Selection Algorithms', 'number': '5'}"
This basic algorithm needs no parameters.,"{'title': '5 Selection Algorithms', 'number': '5'}"
"If desired, it is possible to tune the frequency of selection, by changing the variance of P(MIS) (or the variance of P(ai = a IS) for each parameter), where larger variances increase the rate of disagreement among the committee members.","{'title': '5 Selection Algorithms', 'number': '5'}"
"We implemented this effect by employing a temperature parameter t, used as a multiplier of the variance of the posterior parameter distribution.","{'title': '5 Selection Algorithms', 'number': '5'}"
"A more general algorithm results from allowing (i) a larger number of committee members, k, in order to sample P(MIS) more precisely, and (ii) more refined example selection criteria.","{'title': '5 Selection Algorithms', 'number': '5'}"
"This gives the following general sequential selection algorithm, executed for each unlabeled input example e: It is easy to see that two member sequential selection is a special case of general sequential selection, where any disagreement is considered sufficient for selection.","{'title': '5 Selection Algorithms', 'number': '5'}"
"In order to instantiate the general algorithm for larger committees, we need to define (i) a measure for disagreement (Step 3), and (ii) a selection criterion (Step 4).","{'title': '5 Selection Algorithms', 'number': '5'}"
"Our approach to measuring disagreement is to use the vote entropy, the entropy of the distribution of classifications assigned to an example ('voted for') by the committee members.","{'title': '5 Selection Algorithms', 'number': '5'}"
"Denoting the number of committee members assigning c to e by V(c, e), the vote entropy is: (Dividing by log k normalizes the scale for the number of committee members.)","{'title': '5 Selection Algorithms', 'number': '5'}"
"Vote entropy is maximized when all committee members disagree, and is zero when they all agree.","{'title': '5 Selection Algorithms', 'number': '5'}"
"In bigram tagging, each example consists of a sequence of several words.","{'title': '5 Selection Algorithms', 'number': '5'}"
"In our system, we measure D separately for each word, and use the average entropy over the word sequence as a measurement of disagreement for the example.","{'title': '5 Selection Algorithms', 'number': '5'}"
"We use the average entropy rather than the entropy over the entire sequence, because the number of committee members is small with respect to the total number of possible tag sequences.","{'title': '5 Selection Algorithms', 'number': '5'}"
"Note that we do not look at the entropy of the distribution given by each single model to the possible tags (classes), since we are only interested in the uncertainty of the final classification (see the discussion in Section 7).","{'title': '5 Selection Algorithms', 'number': '5'}"
We consider two alternative selection criteria (for Step 4).,"{'title': '5 Selection Algorithms', 'number': '5'}"
"The simplest is thresholded selection, in which an example is selected for annotation if its vote entropy exceeds some threshold O.","{'title': '5 Selection Algorithms', 'number': '5'}"
"The other alternative is randomized selection, in which an example is selected for annotation based on the flip of a coin biased according to the vote entropy—a higher vote entropy entailing a higher probability of selection.","{'title': '5 Selection Algorithms', 'number': '5'}"
"We define the selection probability as a linear function of vote entropy: p = gD, where g is an entropy gain parameter.","{'title': '5 Selection Algorithms', 'number': '5'}"
"The selection method we used in our earlier work (Dagan and Engelson, 1995) is randomized sequential selection using this linear selection probability model, with parameters k, t and g. An alternative to sequential selection is batch selection.","{'title': '5 Selection Algorithms', 'number': '5'}"
"Rather than evaluating examples individually for their informativeness a large batch of examples is examined, and the m best are selected for annotation.","{'title': '5 Selection Algorithms', 'number': '5'}"
"The batch selection algorithm, executed for each batch B of N examples, is as follows: This procedure is repeated sequentially for successive batches of N examples, returning to the start of the corpus at the end.","{'title': '5 Selection Algorithms', 'number': '5'}"
"If N is equal to the size of the corpus, batch selection selects the m globally best examples in the corpus at each stage (as in (Lewis and Catlett, 1994)).","{'title': '5 Selection Algorithms', 'number': '5'}"
"On the other hand, as N decreases, batch selection becomes closer to sequential selection.","{'title': '5 Selection Algorithms', 'number': '5'}"
"This section presents results of applying committeebased sample selection to bigram part-of-speech tagging, as compared with complete training on all examples in the corpus.","{'title': '6 Experimental Results', 'number': '6'}"
Evaluation was performed using the University of Pennsylvania tagged corpus from the ACL/DCI CD-ROM I.,"{'title': '6 Experimental Results', 'number': '6'}"
"For ease of implementation, we used a complete (closed) lexicon which contains all the words in the corpus.","{'title': '6 Experimental Results', 'number': '6'}"
"The committee-based sampling algorithm was initialized using the first 1,000 words from the corpus, and then sequentially examined the following examples in the corpus for possible labeling.","{'title': '6 Experimental Results', 'number': '6'}"
"The training set consisted of the first million words in the corpus, with sentence ordering randomized to compensate for inhomogeneity in corpus composition.","{'title': '6 Experimental Results', 'number': '6'}"
"The test set was a separate portion of the corpus, consisting of 20,000 words.","{'title': '6 Experimental Results', 'number': '6'}"
"We compare the amount of training required by different selection methods to achieve a given tagging accuracy on the test set, where both the amount of training and tagging accuracy are measured over ambiguous words.'","{'title': '6 Experimental Results', 'number': '6'}"
"The effectiveness of randomized committee-based 5Note that most other work on tagging has measured accuracy over all words, not just ambiguous ones.","{'title': '6 Experimental Results', 'number': '6'}"
"Complete training of our system on 1,000,000 words gave us an accuracy of 93.5% over ambiguous words, which corresponds to an accuracy of 95.9% over all words in the selection for part-of-speech tagging, with 5 and 10 committee members, was demonstrated in (Dagan and Engelson, 1995).","{'title': '6 Experimental Results', 'number': '6'}"
"Here we present and compare results for batch, randomized, thresholded, and two member committee-based selection.","{'title': '6 Experimental Results', 'number': '6'}"
Figure 1 presents the results of comparing the several selection methods against each other.,"{'title': '6 Experimental Results', 'number': '6'}"
The plots shown are for the best parameter settings that we found through manual tuning for each method.,"{'title': '6 Experimental Results', 'number': '6'}"
Figure 1(a) shows the advantage that sample selection gives with regard to annotation cost.,"{'title': '6 Experimental Results', 'number': '6'}"
"For example, complete training requires annotated examples containing 98,000 ambiguous words to achieve a 92.6% accuracy (beyond the scale of the graph), while the selective methods require only 18,000-25,000 ambiguous words to achieve this accuracy.","{'title': '6 Experimental Results', 'number': '6'}"
"We also find test set, comparable to other published results on bigram tagging. that, to a first approximation, all selection methods considered give similar results.","{'title': '6 Experimental Results', 'number': '6'}"
"Thus, it seems that a refined choice of the selection method is not crucial for achieving large reductions in annotation cost.","{'title': '6 Experimental Results', 'number': '6'}"
This equivalence of the different methods also largely holds with respect to computational efficiency.,"{'title': '6 Experimental Results', 'number': '6'}"
"Figure 1(b) plots classification accuracy versus number of words examined, instead of those selected.","{'title': '6 Experimental Results', 'number': '6'}"
"We see that while all selective methods are less efficient in terms of examples examined than complete training, they are comparable to each other.","{'title': '6 Experimental Results', 'number': '6'}"
"Two member selection seems to have a clear, though small, advantage.","{'title': '6 Experimental Results', 'number': '6'}"
In Figure 2 we investigate further the properties of batch selection.,"{'title': '6 Experimental Results', 'number': '6'}"
"Figure 2(a) shows that accuracy increases with batch size only up to a point, and then starts to decrease.","{'title': '6 Experimental Results', 'number': '6'}"
"This result is in line with theoretical difficulties with batch selection (Freund et al., 1993) in that batch selection does not account for the distribution of input examples.","{'title': '6 Experimental Results', 'number': '6'}"
"Hence, once batch size increases past a point, the input distribution has too little influence on which examples are selected, and hence classification accuracy decreases.","{'title': '6 Experimental Results', 'number': '6'}"
"Furthermore, as batch size increases, computational efficiency, in terms of the number of examples examined to attain a given accuracy, decreases tremendously (Figure 2(b)).","{'title': '6 Experimental Results', 'number': '6'}"
The ability of committee-based selection to focus on the more informative parts of the training corpus is analyzed in Figure 3.,"{'title': '6 Experimental Results', 'number': '6'}"
"Here we examined the number of lexical and bigram counts that were stored (i.e, were non-zero) during training, using the two member selection algorithm and complete training.","{'title': '6 Experimental Results', 'number': '6'}"
"As the graphs show, the sample selection method achieves the same accuracy as complete training with fewer lexical and bigram counts.","{'title': '6 Experimental Results', 'number': '6'}"
"This means that many counts in the data are less useful for correct tagging, as replacing them with smoothed estimates works just as well.'","{'title': '6 Experimental Results', 'number': '6'}"
"Committee-based selection ignores such counts, focusing on parameters which improve the model.","{'title': '6 Experimental Results', 'number': '6'}"
This behavior has the practical advantage of reducing the size of the model significantly (by a factor of three here).,"{'title': '6 Experimental Results', 'number': '6'}"
"Also, the average count is lower in a model constructed by selective training than in a fully trained model, suggesting that the selection method avoids using examples which increase the counts for already known parameters.","{'title': '6 Experimental Results', 'number': '6'}"
Why does committee-based sample selection work?,"{'title': '7 Discussion', 'number': '7'}"
Consider the properties of those examples that are selected for training.,"{'title': '7 Discussion', 'number': '7'}"
"In general, a selected training example will contribute data to several statistics, which in turn will improve the estimates of several parameter values.","{'title': '7 Discussion', 'number': '7'}"
An informative example is therefore one whose contribution to the statistics leads to a significantly useful improvement of model parameter estimates.,"{'title': '7 Discussion', 'number': '7'}"
Model parameters for which acquiring additional statistics is most beneficial can be characterized by the following three properties: ters that affect only few examples have low overall utility.,"{'title': '7 Discussion', 'number': '7'}"
The committee-based selection algorithms work because they tend to select examples that affect parameters with the above three properties.,"{'title': '7 Discussion', 'number': '7'}"
Property 1 is addressed by randomly drawing the parameter values for committee members from the posterior distribution given the current statistics.,"{'title': '7 Discussion', 'number': '7'}"
"When the statistics for a parameter are insufficient, the variance of the posterior distribution of the estimates is large, and hence there will be large differences in the values of the parameter chosen for different committee members.","{'title': '7 Discussion', 'number': '7'}"
"Note that property 1 is not addressed when uncertainty in classification is only judged relative to a single model7 (as in, eg, (Lewis and Gale, 1994)).","{'title': '7 Discussion', 'number': '7'}"
Property 2 is addressed by selecting examples for which committee members highly disagree in classification (rather than measuring disagreement in parameter estimates).,"{'title': '7 Discussion', 'number': '7'}"
Committee-based selection thus addresses properties 1 and 2 simultaneously: it acquires statistics just when uncertainty in current parameter estimates entails uncertainty regarding the appropriate classification of the example.,"{'title': '7 Discussion', 'number': '7'}"
Our results show that this effect is achieved even when using only two committee members to sample the space of likely classifications.,"{'title': '7 Discussion', 'number': '7'}"
"By appropriate classification we mean the classification given by a perfectly-trained model, that is, one with accurate parameter values.","{'title': '7 Discussion', 'number': '7'}"
"Note that this type of uncertainty regarding the identity of the appropriate classification, is different than uncertainty regarding the correctness of the classification itself.","{'title': '7 Discussion', 'number': '7'}"
"For example, sufficient statistics may yield an accurate 0.51 probability estimate for a class c in a given example, making it certain that c is the appropriate classification.","{'title': '7 Discussion', 'number': '7'}"
"However, the certainty that c is the correct classification is low, since there is a 0.49 chance that c is the wrong class for the example.","{'title': '7 Discussion', 'number': '7'}"
"A single model can be used to estimate only the second type of uncertainty, which does not correlate directly with the utility of additional training.","{'title': '7 Discussion', 'number': '7'}"
"Finally, property 3 is addressed by independently examining input examples which are drawn from the input distribution.","{'title': '7 Discussion', 'number': '7'}"
"In this way, we implicitly model the distribution of model parameters used for classifying input examples.","{'title': '7 Discussion', 'number': '7'}"
"Such modeling is absent in batch selection, and we hypothesize that this is the reason for its lower effectiveness.","{'title': '7 Discussion', 'number': '7'}"
Annotating large textual corpora for training natural language models is a costly process.,"{'title': '8 Conclusions', 'number': '8'}"
"We propose reducing this cost significantly using committee7The use of a single model is also criticized in (Cohn, Atlas, and Ladner, 1994). based sample selection, which reduces redundant annotation of examples that contribute little new information.","{'title': '8 Conclusions', 'number': '8'}"
"The method can be applied in a semiinteractive process, in which the system selects several new examples for annotation at a time and updates its statistics after receiving their labels from the user.","{'title': '8 Conclusions', 'number': '8'}"
The implicit modeling of uncertainty makes the selection system generally applicable and quite simple to implement.,"{'title': '8 Conclusions', 'number': '8'}"
Our experimental study of variants of the selection method suggests several practical conclusions.,"{'title': '8 Conclusions', 'number': '8'}"
"First, it was found that the simplest version of the committee-based method, using a two-member committee, yields reduction in annotation cost comparable to that of the multi-member committee.","{'title': '8 Conclusions', 'number': '8'}"
"The two-member version is simpler to implement, has no parameters to tune and is computationally more efficient.","{'title': '8 Conclusions', 'number': '8'}"
"Second, we generalized the selection scheme giving several alternatives for optimizing the method for a specific task.","{'title': '8 Conclusions', 'number': '8'}"
"For bigram tagging, comparative evaluation of the different variants of the method showed similar large reductions in annotation cost, suggesting the robustness of the committee-based approach.","{'title': '8 Conclusions', 'number': '8'}"
"Third, sequential selection, which implicitly models the expected utility of an example relative to the example distribution, worked in general better than batch selection.","{'title': '8 Conclusions', 'number': '8'}"
"The latter was found to work well only for small batch sizes, where the method mimics sequential selection.","{'title': '8 Conclusions', 'number': '8'}"
Increasing batch size (approaching 'pure' batch selection) reduces both accuracy and efficiency.,"{'title': '8 Conclusions', 'number': '8'}"
"Finally, we studied the effect of sample selection on the size of the trained model, showing a significant reduction in model size.","{'title': '8 Conclusions', 'number': '8'}"
Our results suggest applying committee-based sample selection to other statistical NLP tasks which rely on estimating probabilistic parameters from an annotated corpus.,"{'title': '8 Conclusions', 'number': '8'}"
"Statistical methods for these tasks typically assign a probability estimate, or some other statistical score, to each alternative analysis (a word sense, a category label, a parse tree, etc.","{'title': '8 Conclusions', 'number': '8'}"
"), and then select the analysis with the highest score.","{'title': '8 Conclusions', 'number': '8'}"
"The score is usually computed as a function of the estimates of several 'atomic' parameters, often binomials or multinomials, such as: • In word sense disambiguation (Hearst, 1991; Gale, Church, and Yarowsky, 1993): P(slf), where s is a specific sense of the ambiguous word in question w, and f is a feature of occurrences of w. Common features are words in the context of w or morphological attributes of it.","{'title': '8 Conclusions', 'number': '8'}"
"• In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment.","{'title': '8 Conclusions', 'number': '8'}"
"Cornmon features are the words involved in the attachment, such as the head verb or noun, the preposition, and the head word of the PP.","{'title': '8 Conclusions', 'number': '8'}"
Applying committee-based selection to supervised training for such tasks can be done analogously to its application in the current papers.,"{'title': '8 Conclusions', 'number': '8'}"
"Furthermore, committee-based selection may be attempted also for training non-probabilistic classifiers, where explicit modeling of information gain is typically impossible.","{'title': '8 Conclusions', 'number': '8'}"
"In such contexts, committee members might be generated by randomly varying some of the decisions made in the learning algorithm.","{'title': '8 Conclusions', 'number': '8'}"
Another important area for future work is in developing sample selection methods which are independent of the eventual learning method to be applied.,"{'title': '8 Conclusions', 'number': '8'}"
This would be of considerable advantage in developing selectively annotated corpora for general research use.,"{'title': '8 Conclusions', 'number': '8'}"
"Recent work on heterogeneous uncertainty sampling (Lewis and Catlett, 1994) supports this idea, using one type of model for example selection and a different type for classification.","{'title': '8 Conclusions', 'number': '8'}"
Acknowledgments.,"{'title': '8 Conclusions', 'number': '8'}"
We thank Yoav Freund and Yishay Mansour for helpful discussions.,"{'title': '8 Conclusions', 'number': '8'}"
The first author gratefully acknowledges the support of the Fulbright Foundation.,"{'title': '8 Conclusions', 'number': '8'}"
