col1,col2
Entity Recognition systems need to integrate a wide variety of information for optimal performance.,{}
This paper demonstrates that a maximum entropy tagger can effectively encode such information and identify named entities with very high accuracy.,{}
"The tagger uses features which can be obtained for a variety of languages and works effectively not only for English, but also for other languages such as German and Dutch.",{}
Named Entity Recognition1 (NER) can be treated as a tagging problem where each word in a sentence is assigned a label indicating whether it is part of a named entity and the entity type.,"{'title': '1 Introduction', 'number': '1'}"
Thus methods used for part of speech (POS) tagging and chunking can also be used for NER.,"{'title': '1 Introduction', 'number': '1'}"
The papers from the CoNLL-2002 shared task which used such methods (e.g.,"{'title': '1 Introduction', 'number': '1'}"
"Malouf (2002), Burger et al. (2002)) reported results significantly lower than the best system (Carreras et al., 2002).","{'title': '1 Introduction', 'number': '1'}"
"However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger.","{'title': '1 Introduction', 'number': '1'}"
"Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method.","{'title': '1 Introduction', 'number': '1'}"
We demonstrate this to be the case by improving on the best Dutch results from CoNLL-2002 using a maximum entropy (ME) tagger.,"{'title': '1 Introduction', 'number': '1'}"
"We report reasonable precision and recall (84.9 F-score) for the CoNLL-2003 English test data, and an F-score of 68.4 for the CoNLL-2003 German test data.","{'title': '1 Introduction', 'number': '1'}"
Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.,"{'title': '1 Introduction', 'number': '1'}"
"In contrast, a ME tagger can easily deal with diverse, overlapping features.","{'title': '1 Introduction', 'number': '1'}"
We also use a Gaussian prior on the parameters for effective smoothing over the large feature space.,"{'title': '1 Introduction', 'number': '1'}"
The ME tagger is based on Ratnaparkhi (1996)’s POS tagger and is described in Curran and Clark (2003) .,"{'title': '2 The ME Tagger', 'number': '2'}"
"The tagger uses models of the form: where y is the tag, x is the context and the fi(x, y) are the features with associated weights λi.","{'title': '2 The ME Tagger', 'number': '2'}"
The probability of a tag sequence y1 ... yn given a sentence w1 ... wn is approximated as follows: where xi is the context for word wi.,"{'title': '2 The ME Tagger', 'number': '2'}"
The tagger uses beam search to find the most probable sequence given the sentence.,"{'title': '2 The ME Tagger', 'number': '2'}"
The features are binary valued functions which pair a tag with various elements of the context; for example: � Generalised Iterative Scaling (GIS) is used to estimate the values of the weights.,"{'title': '2 The ME Tagger', 'number': '2'}"
"The tagger uses a Gaussian prior over the weights (Chen et al., 1999) which allows a large number of rare, but informative, features to be used without overfitting.","{'title': '2 The ME Tagger', 'number': '2'}"
"We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL2002 shared task (Tjong Kim Sang, 2002).","{'title': '3 The Data', 'number': '3'}"
"Each word in the data sets is annotated with a named entity tag plus POS tag, and the words in the German and English data also have a chunk tag.","{'title': '3 The Data', 'number': '3'}"
Our system does not currently exploit the chunk tags.,"{'title': '3 The Data', 'number': '3'}"
"There are 4 types of entities to be recognised: persons, locations, organisations, and miscellaneous entities not belonging to the other three classes.","{'title': '3 The Data', 'number': '3'}"
The 2002 data uses the IOB-2 format in which a B-XXX tag indicates the first word of an entity of type XXX and I-XXX is used for subsequent words in an entity of type XXX.,"{'title': '3 The Data', 'number': '3'}"
The tag O indicates words outside of a named entity.,"{'title': '3 The Data', 'number': '3'}"
"The 2003 data uses a variant of IOB-2, IOB-1, in which I-XXX is used for all words in an entity, including the first word, unless the first word separates contiguous entities of the same type, in which case B-XXX is used.","{'title': '3 The Data', 'number': '3'}"
"Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger.","{'title': '4 The Feature Set', 'number': '4'}"
"The first set of features apply to rare words, i.e. those which appear less than 5 times in the training data.","{'title': '4 The Feature Set', 'number': '4'}"
"The first two kinds of features encode prefixes and suffixes less than length 5, and the remaining rare word features encode other morphological characteristics.","{'title': '4 The Feature Set', 'number': '4'}"
These features are important for tagging unknown and rare words.,"{'title': '4 The Feature Set', 'number': '4'}"
"The remaining features are the word, POS tag, and NE tag history features, using a window size of 2.","{'title': '4 The Feature Set', 'number': '4'}"
Note that the NEi−2NEi−1 feature is a composite feature of both the previous and previous-previous NE tags.,"{'title': '4 The Feature Set', 'number': '4'}"
Table 2 lists the extra features used in our final system.,"{'title': '4 The Feature Set', 'number': '4'}"
These features have been shown to be useful in other NER systems.,"{'title': '4 The Feature Set', 'number': '4'}"
"The additional orthographic features have proved useful in other systems, for example Carreras et al. (2002), Borthwick (1999) and Zhou and Su (2002).","{'title': '4 The Feature Set', 'number': '4'}"
Some of the rows in Table 2 describe sets of contextual predicates.,"{'title': '4 The Feature Set', 'number': '4'}"
The wi is only digits predicates apply to words consisting of all digits.,"{'title': '4 The Feature Set', 'number': '4'}"
They encode the length of the digit string with separate predicates for lengths 1–4 and a single predicate for lengths greater than 4.,"{'title': '4 The Feature Set', 'number': '4'}"
Titlecase applies to words with an initial uppercase letter followed by all lowercase (e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
Mr).,"{'title': '4 The Feature Set', 'number': '4'}"
Mixedcase applies to words with mixed lower- and uppercase (e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
CityBank).,"{'title': '4 The Feature Set', 'number': '4'}"
"The length predicates encode the number of characters in the word from 1 to 15, with a single predicate for lengths greater than 15.","{'title': '4 The Feature Set', 'number': '4'}"
The next set of contextual predicates encode extra information about NE tags in the current context.,"{'title': '4 The Feature Set', 'number': '4'}"
The memory NE tag predicate (see e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
Malouf (2002)) records the NE tag that was most recently assigned to the current word.,"{'title': '4 The Feature Set', 'number': '4'}"
The use of beam-search tagging means that tags can only be recorded from previous sentences.,"{'title': '4 The Feature Set', 'number': '4'}"
This memory is cleared at the beginning of each document.,"{'title': '4 The Feature Set', 'number': '4'}"
The unigram predicates (see e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
Tsukamoto et al. (2002)) encode the most probable tag for the next words in the window.,"{'title': '4 The Feature Set', 'number': '4'}"
The unigram probabilities are relative frequencies obtained from the training data.,"{'title': '4 The Feature Set', 'number': '4'}"
This feature enables us to know something about the likely NE tag of the next word before reaching it.,"{'title': '4 The Feature Set', 'number': '4'}"
"Most systems use gazetteers to encode information about personal and organisation names, locations and trigger words.","{'title': '4 The Feature Set', 'number': '4'}"
There is considerable variation in the size of the gazetteers used.,"{'title': '4 The Feature Set', 'number': '4'}"
Some studies found that gazetteers did not improve performance (e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
Malouf (2002)) whilst others gained significant improvement using gazetteers and triggers (e.g.,"{'title': '4 The Feature Set', 'number': '4'}"
Carreras et al. (2002)).,"{'title': '4 The Feature Set', 'number': '4'}"
Our system incorporates only English and Dutch first name and last name gazetteers as shown in Table 6.,"{'title': '4 The Feature Set', 'number': '4'}"
"These gazetteers are used for predicates applied to the current, previous and next word in the window.","{'title': '4 The Feature Set', 'number': '4'}"
Collins (2002) includes a number of interesting contextual predicates for NER.,"{'title': '4 The Feature Set', 'number': '4'}"
One feature we have adapted encodes whether the current word is more frequently seen lowercase than uppercase in a large external corpus.,"{'title': '4 The Feature Set', 'number': '4'}"
This feature is useful for disambiguating beginning of sentence capitalisation and tagging sentences which are all capitalised.,"{'title': '4 The Feature Set', 'number': '4'}"
The frequency counts have been obtained from 1 billion words of English newspaper text collected by Curran and Osborne (2002).,"{'title': '4 The Feature Set', 'number': '4'}"
Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes.,"{'title': '4 The Feature Set', 'number': '4'}"
This involves mapping characters to classes and merging adjacent characters of the same type.,"{'title': '4 The Feature Set', 'number': '4'}"
"For example, Moody becomes Aa, A.B.C. becomes A.A.A. and 1,345.05 becomes 0,0.0.","{'title': '4 The Feature Set', 'number': '4'}"
"The classes are used to define unigram, bigram and trigram contextual predicates over the window.","{'title': '4 The Feature Set', 'number': '4'}"
"We have also defined additional composite features which are a combination of atomic features; for example, a feature which is active for mid-sentence titlecase words seen more frequently as lowercase than uppercase in a large external corpus.","{'title': '4 The Feature Set', 'number': '4'}"
The baseline development results for English using the supertagger features only are given in Table 3.,"{'title': '5 Results', 'number': '5'}"
The full system results for the English development data are given in Table 7.,"{'title': '5 Results', 'number': '5'}"
Clearly the additional features have a significant impact on both precision and recall scores across all entities.,"{'title': '5 Results', 'number': '5'}"
"We have found that the word type features are particularly useful, as is the memory feature.","{'title': '5 Results', 'number': '5'}"
The performance of the final system drops by 1.97% if these features are removed.,"{'title': '5 Results', 'number': '5'}"
The performance of the system if the gazetteer features are removed is given in Table 4.,"{'title': '5 Results', 'number': '5'}"
The sizes of our gazetteers are given in Table 6.,"{'title': '5 Results', 'number': '5'}"
"We have experimented with removing the other contextual predicates but each time performance was reduced, except for the next-next unigram tag feature which was switched off for all final experiments.","{'title': '5 Results', 'number': '5'}"
The results for the Dutch test data are given in Table 5.,"{'title': '5 Results', 'number': '5'}"
"These improve upon the scores of the best performing system at CoNLL-2002 (Carreras et al., 2002).","{'title': '5 Results', 'number': '5'}"
The final results for the English test data are given in Table 7.,"{'title': '5 Results', 'number': '5'}"
These are significantly lower than the results for the development data.,"{'title': '5 Results', 'number': '5'}"
The results for the German development and test sets are given in Table 7.,"{'title': '5 Results', 'number': '5'}"
For the German NER we removed the lowercase more frequent than uppercase feature.,"{'title': '5 Results', 'number': '5'}"
"Apart from this change, the system was identical.","{'title': '5 Results', 'number': '5'}"
We did not add any extra gazetteer information for German.,"{'title': '5 Results', 'number': '5'}"
Our NER system demonstrates that using a large variety of features produces good performance.,"{'title': '6 Conclusion', 'number': '6'}"
"These features can be defined and extracted in a language independent manner, as our results for German, Dutch and English show.","{'title': '6 Conclusion', 'number': '6'}"
Maximum entropy models are an effective way of incorporating diverse and overlapping features.,"{'title': '6 Conclusion', 'number': '6'}"
"Our maximum entropy tagger employs Gaussian smoothing which allows a large number of sparse, but informative, features to be used without overfitting.","{'title': '6 Conclusion', 'number': '6'}"
"Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002).","{'title': '6 Conclusion', 'number': '6'}"
We would like to thank Jochen Leidner for help collecting the Gazetteers.,"{'title': 'Acknowledgements', 'number': '7'}"
"This research was supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.","{'title': 'Acknowledgements', 'number': '7'}"
