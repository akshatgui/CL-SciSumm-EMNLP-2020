col1,col2
Consumers are often forced to wade through many on-line reviews inorder to make an informed prod uct choice.,Introduction
"This paper introducesOPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evalu ation by reviewers, and their relative quality across products.",Introduction
"Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task.",Introduction
OPINE?s novel use ofrelaxation labeling for finding the semantic orientation of words in con text leads to strong performance on the tasks of finding opinion phrases and their polarity.,Introduction
"The Web contains a wealth of opinions about products, politicians, and more, which are expressed in newsgroupposts, review sites, and elsewhere.",Experiment/Discussion
"As a result, the prob lem of ?opinion mining?",Experiment/Discussion
"has seen increasing attention over the last three years from (Turney, 2002; Hu and Liu, 2004) and many others.",Experiment/Discussion
"This paper focuses on product reviews, though our methods apply to a broader range of opinions.",Experiment/Discussion
"Product reviews on Web sites such as amazon.com and elsewhere often associate meta-data with each review indicating how positive (or negative) it is using a 5-starscale, and also rank products by how they fare in the re views at the site.",Experiment/Discussion
"However, the reader?s taste may differ from the reviewers?.",Experiment/Discussion
"For example, the reader may feel strongly about the quality of the gym in a hotel, whereasmany reviewers may focus on other aspects of the ho tel, such as the decor or the location.",Experiment/Discussion
"Thus, the reader is forced to wade through a large number of reviews looking for information about particular features of interest.",Experiment/Discussion
We decompose the problem of review mining into the following main subtasks: I. Identify product features.,Experiment/Discussion
II.,Experiment/Discussion
Identify opinions regarding product features.,Experiment/Discussion
III.,Experiment/Discussion
Determine the polarity of opinions.,Experiment/Discussion
IV.,Experiment/Discussion
"Rank opinions based on their strength.This paper introduces OPINE, an unsupervised infor mation extraction system that embodies a solution to eachof the above subtasks.",Experiment/Discussion
"OPINE is built on top of the Know ItAll Web information-extraction system (Etzioni et al, 2005) as detailed in Section 3.",Experiment/Discussion
"Given a particular product and a corresponding set of reviews, OPINE solves the opinion mining tasks outlinedabove and outputs a set of product features, each accom panied by a list of associated opinions which are ranked based on strength (e.g., ?abominable?",Experiment/Discussion
is stronger than?bad).,Experiment/Discussion
This output information can then be used to gen erate various types of opinion summaries.This paper focuses on the first 3 review mining sub tasks and our contributions are as follows: 1.,Experiment/Discussion
"We introduce OPINE, a review-mining system whose.",Experiment/Discussion
novel components include the use of relaxation labeling to find the semantic orientation of words in the context of given product features and sentences.,Experiment/Discussion
"review-mining system (Hu and Liu, 2004) and find that OPINE?s precision on the feature extraction task is 22% better though its recall is 3% lower on Hu?s data sets.",Experiment/Discussion
We show that 1/3 of this increase in precision comes from using OPINE?s feature assessment mechanism on review data while the rest is due to Web PMI statistics.,Experiment/Discussion
3.,Experiment/Discussion
While many other systems have used extracted opin-.,Experiment/Discussion
"ion phrases in order to determine the polarity of sentences or documents, OPINE is the first to report its precision andrecall on the tasks of opinion phrase extraction and opin ion phrase polarity determination in the context of known product features and sentences.",Experiment/Discussion
"On the first task, OPINEhas a precision of 79% and a recall of 76%.",Experiment/Discussion
"On the sec ond task, OPINE has a precision of 86% and a recall of 89%.",Experiment/Discussion
"339 Input: product class C, reviews R. Output: set of [feature, ranked opinion list] tuples R??",Experiment/Discussion
parseReviews(R); E?,Experiment/Discussion
"findExplicitFeatures(R?, C); O?",Experiment/Discussion
"findOpinions(R?, E); CO? clusterOpinions(O); I?",Experiment/Discussion
"findImplicitFeatures(CO, E); RO?",Experiment/Discussion
"rankOpinions(CO); {(f , oi, ...oj)...}?outputTuples(RO, I ? E); Figure 1: OPINE Overview.",Experiment/Discussion
"The remainder of this paper is organized as follows: Section 2 introduces the basic terminology, Section 3 gives an overview of OPINE, describes and evaluates its main components, Section 4 describes related work and Section 5 presents our conclusion.",Experiment/Discussion
"A product class (e.g., Scanner) is a set of products (e.g.,Epson1200).",Experiment/Discussion
"OPINE extracts the following types of prod uct features: properties, parts, features of product parts, related concepts, parts and properties of related concepts(see Table 1 for examples of such features in the Scan ner domains).",Experiment/Discussion
Related concepts are concepts relevant to the customers?,Experiment/Discussion
"experience with the main product (e.g.,the company that manufactures a scanner).",Experiment/Discussion
"The relation ships between the main product and related concepts are typically expressed as verbs (e.g., ?Epson manufacturesscanners?)",Experiment/Discussion
or prepositions (?scanners from Epson?).,Experiment/Discussion
Features can be explicit (?good scan quality?),Experiment/Discussion
or im plicit (?good scans?,Experiment/Discussion
"implies good ScanQuality).OPINE also extracts opinion phrases, which are adjec tive, noun, verb or adverb phrases representing customer opinions.",Experiment/Discussion
"Opinions can be positive or negative and vary in strength (e.g., ?fantastic?",Experiment/Discussion
is stronger than ?good?).,Experiment/Discussion
"This section gives an overview of OPINE (see Figure 1)and describes its components and their experimental eval uation.Goal Given product class C with instances I and reviews R, OPINE?s goal is to find a set of (feature, opin ions) tuples {(f, oi, ...oj)} s.t. f ? F and oi, ...oj ? O, where: a) F is the set of product class features in R. b) O is the set of opinion phrases in R. c) f is a feature of a particular product instance.",Experiment/Discussion
d) o is an opinion about f in a particular sentence.,Experiment/Discussion
d) the opinions associated with each feature f are ranked based on their strength.Solution The steps of our solution are outlined in Figure 1 above.,Experiment/Discussion
"OPINE parses the reviews using MINI PAR (Lin, 1998) and applies a simple pronoun-resolution module to parsed review data.",Experiment/Discussion
OPINE then uses the datato find explicit product features (E).,Experiment/Discussion
OPINE?s Feature As sessor and its use of Web PMI statistics are vital for the extraction of high-quality features (see 3.2).,Experiment/Discussion
OPINE then identifies opinion phrases associated with features in Eand finds their polarity.,Experiment/Discussion
"OPINE?s novel use of relaxationlabeling techniques for determining the semantic orien tation of potential opinion words in the context of given features and sentences leads to high precision and recall on the tasks of opinion phrase extraction and opinion phrase polarity extraction (see 3.3).In this paper, we only focus on the extraction of explicit features, identifying corresponding customer opin ions about these features and determining their polarity.We omit the descriptions of the opinion clustering, im plicit feature generation and opinion ranking algorithms.",Experiment/Discussion
3.0.1 The KnowItAll System.,Experiment/Discussion
"OPINE is built on top of KnowItAll, a Web-based,domain-independent information extraction system (Et zioni et al, 2005).",Experiment/Discussion
"Given a set of relations of interest,KnowItAll instantiates relation-specific generic extrac tion patterns into extraction rules which find candidate facts.",Experiment/Discussion
KnowItAll?s Assessor then assigns a probability to each candidate.,Experiment/Discussion
"The Assessor uses a form of Point-wiseMutual Information (PMI) between phrases that is esti mated from Web search engine hit counts (Turney, 2001).",Experiment/Discussion
"It computes the PMI between each fact and automatically generated discriminator phrases (e.g., ?is a scanner?",Experiment/Discussion
for the isA() relationship in the context of the Scanner class).,Experiment/Discussion
"Given fact f and discriminator d, the computed PMI score is: PMI(f, d) = Hits(d+ f )Hits(d)?Hits(f ) The PMI scores are converted to binary features for aNaive Bayes Classifier, which outputs a probability asso ciated with each fact (Etzioni et al, 2005).",Experiment/Discussion
3.1 Finding Explicit Features.,Experiment/Discussion
OPINE extracts explicit features for the given productclass from parsed review data.,Experiment/Discussion
"First, the system recur sively identifies both the parts and the properties of the given product class and their parts and properties, in turn,continuing until no candidates are found.",Experiment/Discussion
"Then, the sys tem finds related concepts as described in (Popescu et al., 2004) and extracts their parts and properties.",Experiment/Discussion
Table 1 shows that each feature type contributes to the set of final features (averaged over 7 product classes).,Experiment/Discussion
Explicit Features Examples % Total Properties ScannerSize 7% Parts ScannerCover 52% Features of Parts BatteryLife 24% Related Concepts ScannerImage 9% Related Concepts?,Experiment/Discussion
"Features ScannerImageSize 8% Table 1: Explicit Feature Information 340In order to find parts and properties, OPINE first ex tracts the noun phrases from reviews and retains thosewith frequency greater than an experimentally set threshold.",Experiment/Discussion
"OPINE?s Feature Assessor, which is an instantia tion of KnowItAll?s Assessor, evaluates each noun phrase by computing the PMI scores between the phrase and meronymy discriminators associated with the product class (e.g., ?of scanner?, ?scanner has?, ?scanner comeswith?, etc. for the Scanner class).",Experiment/Discussion
"OPINE distinguishes parts from properties using WordNet?s IS-A hi erarchy (which enumerates different kinds of properties) and morphological cues (e.g., ?-iness?, ?-ity?",Experiment/Discussion
suffixes).,Experiment/Discussion
3.2 Experiments: Explicit Feature Extraction.,Experiment/Discussion
"In our experiments we use sets of reviews for 7 product classes (1621 total reviews) which include the pub licly available data sets for 5 product classes from (Huand Liu, 2004).",Experiment/Discussion
Hu?s system is the review mining sys tem most relevant to our work.,Experiment/Discussion
It uses association rulemining to extract frequent review noun phrases as features.,Experiment/Discussion
Frequent features are used to find potential opinion words (only adjectives) and the system uses Word Net synonyms/antonyms in conjunction with a set of seedwords in order to find actual opinion words.,Experiment/Discussion
"Finally, opinion words are used to extract associated infrequent fea tures.",Experiment/Discussion
"The system only extracts explicit features.On the 5 datasets in (Hu and Liu, 2004), OPINE?s precision is 22% higher than Hu?s at the cost of a 3% re call drop.",Experiment/Discussion
There are two important differences between OPINE and Hu?s system: a) OPINE?s Feature Assessor uses PMI assessment to evaluate each candidate feature and b) OPINE incorporates Web PMI statistics in addition to review data in its assessment.,Experiment/Discussion
"In the following, we quantify the performance gains from a) and b).",Experiment/Discussion
"a) In order to quantify the benefits of OPINE?s Feature Assessor, we use it to evaluate the features extracted by Hu?s algorithm on review data (Hu+A/R).",Experiment/Discussion
The Feature Assessor improves Hu?s precision by 6%.,Experiment/Discussion
"b) In order to evaluate the impact of using Web PMI statistics, we assess OPINE?s features first on reviews (OP/R) and then on reviews in conjunction with the Web (the corresponding methods are Hu+A/R+W andOPINE).",Experiment/Discussion
Web PMI statistics increase precision by an av erage of 14.5%.,Experiment/Discussion
"Overall, 1/3 of OPINE?s precision increase over Hu?s system comes from using PMI assessment on reviews and the other 2/3 from the use of the Web PMI statistics.",Experiment/Discussion
"In order to show that OPINE?s performance is robustacross multiple product classes, we used two sets of reviews downloaded from tripadvisor.com for Hotels and amazon.com for Scanners.",Experiment/Discussion
Two annotators la beled a set of unique 450 OPINE extractions as correct or incorrect.,Experiment/Discussion
The inter-annotator agreement was 86%.,Experiment/Discussion
"The extractions on which the annotators agreed were usedto compute OPINE?s precision, which was 89%.",Experiment/Discussion
Fur Data Explicit Feature Extraction: Precision Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.75 +0.05 +0.17 +0.07 +0.19 D2 0.71 +0.03 +0.19 +0.08 +0.22 D3 0.72 +0.03 +0.25 +0.09 +0.23 D4 0.69 +0.06 +0.22 +0.08 +0.25 D5 0.74 +0.08 +0.19 +0.04 +0.21 Avg 0.72 +0.06 + 0.20 +0.07 +0.22Table 2: Precision Comparison on the Explicit Feature Extraction Task.,Experiment/Discussion
OPINE?s precision is 22% better than Hu?sprecision; Web PMI statistics are responsible for 2/3 of the pre cision increase.,Experiment/Discussion
All results are reported with respect to Hu?s. Data Explicit Feature Extraction: Recall Hu Hu+A/R Hu+A/R+W OP/R OPINE D1 0.82 -0.16 -0.08 -0.14 -0.02 D2 0.79 -0.17 -0.09 -0.13 -0.06 D3 0.76 -0.12 -0.08 -0.15 -0.03 D4 0.82 -0.19 -0.04 -0.17 -0.03 D5 0.80 -0.16 -0.06 -0.12 -0.02 Avg 0.80 -0.16 -0.07 -0.14 -0.03Table 3: Recall Comparison on the Explicit Feature Extraction Task.,Experiment/Discussion
OPINE?s recall is 3% lower than the recall of Hu?s original system (precision level = 0.8).,Experiment/Discussion
"All results are reported with respect to Hu?s. thermore, the annotators extracted explicit features from800 review sentences (400 for each domain).",Experiment/Discussion
The inter annotator agreement was 82%.,Experiment/Discussion
OPINE?s recall on the set of 179 features on which both annotators agreed was 73%.,Experiment/Discussion
3.3 Finding Opinion Phrases and Their Polarity.,Experiment/Discussion
"This subsection describes how OPINE extracts potentialopinion phrases, distinguishes between opinions and non opinions, and finds the polarity of each opinion in thecontext of its associated feature in a particular review sen tence.",Experiment/Discussion
3.3.1 Extracting Potential Opinion PhrasesOPINE uses explicit features to identify potential opinion phrases.,Experiment/Discussion
Our intuition is that an opinion phrase as sociated with a product feature will occur in its vicinity.,Experiment/Discussion
"This idea is similar to that of (Kim and Hovy, 2004) and (Hu and Liu, 2004), but instead of using a window of size k or the output of a noun phrase chunker, OPINE takes advantage of the syntactic dependencies computed by theMINIPAR parser.",Experiment/Discussion
"Our intuition is embodied by 10 ex traction rules, some of which are shown in Table 4.",Experiment/Discussion
"If an explicit feature is found in a sentence, OPINE applies the extraction rules in order to find the heads of potentialopinion phrases.",Experiment/Discussion
Each head word together with its modi 341 fiers is returned as a potential opinion phrase1.,Experiment/Discussion
"Extraction Rules Examples if ?(M,NP = f)?",Experiment/Discussion
"po = M (expensive) scanner if ?(S = f, P,O)?",Experiment/Discussion
"po = O lamp has (problems) if ?(S, P,O = f)?",Experiment/Discussion
"po = P I (hate) this scanner if ?(S = f, P,O)?",Experiment/Discussion
po = P program (crashed) Table 4: Examples of Domain-independent Rules forthe Extraction of Potential Opinion Phrases.,Experiment/Discussion
"Nota tion: po=potential opinion, M=modifier, NP=noun phrase,S=subject, P=predicate, O=object.",Experiment/Discussion
Extracted phrases are en closed in parentheses.,Experiment/Discussion
Features are indicated by the typewriter font.,Experiment/Discussion
The equality conditions on the left-hand side use po?s head.,Experiment/Discussion
"Rule Templates Rules dep(w,w?) m(w,w?) ?v s.t. dep(w, v), dep(v, w?)",Experiment/Discussion
"?v s.t. m(w, v), o(v, w?)",Experiment/Discussion
"?v s.t. dep(w, v), dep(w?, v) ?v s.t. m(w, v), o(w?, v) Table 5: Dependency Rule Templates For Finding Words w, w?",Experiment/Discussion
with Related SO Labels . OPINE instantiates these templates in order to obtain extraction rules.,Experiment/Discussion
"Notation: dep=dependent, m=modifier, o=object, v,w,w?=words.",Experiment/Discussion
OPINE examines the potential opinion phrases in order to identify the actual opinions.,Experiment/Discussion
"First, the system finds thesemantic orientation for the lexical head of each poten tial opinion phrase.",Experiment/Discussion
Every phrase whose head word has a positive or negative semantic orientation is then retained as an opinion phrase.,Experiment/Discussion
"In the following, we describe how OPINE finds the semantic orientation of words.",Experiment/Discussion
"3.3.2 Word Semantic Orientation OPINE finds the semantic orientation of a word w in the context of an associated feature f and sentence s. We restate this task as follows: Task Given a set of semantic orientation (SO) labels ({positive, negative, neutral}), a set of reviews and a set of tuples (w, f , s), where w is a potential opinion word associated with feature f in sentence s, assign a SO label to each tuple (w, f , s).",Experiment/Discussion
"For example, the tuple (sluggish, driver, ?I am not happy with this sluggish driver?)",Experiment/Discussion
would be assigned a negative SO label.,Experiment/Discussion
Note: We use ?word?,Experiment/Discussion
to refer to a potential opinion word w and ?feature?,Experiment/Discussion
to refer to the word or phrase which represents the explicit feature f . Solution OPINE uses the 3-step approach below: 1.,Experiment/Discussion
"Given the set of reviews, OPINE finds a SO label for.",Experiment/Discussion
each word w. 2.,Experiment/Discussion
Given the set of reviews and the set of SO labels for.,Experiment/Discussion
"words w, OPINE finds a SO label for each (w, f ) pair.",Experiment/Discussion
"1The (S,P,O) tuples in Table 4 are automatically generated from MINIPAR?s output.",Experiment/Discussion
3.,Experiment/Discussion
"Given the set of SO labels for (w, f ) pairs, OPINE.",Experiment/Discussion
"finds a SO label for each (w, f , s) input tuple.Each of these subtasks is cast as an unsupervised col lective classification problem and solved using the samemechanism.",Experiment/Discussion
"In each case, OPINE is given a set of objects (words, pairs or tuples) and a set of labels (SO labels); OPINE then searches for a global assignment of la bels to objects.",Experiment/Discussion
"In each case, OPINE makes use of local constraints on label assignments (e.g., conjunctions and disjunctions constraining the assignment of SO labels to words (Hatzivassiloglou and McKeown, 1997)).",Experiment/Discussion
"A key insight in OPINE is that the problem of searching for a global SO label assignment to words, pairs or tupleswhile trying to satisfy as many local constraints on as signments as possible is analogous to labeling problems in computer vision (e.g., model-based matching).",Experiment/Discussion
"OPINE uses a well-known computer vision technique, relaxation labeling (Hummel and Zucker, 1983), in order to solve the three subtasks described above.",Experiment/Discussion
"3.3.3 Relaxation Labeling Overview Relaxation labeling is an unsupervised classification technique which takes as input: a) a set of objects (e.g., words) b) a set of labels (e.g., SO labels) c) initial probabilities for each object?s possible labels d) the definition of an object o?s neighborhood (a set of other objects which influence the choice of o?s label) e) the definition of neighborhood features f) the definition of a support function for an object labelThe influence of an object o?s neighborhood on its label L is quantified using the support function.",Experiment/Discussion
The support function computes the probability of the label L being assigned to o as a function of o?s neighborhood fea tures.,Experiment/Discussion
"Examples of features include the fact that a certainlocal constraint is satisfied (e.g., the word nice partic ipates in the conjunction and together with some other word whose SO label is estimated to be positive).",Experiment/Discussion
Relaxation labeling is an iterative procedure whoseoutput is an assignment of labels to objects.,Experiment/Discussion
"At each itera tion, the algorithm uses an update equation to reestimate the probability of an object label based on its previous probability estimate and the features of its neighborhood.",Experiment/Discussion
The algorithm stops when the global label assignment stays constant over multiple consecutive iterations.We employ relaxation labeling for the following rea sons: a) it has been extensively used in computer-vision with good results b) its formalism allows for many typesof constraints on label assignments to be used simulta neously.,Experiment/Discussion
"As mentioned before, constraints are integratedinto the algorithm as neighborhood features which influ ence the assignment of a particular label to a particular object.",Experiment/Discussion
OPINE uses the following sources of constraints: 342 a) conjunctions and disjunctions in the review textb) manually-supplied syntactic dependency rule templates (see Table 5).,Experiment/Discussion
"The templates are automatically instantiated by our system with different dependency re lationships (premodifier, postmodifier, subject, etc.) in order to obtain syntactic dependency rules which find words with related SO labels.",Experiment/Discussion
"c) automatically derived morphological relationships (e.g., ?wonderful?",Experiment/Discussion
and ?wonderfully?,Experiment/Discussion
are likely to have similar SO labels).,Experiment/Discussion
"d) WordNet-supplied synonymy, antonymy, IS-A andmorphological relationships between words.",Experiment/Discussion
"For exam ple, clean and neat are synonyms and so they are likely to have similar SO labels.",Experiment/Discussion
Each of the SO label assignment subtasks previously identified is solved using a relaxation labeling step.,Experiment/Discussion
"In the following, we describe in detail how relaxation labeling is used to find SO labels for words in the given review sets.",Experiment/Discussion
"3.3.4 Finding SO Labels for Words For many words, a word sense or set of senses is used throughout the review corpus with a consistently positive, negative or neutral connotation (e.g., ?great?, ?awful?, etc.).",Experiment/Discussion
"Thus, in many cases, a word w?s SO label in the context of a feature f and sentence s will be the same as its SO label in the context of other features and sentences.In the following, we describe how OPINE?s relaxation la beling mechanism is used to find a word?s dominant SO label in a set of reviews.",Experiment/Discussion
"For this task, a word?s neighborhood is defined as the set of words connected to it through conjunctions,disjunctions and all other relationships previously intro duced as sources of constraints.RL uses an update equation to re-estimate the probability of a word label based on its previous probabil ity estimate and the features of its neighborhood (see Neighborhood Features).",Experiment/Discussion
"At iteration m, let q(w,L)(m) denote the support function for label L of w and let P (l(w) = L)(m) denote the probability that L is the label of w. P (l(w) = L)(m+1) is computed as follows: RL Update Equation (Rangarajan, 2000) P (l(w) = L)(m+1) = P (l(w) = L)(m)(1 + ?q(w,L)(m)) P L?",Experiment/Discussion
"P (l(w) = L ?)(m)(1 + ?q(w,L?)(m)) where L?",Experiment/Discussion
"{pos, neg, neutral} and ? > 0 is an experimentally set constant keeping the numerator and probabilities positive.",Experiment/Discussion
RL?s output is an assignment of dominant SO labels to words.,Experiment/Discussion
"In the following, we describe in detail the initialization step, the derivation of the support function formula and the use of neighborhood features.RL Initialization Step OPINE uses a version of Turney?s PMI-based approach (Turney, 2003) in order to de rive the initial probability estimates (P (l(w) = L)(0)) for a subset S of the words.",Experiment/Discussion
"OPINE computes a SO score so(w) for each w in S as the difference between the PMI of w with positive keywords (e.g., ?excellent?)",Experiment/Discussion
"and the PMI of w with negative keywords (e.g., ?awful?).When so(w) is small, or w rarely co-occurs with the key words, w is classified as neutral.",Experiment/Discussion
"If so(w) > 0, then w is positive, otherwise w is negative.",Experiment/Discussion
"OPINE then uses the labeled S set in order to compute prior probabilities P (l(w) = L), L ? {pos, neg, neutral} by computing the ratio between the number of words in S labeled Land |S|.",Experiment/Discussion
Such probabilities are used as initial probabil ity estimates associated with the labels of the remaining words.,Experiment/Discussion
"Support Function The support function computes the probability of each label for word w based on the labels of objects in w?s neighborhood N .Let Ak = {(wj , Lj)|wj ? N} , 0 < k ? 3|N | rep resent one of the potential assignments of labels to the words in N . Let P (Ak)(m) denote the probability of thisparticular assignment at iteration m. The support for la bel L of word w at iteration m is : q(w,L)(m) = 3|N|X k=1 P (l(w) = L|Ak)(m) ? P (Ak)(m)We assume that the labels of w?s neighbors are inde pendent of each other and so the formula becomes: q(w,L)(m) = 3|N|X k=1 P (l(w) = L|Ak)(m)?",Experiment/Discussion
|N|Y j=1 P (l(wj) = Lj)(m) Every P (l(wj) = Lj)(m) term is the estimate for theprobability that l(wj) = Lj (which was computed at it eration m using the RL update equation).,Experiment/Discussion
The P (l(w) = L|Ak)(m) term quantifies the influence of a particular label assignment to w?s neighborhood over w?s label.,Experiment/Discussion
"In the following, we describe how we estimate this term.",Experiment/Discussion
"Neighborhood Features Each type of word relationship which constrains the assignment of SO labels to words (synonymy, antonymy, etc.) is mapped by OPINE to a neighborhood feature.",Experiment/Discussion
Thismapping allows OPINE to use simultaneously use multi ple independent sources of constraints on the label of aparticular word.,Experiment/Discussion
"In the following, we formalize this map ping.Let T denote the type of a word relationship in R (syn onym, antonym, etc.) and let Ak,T represent the labelsassigned by Ak to neighbors of a word w which are con nected to w through a relationship of type T . We have Ak = ? T Ak,T and P (l(w) = L|Ak)(m) = P (l(w) = L| [ T Ak,T )(m) For each relationship type T , OPINE defines a neighborhood feature fT (w,L,Ak,T ) which computes P (l(w) = L|Ak,T ), the probability that w?s label is L given Ak,T (see below).",Experiment/Discussion
"P (l(w) = L| ? T Ak,T )(m) isestimated combining the information from various fea tures about w?s label using the sigmoid function ?(): 343 P (l(w) = L|Ak)(m) = ?( jX i=1 f i(w,L,Ak,i)(m) ? ci) where c0, ...cj are weights whose sum is 1 and which reflect OPINE ?s confidence in each type of feature.Given word w, label L, relationship type T and neigh borhood label assignment Ak, let NT represent the subsetof w?s neighbors connected to w through a type T rela tionship.",Experiment/Discussion
"The feature fT computes the probability that w?s label is L given the labels assigned by Ak to wordsin NT . Using Bayes?s Law and assuming that these la bels are independent given l(w), we have the following formula for fT at iteration m: fT (w,L,Ak,T )(m) = P (l(w) = L)(m)?",Experiment/Discussion
"|NT |Y j=1 P (Lj |l(w) = L) P (Lj |l(w) = L) is the probability that word wj has label Lj if wj and w are linked by a relationship of type T and w has label L. We make the simplifying assumption that this probability is constant and depends only of T , L and L?, not of the particular words wj and w. For each tuple (T , L, Lj), L,Lj ? {pos, neg, neutral}, OPINE buildsa probability table using a small set of bootstrapped pos itive, negative and neutral words.",Experiment/Discussion
"3.3.5 Finding (Word, Feature) SO Labels This subtask is motivated by the existence of frequent words which change their SO label based on associatedfeatures, but whose SO labels in the context of the respec tive features are consistent throughout the reviews (e.g.,in the Hotel domain, ?hot water?",Experiment/Discussion
"has a consistently posi tive connotation, whereas ?hot room?",Experiment/Discussion
has a negative one).,Experiment/Discussion
"In order to solve this task, OPINE first assigns each (w, f) pair an initial SO label which is w?s SO label.",Experiment/Discussion
"The system then executes a relaxation labeling step duringwhich syntactic relationships between words and, respec tively, between features, are used to update the default SO labels whenever necessary.",Experiment/Discussion
"For example, (hot, room) appears in the proximity of (broken, fan).",Experiment/Discussion
If ?room?and ?fan?,Experiment/Discussion
"are conjoined by and, this suggests that ?hot?",Experiment/Discussion
and ?broken?,Experiment/Discussion
have similar SO labels in the context of their respective features.,Experiment/Discussion
If ?broken?,Experiment/Discussion
"has a strongly negativesemantic orientation, this fact contributes to OPINE?s be lief that ?hot?",Experiment/Discussion
may also be negative in this context.,Experiment/Discussion
"Since (hot, room) occurs in the vicinity of other such phrases (e.g., stifling kitchen), ?hot?",Experiment/Discussion
acquires a negative SO label in the context of ?room?.,Experiment/Discussion
"3.3.6 Finding (Word, Feature, Sentence) SO Labels This subtask is motivated by the existence of (w,f ) pairs (e.g., (big, room)) for which w?s orientation changes based on the sentence in which the pair appears (e.g., ? I hated the big, drafty room because I ended up freezing.?",Experiment/Discussion
"vs. ?We had a big, luxurious room?.)",Experiment/Discussion
"In order to solve this subtask, OPINE first assigns each(w, f, s) tuple an initial label which is simply the SO la bel for the (w, f) pair.",Experiment/Discussion
"The system then uses syntactic relationships between words and, respectively, features in order to update the SO labels when necessary.",Experiment/Discussion
"For example, in the sentence ?I hated the big, drafty room because I ended up freezing.?, ?big?",Experiment/Discussion
and ?hate?,Experiment/Discussion
satisfy condition 2 in Table 5 and therefore OPINE expects themto have similar SO labels.,Experiment/Discussion
Since ?hate?,Experiment/Discussion
"has a strong neg ative connotation, ?big?",Experiment/Discussion
acquires a negative SO label in this context.,Experiment/Discussion
"In order to correctly update SO labels in this last step, OPINE takes into consideration the presence of negation modifiers.",Experiment/Discussion
"For example, in the sentence ?I don?t like a large scanner either?, OPINE first replaces the positive (w, f) pair (like, scanner) with the negative labeled pair (not like, scanner) and then infers that ?large?",Experiment/Discussion
is likely to have a negative SO label in this context.,Experiment/Discussion
"3.3.7 Identifying Opinion Phrases After OPINE has computed the most likely SO labels for the head words of each potential opinion phrase in thecontext of given features and sentences, OPINE can ex tract opinion phrases and establish their polarity.",Experiment/Discussion
Phraseswhose head words have been assigned positive or nega tive labels are retained as opinion phrases.,Experiment/Discussion
"Furthermore,the polarity of an opinion phrase o in the context of a fea ture f and sentence s is given by the SO label assigned to the tuple (head(o), f, s) (3.3.6 shows how OPINE takes into account negation modifiers).",Experiment/Discussion
3.4 Experiments.,Experiment/Discussion
"In this section we evaluate OPINE?s performance on thefollowing tasks: finding SO labels of words in the context of known features and sentences (SO label extrac tion); distinguishing between opinion and non-opinion phrases in the context of known features and sentences (opinion phrase extraction); finding the correct polarityof extracted opinion phrases in the context of known fea tures and sentences (opinion phrase polarity extraction).While other systems, such as (Hu and Liu, 2004; Tur ney, 2002), have addressed these tasks to some degree, OPINE is the first to report results.",Experiment/Discussion
We first ran OPINE on 13841 sentences and 538 previously extracted features.OPINE searched for a SO label assignment for 1756 different words in the context of the given features and sentences.,Experiment/Discussion
"We compared OPINE against two baseline meth ods, PMI++ and Hu++.",Experiment/Discussion
"PMI++ is an extended version of (Turney, 2002)?smethod for finding the SO label of a phrase (as an at tempt to deal with context-sensitive words).",Experiment/Discussion
"For a given(word, feature, sentence) tuple, PMI++ ignores the sentence, generates a phrase based on the word and the fea ture (e.g., (clean, room): ?clean room?)",Experiment/Discussion
and finds its SO label using PMI statistics.,Experiment/Discussion
"If unsure of the label, PMI++ tries to find the orientation of the potential opinion word instead.",Experiment/Discussion
"The search engine queries use domain-specific keywords (e.g., ?scanner?), which are dropped if they 344 lead to low counts.",Experiment/Discussion
Hu++ is a WordNet-based method for finding a word?s context-independent semantic orientation.,Experiment/Discussion
"It extends Hu?s adjective labeling method in a number of ways in order to handle nouns, verbs and adverbs in addition to adjectives and in order to improve coverage.",Experiment/Discussion
Hu?s method starts with two sets of positive and negative words and iteratively grows each one by including synonyms andantonyms from WordNet.,Experiment/Discussion
The final sets are used to pre dict the orientation of an incoming word.,Experiment/Discussion
Type PMI++ Hu++ OPINE P R P R P R adj 0.73 0.91 +0.02 -0.17 +0.07 -0.03 nn 0.63 0.92 +0.04 -0.24 +0.11 -0.08 vb 0.71 0.88 +0.03 -0.12 +0.01 -0.01 adv 0.82 0.92 +0.02 -0.01 +0.06 +0.01 Avg 0.72 0.91 +0.03 -0.14 +0.06 -0.03 Table 6: Finding SO Labels of Potential Opinion Words in the Context of Given Product Features and Sentences.,Experiment/Discussion
OPINE?s precision is higher than that of PMI++ and Hu++.,Experiment/Discussion
"All results are reported with respect to PMI++ . Notation: adj=adjectives, nn=nouns, vb=verbs, adv=adverbs 3.4.1 Experiments: SO LabelsOn the task of finding SO labels for words in the con text of given features and review sentences, OPINE obtains higher precision than both baseline methods at a smallloss in recall with respect to PMI++.",Experiment/Discussion
"As described be low, this result is due in large part to OPINE?s ability to handle context-sensitive opinion words.",Experiment/Discussion
"We randomly selected 200 (word, feature, sentence) tuples for each word type (adjective, adverb, etc.) andobtained a test set containing 800 tuples.",Experiment/Discussion
"Two annota tors assigned positive, negative and neutral labels to eachtuple (the inter-annotator agreement was 78%).",Experiment/Discussion
We re tained the tuples on which the annotators agreed as the gold standard.,Experiment/Discussion
We ran PMI++ and Hu++ on the test data and compared the results against OPINE?s results on the same data.,Experiment/Discussion
"In order to quantify the benefits of each of the threesteps of our method for finding SO labels, we also compared OPINE with a version which only finds SO la bels for words and a version which finds SO labels for words in the context of given features, but doesn?t take into account given sentences.",Experiment/Discussion
We have learned from this comparison that OPINE?s precision gain over PMI++ andHu++ is mostly due to to its ability to handle context sensitive words in a large number of cases.,Experiment/Discussion
"Although Hu++ does not handle context-sensitive SO label assignment, its average precision was reasonable (75%) and better than that of PMI++.",Experiment/Discussion
"Finding a word?s SO label is good enough in the case of strongly positiveor negative opinion words, which account for the major ity of opinion instances.",Experiment/Discussion
"The method?s loss in recall is due to not recognizing words absent from WordNet (e.g., ?depth-adjustable?)",Experiment/Discussion
or not having enough information to classify some words in WordNet.,Experiment/Discussion
PMI++ typically does well in the presence of strongly positive or strongly negative words.,Experiment/Discussion
"Its high recall iscorrelated with decreased precision, but overall this sim ple approach does well.",Experiment/Discussion
PMI++?s main shortcoming is misclassifying terms such as ?basic?,Experiment/Discussion
or ?visible?,Experiment/Discussion
which change orientation based on context.,Experiment/Discussion
"3.4.2 Experiments: Opinion Phrases In order to evaluate OPINE on the tasks of opinion phrase extraction and opinion phrase polarity extraction in the context of known features and sentences, we used aset of 550 sentences containing previously extracted fea tures.",Experiment/Discussion
The sentences were annotated with the opinion phrases corresponding to the known features and with the opinion polarity.,Experiment/Discussion
We compared OPINE with PMI++ and Hu++ on the tasks of interest.,Experiment/Discussion
We found that OPINE hadthe highest precision on both tasks at a small loss in re call with respect to PMI++.,Experiment/Discussion
OPINE?s ability to identify a word?s SO label in the context of a given feature and sentence allows the system to correctly extract opinionsexpressed by words such as ?big?,Experiment/Discussion
"or ?small?, whose se mantic orientation varies based on context.",Experiment/Discussion
Measure PMI++ Hu++ OPINE OP Extraction: Precision 0.71 +0.06 +0.08 OP Extraction: Recall 0.78 -0.08 -0.02 OP Polarity: Precision 0.80 -0.04 +0.06 OP Polarity: Recall 0.93 +0.07 -0.04 Table 7: Extracting Opinion Phrases and Opinion Phrase Polarity Corresponding to Known Features and Sentences.,Experiment/Discussion
OPINE?s precision is higher than that of PMI++ and of Hu++.,Experiment/Discussion
All results are reported with respect to PMI++.,Experiment/Discussion
The key components of OPINE described in this paper are the PMI feature assessment which leads to high-precisionfeature extraction and the use of relaxation-labeling in or der to find the semantic orientation of potential opinionwords.,Experiment/Discussion
"The review-mining work most relevant to our re search is that of (Hu and Liu, 2004) and (Kobayashi et al., 2004).",Experiment/Discussion
"Both identify product features from reviews, but OPINE significantly improves on both.",Experiment/Discussion
"(Hu and Liu, 2004) doesn?t assess candidate features, so its precision is lower than OPINE?s.",Experiment/Discussion
"(Kobayashi et al, 2004) employsan iterative semi-automatic approach which requires human input at every iteration.",Experiment/Discussion
Neither model explicitly ad dresses composite (feature of feature) or implicit features.,Experiment/Discussion
"Other systems (Morinaga et al, 2002; Kushal et al, 2003) also look at Web product reviews but they do not extract 345 opinions about particular product features.",Experiment/Discussion
"OPINE?s use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004).",Experiment/Discussion
"Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; Riloff et al, 2003; Wiebe, 2000; Hatzivassiloglou and McKeown, 1997).",Experiment/Discussion
"Most recently, (Takamura et al, 2005) reports on the use of spin models to infer the semantic orientation of words.",Experiment/Discussion
"The paper?s global optimization approach and use of multiple sources of constraints on a word?s semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information.",Experiment/Discussion
"Subjective phrases are used by (Turney, 2002; Pang and Vaithyanathan, 2002; Kushal et al, 2003; Kim and Hovy, 2004) and others in order to classify reviews or sentences as positive or negative.",Experiment/Discussion
"So far, OPINE?s focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.",Experiment/Discussion
"OPINE is an unsupervised information extraction systemwhich extracts fine-grained features, and associated opinions, from reviews.",Results/Conclusion
OPINE?s use of the Web as a corpus helps identify product features with improved preci sion compared with previous work.,Results/Conclusion
OPINE uses a novel relaxation-labeling technique to determine the semantic orientation of potential opinion words in the context ofthe extracted product features and specific review sentences; this technique allows the system to identify cus tomer opinions and their polarity with high precision and recall.,Results/Conclusion
We would like to thank the KnowItAll project and theanonymous reviewers for their comments.,Results/Conclusion
"Michael Gamon, Costas Boulis and Adam Carlson have also pro vided valuable feedback.",Results/Conclusion
We thank Minquing Hu andBing Liu for providing their data sets and for their com ments.,Results/Conclusion
"Finally, we are grateful to Bernadette Minton and Fetch Technologies for their help in collecting additional reviews.",Results/Conclusion
"This research was supported in part by NSF grant IIS-0312988, DARPA contract NBCHD030010, ONR grant N00014-02-1-0324 as well as gifts from Google and the Turing Center.",Results/Conclusion
