col1,col2
We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.,{}
"Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.",{}
"We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.",{}
The ability to detect complex patterns in data is limited by the complexity of the data’s representation.,"{'title': '1 Introduction', 'number': '1'}"
"In the case of text, a more structured data source (e.g. a relational database) allows richer queries than does an unstructured data source (e.g. a collection of news articles).","{'title': '1 Introduction', 'number': '1'}"
"For example, current web search engines would not perform well on the query, “list all California-based CEOs who have social ties with a United States Senator.” Only a structured representation of the data can effectively provide such a list.","{'title': '1 Introduction', 'number': '1'}"
The goal of Information Extraction (IE) is to discover relevant segments of information in a data stream that will be useful for structuring the data.,"{'title': '1 Introduction', 'number': '1'}"
"In the case of text, this usually amounts to finding mentions of interesting entities and the relations that join them, transforming a large corpus of unstructured text into a relational database with entries such as those in Table 1.","{'title': '1 Introduction', 'number': '1'}"
"IE is commonly viewed as a three stage process: first, an entity tagger detects all mentions of interest; second, coreference resolution resolves disparate mentions of the same entity; third, a relation extractor finds relations between these entities.","{'title': '1 Introduction', 'number': '1'}"
"Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003).","{'title': '1 Introduction', 'number': '1'}"
"Coreference resolution is an active area of research not investigated here (Pasula et al., 2002; McCallum and Wellner, 2003).","{'title': '1 Introduction', 'number': '1'}"
We describe a relation extraction technique based on kernel methods.,"{'title': '1 Introduction', 'number': '1'}"
"Kernel methods are nonparametric density estimation techniques that compute a kernel function between data instances, where a kernel function can be thought of as a similarity measure.","{'title': '1 Introduction', 'number': '1'}"
"Given a set of labeled instances, kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function.","{'title': '1 Introduction', 'number': '1'}"
"Nearest neighbor classification and support-vector machines (SVMs) are two popular examples of kernel methods (Fukunaga, 1990; Cortes and Vapnik, 1995).","{'title': '1 Introduction', 'number': '1'}"
An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach.,"{'title': '1 Introduction', 'number': '1'}"
"This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances, as described in the Section 3.","{'title': '1 Introduction', 'number': '1'}"
Working in such a large feature space can lead to over-fitting in many machine learning algorithms.,"{'title': '1 Introduction', 'number': '1'}"
"To address this problem, we apply SVMs to the task of relation extraction.","{'title': '1 Introduction', 'number': '1'}"
SVMs find a boundary between instances of different classes such that the distance between the boundary and the nearest instances is maximized.,"{'title': '1 Introduction', 'number': '1'}"
"This characteristic, in addition to empirical validation, indicates that SVMs are particularly robust to over-fitting.","{'title': '1 Introduction', 'number': '1'}"
"Here we are interested in detecting and classifying instances of relations, where a relation is some meaningful connection between two entities (Table 2).","{'title': '1 Introduction', 'number': '1'}"
We represent each relation instance as an augmented dependency tree.,"{'title': '1 Introduction', 'number': '1'}"
A dependency tree represents the grammatical dependencies in a sentence; we augment this tree with features for each node (e.g. part of speech) We choose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees.,"{'title': '1 Introduction', 'number': '1'}"
The task of the kernel function is to find these similarities.,"{'title': '1 Introduction', 'number': '1'}"
We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents.,"{'title': '1 Introduction', 'number': '1'}"
"The tree kernel approach consistently outperforms the bag-ofwords kernel, suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations.","{'title': '1 Introduction', 'number': '1'}"
"Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space.","{'title': '2 Related Work', 'number': '2'}"
Haussler (1999) describes a framework for calculating kernels over discrete structures such as strings and trees.,"{'title': '2 Related Work', 'number': '2'}"
"String kernels for text classification are explored in Lodhi et al. (2000), and tree kernel variants are described in (Zelenko et al., 2003; Collins and Duffy, 2002; Cumby and Roth, 2003).","{'title': '2 Related Work', 'number': '2'}"
Our algorithm is similar to that described by Zelenko et al. (2003).,"{'title': '2 Related Work', 'number': '2'}"
"Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity.","{'title': '2 Related Work', 'number': '2'}"
"Brin (1998) and Agichtein and Gravano (2000) apply pattern matching and wrapper techniques for relation extraction, but these approaches do not scale well to fastly evolving corpora.","{'title': '2 Related Work', 'number': '2'}"
Miller et al. (2000) propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types.,"{'title': '2 Related Work', 'number': '2'}"
"Whereas Miller et al. (2000) use a generative model to produce parse information as well as relation information, we hypothesize that a technique discriminatively trained to classify relations will achieve better performance.","{'title': '2 Related Work', 'number': '2'}"
"Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously.","{'title': '2 Related Work', 'number': '2'}"
We experiment with a more challenging set of relation types and a larger corpus.,"{'title': '2 Related Work', 'number': '2'}"
"In traditional machine learning, we are provided a set of training instances S = {x1 ... xN}, where each instance xi is represented by some ddimensional feature vector.","{'title': '3 Kernel Methods', 'number': '3'}"
"Much time is spent on the task of feature engineering – searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection (Scott and Matwin, 1999).","{'title': '3 Kernel Methods', 'number': '3'}"
"For example, in entity detection the original instance representation is generally a word vector corresponding to a sentence.","{'title': '3 Kernel Methods', 'number': '3'}"
"Feature extraction and induction may result in features such as part-ofspeech, word n-grams, character n-grams, capitalization, and conjunctions of these features.","{'title': '3 Kernel Methods', 'number': '3'}"
"In the case of more structured objects, such as parse trees, features may include some description of the object’s structure, such as “has an NP-VP subtree.” Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects.","{'title': '3 Kernel Methods', 'number': '3'}"
"By calculating the similarity between two objects, kernel methods can employ dynamic programming solutions to efficiently enumerate over substructures that would be too costly to explicitly include as features.","{'title': '3 Kernel Methods', 'number': '3'}"
"Formally, a kernel function K is a mapping tion over the instance x.","{'title': '3 Kernel Methods', 'number': '3'}"
"The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite.","{'title': '3 Kernel Methods', 'number': '3'}"
"By positive-semidefinite, we require that the if x1, ... , xn E X, then the n x n matrix G defined by Gij = K(xi, xj) is positive semidefinite.","{'title': '3 Kernel Methods', 'number': '3'}"
"It has been shown that any function that takes the dot product of feature vectors is a kernel function (Haussler, 1999).","{'title': '3 Kernel Methods', 'number': '3'}"
A simple kernel function takes the dot product of the vector representation of instances being compared.,"{'title': '3 Kernel Methods', 'number': '3'}"
"For example, in document classification, each document can be represented by a binary vector, where each element corresponds to the presence or absence of a particular word in that document.","{'title': '3 Kernel Methods', 'number': '3'}"
"Here, Oi(x) = 1 if word i occurs in document x.","{'title': '3 Kernel Methods', 'number': '3'}"
"Thus, the kernel function K(x, y) returns the number of words in common between x and y.","{'title': '3 Kernel Methods', 'number': '3'}"
"We refer to this kernel as the “bag-of-words” kernel, since it ignores word order.","{'title': '3 Kernel Methods', 'number': '3'}"
"When instances are more structured, as in the case of dependency trees, more complex kernels become necessary.","{'title': '3 Kernel Methods', 'number': '3'}"
"Haussler (1999) describes convolution kernels, which find the similarity between two structures by summing the similarity of their substructures.","{'title': '3 Kernel Methods', 'number': '3'}"
"As an example, consider a kernel over strings.","{'title': '3 Kernel Methods', 'number': '3'}"
"To determine the similarity between two strings, string kernels (Lodhi et al., 2000) count the number of common subsequences in the two strings, and weight these matches by their length.","{'title': '3 Kernel Methods', 'number': '3'}"
"Thus, Oi(x) is the number of times string x contains the subsequence referenced by i.","{'title': '3 Kernel Methods', 'number': '3'}"
"These matches can be found efficiently through a dynamic program, allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method.","{'title': '3 Kernel Methods', 'number': '3'}"
"Given a training set S = {xs ... xN}, kernel methods compute the Gram matrix G such that Gij = K(xi,xj).","{'title': '3 Kernel Methods', 'number': '3'}"
"Given G, the classifier finds a hyperplane which separates instances of different classes.","{'title': '3 Kernel Methods', 'number': '3'}"
"To classify an unseen instance x, the classifier first projects x into the feature space defined by the kernel function.","{'title': '3 Kernel Methods', 'number': '3'}"
Classification then consists of determining on which side of the separating hyperplane x lies.,"{'title': '3 Kernel Methods', 'number': '3'}"
"A support vector machine (SVM) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem (Cristianini and Shawe-Taylor, 2000).","{'title': '3 Kernel Methods', 'number': '3'}"
Support vector machines attempt to find a hyperplane that not only separates the classes but also maximizes the margin between them.,"{'title': '3 Kernel Methods', 'number': '3'}"
The hope is that this will lead to better generalization performance on unseen instances.,"{'title': '3 Kernel Methods', 'number': '3'}"
Our task is to detect and classify relations between entities in text.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"We assume that entity tagging has been performed; so to generate potential relation instances, we iterate over all pairs of entities occurring in the same sentence.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"For each entity pair, we create an augmented dependency tree (described below) representing this instance.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"Given a labeled training set of potential relations, we define a tree kernel over dependency trees which we then use in an SVM to classify test instances.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
A dependency tree is a representation that denotes grammatical relations between words in a sentence (Figure 1).,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
A set of rules maps a parse tree to a dependency tree.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"For example, subjects are dependent on their verbs and adjectives are dependent the dependency tree. on the nouns they modify.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"Note that for the purposes of this paper, we do not consider the link labels (e.g.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"“object”, “subject”); instead we use only the dependency structure.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"To generate the parse tree of each sentence, we use MXPOST, a maximum entropy statistical parser1; we then convert this parse tree to a dependency tree.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
Note that the left-to-right ordering of the sentence is maintained in the dependency tree only among siblings (i.e. the dependency tree does not specify an order to traverse the tree to recover the original sentence).,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"For each pair of entities in a sentence, we find the smallest common subtree in the dependency tree that includes both entities.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
We choose to use this subtree instead of the entire tree to reduce noise and emphasize the local characteristics of relations.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
We then augment each node of the tree with a feature vector (Table 3).,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
The relation-argument feature specifies whether an entity is the first or second argument in a relation.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
This is required to learn asymmetric relations (e.g.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
X OWNS Y).,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"Formally, a relation instance is a dependency tree T with nodes It0 ... tn}.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
The features of node ti are given by 0(ti) = Iv1 ... vd}.,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"We refer to the jth child of node ti as ti[j], and we denote the set of all children of node ti as ti[c].","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
We reference a subset j of children of ti by ti[j] C_ ti[c].,"{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"Finally, we refer to the parent of node ti as ti.p.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
"From the example in Figure 1, t0[1] = t2, t0[I0,1}] = It1, t2}, and t1.p = t0.","{'title': '4 Augmented Dependency Trees', 'number': '4'}"
We now define a kernel function for dependency trees.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"The tree kernel is a function K(T1, T2) that returns a normalized, symmetric similarity score in the range (0, 1) for two trees T1 and T2.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
We define a slightly more general version of the kernel described by Zelenko et al. (2003).,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"We first define two functions over the features of tree nodes: a matching function m(ti, tj) E I0, 1} and a similarity function s(ti, tj) E (0, oc].","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
Let the feature vector 0(ti) = Iv1 ... vd} consist of two possibly overlapping subsets 0m(ti) C_ 0(ti) and 0s(ti) C_ 0(ti).,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
We use 0m(ti) in the matching function and 0s(ti) in the similarity function.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"We define and where C(vq, vr) is some compatibility function between two feature values.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"For example, in the simplest case where s(ti, tj) returns the number of feature values in common between feature vectors 0s(ti) and 0s(tj).","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"We can think of the distinction between functions m(ti, tj) and s(ti, tj) as a way to discretize the similarity between two nodes.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"If 0m(ti) =� 0m(tj), then we declare the two nodes completely dissimilar.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"However, if 0m(ti) = 0m(tj), then we proceed to compute the similarity s(ti, tj).","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Thus, restricting nodes by m(ti, tj) is a way to prune the search space of matching subtrees, as shown below.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"For two dependency trees T1, T2, with root nodes r1 and r2, we define the tree kernel K(T1, T2) as where Kc is a kernel function over children.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Let a and b be sequences of indices such that a is a sequence a1 < a2 < ... < an, and likewise for b.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
Let d(a) = an − a1 + 1 and l(a) be the length of a.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Then we have Kc(ti[c], tj[c]) = The constant 0 < A < 1 is a decay factor that penalizes matching subsequences that are spread out within the child sequences.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
See Zelenko et al. (2003) for a proof that K is kernel function.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Intuitively, whenever we find a pair of matching nodes, we search for all matching subsequences of the children of each node.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"A matching subsequence of children is a sequence of children a and b such that m(ai, bi) = 1 (bi < n).","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"For each matching pair of nodes (ai, bi) in a matching subsequence, we accumulate the result of the similarity function s(ai, bj) and then recursively search for matching subsequences of their children ai[c], bj[c].","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
We implement two types of tree kernels.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
A contiguous kernel only matches children subsequences that are uninterrupted by non-matching nodes.,"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Therefore, d(a) = l(a).","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"A sparse tree kernel, by contrast, allows non-matching nodes within matching subsequences.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Figure 2 shows two relation instances, where each node contains the original text plus the features used for the matching function, 0m(ti) = Igeneralpos, entity-type, relation-argument}.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
(“NA” denotes the feature is not present for this node.),"{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"The contiguous kernel matches the following substructures: It0[0], u0[0]}, It0[2], u0[1]}, It3[0], u2[0]}.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Because the sparse kernel allows non-contiguous matching sequences, it matches an additional substructure It0[0, *, 2], u0[0, *,1]}, where (*) indicates an arbitrary number of non-matching nodes.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
"Zelenko et al. (2003) have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3), where m and n are the number of children in trees T1 and T2 respectively.","{'title': '5 Tree kernels for dependency trees', 'number': '5'}"
We extract relations from the Automatic Content Extraction (ACE) corpus provided by the National Institute for Standards and Technology (NIST).,"{'title': '6 Experiments', 'number': '6'}"
The data consists of about 800 annotated text documents gathered from various newspapers and broadcasts.,"{'title': '6 Experiments', 'number': '6'}"
"Five entities have been annotated (PERSON, ORGANIZATION, GEO-POLITICAL ENTITY, LOCATION, FACILITY), along with 24 types of relations (Table 2).","{'title': '6 Experiments', 'number': '6'}"
"As noted from the distribution of relationship types in the training data (Figure 3), data imbalance and sparsity are potential problems.","{'title': '6 Experiments', 'number': '6'}"
"In addition to the contiguous and sparse tree kernels, we also implement a bag-of-words kernel, which treats the tree as a vector of features over nodes, disregarding any structural information.","{'title': '6 Experiments', 'number': '6'}"
We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel.,"{'title': '6 Experiments', 'number': '6'}"
"Joachims et al. (2001) have shown that given two kernels K1, K2, the composite kernel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a kernel.","{'title': '6 Experiments', 'number': '6'}"
We find that this composite kernel improves performance when the Gram matrix G is sparse (i.e. our instances are far apart in the kernel space).,"{'title': '6 Experiments', 'number': '6'}"
The features used to represent each node are shown in Table 3.,"{'title': '6 Experiments', 'number': '6'}"
"After initial experimentation, the set of features we use in the matching function is φm(ti) = {general-pos, entity-type, relationargument}, and the similarity function examines the remaining features.","{'title': '6 Experiments', 'number': '6'}"
"In our experiments we tested the following five kernels: We also experimented with the function C(vq, vr), the compatibility function between two feature values.","{'title': '6 Experiments', 'number': '6'}"
"For example, we can increase the importance of two nodes having the same Wordnet hypernym2.","{'title': '6 Experiments', 'number': '6'}"
"If vq, vr are hypernym features, then we can define When > 1, we increase the similarity of nodes that share a hypernym.","{'title': '6 Experiments', 'number': '6'}"
"We tested a number of weighting schemes, but did not obtain a set of weights that produced consistent significant improvements.","{'title': '6 Experiments', 'number': '6'}"
See Section 8 for altern α ate approaches to setting C. Table 4 shows the results of each kernel within an SVM.,"{'title': '6 Experiments', 'number': '6'}"
(We augment the LibSVM3 implementation to include our dependency tree kernel.),"{'title': '6 Experiments', 'number': '6'}"
"Note that, although training was done over all 24 relation subtypes, we evaluate only over the 5 high-level relation types.","{'title': '6 Experiments', 'number': '6'}"
"Thus, classifying a RESIDENCE relation as a LOCATED relation is deemed correct4.","{'title': '6 Experiments', 'number': '6'}"
Note also that K0 is not included in Table 4 because of burdensome computational time.,"{'title': '6 Experiments', 'number': '6'}"
"Table 4 shows that precision is adequate, but recall is low.","{'title': '6 Experiments', 'number': '6'}"
"This is a result of the aforementioned class imbalance – very few of the training examples are relations, so the classifier is less likely to identify a testing instances as a relation.","{'title': '6 Experiments', 'number': '6'}"
"Because we treat every pair of mentions in a sentence as a possible relation, our training set contains fewer than 15% positive relation instances.","{'title': '6 Experiments', 'number': '6'}"
"To remedy this, we retrain each SVMs for a binary classification task.","{'title': '6 Experiments', 'number': '6'}"
"Here, we detect, but do not classify, relations.","{'title': '6 Experiments', 'number': '6'}"
"This allows us to combine all positive relation instances into one class, which provides us more training samples to estimate the class boundary.","{'title': '6 Experiments', 'number': '6'}"
We then threshold our output to achieve an optimal operating point.,"{'title': '6 Experiments', 'number': '6'}"
"As seen in Table 5, this method of relation detection outperforms that of the multi-class classifier.","{'title': '6 Experiments', 'number': '6'}"
"We then use these binary classifiers in a cascading scheme as follows: First, we use the binary SVM to detect possible relations.","{'title': '6 Experiments', 'number': '6'}"
"Then, we use the SVM trained only on positive relation instances to classify each predicted relation.","{'title': '6 Experiments', 'number': '6'}"
These results are shown in Table 6.,"{'title': '6 Experiments', 'number': '6'}"
"The first result of interest is that the sparse tree kernel, K0, does not perform as well as the contiguous tree kernel, K1.","{'title': '6 Experiments', 'number': '6'}"
"Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel, we performed the experiment with different values for the decay factor A = {.9,.5,.1}, but obtained no improvement.","{'title': '6 Experiments', 'number': '6'}"
"The second result of interest is that all tree kernels outperform the bag-of-words kernel, K2, most noticeably in recall performance, implying that the and C denote the kernel used for relation detection and classification, respectively. structural information the tree kernel provides is extremely useful for relation detection.","{'title': '6 Experiments', 'number': '6'}"
"Note that the average results reported here are representative of the performance per relation, except for the NEAR relation, which had slightly lower results overall due to its infrequency in training.","{'title': '6 Experiments', 'number': '6'}"
We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel.,"{'title': '7 Conclusions', 'number': '7'}"
"While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.","{'title': '7 Conclusions', 'number': '7'}"
"Detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous, and is therefore difficult to characterize with a similarity metric.","{'title': '7 Conclusions', 'number': '7'}"
An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations.,"{'title': '7 Conclusions', 'number': '7'}"
"The most immediate extension is to automatically learn the feature compatibility function C(vq, vr).","{'title': '8 Future Work', 'number': '8'}"
A first approach might use tf-idf to weight each feature.,"{'title': '8 Future Work', 'number': '8'}"
Another approach might be to calculate the information gain for each feature and use that as its weight.,"{'title': '8 Future Work', 'number': '8'}"
A more complex system might learn a weight for each pair of features; however this seems computationally infeasible for large numbers of features.,"{'title': '8 Future Work', 'number': '8'}"
"One could also perform latent semantic indexing to collapse feature values into similar “categories” — for example, the words “football” and “baseball” might fall into the same category.","{'title': '8 Future Work', 'number': '8'}"
"Here, C(vq, vr) might return α1 if vq = vr, and α2 if vq and vr are in the same category, where α1 > α2 > 0.","{'title': '8 Future Work', 'number': '8'}"
Any method that provides a “soft” match between feature values will sharpen the granularity of the kernel and enhance its modeling power.,"{'title': '8 Future Work', 'number': '8'}"
Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.,"{'title': '8 Future Work', 'number': '8'}"
"These results contradict those given in Zelenko et al. (2003), where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel.","{'title': '8 Future Work', 'number': '8'}"
"It is worthwhile to characterize relation types that are better captured by the sparse kernel, and to determine when using the sparse kernel is worth the increased computational burden.","{'title': '8 Future Work', 'number': '8'}"
