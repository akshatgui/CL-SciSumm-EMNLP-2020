col1,col2
"In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing.",{}
"We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.",{}
The field of research in natural language processing (NLP) applications for L2 language is constantly growing.,"{'title': 'Introduction', 'number': '1'}"
"This is largely driven by the ex panding population of L2 English speakers, whose varying levels of ability may require different types of NLP tools from those designed primarily for native speakers of the language.","{'title': 'Introduction', 'number': '1'}"
These include applications for use by the individual and within instructional contexts.,"{'title': 'Introduction', 'number': '1'}"
"Among the key tools are error-checking applications, focusing particularly on areas which learners find the most challenging.","{'title': 'Introduction', 'number': '1'}"
"Prepositions and determiners are known to be oneof the most frequent sources of error for L2 En glish speakers, a finding supported by our analysisof a small error-tagged corpus we created (determiners 17% of errors, prepositions 12%).","{'title': 'Introduction', 'number': '1'}"
"There fore, in developing a system for automatic error detection in L2 writing, it seems desirable to focus on these problematic, and very common, parts of speech (POS).This paper gives a brief overview of the prob lems posed by these POS and of related work.","{'title': 'Introduction', 'number': '1'}"
We c ? 2008.,"{'title': 'Introduction', 'number': '1'}"
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).,"{'title': 'Introduction', 'number': '1'}"
Some rights reserved.,"{'title': 'Introduction', 'number': '1'}"
then present our proposed approach on both L1 and L2 data and discuss the results obtained so far.,"{'title': 'Introduction', 'number': '1'}"
2.1 Prepositions.,"{'title': 'The problem. ', 'number': '2'}"
Prepositions are challenging for learners because they can appear to have an idiosyncratic behaviour which does not follow any predictable pattern even across nearly identical contexts.,"{'title': 'The problem. ', 'number': '2'}"
"For example, we say I study in Boston but I study at MIT; or He is independent of his parents, but dependent on his son.","{'title': 'The problem. ', 'number': '2'}"
"As it is hard even for L1 speakers to articulatethe reasons for these differences, it is not surprising that learners find it difficult to master preposi tions.","{'title': 'The problem. ', 'number': '2'}"
2.2 Determiners.,"{'title': 'The problem. ', 'number': '2'}"
"Determiners pose a somewhat different problem from prepositions as, unlike them, their choice is more dependent on the wider discourse contextthan on individual lexical items.","{'title': 'The problem. ', 'number': '2'}"
"The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context.","{'title': 'The problem. ', 'number': '2'}"
"For example, wecan say boys like sport or the boys like sport, depending on whether we are making a general state ment about all boys or referring to a specific group.Equally, both she ate an apple and she ate the ap ple are grammatically well-formed sentences, butonly one may be appropriate in a given context, de pending on whether the apple has been mentioned previously.","{'title': 'The problem. ', 'number': '2'}"
"Therefore, here, too, it is very hard tocome up with clear-cut rules predicting every pos sible kind of occurrence.","{'title': 'The problem. ', 'number': '2'}"
169,"{'title': 'The problem. ', 'number': '2'}"
"Although in the past there has been some research on determiner choice in L1 for applications such as generation and machine translation output, work to date on automatic error detection in L2 writing hasbeen fairly limited.","{'title': 'Related work. ', 'number': '3'}"
Izumi et al (2004) train a maximum entropy classifier to recognise various er rors using contextual features.,"{'title': 'Related work. ', 'number': '3'}"
"They report results for different error types (e.g. omission - precision 75.7%, recall 45.67%; replacement - P 31.17%, R 8%), but there is no break-down of results byindividual POS.","{'title': 'Related work. ', 'number': '3'}"
"Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.","{'title': 'Related work. ', 'number': '3'}"
"Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter.","{'title': 'Related work. ', 'number': '3'}"
They report 80% precision and 30% recall.,"{'title': 'Related work. ', 'number': '3'}"
"Finally, Gamon etal.","{'title': 'Related work. ', 'number': '3'}"
"(2008) use a complex system including a decision tree and a language model for both preposi tion and determiner errors, while Yi et al (2008)propose a web count-based system to correct de terminer errors (P 62%, R 41%).The work presented here displays some similar ities to the papers mentioned above in its use of a maximum entropy classifier and a set of features.However, our feature set is more linguistically sophisticated in that it relies on a full syntactic analysis of the data.","{'title': 'Related work. ', 'number': '3'}"
It includes some semantic compo nents which we believe play a role in correct class assignment.,"{'title': 'Related work. ', 'number': '3'}"
determiners 4.1 Feature set.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"The approach proposed in this paper is based on the belief that although it is difficult to formulatehard and fast rules for correct preposition and determiner usage, there is enough underlying regularity of characteristic syntactic and semantic con texts to be able to predict usage to an acceptabledegree of accuracy.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
We use a corpus of grammat ically correct English to train a maximum entropyclassifier on examples of correct usage.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"The classifier can therefore learn to associate a given preposition or determiner to particular contexts, and re liably predict a class when presented with a novel instance of a context for one or the other.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
The L1 source we use is the British National Head noun ?apple?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
Number singular Noun type count Named entity?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"no WordNet category food, plant Prep modification?","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"yes, ?on?","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
Object of Prep?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
no Adj modification?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"yes, ?juicy?","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"Adj grade superlative POS ?3 VV, DT, JJS, IN, DT, NN Table 1: Determiner feature set for Pick the juiciest apple on the tree.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
POS modified verb Lexical item modified ?drive?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
WordNet Category motion Subcat frame pp to POS of object noun Object lexical item ?London?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
Named entity?,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"yes, type = location POS ?3 NNP, VBD, NNP Grammatical relation iobj Table 2: Preposition feature set for John drove to London.Corpus (BNC) as we believe this offers a represen tative sample of different text types.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
We represent training and testing items as vectors of values for linguistically motivated contextual features.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
Our feature vectors include 18 feature categories for determiners and 13 for prepositions; the main ones are illustrated in Table 1 and Table 2 respectively.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"Further determiner features note whether the nounis modified by a predeterminer, possessive, nu meral, and/or a relative clause, and whether it ispart of a ?there is. . . ?","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
phrase.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"Additional preposi tion features refer to the grade of any adjectives or adverbs modified (base, comparative, superlative) and to whether the items modified are modified by more than one PP 1 . In De Felice and Pulman (2007), we described some of the preprocessing required and offered some motivation for this approach.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"As for ourchoice of features, we aim to capture all the ele ments of a sentence which we believe to have an effect on preposition and determiner choice, and which can be easily extracted automatically - this is a key consideration as all the features derivedrely on automatic processing of the text.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
Grammatical relations refer to RASP-style grammatical re lations between heads and complements in which the preposition occurs (see e.g.,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"(Briscoe et al, 1 A full discussion of each feature, including motivation for its inclusion and an assessment of its contribution to the model, is found in De Felice (forthcoming).","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
170 Author Accuracy Baseline 26.94% Gamon et al 08 64.93% Chodorow et al 07 69.00% Our model 70.06% Table 3: Classifier performance on L1 prepositions 2006)).,"{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"Semantic word type information is takenfrom WordNet lexicographer classes, 40 broad se mantic categories which all nouns and verbs in WordNet belong to 2 (e.g. ?verb of motion?, ?noun denoting food?), while the POStags are from the Penn Treebank tagset - we note the POS of three words either side of the target word 3 . For each.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
"occurrence of a preposition or determiner in the corpus, we obtain a feature vector consisting ofthe preposition or determiner and its context, de scribed in terms of the features noted above.","{'title': 'Contextual models for prepositions and. ', 'number': '4'}"
5.1 Prepositions.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"At the moment, we restrict our analysis to the nine most frequent prepositions in the data: at, by, for, from, in, of, on, to, and with, to ensure a sufficient amount of data for training.","{'title': 'Acquiring the models. ', 'number': '5'}"
"This gives a training dataset comprising 8,898,359 instances.","{'title': 'Acquiring the models. ', 'number': '5'}"
"We use a standard maximum entropy classifier 4 and donot omit any features, although we plan to experiment with different feature combinations to deter mine if, and how, this would impact the classifier?s performance.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Before testing our model on learner data, it is important to ascertain that it can correctlyassociate prepositions to a given context in gram matical, well-edited data.","{'title': 'Acquiring the models. ', 'number': '5'}"
"We therefore tested themodel on a section of the BNC not used in train ing, section J. Our best result to date is 70.06% accuracy (test set size: 536,193).","{'title': 'Acquiring the models. ', 'number': '5'}"
Table 3 relates our results to others reported in the literature on comparable tasks.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"The baseline refers to always choosing the most frequent option, namely of.We can see that our model?s performance com pares favourably to the best results in the literature, although direct comparisons are hard to draw sincedifferent groups train and test on different preposi tion sets and on different types of data (British vs. American English, BNC vs. news reports, and so 2 No word sense disambiguation was performed at this stage.","{'title': 'Acquiring the models. ', 'number': '5'}"
"3 In NPs with a null determiner, the target is the head noun.","{'title': 'Acquiring the models. ', 'number': '5'}"
4 Developed by James Curran.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"Proportion of training data Precision Recall of 27.83% (2,501,327) 74.28% 90.47% to 20.64% (1,855,304) 85.99% 81.73% in 17.68% (1,589,718) 60.15% 67.60% for 8.01% (720,369) 55.47% 43.78% on 6.54% (587,871) 58.52% 45.81% with 6.03% (541,696) 58.13% 46.33% at 4.72% (424,539) 57.44% 52.12% by 4.69% (421,430) 63.83% 56.51% from 3.86% (347,105) 59.20% 32.07% Table 4: L1 results - individual prepositions on).","{'title': 'Acquiring the models. ', 'number': '5'}"
"Furthermore, it should be noted that Gamon et al report more than one figure in their results, as there are two components to their model: one determining whether a preposition is needed, and the other deciding what the preposition should be.","{'title': 'Acquiring the models. ', 'number': '5'}"
"The figure reported here refers to the latter task,as it is the most similar to the one we are evalu ating.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Additionally, Chodorow et al also discusssome modifications to their model which can in crease accuracy; the result noted here is the one more directly comparable to our own approach.","{'title': 'Acquiring the models. ', 'number': '5'}"
"5.1.1 Further discussion To fully assess the model?s performance on the L1data, it is important to consider factors such as performance on individual prepositions, the relation ship between training dataset size and accuracy, and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in dividual prepositions together with the size of their training datasets.","{'title': 'Acquiring the models. ', 'number': '5'}"
"At first glance, a clear correlationappears between the amount of data seen in training and precision and recall, as evidenced for ex ample by of or to, for which the classifier achievesa very high score.","{'title': 'Acquiring the models. ', 'number': '5'}"
"In other cases, however, the cor relation is not so clear-cut.","{'title': 'Acquiring the models. ', 'number': '5'}"
"For example by has one of the smallest data sets in training but higher scores than many of the other prepositions, whilefor is notable for the opposite reason, namely hav ing a large dataset but some of the lowest scores.","{'title': 'Acquiring the models. ', 'number': '5'}"
The absence of a definite relation between dataset size and performance suggests that theremight be a cline of ?learnability?,"{'title': 'Acquiring the models. ', 'number': '5'}"
for these prepo sitions: different prepositions?,"{'title': 'Acquiring the models. ', 'number': '5'}"
"contexts may be more or less uniquely identifiable, or they mayhave more or fewer senses, leading to less confusion for the classifier.","{'title': 'Acquiring the models. ', 'number': '5'}"
One simple way of verify ing the latter case is by looking at the number of senses assigned to the prepositions by a resource 171 Target prep Confused with at by for from in of on to with at xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85% by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10% for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28% from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07% in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59% of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43% on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71% to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95% with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xx Table 5: Confusion matrix for L1 data - prepositions such as the Oxford English Dictionary.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"However, we find no good correlation between the two as the preposition with the most senses is of (16), and that with the fewest is from (1), thus negating the idea that fewer senses make a preposition easierto learn.","{'title': 'Acquiring the models. ', 'number': '5'}"
"The reason may therefore be found else where, e.g. in the lexical properties of the contexts.","{'title': 'Acquiring the models. ', 'number': '5'}"
"A good picture of the model?s errors can be had by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas sifier?s incorrect decision was.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Analysis of these errors may establish whether they are related to thedataset size issue noted above, or have a more lin guistically grounded explanation.From the table, the frequency effect appears evi dent: in almost every case, the three most frequentwrong choices are the three most frequent prepo sitions, to, of, and in, although interestingly not inthat order, in usually being the first choice.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Conversely, the less frequent prepositions are less of ten suggested as the classifier?s choice.","{'title': 'Acquiring the models. ', 'number': '5'}"
This effectprecludes the possibility at the moment of draw ing any linguistic conclusions.,"{'title': 'Acquiring the models. ', 'number': '5'}"
These may only be gleaned by looking at the errors for the three more frequent prepositions.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"We see for example that there seems to be a strong relation between of and for, the cause of which is not immediately clear: perhaps they both often occur within noun phrases(e.g. book of recipes, book for recipes).","{'title': 'Acquiring the models. ', 'number': '5'}"
"More pre dictable is the confusion between to and from, andbetween locative prepositions such as to and at, al though the effect is less strong for other potentially confusable pairs such as in and at or on.","{'title': 'Acquiring the models. ', 'number': '5'}"
Table 6 gives some examples of instances where the classifier?s chosen preposition differs from thatfound in the original text.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"In most cases, the clas sifier?s suggestion is also grammatically correct, Classifier choice Correct phrase demands of the sector demands for.","{'title': 'Acquiring the models. ', 'number': '5'}"
condition for development condition of.,"{'title': 'Acquiring the models. ', 'number': '5'}"
travel to speed travel at.,"{'title': 'Acquiring the models. ', 'number': '5'}"
look at the USA look to.,"{'title': 'Acquiring the models. ', 'number': '5'}"
.Table 6: Examples of classifier errors on preposi tion L1 task Author Accuracy Baseline 59.83% Han et al 06 83.00% Gamon et al 08 86.07% Turner and Charniak 07 86.74% Our model 92.15% Table 7: Classifier performance - L1 determiners but the overall meaning of the phrases changes somewhat.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"For example, while the demands of the sector are usually made by the sector itself, the demands for the sector suggest that someoneelse may be making them.","{'title': 'Acquiring the models. ', 'number': '5'}"
These are subtle dif ferences which it may be impossible to capture without a more sophisticated understanding of the wider context.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"The example with travel, on the other hand, yields an ungrammatical result.","{'title': 'Acquiring the models. ', 'number': '5'}"
We assume thatthe classifier has acquired a very strong link be tween the lexical item travel and the preposition tothat directs it towards this choice (cf.,"{'title': 'Acquiring the models. ', 'number': '5'}"
also the ex ample of look at/to).,"{'title': 'Acquiring the models. ', 'number': '5'}"
This suggests that individual lexical items play an important role in preposition choice along with other more general syntactic and semantic properties of the context.,"{'title': 'Acquiring the models. ', 'number': '5'}"
172 %of training data Prec.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"Recall a 9.61% (388,476) 70.52% 53.50% the 29.19% (1,180,435) 85.17% 91.51% null 61.20% (2,475,014) 98.63% 98.79% Table 8: L1 results - individual determiners 5.2 Determiners.","{'title': 'Acquiring the models. ', 'number': '5'}"
"For the determiner task, we also consider only the three most frequent cases (a, the, null), which gives us a training dataset consisting of 4,043,925 instances.","{'title': 'Acquiring the models. ', 'number': '5'}"
"We achieve accuracy of 92.15% on theL1 data (test set size: 305,264), as shown in Table 7.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Again, the baseline refers to the most fre quent class, null.","{'title': 'Acquiring the models. ', 'number': '5'}"
The best reported results to date on determiner selection are those in Turner and Charniak (2007).,"{'title': 'Acquiring the models. ', 'number': '5'}"
Our model outperforms their n-gram languagemodel approach by over 5%.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"Since the two approaches are not tested on the same data this com parison is not conclusive, but we are optimistic that there is a real difference in accuracy since the type of texts used are not dissimilar.","{'title': 'Acquiring the models. ', 'number': '5'}"
"As in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across thethree classes; this information is reported in Ta ble 8.","{'title': 'Acquiring the models. ', 'number': '5'}"
Here we can see that there is a very strongcorrelation between amount of data seen in training and precision and recall.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"The indefinite arti cle?s lower ?learnability?, and its lower frequency appears not to be peculiar to our data, as it is also found by Gamon et al among others.The disparity in training is a reflection of the dis tribution of determiners in the English language.","{'title': 'Acquiring the models. ', 'number': '5'}"
"Perhaps if this imbalance were addressed, the model would more confidently learn contexts of use for a, too, which would be desirable in view of using this information for error correction.","{'title': 'Acquiring the models. ', 'number': '5'}"
"On theother hand, this would create a distorted represen tation of the composition of English, which maynot be what we want in a statistical model of lan guage.","{'title': 'Acquiring the models. ', 'number': '5'}"
"We plan to experiment with smaller scale, more similar datasets to ascertain whether the issue is one of training size or of inherent difficulty in learning about the indefinite article?s occurrence.In looking at the confusion matrix for determin ers (Table 9), it is interesting to note that for theclassifier?s mistakes involving a or the, the erroneous choice is in the almost always the other de terminer rather than the null case.","{'title': 'Acquiring the models. ', 'number': '5'}"
"This suggeststhat the frequency effect is not so strong as to over Target det Confused with a the null a xx 92.92% 7.08% the 80.66% xx 19.34% null 14.51% 85.49% xx Table 9: Confusion matrix for L1 determiners ride any true linguistic information the model has acquired, otherwise the predominant choice wouldalways be the null case.","{'title': 'Acquiring the models. ', 'number': '5'}"
"On the contrary, these results show that the model is indeed capable of distinguishing between contexts which require a determiner and those which do not, but requires fur ther fine tuning to perform better in knowing which of the two determiner options to choose.","{'title': 'Acquiring the models. ', 'number': '5'}"
Perhaps the introduction of a discourse dimension might assist in this respect.,"{'title': 'Acquiring the models. ', 'number': '5'}"
"We plan to experiment withsome simple heuristics: for example, given a se quence ?Determiner Noun?, has the noun appeared in the preceding few sentences?","{'title': 'Acquiring the models. ', 'number': '5'}"
"If so, we might expect the to be the correct choice rather than a.","{'title': 'Acquiring the models. ', 'number': '5'}"
6.1 Working with L2 text.,"{'title': 'Testing the model. ', 'number': '6'}"
"To evaluate the model?s performance on learner data, we use a subsection of the Cambridge Learner Corpus (CLC) 5 . We envisage our model to.","{'title': 'Testing the model. ', 'number': '6'}"
eventually be of assistance to learners in analysingtheir writing and identifying instances of preposi tion or determiner usage which do not correspond to what it has been trained to expect; the more probable instance would be suggested as a more appropriate alternative.,"{'title': 'Testing the model. ', 'number': '6'}"
"In using NLP tools and techniques which have been developed with and for L1 language, a loss of performance on L2 data is to be expected.","{'title': 'Testing the model. ', 'number': '6'}"
"These methods usually expect grammatically well-formed input; learner text is often ungrammatical, misspelled, and different in content and structure from typical L1 resources such as the WSJ and the BNC.","{'title': 'Testing the model. ', 'number': '6'}"
6.2 Prepositions.,"{'title': 'Testing the model. ', 'number': '6'}"
"For the preposition task, we extract 2523 instances of preposition use from the CLC (1282 correct, 1241 incorrect) and ask the classifier to mark them 5 The CLC is a computerised database of contemporary written learner English (currently over 25m words).","{'title': 'Testing the model. ', 'number': '6'}"
It wasdeveloped jointly by Cambridge ESOL and Cambridge Uni versity Press.,"{'title': 'Testing the model. ', 'number': '6'}"
The Cambridge Error Coding System has been developed and applied manually to the data by Cambridge University Press.,"{'title': 'Testing the model. ', 'number': '6'}"
173 Instance type Accuracy Correct 66.7% Incorrect 70%Table 10: Accuracy on L2 data - prepositions.,"{'title': 'Testing the model. ', 'number': '6'}"
Ac curacy on incorrect instances refers to the classifier successfully identifying the preposition in the text as not appropriate for that context.,"{'title': 'Testing the model. ', 'number': '6'}"
as correct or incorrect.,"{'title': 'Testing the model. ', 'number': '6'}"
The results from this taskare presented in Table 10.,"{'title': 'Testing the model. ', 'number': '6'}"
"These first results sug gest that the model is fairly robust: the accuracy rate on the correct data, for example, is not much lower than that on the L1 data.","{'title': 'Testing the model. ', 'number': '6'}"
"In an application designed to assist learners, it is important to aim to reduce the rate of false alarms - cases where the original is correct, but the model flags an error - toa minimum, so it is positive that this result is com paratively high.","{'title': 'Testing the model. ', 'number': '6'}"
Accuracy on error identification is at first glance even more encouraging.,"{'title': 'Testing the model. ', 'number': '6'}"
"However, ifwe look at the suggestions the model makes to re place the erroneous preposition, we find that theseare correct only 51.5% of the time, greatly reduc ing its usefulness.","{'title': 'Testing the model. ', 'number': '6'}"
6.2.1 Further discussion A first analysis of the classifier?s decisions and itserrors points to various factors which could be im pairing its performance.,"{'title': 'Testing the model. ', 'number': '6'}"
Spelling mistakes in theinput are one of the most immediate ones.,"{'title': 'Testing the model. ', 'number': '6'}"
"For ex ample, in the sentence I?m Franch, responsable on the computer services, the classifier is not able to suggest a correct alternative to the erroneous on:since it does not recognise the adjective as a misspelling of responsible, it loses the information associated with this lexical feature, which could po tentially determine the preposition choice.","{'title': 'Testing the model. ', 'number': '6'}"
"A more complex problem arises when poor grammar in the input misleads the parser so thatthe information it gives for a sentence is incor rect, especially as regards PP attachment.","{'title': 'Testing the model. ', 'number': '6'}"
"In this example, I wold like following equipment to my speech: computer, modem socket and microphone, the missing the leads the parser to treat following as a verb, and believes it to be the verb to which the preposition is attached.","{'title': 'Testing the model. ', 'number': '6'}"
"It therefore suggests from as a correction, which is a reasonable choice given the frequency of phrases such as to follow from.","{'title': 'Testing the model. ', 'number': '6'}"
"However, this was not what the PP was meant to modify: impaired performance from the parser could be a significant negative factor in the model?s performance.","{'title': 'Testing the model. ', 'number': '6'}"
"It would be interesting to test themodel on texts written by students of different lev els of proficiency, as their grammar may be more error-free and more likely to be parsed correctly.","{'title': 'Testing the model. ', 'number': '6'}"
"Alternatively, we could modify the parser so as to skip cases where it requires several attempts before producing a parse, as these more challenging casescould be indicative of very poorly structured sentences in which misused prepositions are depen dent on more complex errors.A different kind of problem impacting our accu racy scores derives from those instances where theclassifier selects a preposition which can be cor rect in the given context, but is not the correct one in that particular case.","{'title': 'Testing the model. ', 'number': '6'}"
"In the example I received a beautiful present at my birthday, the classifier identifies the presence of the error, and suggests the grammatically and pragmatically appropriate correction for.","{'title': 'Testing the model. ', 'number': '6'}"
"The corpus annotators, however, indicate on as the correct choice.","{'title': 'Testing the model. ', 'number': '6'}"
"Since we use their annotations as the benchmark against which to evaluate the model, this instance is counted as the classifier being wrong because it disagrees with the annotators.","{'title': 'Testing the model. ', 'number': '6'}"
"A better indication of the model?sperformance may be to independently judge its de cisions, to avoid being subject to the annotators?bias.","{'title': 'Testing the model. ', 'number': '6'}"
"Finally, we are beginning to look at the rela tions between preposition errors and other types oferror such as verb choice, and how these are anno tated in the data.","{'title': 'Testing the model. ', 'number': '6'}"
An overview of the classifier?s error patterns forthe data in this task shows that they are largely similar to those observed in the L1 data.,"{'title': 'Testing the model. ', 'number': '6'}"
"This sug gests that the gap in performance between L1 and L2 is due more to the challenges posed by learner text than by inherent shortcomings in the model, and therefore that the key to better performance is likely to lie in overcoming these problems.","{'title': 'Testing the model. ', 'number': '6'}"
In future work we plan to use L2 data where someof the spelling errors and non-preposition or deter miner errors have been corrected so that we can see which of the other errors are worth focussing on first.,"{'title': 'Testing the model. ', 'number': '6'}"
6.3 Determiners.,"{'title': 'Testing the model. ', 'number': '6'}"
Our work on determiner error correction is still in the early stages.,"{'title': 'Testing the model. ', 'number': '6'}"
"We follow a similar procedure to the prepositions task, selecting a number of both correct and incorrect instances.","{'title': 'Testing the model. ', 'number': '6'}"
On the former (set size 2000) accuracy is comparable to that on L1data: 92.2%.,"{'title': 'Testing the model. ', 'number': '6'}"
"The danger of false alarms, then, ap pears not to be as significant as for the prepositions 174 task.","{'title': 'Testing the model. ', 'number': '6'}"
On the incorrect instances (set size ca.,"{'title': 'Testing the model. ', 'number': '6'}"
"1200), however, accuracy is less than 10%.","{'title': 'Testing the model. ', 'number': '6'}"
"Preliminary error analysis shows that the modelis successful at identifying cases of misused deter miner, e.g. a for the or vice versa, doing so in overtwo-thirds of cases.","{'title': 'Testing the model. ', 'number': '6'}"
"However, by far the most fre quent error type for determiners is not confusion between indefinite and definite article, but omitting an article where one is needed.","{'title': 'Testing the model. ', 'number': '6'}"
"At the moment, themodel detects very few of these errors, no doubt in fluenced by the preponderance of null cases seen in training.","{'title': 'Testing the model. ', 'number': '6'}"
"Furthermore, some of the issues raised earlier in discussing the application of NLP tools to L2 language hold for this task, too.","{'title': 'Testing the model. ', 'number': '6'}"
"In addition to those, though, in this task more than for prepositions we believe that differences intext type between the training texts - the BNC and the testing material - learner essays - has a sig nificant negative effect on the model.","{'title': 'Testing the model. ', 'number': '6'}"
"In this task,the lexical items play a crucial role in class assign ment.","{'title': 'Testing the model. ', 'number': '6'}"
"If the noun in question has not been seen in training, the classifier may be unable to make an informed choice.","{'title': 'Testing the model. ', 'number': '6'}"
"Although the BNC comprises a wide variety of texts, there may not be a sufficient number covering topics typical of learner essays, such as ?business letters?","{'title': 'Testing the model. ', 'number': '6'}"
"or ?postcards to penpals?.Also, the BNC was created with material from almost 20 years ago, and learners writing in contem porary English may use lexical items which are notvery frequently seen in the BNC.","{'title': 'Testing the model. ', 'number': '6'}"
"A clear exam ple of this discrepancy is the noun internet, which requires the definite article in English, but not inseveral other languages, leading to countless sen tences such as I saw it in internet, I booked it on internet, and so on.","{'title': 'Testing the model. ', 'number': '6'}"
This is one of the errors themodel never detects: a fact which is not surpris ing when we consider that this noun occurs only four times in the whole of the training data.,"{'title': 'Testing the model. ', 'number': '6'}"
It may be therefore necessary to consider using alternative sources of training data to overcome this problem and improve the classifier?s performance.,"{'title': 'Testing the model. ', 'number': '6'}"
"In developing this model, our first aim was not to create something which learns like a human, butsomething that works in the best and most effi cient possible way.","{'title': 'Comparison to human learners. ', 'number': '7'}"
"However, it is interesting to see whether human learners and classifiers display similar patterns of errors in preposition choice.This information has twofold value: as well as being of pedagogical assistance to instructors of En glish L2, were the classifier to display student-like error patterns, insights into ?error triggers?","{'title': 'Comparison to human learners. ', 'number': '7'}"
could be derived from the L2 pedagogical literature to improve the classifier.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
The analysis of the typesof errors made by human learners yields some insights which might be worthy of further investi gation.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
"A clear one is the confusion between the three locative and temporal prepositions at, in, and on (typical sentence: The training programme will start at the 1st August).","{'title': 'Comparison to human learners. ', 'number': '7'}"
"This type of error is made often by both learners and the model on both types of data, suggesting that perhaps further attentionto features might be necessary to improve discrim ination between these three prepositions.There are also interesting divergences.","{'title': 'Comparison to human learners. ', 'number': '7'}"
"For ex ample, a common source of confusion in learners is between by and from, as in I like it becauseit?s from my favourite band.","{'title': 'Comparison to human learners. ', 'number': '7'}"
"However, this confu sion is not very frequent in the model, a difference which could be explained either by the fact that, as noted above, performance on from is very low and so the classifier is unlikely to suggest it, or that in training the contexts seen for by are sufficiently distinctive that the classifier is not misled like the learners.","{'title': 'Comparison to human learners. ', 'number': '7'}"
"Finally, a surprising difference comes from looking at what to is confused with.","{'title': 'Comparison to human learners. ', 'number': '7'}"
The model often suggests at where to would be correct.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
This is perhaps not entirely unusual as both can occur with locative complements (one can go to a placeor be at a place) and this similarity could be con fusing the classifier.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
"Learners, however, although they do make this kind of mistake, are much more hampered by the confusion between for and to, as in She was helpful for me or This is interesting for you.","{'title': 'Comparison to human learners. ', 'number': '7'}"
"In other words, for learners it seems that the abstract use of this preposition, its benefactive sense, is much more problematic than the spatial sense.","{'title': 'Comparison to human learners. ', 'number': '7'}"
We can hypothesise that the classifier is less distracted by these cases because the effect of the lexical features is stronger.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
A more detailed discussion of the issues arising from the comparison of confusion pairs cannot be had here.,"{'title': 'Comparison to human learners. ', 'number': '7'}"
"However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them.","{'title': 'Comparison to human learners. ', 'number': '7'}"
175,"{'title': 'Comparison to human learners. ', 'number': '7'}"
"This paper discussed a contextual feature based approach to the automatic acquisition of models of use for prepositions and determiners, whichachieve an accuracy of 70.06% and 92.15% re spectively, and showed how it can be applied to anerror correction task for L2 writing, with promis ing early results.","{'title': 'Conclusions and future directions. ', 'number': '8'}"
There are several directions that can be pursued to improve accuracy on both types of data.,"{'title': 'Conclusions and future directions. ', 'number': '8'}"
The classifier can be further fine-tuned to acquire more reliable models of use for the two POS.,"{'title': 'Conclusions and future directions. ', 'number': '8'}"
"We can also experiment with its confidencethresholds, for example allowing it to make an other suggestion when its confidence in its first choice is low.","{'title': 'Conclusions and future directions. ', 'number': '8'}"
"Furthermore, issues relating to the use of NLP tools with L2 data must be addressed, such as factoring out spelling or other errors in the data, and perhaps training on text types which are more similar to the CLC.","{'title': 'Conclusions and future directions. ', 'number': '8'}"
"In the longer term, we also envisage mining the information implicit inour training data to create a lexical resource de scribing the statistical tendencies observed.","{'title': 'Conclusions and future directions. ', 'number': '8'}"
AcknowledgementsWe wish to thank Stephen Clark and Laura Rimell for stim ulating discussions and the anonymous reviewers for their helpful comments.,"{'title': 'Conclusions and future directions. ', 'number': '8'}"
We acknowledge Cambridge University Press?s assistance in accessing the Cambridge Learner Corpusdata.,"{'title': 'Conclusions and future directions. ', 'number': '8'}"
Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.,"{'title': 'Conclusions and future directions. ', 'number': '8'}"
