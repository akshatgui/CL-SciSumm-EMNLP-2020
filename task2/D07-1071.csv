col1,col2
We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).,Introduction
"A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?",Introduction
with learned costs.,Introduction
"We also present a new, online algorithm for inducing a weighted CCG.",Introduction
"Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).",Introduction
"Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.",Experiment/Discussion
"In one such approach (Zettlemoyer and Collins, 2005) (ZC05), the input to the learning algorithm is a training set consisting of sentences paired with lambda-calculus expressions.",Experiment/Discussion
"For instance, the training data might contain the following example: Sentence: list flights to boston Logical Form: ?x.flight(x) ? to(x, boston) In this case the lambda-calculus expression denotes the set of all flights that land in Boston.",Experiment/Discussion
"In ZC05 it is assumed that training examples do not include additional information, for example parse trees or a) on may four atlanta to denver delta flight 257 ?x.month(x,may) ? day number(x, fourth)?",Experiment/Discussion
"from(x, atlanta) ? to(x, denver)?",Experiment/Discussion
"airline(x, delta air lines) ? flight(x)?",Experiment/Discussion
"flight number(x, 257) b) show me information on american airlines from fort worth texas to philadelphia ?x.airline(x, american airlines)?",Experiment/Discussion
"from(x, fort worth) ? to(x, philadelphia) c) okay that one?s great too now we?re going to go on april twenty second dallas to washington the latest nighttime departure one way argmax(?x.flight(x) ? from(x, dallas)?",Experiment/Discussion
"to(x,washington) ? month(x, april)?",Experiment/Discussion
"day number(x, 22) ? during(x, night)?",Experiment/Discussion
"one way(x), ?y.depart time(y)) Figure 1: Three sentences from the ATIS domain.other derivations.",Experiment/Discussion
"The output from the learning algo rithm is a combinatory categorial grammar (CCG),together with parameters that define a log-linear distribution over parses under the grammar.",Experiment/Discussion
"Experi ments show that the approach gives high accuracy on two database-query problems, introduced by Zelle and Mooney (1996) and Tang and Mooney (2000).",Experiment/Discussion
"The use of a detailed grammatical formalism such as CCG has the advantage that it allows a system tohandle quite complex semantic effects, such as co ordination or scoping phenomena.",Experiment/Discussion
"In particular, it allows us to leverage the considerable body of work on semantics within these formalisms, for example see Carpenter (1997).",Experiment/Discussion
"However, a grammar based on a formalism such as CCG can be somewhat rigid, and this can cause problems when a system is faced with spontaneous, unedited natural language input, as is commonly seen in natural language interface applications.",Experiment/Discussion
"For example, consider the sentences shown in figure 1, which were taken from the ATIS travel-planning domain (Dahl et al, 1994).",Experiment/Discussion
Thesesentences exhibit characteristics which present significant challenges to the approach of ZC05.,Experiment/Discussion
"For ex 678 ample, the sentences have quite flexible word order, and include telegraphic language where some words are effectively omitted.",Experiment/Discussion
"In this paper we describe a learning algorithm that retains the advantages of using a detailed grammar, but is highly effective in dealing with phenomenaseen in spontaneous natural language, as exempli fied by the ATIS domain.",Experiment/Discussion
A key idea is to extendthe approach of ZC05 by allowing additional nonstandard CCG combinators.,Experiment/Discussion
"These combinators relax certain parts of the grammar?for example al lowing flexible word order, or insertion of lexical items?with learned costs for the new operations.This approach has the advantage that it can be seam lessly integrated into CCG learning algorithms such as the algorithm described in ZC05.A second contribution of the work is a new, online algorithm for CCG learning.",Experiment/Discussion
The approach in volves perceptron training of a model with hidden variables.,Experiment/Discussion
In this sense it is related to the algorithmof Liang et al (2006).,Experiment/Discussion
However it has the addi tional twist of also performing grammar induction(lexical learning) in an online manner.,Experiment/Discussion
"In our experiments, we show that the new algorithm is consid erably more efficient than the ZC05 algorithm; this is important when training on large training sets, for example the ATIS data used in this paper.",Experiment/Discussion
"Results for the approach on ATIS data show 86%F-measure accuracy in recovering fully correct semantic analyses, and 95.9% F-measure by a partial match criterion described by He and Young (2006).",Experiment/Discussion
"The latter figure contrasts with a figure of 90.3% for the approach reported by He and Young (2006).1Results on the Geo880 domain also show an im provement in accuracy, with 88.9% F-measure for the new approach, compared to 87.0% F-measure for the method in ZC05.",Experiment/Discussion
2.1 Semantics.,Experiment/Discussion
Training examples in our approach consist of sen tences paired with lambda-calculus expressions.,Experiment/Discussion
"We use a version of the lambda calculus that is closely related to the one presented by Carpenter (1997).There are three basic types: t, the type of truth val 1He and Young (2006) do not give results for recovering fully correct parses.",Experiment/Discussion
"ues; e, the type for entities; and r, the type for realnumbers.",Experiment/Discussion
"Functional types are defined by specify ing their input and output types, for example ?e, t?is the type of a function from entities to truth val ues.",Experiment/Discussion
"In general, declarative sentences have a logical form of type t. Question sentences generally have functional types.2 Each expression is constructed from constants, logical connectors, quantifiers and lambda functions.",Experiment/Discussion
2.2 Combinatory Categorial Grammars.,Experiment/Discussion
"Combinatory categorial grammar (CCG) is a syn tactic theory that models a wide range of linguistic phenomena (Steedman, 1996; Steedman, 2000).",Experiment/Discussion
The core of a CCG grammar is a lexicon ?.,Experiment/Discussion
"For example, consider the lexicon flights := N : ?x.flight(x) to := (N\N)/NP : ?y.?f.?x.f(x) ? to(x, y) boston := NP : boston Each entry in the lexicon is a pair consisting of aword and an associated category.",Experiment/Discussion
The category con tains both syntactic and semantic information.,Experiment/Discussion
"For example, the first entry states that the word flightscan have the category N : ?x.flight(x).",Experiment/Discussion
"This cat egory consists of a syntactic type N , together withthe semantics ?x.flight(x).",Experiment/Discussion
"In general, the seman tic entries for words in the lexicon can consist of anylambda-calculus expression.",Experiment/Discussion
"Syntactic types can ei ther be simple types such as N , NP , or S, or can be more complex types that make use of slash notation, for example (N\N)/NP . CCG makes use of a set of combinators which are used to combine categories to form larger pieces of syntactic and semantic structure.",Experiment/Discussion
The simplest such rules are the functional application rules: A/B : f B : g ? A : f(g) (>) B : g A\B : f ? A : f(g) (<) The first rule states that a category with syntactic type A/B can be combined with a category to the right of syntactic type B to create a new category of type A. It also states that the new semantics will be formed by applying the function f tothe expression g. The second rule handles argu ments to the left.,Experiment/Discussion
"Using these rules, we can parse the 2For example, many question sentences have semantics of type ?e, t?, as in ?x.flight(x) ? to(x, boston).",Experiment/Discussion
"679 following phrase to create a new category of typeN : flights to boston N (N\N)/NP NP ?x.flight(x) ?y.?f.?x.f(x) ? to(x, y) boston > (N\N) ?f.?x.f(x) ? to(x, boston) < N ?x.flight(x) ? to(x, boston) The top-most parse operations pair each word with a corresponding category from the lexicon.",Experiment/Discussion
The later steps are labeled ?> (for each instance of forward application) or ?< (for backward application).,Experiment/Discussion
A second set of combinators in CCG grammars are the rules of functional composition: A/B : f B/C : g ? A/C : ?x.f(g(x)) (> B) B\C : g A\B : f ? A\C : ?x.f(g(x)) (< B)These rules allow for an unrestricted notion of con stituency that is useful for modeling coordination and other linguistic phenomena.,Experiment/Discussion
"As we will see, theyalso turn out to be useful when modeling construc tions with relaxed word order, as seen frequently in domains such as ATIS.",Experiment/Discussion
"In addition to the application and compositionrules, we will also make use of type raising and co ordination combinators.",Experiment/Discussion
A full description of these combinators goes beyond the scope of this paper.Steedman (1996; 2000) presents a detailed descrip tion of CCG.,Experiment/Discussion
2.3 Log-Linear CCGs.,Experiment/Discussion
"We can generalize CCGs to weighted, or probabilis tic, models as follows.",Experiment/Discussion
"Our models are similar to several other approaches (Ratnaparkhi et al, 1994; Johnson et al, 1999; Lafferty et al, 2001; Collins,2004; Taskar et al, 2004).",Experiment/Discussion
"We will write x to de note a sentence, and y to denote a CCG parse for asentence.",Experiment/Discussion
We use GEN(x; ?) to refer to all possi ble CCG parses for x under some CCG lexicon ?.,Experiment/Discussion
"We will define f(x, y) ? Rd to be a d-dimensional feature?vector that represents a parse tree y pairedwith an input sentence x. In principle, f could include features that are sensitive to arbitrary sub structures within the pair (x, y).",Experiment/Discussion
We will define w ? Rd to be a parameter vector.,Experiment/Discussion
The optimal parse for a sentence x under parameters w and lexicon ? is then defined as y?(x) = arg max y?GEN(x;?),Experiment/Discussion
"w ? f(x, y) . Assuming sufficiently local features3 in f , search fory?",Experiment/Discussion
"can be achieved using dynamic-programming style algorithms, typically with some form of beam search.4 Training a model of this form involves learning the parameters w and potentially also thelexicon ?.",Experiment/Discussion
"This paper focuses on a method for learn ing a (w,?)",Experiment/Discussion
pair from a training set of sentences paired with lambda-calculus expressions.,Experiment/Discussion
2.4 Zettlemoyer and Collins 2005.,Experiment/Discussion
We now give a description of the approach of Zettle moyer and Collins (2005).,Experiment/Discussion
"This method will form the basis for our approach, and will be one of the baseline models for the experimental comparisons.The input to the ZC05 algorithm is a set of train ing examples (xi, zi) for i = 1 . . .",Experiment/Discussion
"n. Each xi isa sentence, and each zi is a corresponding lambda expression.",Experiment/Discussion
"The output from the algorithm is a pair (w,?)",Experiment/Discussion
"specifying a set of parameter values, and a CCG lexicon.",Experiment/Discussion
"Note that for a given training example (xi, zi), there may be many possible parses y which lead to the correct semantics zi.5 For this reason the training problem is a hidden-variable problem,where the training examples contain only partial information, and the CCG lexicon and parse deriva tions must be learned without direct supervision.",Experiment/Discussion
"A central part of the ZC05 approach is a function GENLEX(x, z) which maps a sentence x together with semantics z to a set of potential lexical entries.",Experiment/Discussion
"The function GENLEX is defined through a set of rules?see figure 2?that consider the expression z, and generate a set of categories that may help in building the target semantics z. An exhaustive setof lexical entries is then generated by taking all categories generated by the GENLEX rules, and pair ing themwith all possible sub-strings of the sentencex.",Experiment/Discussion
"Note that our lexicon can contain multi-word en tries, where a multi-word string such as New Yorkcan be paired with a CCG category.",Experiment/Discussion
"The final out 3For example, features which count the number of lexical entries of a particular type, or features that count the number of applications of a particular CCG combinator.4In our experiments we use a parsing algorithm that is simi lar to a CKY-style parser with dynamic programming.",Experiment/Discussion
"Dynamic programming is used but each entry in the chart maintains a full semantic expression, preventing a polynomial-time algorithm; beam search is used to make the approach tractable.",Experiment/Discussion
"5This problem is compounded by the fact that the lexicon is unknown, so that many of the possible hidden derivations involve completely spurious lexical entries.",Experiment/Discussion
"680 Rules Example categories produced from the logical form Input Trigger Output Category argmax(?x.flight(x) ? from(x, boston), ?x.cost(x)) constant c NP : c NP : boston arity one predicate p N : ?x.p(x) N : ?x.flight(x) arity one predicate p S\NP : ?x.p(x) S\NP : ?x.flight(x) arity two predicate p2 (S\NP )/NP : ?x.?y.p2(y, x) (S\NP )/NP : ?x.?y.from(y, x) arity two predicate p2 (S\NP )/NP : ?x.?y.p2(x, y) (S\NP )/NP : ?x.?y.from(x, y) arity one predicate p1 N/N : ?g.?x.p1(x) ? g(x) N/N : ?g.?x.flight(x) ? g(x) literal with arity two predicate p2 and constant second argument c N/N : ?g.?x.p2(x, c) ? g(x) N/N : ?g.?x.from(x, boston) ? g(x) arity two predicate p2 (N\N)/NP : ?y.?g.?x.p2(x, y) ? g(x) (N\N)/NP : ?y.?g.?x.from(x, y) ? g(x) an argmax /min with second argument arity one function f NP/N : ?g. argmax /min(g, ?x.f(x)) NP/N : ?g. argmax(g, ?x.cost(x)) arity one function f S/NP : ?x.f(x) S/NP : ?x.cost(x) arity one function f (N\N)/NP : ?y.?f.?x.g(x) ? f(x) >/< y (N\N)/NP : ?y.?f.?x.g(x) ? cost(x) > y no trigger S/NP : ?x.x, S/N : ?f.?x.f(x) S/NP : ?x.x, S/N : ?f.?x.f(x) Figure 2: Rules used in GENLEX.",Experiment/Discussion
Each row represents a rule.,Experiment/Discussion
The first column lists the triggers that identify some sub-structure within a logical form.,Experiment/Discussion
The second column lists the category that is created.,Experiment/Discussion
The third column lists categories that are created when the rule is applied to the logical form at the top of this column.,Experiment/Discussion
"We use the 10 rules described in ZC05 and add two new rules, listed in the last two rows above.",Experiment/Discussion
This first new rule is instantiated for greater than (>) and less than (<) comparisions.,Experiment/Discussion
The second new rule has no trigger; it is always applied.,Experiment/Discussion
It generates categories that are used to learn lexical entries for semantically vacuous sentence prefixes such as the phrase show me information on in the example in figure 1(b).,Experiment/Discussion
"put from GENLEX(x, z) is a large set of potentiallexical entries, with the vast majority of those en tries being spurious.",Experiment/Discussion
The algorithm in ZC05 embeds GENLEX within an overall learning approach that simultaneously selects a small subset of all entriesgenerated by GENLEX and estimates parameter val uesw.,Experiment/Discussion
Zettlemoyer and Collins (2005) present more complete details.,Experiment/Discussion
"In section 4.2 we describe a new, online algorithm that uses GENLEX.",Experiment/Discussion
This section describes a set of CCG combinators which we add to the conventional CCG combinatorsdescribed in section 2.2.,Experiment/Discussion
"These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.",Experiment/Discussion
We first describe a set of combinators that allow the parser to significantly relax con straints on word order.,Experiment/Discussion
"We then describe a set of type-raising rules which allow the parser to copewith telegraphic input (in particular, missing func tion words).",Experiment/Discussion
In both cases these additional rules lead to significantly more parses for any sentence x given a lexicon ?.,Experiment/Discussion
Many of these parses will be suspect from a linguistic perspective; broadening theset of CCG combinators in this way might be con sidered a dangerous move.,Experiment/Discussion
"However, the learning algorithm in our approach can learn weights for the new rules, effectively allowing the model to learn touse them only in appropriate contexts; in the exper iments we show that the rules are highly effective additions when used within a weighted CCG.",Experiment/Discussion
3.1 Application and Composition Rules.,Experiment/Discussion
The first new combinators we consider are the relaxed functional application rules: A\B : f B : g ? A : f(g) (&) B : g A/B : f ? A : f(g) (.),Experiment/Discussion
"These are variants of the original applicationrules, where the slash direction on the principal categories (A/B or A\B) is reversed.6 These rules allow simple reversing of regular word order, for ex ample flights one way N N/N ?x.flight(x) ?f.?x.f(x) ? one way(x) . N ?x.flight(x) ? one way(x) Note that we can recover the correct analysis for this fragment, with the same lexical entries as those used for the conventional word order, one-way flights.",Experiment/Discussion
A second set of new combinators are the relaxed functional composition rules: A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B) B\C : g A/B : f ? A\C : ?x.f(g(x)) (.,Experiment/Discussion
"B)These rules are variantions of the standard func tional composition rules, where the slashes of the principal categories are reversed.",Experiment/Discussion
6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.,Experiment/Discussion
"Steedman (2000) only considers rulesthat do not violate this principle?for example, crossed compo sition rules, which we consider later, and which Steedman also considers, do not violate this principle.",Experiment/Discussion
681An important point is that that these new composition and application rules can deal with quite flex ible word orders.,Experiment/Discussion
"For example, take the fragment to washington the latest flight.",Experiment/Discussion
In this case the parse is to washington the latest flight N\N NP/N N ?f.?x.f(x)?,Experiment/Discussion
"?f. argmax(f, ?x.flight(x) to(x,washington) ?y.depart time(y)) .B NP\N ?f. argmax(?x.f(x)?",Experiment/Discussion
"to(x,washington), ?y.depart time(y)) & NP argmax(?x.flight(x) ? to(x,washington), ?y.depart time(y))Note that in this case the substring the latest has cat egory NP/N , and this prevents a naive parse wherethe latest first combines with flight, and to washington then combines with the latest flight.",Experiment/Discussion
"The func tional composition rules effectively allow the latest to take scope over flight and to washington, in spite of the fact that the latest appears between the twoother sub-strings.",Experiment/Discussion
"Examples like this are quite fre quent in domains such as ATIS.We add features in the model which track the oc currences of each of these four new combinators.Specifically, we have four new features in the def inition of f; each feature tracks the number of times one of the combinators is used in a CCG parse.",Experiment/Discussion
"Themodel learns parameter values for each of these fea tures, allowing it to learn to penalise these rules to the correct extent.",Experiment/Discussion
3.2 Additional Rules of Type-Raising.,Experiment/Discussion
We now describe new CCG operations designed todeal with cases where words are in some sense miss ing in the input.,Experiment/Discussion
"For example, in the string flights Boston to New York, one style of analysis would assume that the preposition from had been deleted from the position before Boston.The first set of rules is generated from the follow ing role-hypothesising type shifting rules template: NP : c ? N\N : ?f.?x.f(x) ? p(x, c) (TR) This rule can be applied to any NP with semantics c, and any arity-two function p such that the secondargument of p has the same type as c. By ?any?",Experiment/Discussion
"aritytwo function, we mean any of the arity-two func tions seen in training data.",Experiment/Discussion
"We define features within the feature-vector f that are sensitive to the number of times these rules are applied in a parse; a separate feature is defined for each value of p. In practice, in our experiments most rules of this form have p as the semantics of some preposition, for example from or to.",Experiment/Discussion
"A typical example of a use of this rule would be the following: flights boston to new york N NP N\N ?x.flight(x) bos ?f.?x.f(x) ?to(x, new york) TR N\N ?f.?x.f(x) ? from(x, bos) < N ?f.?x.flight(x) ? from(x, bos) < N ?x.flight(x) ? to(x, new york) ? from(x, bos) The second rule we consider is the null-head type shifting rule: N\N : f ? N : f(?x.true) (TN)This rule allows parses of fragments such as Amer ican Airlines from New York, where there is again aword that is in some sense missing (it is straightfor ward to derive a parse for American Airlines flights from New York).",Experiment/Discussion
"The analysis would be as follows: American Airlines from New York N/N N\N ?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york) TN N ?x.from(x, new york) > N ?x.airline(x, aa) ? from(x, new york)The new rule effectively allows the preposi tional phrase from New York to type-shift to an entry with syntactic type N and semantics ?x.from(x, new york), representing the set of all things from New York.7 We introduce a single additional feature which counts the number of times this rule is used.",Experiment/Discussion
3.3 Crossed Composition Rules.,Experiment/Discussion
"Finally, we include crossed functional composition rules: A/B : f B\C : g ? A\C : ?x.f(g(x)) (>B?) B/C : g A\B : f ? A/C : ?x.f(g(x)) (<B?) These rules are standard CCG operators but they were not used by the parser described in ZC05.When used in unrestricted contexts, they can sig nificantly relax word order.",Experiment/Discussion
"Again, we address this 7Note that we do not analyze this prepositional phrase as having the semantics ?x.flight(x) ? from(x, new york)?although in principle this is possible?as the flight(x) predi cate is not necessarily implied by this utterance.",Experiment/Discussion
"682 dallas to washington the latest on friday NP (N\N)/NP NP NP/N (N\N)/NP NP dallas ?y.?f.?x.f(x) washington ?f. argmax(f, ?y.?f.?x.f(x) friday ?to(x, y) ?y.depart time(y)) ?day(x, y) TR > > N\N N\N N\N ?f.?x.f(x) ? from(x, dallas) ?f.?x.f(x) ? to(x,washington) ?f.?x.f(x) ? day(x, friday) <B TN N\N N ?f.?x.f(x) ? from(x, dallas) ? to(x,washington) ?x.day(x, friday) .B NP\N ?f. argmax(?x.f(x) ? from(x, dallas) ? to(x,washington), ?y.depart time(y)) & NP argmax(?x.day(x, friday) ? from(x, dallas) ? to(x,washington), ?y.depart time(y)) Figure 3: A parse with the flexible parser.problem by introducing features that count the num ber of times they are used in a parse.8 3.4 An Example.",Experiment/Discussion
"As a final point, to see how these rules can interact in practice, see figure 3.",Experiment/Discussion
"This example demonstrates the use of the relaxed application and composition rules, as well as the new type-raising rules.",Experiment/Discussion
This section describes an approach to learning in ourmodel.,Experiment/Discussion
We first define the features used and then de scribe a new online learning algorithm for the task.,Experiment/Discussion
4.1 Features in the Model.,Experiment/Discussion
"Section 2.3 described the use of a function f(x, y) which maps a sentence x together with a CCG parse y to a feature vector.",Experiment/Discussion
"As described in section 3,we introduce features for the new CCG combinators.",Experiment/Discussion
"In addition, we follow ZC05 in defining fea tures which track the number of times each lexical item in ? is used.",Experiment/Discussion
"For example, we would have one feature tracking the number of times the lexical entry flights := N : ?x.flights(x) is used in a parse, and similar features for all other members of ?.",Experiment/Discussion
"Finally, we introduce new features which directly consider the semantics of a parse.",Experiment/Discussion
"For each predicate f seen in training data, we introduce a feature that counts the number of times f is conjoined with itself at some level in the logical form.",Experiment/Discussion
"For example, the expression ?x.flight(x) ? from(x, new york) ? from(x, boston) would trigger the new feature for 8In general, applications of the crossed composition rules can be lexically governed, as described in work on Multi-ModalCCG (Baldridge, 2002).",Experiment/Discussion
In the future we would like to incorpo rate more fine-grained lexical distinctions of this type.,Experiment/Discussion
the from predicate signaling that the logical-form describes flights with more than one origin city.,Experiment/Discussion
We introduce similar features which track disjunction as opposed to conjunction.,Experiment/Discussion
4.2 An Online Learning Algorithm.,Experiment/Discussion
"Figure 4 shows a learning algorithm that takes a training set of (xi, zi) pairs as input, and returns a weighted CCG (i.e., a pair (w,?)) as its output.The algorithm is online, in that it visits each example in turn, and updates both w and ? if neces sary.",Experiment/Discussion
"In Step 1 on each example, the input xi isparsed.",Experiment/Discussion
"If it is parsed correctly, the algorithm im mediately moves to the next example.",Experiment/Discussion
"In Step 2,the algorithm temporarily introduces all lexical en tries seen in GENLEX(xi, zi), and finds the highest scoring parse that leads to the correct semantics zi.",Experiment/Discussion
"A small subset of GENLEX(xi, zi)?namely, only those lexical entries that are contained in the highest scoring parse?are added to ?.",Experiment/Discussion
"In Step 3, a simple perceptron update (Collins, 2002) is performed.",Experiment/Discussion
"The hypothesis is parsed again with the new lexicon, andan update to the parameters w is made if the result ing parse does not have the correct logical form.",Experiment/Discussion
This algorithm differs from the approach in ZC05in a couple of important respects.,Experiment/Discussion
"First, the ZC05 al gorithm performed learning of the lexicon ? at each iteration in a batch method, requiring a pass over the entire training set.",Experiment/Discussion
"The new algorithm is fully online, learning both ? and w in an example-by-example fashion.",Experiment/Discussion
This has important consequences for the efficiency of the algorithm.,Experiment/Discussion
"Second, the parameter estimation method in ZC05 was based on stochasticgradient descent on a log-likelihood objective func tion.",Experiment/Discussion
"The new algorithm makes use of perceptron 683 Inputs: Training examples {(xi, zi) : i = 1 . . .",Experiment/Discussion
"n} where each xi is a sentence, each zi is a logical form.",Experiment/Discussion
An initial lexicon ?0.,Experiment/Discussion
"Number of training iterations, T . Definitions: GENLEX(x, z) takes as input a sentence x anda logical form z and returns a set of lexical items as de scribed in section 2.4.",Experiment/Discussion
GEN(x; ?) is the set of all parses for x with lexicon ?.,Experiment/Discussion
"GEN(x, z; ?) is the set of all parses for x with lexicon ?, which have logical form z. Thefunction f(x, y) represents the features described in sec tion 4.1.",Experiment/Discussion
The function L(y) maps a parse tree y to its associated logical form.,Experiment/Discussion
Initialization: Set parameters w to initial values described in section 6.2.,Experiment/Discussion
Set ? = ?0.,Experiment/Discussion
Algorithm: ? For t = 1 . . .,Experiment/Discussion
"T, i = 1 . . .",Experiment/Discussion
n : Step 1: (Check correctness) ? Let y?,Experiment/Discussion
= argmaxy?GEN(xi;?),Experiment/Discussion
"w ? f(xi, y) . ? If L(y?) = zi, go to the next example.",Experiment/Discussion
Step 2: (Lexical generation) ? Set ? = ? ?,Experiment/Discussion
"GENLEX(xi, zi) . ? Let y?",Experiment/Discussion
"= argmaxy?GEN(xi,zi;?)",Experiment/Discussion
"w ? f(xi, y) . ? Define ?i to be the set of lexical entries in y?.",Experiment/Discussion
Set lexicon to ? = ? ?,Experiment/Discussion
?i . Step 3: (Update parameters) ? Let y?,Experiment/Discussion
= argmaxy?GEN(xi;?),Experiment/Discussion
"w ? f(xi, y) . ? If L(y?) 6= zi : ? Set w = w + f(xi, y?)",Experiment/Discussion
"f(xi, y?)",Experiment/Discussion
Output: Lexicon ? together with parameters w. Figure 4: An online learning algorithm.,Experiment/Discussion
"updates, which are simpler and cheaper to compute.As in ZC05, the algorithm assumes an initial lex icon ?0 that contains two types of entries.",Experiment/Discussion
"First, we compile entries such as Boston := NP : boston for entities such as cities, times and month-names that occur in the domain or underlying database.",Experiment/Discussion
In practice it is easy to compile a list of these atomic entities.,Experiment/Discussion
"Second, the lexicon has entries for some function words such as wh-words, and determiners.9",Experiment/Discussion
There has been a significant amount of previous work on learning to map sentences to under lying semantic representations.,Experiment/Discussion
"A wide variety 9Our assumption is that these entries are likely to be domain independent, so it is simple enough to compile a list that can be reused in new domains.",Experiment/Discussion
"Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned.of techniques have been considered including ap proaches based on machine translation techniques (Papineni et al, 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques(Miller et al, 1996; Ge and Mooney, 2006), tech niques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al, 2005), andideas from string kernels and support vector ma chines (Kate and Mooney, 2006; Nguyen et al, 2006).",Experiment/Discussion
"In our experiments we compare to He and Young (2006) on the ATIS domain and Zettlemoyerand Collins (2005) on the Geo880 domain, because these systems currently achieve the best per formance on these problems.",Experiment/Discussion
The approach of Zettlemoyer and Collins (2005) was presented in section 2.4.,Experiment/Discussion
He and Young (2005) describe an algorithm that learns a probabilisticpush-down automaton that models hierarchical de pendencies but can still be trained on a data set that does not have full treebank-style annotations.,Experiment/Discussion
"Thisapproach has been integrated with a speech recog nizer and shown to be robust to recognition errors (He and Young, 2006).There is also related work in the CCG litera ture.",Experiment/Discussion
Clark and Curran (2003) present a method forlearning the parameters of a log-linear CCG pars ing model from fully annotated normal?form parse trees.,Experiment/Discussion
Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexiconsthat does not represent the semantics of the training sentences.,Experiment/Discussion
Bos et al (2004) present an al gorithm that learns CCG lexicons with semantics but requires fully?specified CCG derivations in thetraining data.,Experiment/Discussion
Bozsahin (1998) presents work on us ing CCG to model languages with free word order.,Experiment/Discussion
"In addition, there is related work that focuses on modeling child language learning.",Experiment/Discussion
Siskind (1996) presents an algorithm that learns word-to-meaning mappings from sentences that are paired with a set of possible meaning representations.,Experiment/Discussion
"Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information.Both of these approaches use sentences from child directed speech, which differ significantly from the natural language interface queries we consider.",Experiment/Discussion
"Finally, there is work on manually developing parsing techniques to improve robustness (Carbonell 684and Hayes, 1983; Seneff, 1992).",Experiment/Discussion
"In contrast, our ap proach is integrated into a learning framework.",Experiment/Discussion
The main focus of our experiments is on the ATIS travel planning domain.,Results/Conclusion
"For development, we used4978 sentences, split into a training set of 4500 ex amples, and a development set of 478 examples.",Results/Conclusion
"Fortest, we used the ATIS NOV93 test set which con tains 448 examples.",Results/Conclusion
"To create the annotations, wecreated a script that maps the original SQL annotations provided with the data to lambda-calculus ex pressions.",Results/Conclusion
"He and Young (2006) previously reported results on the ATIS domain, using a learning approachwhich also takes sentences paired with semantic annotations as input.",Results/Conclusion
"In their case, the semantic struc tures resemble context-free parses with semantic (asopposed to syntactic) non-terminal labels.",Results/Conclusion
"In our experiments we have used the same split into training and test data as He and Young (2006), ensur ing that our results are directly comparable.",Results/Conclusion
"He and Young (2006) report partial match figures for their parser, based on precision and recall in recovering attribute-value pairs.",Results/Conclusion
"(For example, the sentence flights to Boston would have a single attribute-valueentry, namely destination = Boston.)",Results/Conclusion
"It is sim ple for us to map from lambda-calculus expressions to attribute-value entries of this form; for example, the expression to(x,Boston) would be mapped to destination = Boston.",Results/Conclusion
"He and Young (2006) gave us their data and annotations, so we can directly compare results on the partial-match criterion.",Results/Conclusion
"Wealso report accuracy for exact matches of lambda calculus expressions, which is a stricter criterion.",Results/Conclusion
"In addition, we report results for the method on the Geo880 domain.",Results/Conclusion
"This allows us to compare directly to the previous work of Zettlemoyer and Collins (2005), using the same split of the data intotraining and test sets of sizes 600 and 280 respec tively.",Results/Conclusion
"We use cross-validation of the training set, asopposed to a separate development set, for optimiza tion of parameters.",Results/Conclusion
6.1 Improving Recall.,Results/Conclusion
The simplest approach to the task is to train the parser and directly apply it to test sentences.,Results/Conclusion
"In our experiments we will see that this produces resultswhich have high precision, but somewhat lower recall, due to some test sentences failing to parse (usu ally due to words in the test set which were neverobserved in training data).",Results/Conclusion
A simple strategy to alle viate this problem is as follows.,Results/Conclusion
"If the sentence failsto parse, we parse the sentence again, this time al lowing parse moves which can delete words at some cost.",Results/Conclusion
The cost of this deletion operation is optimizedon development data.,Results/Conclusion
This approach can significantly improve F-measure on the partial-match cri terion in particular.,Results/Conclusion
We report results both with and without this second pass strategy.,Results/Conclusion
6.2 Parameters in the Approach.,Results/Conclusion
"The algorithm in figure 4 has a number of param eters, the set {T, ?, ?, ?}, which we now describe.The values of these parameters were chosen to op timize the performance on development data.",Results/Conclusion
"T is the number of passes over the training set, and was set to be 4.",Results/Conclusion
Each lexical entry in the initial lexicon?0 has an associated feature which counts the num ber of times this entry is seen in a parse.,Results/Conclusion
The initial parameter value in w for all features of this form was chosen to be some value ?.,Results/Conclusion
"Each of the newCCG rules?the application, composition, crossedcomposition, and type-raising rules described in sec tion 3?has an associated parameter.",Results/Conclusion
We set al of these parameters to the same initial value ?.,Results/Conclusion
"Finally, when new lexical entries are added to ?",Results/Conclusion
"(in step 2 of the algorithm), their initial weight is set to some value ?.",Results/Conclusion
"In practice, optimization on developmentdata led to a positive value for ?, and negative val ues for ? and ?.",Results/Conclusion
6.3 Results.,Results/Conclusion
Table 1 shows accuracy for the method by the exact match criterion on the ATIS test set.,Results/Conclusion
"The two passstrategy actually hurts F-measure in this case, al though it does improve recall of the method.Table 2 shows results under the partial-match cri terion.",Results/Conclusion
"The results for our approach are higher than those reported by He and Young (2006) even without the second, high-recall, strategy.",Results/Conclusion
"With the two-pass strategy our method has more than halved the F-measure error rate, giving improvements from 90.3% F-measure to 95.9% F-measure.",Results/Conclusion
Table 3 shows results on the Geo880 domain.,Results/Conclusion
The 685 Precision Recall F1 Single-Pass Parsing 90.61 81.92 86.05 Two-Pass Parsing 85.75 84.6 85.16 Table 1: Exact-match accuracy on the ATIS test set.,Results/Conclusion
Precision Recall F1 Single-Pass Parsing 96.76 86.89 91.56 Two-Pass Parsing 95.11 96.71 95.9 He and Young (2006) ? ?,Results/Conclusion
90.3 Table 2: Partial-credit accuracy on the ATIS test set.,Results/Conclusion
"new method gives improvements in performance both with and without the two pass strategy, showingthat the new CCG combinators, and the new learn ing algorithm, give some improvement on even this domain.",Results/Conclusion
The improved performance comes from aslight drop in precision which is offset by a large in crease in recall.,Results/Conclusion
"Table 4 shows ablation studies on the ATIS data, where we have selectively removed various aspectsof the approach, to measure their impact on performance.",Results/Conclusion
"It can be seen that accuracy is seriously de graded if the new CCG rules are removed, or if the features associated with these rules (which allow the model to penalize these rules) are removed.Finally, we report results concerning the effi ciency of the new online algorithm as compared to the ZC05 algorithm.",Results/Conclusion
"We compared running times for the new algorithm, and the ZC05 algorithm, on the geography domain, with both methods making 4 passes over the training data.",Results/Conclusion
"The new algorithm took less than 4 hours, compared to over 12 hours for the ZC05 algorithm.",Results/Conclusion
"The main explanation for this improved performance is that on many trainingexamples,10 in step 1 of the new algorithm a cor rect parse is found, and the algorithm immediately moves on to the next example.",Results/Conclusion
"Thus GENLEX is not required, and in particular parsing the example with the large set of entries generated by GENLEX is not required.",Results/Conclusion
"We presented a new, online algorithm for learning a combinatory categorial grammar (CCG), together with parameters that define a log-linear pars ing model.",Results/Conclusion
"We showed that the use of non-standardCCG combinators is highly effective for parsing sen 10Measurements on the Geo880 domain showed that in the 4 iterations, 83.3% of all parses were successful at step 1.",Results/Conclusion
Precision Recall F1 Single-Pass Parsing 95.49 83.2 88.93 Two-Pass Parsing 91.63 86.07 88.76 ZC05 96.25 79.29 86.95 Table 3: Exact-match accuracy on the Geo880 test set.,Results/Conclusion
Precision Recall F1 Full Online Method 87.26 74.44 80.35 Without control features 70.33 42.45 52.95 Without relaxed word order 82.81 63.98 72.19 Without word insertion 77.31 56.94 65.58 Table 4: Exact-match accuracy on the ATIS development setfor the full algorithm and restricted versions of it.,Results/Conclusion
The sec ond row reports results of the approach without the featuresdescribed in section 3 that control the use of the new combi nators.,Results/Conclusion
The third row presents results without the combinators from section 3.1 that relax word order.,Results/Conclusion
"The fourth row reports experiments without the type-raising combinators presented in section 3.2.tences with the types of phenomena seen in spontaneous, unedited natural language.",Results/Conclusion
The resulting sys tem achieved significant accuracy improvements in both the ATIS and Geo880 domains.,Results/Conclusion
Acknowledgements Wewould like to thank Yulan He and Steve Young for their help with obtaining the ATIS data set.,Results/Conclusion
We also acknowledge the support for this research.,Results/Conclusion
Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.,Results/Conclusion
