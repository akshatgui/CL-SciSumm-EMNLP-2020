col1,col2
graphs: An efficient interface between continous speech recognition and language understanding.,{}
"International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.",{}
"Stefan Ortmanns, Hermann Ney, and Xavier Aubert.",{}
1997.,{}
"A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.",{}
Christoph Tillmann and Hermann Ney.,{}
2000.,{}
Word re-ordering and DP-based search in statistical matranslation.,{}
In '00: The 18th Int.,{}
whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.,"{'title': '', 'number': '1'}"
"Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.","{'title': '', 'number': '1'}"
"After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.","{'title': '', 'number': '1'}"
Those hypotheses that survive the pruning are called the active hypotheses.,"{'title': '', 'number': '1'}"
"The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.","{'title': '', 'number': '1'}"
It is widely accepted in the community that a significant improvement in translation quality will come from more sophisticated translation and language models.,"{'title': '3 Word Graphs', 'number': '2'}"
"For example, a language model that goes beyond m-gram dependencies could be used, but this would be difficult to integrate into the search process.","{'title': '3 Word Graphs', 'number': '2'}"
"As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.","{'title': '3 Word Graphs', 'number': '2'}"
We can then apply rescoring with a refined model to those hypotheses.,"{'title': '3 Word Graphs', 'number': '2'}"
One efficient way to store the different alternatives is a word graph.,"{'title': '3 Word Graphs', 'number': '2'}"
"Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993).","{'title': '3 Word Graphs', 'number': '2'}"
"(Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation.","{'title': '3 Word Graphs', 'number': '2'}"
"In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.","{'title': '3 Word Graphs', 'number': '2'}"
"During search, we keep a bookkeeping tree.","{'title': '3 Word Graphs', 'number': '2'}"
"It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: After the search has finished, i.e. when all source sentence positions have been translated, we trace back the best sentence in the bookkeeping tree.","{'title': '3 Word Graphs', 'number': '2'}"
"To generate the N best hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined.","{'title': '3 Word Graphs', 'number': '2'}"
"Thus, many hypotheses with a high probability have not been stored.","{'title': '3 Word Graphs', 'number': '2'}"
"To overcome this problem, we enhance the bookkeeping concept and generate a word graph as described in Section 3.3.","{'title': '3 Word Graphs', 'number': '2'}"
"If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.","{'title': '3 Word Graphs', 'number': '2'}"
"Thus, an entry in the bookkeeping structure may have several backpointers to different preceding entries.","{'title': '3 Word Graphs', 'number': '2'}"
The bookkeeping structure is no longer a tree but a network where the source is the bookkeeping entry with zero covered source sentence positions and the sink is a node accounting for complete hypotheses (see Figure 3).,"{'title': '3 Word Graphs', 'number': '2'}"
"This leads us to the concept of word graph nodes and edges containing the following information: — the probabilities according to the different models: the language model and the translation submodels, — the backpointer to the preceding bookkeeping entry.","{'title': '3 Word Graphs', 'number': '2'}"
"After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure.","{'title': '3 Word Graphs', 'number': '2'}"
"Thus, we can perform garbage collection and remove all those bookkeeping entries that cannot be reached from the backpointers of the active hypotheses.","{'title': '3 Word Graphs', 'number': '2'}"
This reduces the size of the bookkeeping structure significantly.,"{'title': '3 Word Graphs', 'number': '2'}"
An example of a word graph can be seen in Figure 3.,"{'title': '3 Word Graphs', 'number': '2'}"
"To keep the presentation simple, we chose an example without reordering of sentence positions.","{'title': '3 Word Graphs', 'number': '2'}"
"The words on the edges are the produced target words, and the bitvectors in the nodes show the covered source sentence positions.","{'title': '3 Word Graphs', 'number': '2'}"
"If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.","{'title': '3 Word Graphs', 'number': '2'}"
"The reference translation 'what did you say ?' is contained in the graph, but it has a slightly lower probability than the sentence 'what do you say ?","{'title': '3 Word Graphs', 'number': '2'}"
"', which is then chosen by the single best search.","{'title': '3 Word Graphs', 'number': '2'}"
"The recombination of hypotheses can be seen in the nodes with two or more incoming edges: those hypotheses have been recombined, because they were indistinguishable by translation and language model state.","{'title': '3 Word Graphs', 'number': '2'}"
"To study the effect of the word graph size on the translation quality, we produce a conservatively large word graph.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
Then we apply word graph pruning with a threshold t < 1 and study the change of graph error rate (see Section 5).,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
The pruning is based on the beam search concept also used in the single best search: we determine the probability of the best sentence hypothesis in the word graph.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
All hypotheses in the graph which probability is lower than this maximum probability multiplied with the pruning threshold are discarded.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"In single best search, a standard trigram language model is used.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Search with a bigram language model is much faster, but it yields a lower translation quality.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).","{'title': '4 Pruning and Rescoring', 'number': '3'}"
This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"The rescoring algorithm is based on dynamic programming; a description can be found in (Ortmanns et al., 1997).","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The results of the comparison of the one-pass and the two-pass search are given in Section 5.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"We use A* search for finding the N best sentences in a word graph: starting in the root of the graph, we successively expand the sentence hypotheses.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"As rest cost estimation, we use the probabilities determined in a backward pass as follows: for each node in the graph, we calculate the probability of a best path from this node to the goal node, i.e. the highest probability for completing a partial hypothesis.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Thus, the N best hypothesis are extracted from the graph without additional overhead of finding sentences with a lower probability.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Of course, the hypotheses must not be recombined during this search.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
We have to keep every partial hypothesis in the priority queue in order to determine the N best sentences.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Otherwise, we might lose one of them by recombination.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The calculation of the graph error rate is performed by a dynamic programming based algorithm.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
Its space complexity is the number of graph nodes times the length of the reference translation.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"In our experiments, we varied the word graph pruning threshold in order to obtain word graphs of different densities, i.e. different numbers of hypotheses.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words — analogously to the word graph density in speech recognition.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
The effect of pruning on the graph error rate is shown in Table 3.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
The value of the pruning threshold is given as the negative logarithm of the probability.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"Thus, t = 0 refers to pruning everything but the best hypothesis.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
Figure 4 shows the change in graph error rate in relation to the average graph density.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
"We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.","{'title': '4 Pruning and Rescoring', 'number': '3'}"
The saturation point of the GER lies at 13% and is reached for an average graph density about 1000 which relates to a pruning threshold of 20.,"{'title': '4 Pruning and Rescoring', 'number': '3'}"
We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.,"{'title': '6 Conclusion', 'number': '4'}"
Experiments have shown that the graph error rate significantly decreases for rising word graph densities.,"{'title': '6 Conclusion', 'number': '4'}"
The quality of the hypotheses contained in a word graph is better than of those in an N-best list.,"{'title': '6 Conclusion', 'number': '4'}"
This indicates that word graph rescoring can yield a significant gain in translation quality.,"{'title': '6 Conclusion', 'number': '4'}"
"For the future, we plan the application of refined translation and language models for rescoring on word graphs.","{'title': '6 Conclusion', 'number': '4'}"
