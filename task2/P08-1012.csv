col1,col2
We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs.,{}
"The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.",{}
Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.,{}
"Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.",{}
This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.,{}
Most state-of-the-art statistical machine translation systems are based on large phrase tables extracted from parallel text using word-level alignments.,"{'title': '1 Introduction', 'number': '1'}"
These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. (1993) and Vogel et al.,"{'title': '1 Introduction', 'number': '1'}"
(1996).,"{'title': '1 Introduction', 'number': '1'}"
"As these word-level alignment models restrict the word alignment complexity by requiring each target word to align to zero or one source words, results are improved by aligning both source-to-target as well as target-to-source, then heuristically combining these alignments.","{'title': '1 Introduction', 'number': '1'}"
"Finally, the set of phrases consistent with the word alignments are extracted from every sentence pair; these form the basis of the decoding process.","{'title': '1 Introduction', 'number': '1'}"
"While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems.","{'title': '1 Introduction', 'number': '1'}"
A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.,"{'title': '1 Introduction', 'number': '1'}"
"Ideally, such a procedure would remedy the deficiencies of word-level alignment models, including the strong restrictions on the form of the alignment, and the strong independence assumption between words.","{'title': '1 Introduction', 'number': '1'}"
Furthermore it would obviate the need for heuristic combination of word alignments.,"{'title': '1 Introduction', 'number': '1'}"
"A unified procedure may also improve the identification of non-compositional phrasal translations, and the attachment decisions for unaligned words.","{'title': '1 Introduction', 'number': '1'}"
"In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting.","{'title': '1 Introduction', 'number': '1'}"
"Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we attempt to address these two issues in order to apply EM above the word level.","{'title': '1 Introduction', 'number': '1'}"
"We attack computational complexity by adopting the polynomial-time Inversion Transduction Grammar framework, and by only learning small noncompositional phrases.","{'title': '1 Introduction', 'number': '1'}"
"We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases.","{'title': '1 Introduction', 'number': '1'}"
"We test our model by extracting longer phrases from our model’s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments.","{'title': '1 Introduction', 'number': '1'}"
"We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
They split the left-hand side constituent which represents a phrase pair into two smaller phrase pairs on the right-hand side and order them according to one of the two possible permutations.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
The rewriting process continues until the third rule is invoked.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"C is our unique pre-terminal for generating terminal multi-word pairs: We parameterize our probabilistic model in the manner of a PCFG: we associate a multinomial distribution with each nonterminal, where each outcome in this distribution corresponds to an expansion of that nonterminal.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"Specifically, we place one multinomial distribution θX over the three expansions of the nonterminal X, and another multinomial distribution θC over the expansions of C. Thus, the parameters in our model can be listed as where Phi is for the inverted rule, PD for the straight rule, PC for the third rule, satisfying Phi+PD+PC = 1, and where &/f P(e/f) = 1 is a multinomial distribution over phrase pairs.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
This is our model in a nutshell.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"We can train this model using a two-dimensional extension of the inside-outside algorithm on bilingual data, assuming every phrase pair that can appear as a leaf in a parse tree of the grammar a valid candidate.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"However, it is easy to show that the maximum likelihood training will lead to the saturated solution where PC = 1 — each sentence pair is generated by a single phrase spanning the whole sentence.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"From the computational point of view, the full EM algorithm runs in O(n6) where n is the average length of the two input sentences, which is too slow in practice.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"The key is to control the number of parameters, and therefore the size of the set of candidate phrases.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
We deal with this problem in two directions.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
First we change the objective function by incorporating a prior over the phrasal parameters.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
This has the effect of preferring parameter vectors in θC with fewer non-zero values.,"{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"First we train a lower level word alignment model, then we place hard constraints on the phrasal alignment space using confident word links from this simpler model.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
"Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model.","{'title': '2 Phrasal Inversion Transduction Grammar', 'number': '2'}"
Goldwater and Griffiths (2007) and Johnson (2007) show that modifying an HMM to include a sparse prior over its parameters and using Bayesian estimation leads to improved accuracy for unsupervised part-of-speech tagging.,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"In this section, we describe a Bayesian estimator for ITG: we select parameters that optimize the probability of the data given a prior.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"The traditional estimation method for word alignment models is the EM algorithm (Brown et al., 1993) which iteratively updates parameters to maximize the likelihood of the data.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
The drawback of maximum likelihood is obvious for phrase-based models.,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"If we do not put any constraint on the distribution of phrases, EM overfits the data by memorizing every sentence pair.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
A sparse prior over a multinomial distribution such as the distribution of phrase pairs may bias the estimator toward skewed distributions that generalize better.,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"In the context of phrasal models, this means learning the more representative phrases in the space of all possible phrases.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"The Dirichlet distribution, which is parameterized by a vector of real values often interpreted as pseudo-counts, is a natural choice for the prior, for two main reasons.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"First, the Dirichlet is conjugate to the multinomial distribution, meaning that if we select a Dirichlet prior and a multinomial likelihood function, the posterior distribution will again be a Dirichlet.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
This makes parameter estimation quite simple.,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"Second, Dirichlet distributions with small, non-zero parameters place more probability mass on multinomials on the edges or faces of the probability simplex, distributions with fewer non-zero parameters.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"Starting from the model from Section 2, we propose the following Bayesian extension, where A — Dir(B) means the random variable A is distributed according to a Dirichlet with parameter B: The parameters αX and αC control the sparsity of the two distributions in our model.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
One is the distribution of the three possible branching choices.,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"The other is the distribution of the phrase pairs. αC is crucial, since the multinomial it is controlling has a high dimension.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"By adjusting αC to a very small number, we hope to place more posterior mass on parsimonious solutions with fewer but more confident and general phrase pairs.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"Having defined the Bayesian model, it remains to decide the inference procedure.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"We chose Variational Bayes, for its procedural similarity to EM and ease of implementation.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
Another potential option would be Gibbs sampling (or some other sampling technique).,"{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipf’s law, which is prominent in natural languages.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"As in the case of maximum likelihood estimation, Bayesian estimation for ITGs is very similar to PCFGs, which follows due to the strong isomorphism between the two models.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"Specific to our ITG case, the M step becomes: where ψ is the digamma function (Beal, 2003), s = 3 is the number of right-hand-sides for X, and m is the number of observed phrase pairs in the data.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"The sole difference between EM and VB with a sparse prior α is that the raw fractional counts c are replaced by exp(ψ(c + α)), an operation that resembles smoothing.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"As pointed out by Johnson (2007), in effect this expression adds to c a small value that asymptotically approaches α — 0.5 as c approaches oc, and 0 as c approaches 0.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
"For small values of α the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more likely ones.","{'title': '3 Variational Bayes for ITG', 'number': '3'}"
ITG is slow mainly because it considers every pair of spans in two sentences as a possible chart element.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"In reality, the set of useful chart elements is much smaller than the possible scriptO(n4), where n is the average sentence length.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Pruning the span pairs (bitext cells) that can participate in a tree (either as terminals or non-terminals) serves to not only speed up ITG parsing, but also to provide a kind of initialization hint to the training procedures, encouraging it to focus on promising regions of the alignment space.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Given a bitext cell defined by the four boundary indices (i, j,l, m) as shown in Figure 1a, we prune based on a figure of merit V (i, j,l, m) approximating the utility of that cell in a full ITG parse.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005).","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Like Zhang and Gildea (2005), it is used to prune bitext cells rather than score phrases.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"The total score is the product of the Model 1 probabilities for each column; “inside” columns in the range [l, m] are scored according to the sum (or maximum) of Model 1 probabilities for [i, j], and “outside” columns use the sum (or maximum) of all probabilities not in the range [i, j].","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
Our pruning differs from Zhang and Gildea (2005) in two major ways.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"First, we perform pruning using both directions of the IBM Model 1 scores; instead of a single figure of merit V , we have two: VF and VB.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
Only those spans that pass the pruning threshold in both directions are kept.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Second, we allow whole spans to be pruned.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"The figure of merit for a span is VF (i, j) = maxl,m VF (i, j,l, m).","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"However, even this can be slow for large values of n. Therefore we describe an improved algorithm with best case n3 performance.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Although the worst case performance is also O(n4), in practice it is significantly faster.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"To begin, let us restrict our attention to the forward direction for a fixed source span (i, j).","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Pruning bitext spans and cells requires VF (i, j), the score of the best bitext cell within a given span, as well as all cells within a given threshold of that best score.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"For a fixed i and j, we need to search over the starting and ending points l and m of the inside region.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Note that there is an isomorphism between the set of spans and a simple finite state machine: any span (l, m) can be represented by a sequence of l OUTSIDE columns, followed by m−l+1 INSIDE columns, followed by n − m + 1 OUTSIDE columns.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"This simple machine has the restricted form described in Figure 1c: it has three states, L, M, and R; each transition generates either an OUTSIDE column O or an INSIDE column I.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"The cost of generating an OUTSIDE at position a is O(a) = P(ta|NULL) + &0[i,j] P(ta|sb); likewise the cost of generating an INSIDE column Directly computing O and I would take time O(n2) for each source span, leading to an overall runtime of O(n4).","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
Luckily there are faster ways to find the inside and outside scores.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
First we can precompute following arrays in O(n2) time and space: I(a).,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
Thus we have linear time updates for O and I.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
We can then find the best scoring sequence using the familiar Viterbi algorithm.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Let S[a, Q] be the cost of the best scoring sequence ending at in state Q at time a: phism between state sequences and spans.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
This linear time algorithm allows us to compute span pruning in O(n3) time.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
The same algorithm may be performed using the backward figure of merit after transposing rows and columns.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Having cast the problem in terms of finite state automata, we can use finite state algorithms for pruning.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"For instance, fixing a source span we can enumerate the target spans in decreasing order by score (Soong and Huang, 1991), stopping once we encounter the first span below threshold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"In practice the overhead of maintaining the priority queue outweighs any benefit, as seen in Figure 2.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
An alternate approach that avoids this overhead is to enumerate spans by position.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Note that S[m, R] · Qna=m+1 O(a) is within threshold iff there is a span with right boundary m′ < m within threshold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Furthermore if S[m, M] · Qna=m+1 O(a) is within threshold, then m is the right boundary within threshold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Using these facts, we can gradually sweep the right boundary m from n toward 1 until the first condition fails to hold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"For each value where the second condition holds, we pause to search for the set of left boundaries within threshold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Likewise for the left edge, S[l, M] · Qma=l+1 I(a) · is a bitext cell within threshold.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"For right edges that are known to be within threshold, we can sweep the left edges leftward until the first condition no longer holds, keeping only those spans for which the second condition holds.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
The filtering algorithm behaves extremely well.,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
"Although the worst case runtime is still O(n4), the best case has improved to n3; empirically it seems to significantly reduce the amount of time spent exploring spans.","{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).,"{'title': '4 Bitext Pruning Strategy', 'number': '4'}"
This section introduces a technique that bootstraps candidate phrase pairs for phrase-based ITG from word-based ITG Viterbi alignments.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"The wordbased ITG uses the same expansions for the nonterminal X, but the expansions of C are limited to generate only 1-1, 1-0, and 0-1 alignments: where E indicates that no word was generated.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"Broadly speaking, the goal of this section is the same as the previous section, namely, to limit the set of phrase pairs that needs to be considered in the training process.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
The tic-tac-toe pruning relies on IBM model 1 for scoring a given aligned area.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
The heuristic method is based on the NonCompositional Constraint of Cherry and Lin (2007).,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
Cherry and Lin (2007) use GIZA++ intersections which have high precision as anchor points in the bitext space to constraint ITG phrases.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
We use ITG Viterbi alignments instead.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
The benefit is two-fold.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"First of all, we do not have to run a GIZA++ aligner.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"Second, we do not need to worry about non-ITG word alignments, such as the (2, 4, 1, 3) permutation patterns.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"GIZA++ does not limit the set of permutations allowed during translation, so it can produce permutations that are not reachable using an ITG.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"Mathematically, let e(i, j) count the number of word links that are emitted from the substring ez...j, and f(l, m) count the number of word links emitted from the substring fl...ry,t.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"The non-compositional phrase pairs satisfy e(i, j) = f(l, m) G 1.","{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
Figure 3 (a) shows all possible non-compositional phrases given the Viterbi word alignment of the example sentence pair.,"{'title': '5 Bootstrapping Phrasal ITG from Word-based ITG', 'number': '5'}"
"We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this paper: Variational Bayes, tic-tactoe pruning, and word-to-phrase bootstrapping.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
We start from sentence-aligned bilingual data and run IBM Model 1 in both directions to obtain two translation tables.,"{'title': '6 Summary of the Pipeline', 'number': '6'}"
Then we use the efficient bidirectional tic-tac-toe pruning to prune the bitext space within each of the sentence pairs; ITG parsing will be carried out on only this this sparse set of bitext cells.,"{'title': '6 Summary of the Pipeline', 'number': '6'}"
"The first stage of training is word-based ITG, using the standard iterative training procedure, except VB replaces EM to focus on a sparse prior.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
"After several training iterations, we obtain the Viterbi alignments on the training data according to the final model.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
Now we transition into the second stage – the phrasal training.,"{'title': '6 Summary of the Pipeline', 'number': '6'}"
"Before the training starts, we apply the non-compositional constraints over the pruned bitext space to further constrain the space of phrase pairs.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
"Finally, we run phrasal ITG iterative training using VB for a certain number of iterations.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
"In the end, a Viterbi pass for the phrasal ITG is executed to produce the non-compositional phrasal alignments.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
"From this alignment, phrase pairs are extracted in the usual manner, and a phrase-based translation system is trained.","{'title': '6 Summary of the Pipeline', 'number': '6'}"
AER,"{'title': '6 Summary of the Pipeline', 'number': '6'}"
"The training data was a subset of 175K sentence pairs from the NIST Chinese-English training data, automatically selected to maximize character-level overlap with the source side of the test data.","{'title': '7 Experiments', 'number': '7'}"
"We put a length limit of 35 on both sides, producing a training set of 141K sentence pairs.","{'title': '7 Experiments', 'number': '7'}"
500 Chinese-English pairs from this set were manually aligned and used as a gold standard.,"{'title': '7 Experiments', 'number': '7'}"
"First, using evaluations of alignment quality, we demonstrate the effectiveness of VB over EM, and explore the effect of the prior.","{'title': '7 Experiments', 'number': '7'}"
"Figure 4 examines the difference between EM and VB with varying sparse priors for the word-based model of ITG on the 500 sentence pairs, both after 10 iterations of training.","{'title': '7 Experiments', 'number': '7'}"
"Using EM, because of overfitting, AER drops first and increases again as the number of iterations varies from 1 to 10.","{'title': '7 Experiments', 'number': '7'}"
"The lowest AER using EM is achieved after the second iteration, which is .40.","{'title': '7 Experiments', 'number': '7'}"
"At iteration 10, AER for EM increases to .42.","{'title': '7 Experiments', 'number': '7'}"
"On the other hand, using VB, AER decreases monotonically over the 10 iterations and stabilizes at iteration 10.","{'title': '7 Experiments', 'number': '7'}"
"When αC is 1e − 9, VB gets AER close to .35 at iteration 10.","{'title': '7 Experiments', 'number': '7'}"
"As we increase the bias toward sparsity, the AER decreases, following a long slow plateau.","{'title': '7 Experiments', 'number': '7'}"
"Although the magnitude of improvement is not large, the trend is encouraging.","{'title': '7 Experiments', 'number': '7'}"
These experiments also indicate that a very sparse prior is needed for machine translation tasks.,"{'title': '7 Experiments', 'number': '7'}"
"Unlike Johnson (2007), who found optimal performance when α was approximately 10−4, we observed monotonic increases in performance as α dropped.","{'title': '7 Experiments', 'number': '7'}"
"The dimensionality of this MT problem is significantly larger than that of the sequence problem, though, therefore it may take a stronger push from the prior to achieve the desired result.","{'title': '7 Experiments', 'number': '7'}"
"Given an unlimited amount of time, we would tune the prior to maximize end-to-end performance, using an objective function such as BLEU.","{'title': '7 Experiments', 'number': '7'}"
Unfortunately these experiments are very slow.,"{'title': '7 Experiments', 'number': '7'}"
"Since we observed monotonic increases in alignment performance with smaller values of αC, we simply fixed the prior at a very small value (10−100) for all translation experiments.","{'title': '7 Experiments', 'number': '7'}"
We do compare VB against EM in terms of final BLEU scores in the translation experiments to ensure that this sparse prior has a significant impact on the output.,"{'title': '7 Experiments', 'number': '7'}"
"We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4.","{'title': '7 Experiments', 'number': '7'}"
We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1.,"{'title': '7 Experiments', 'number': '7'}"
These values were used to perform tic-tac-toe pruning with τb = 1 × 10−3 and τ3 = 1 × 10−6.,"{'title': '7 Experiments', 'number': '7'}"
"Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB.","{'title': '7 Experiments', 'number': '7'}"
The charts were then pruned further by applying the non-compositional constraint from the Viterbi alignment links of that model.,"{'title': '7 Experiments', 'number': '7'}"
"Finally we ran 10 iterations of phrase-based ITG over the residual charts, using EM or VB, and extracted the Viterbi alignments.","{'title': '7 Experiments', 'number': '7'}"
"For translation, we used the standard phrasal decoding approach, based on a re-implementation of the Pharaoh system (Koehn, 2004).","{'title': '7 Experiments', 'number': '7'}"
The output of the word alignment systems (GIZA++ or ITG) were fed to a standard phrase extraction procedure that extracted all phrases of length up to 7 and estimated the conditional probabilities of source given target and target given source using relative frequencies.,"{'title': '7 Experiments', 'number': '7'}"
Thus our phrasal ITG learns only the minimal non-compositional phrases; the standard phrase-extraction algorithm learns larger combinations of these minimal units.,"{'title': '7 Experiments', 'number': '7'}"
In addition the phrases were annotated with lexical weights using the IBM Model 1 tables.,"{'title': '7 Experiments', 'number': '7'}"
"The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features.","{'title': '7 Experiments', 'number': '7'}"
"Minimum Error Rate training (Och, 2003) over BLEU was used to optimize the weights for each of these models over the development test data.","{'title': '7 Experiments', 'number': '7'}"
"We used the NIST 2002 evaluation datasets for tuning and evaluation; the 10-reference development set was used for minimum error rate training, and the 4-reference test set was used for evaluation.","{'title': '7 Experiments', 'number': '7'}"
"We trained several phrasal translation systems, varying only the word alignment (or phrasal alignment) method.","{'title': '7 Experiments', 'number': '7'}"
"Table 1 compares the four systems: the GIZA++ baseline, the ITG word-based model, the ITG multiword model using EM training, and the ITG multiword model using VB training.","{'title': '7 Experiments', 'number': '7'}"
ITG-mwm-VB is our best model.,"{'title': '7 Experiments', 'number': '7'}"
We see an improvement of nearly 2 points dev set and nearly 1 point of improvement on the test set.,"{'title': '7 Experiments', 'number': '7'}"
We also observe the consistent superiority of VB over EM.,"{'title': '7 Experiments', 'number': '7'}"
"The gain is especially large on the test data set, indicating VB is less prone to overfitting.","{'title': '7 Experiments', 'number': '7'}"
We have presented an improved and more efficient method of estimating phrase pairs directly.,"{'title': '8 Conclusion', 'number': '8'}"
"By both changing the objective function to include a bias toward sparser models and improving the pruning techniques and efficiency, we achieve significant gains on test data with practical speed.","{'title': '8 Conclusion', 'number': '8'}"
"In addition, these gains were shown without resorting to external models, such as GIZA++.","{'title': '8 Conclusion', 'number': '8'}"
We have shown that VB is both practical and effective for use in MT models.,"{'title': '8 Conclusion', 'number': '8'}"
"However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession.","{'title': '8 Conclusion', 'number': '8'}"
We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases.,"{'title': '8 Conclusion', 'number': '8'}"
Both the tic-tac-toe pruning and the non-compositional constraint address this problem by reducing the space of possible phrase pairs.,"{'title': '8 Conclusion', 'number': '8'}"
"On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns.","{'title': '8 Conclusion', 'number': '8'}"
"Acknowledgments This work was done while the first author was at Microsoft Research; thanks to Xiaodong He, Mark Johnson, and Kristina Toutanova.","{'title': '8 Conclusion', 'number': '8'}"
The last author was supported by NSF IIS-0546554.,"{'title': '8 Conclusion', 'number': '8'}"
