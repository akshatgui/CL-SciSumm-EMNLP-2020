col1,col2
"This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.",{}
Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.,{}
The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.,{}
"We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.",{}
Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.,{}
A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning.,"{'title': '1 Introduction', 'number': '1'}"
"Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).","{'title': '1 Introduction', 'number': '1'}"
"For example, the training data might consist of English sentences paired with lambda-calculus meaning representations: Given pairs like this, the goal is to learn to map new, unseen, sentences to their corresponding meaning.","{'title': '1 Introduction', 'number': '1'}"
"Previous approaches to this problem have been tailored to specific natural languages, specific meaning representations, or both.","{'title': '1 Introduction', 'number': '1'}"
"Here, we develop an approach that can learn to map any natural language to a wide variety of logical representations of linguistic meaning.","{'title': '1 Introduction', 'number': '1'}"
"In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir Meaning: answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008).","{'title': '1 Introduction', 'number': '1'}"
The reason for generalizing to multiple languages is obvious.,"{'title': '1 Introduction', 'number': '1'}"
The need to learn over multiple representations arises from the fact that there is no standard representation for logical form for natural language.,"{'title': '1 Introduction', 'number': '1'}"
"Instead, existing representations are ad hoc, tailored to the application of interest.","{'title': '1 Introduction', 'number': '1'}"
"For example, the variable-free representation above was designed for building natural language interfaces to databases.","{'title': '1 Introduction', 'number': '1'}"
"Our approach works by inducing a combinatory categorial grammar (CCG) (Steedman, 1996, 2000).","{'title': '1 Introduction', 'number': '1'}"
"A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations.","{'title': '1 Introduction', 'number': '1'}"
"The learning process starts by postulating, for each sentence in the training data, a single multi-word lexical item pairing that sentence with its complete logical form.","{'title': '1 Introduction', 'number': '1'}"
"These entries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning.","{'title': '1 Introduction', 'number': '1'}"
"For the data sets we consider, the space of possible grammars is too large to explicitly enumerate.","{'title': '1 Introduction', 'number': '1'}"
"The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence.","{'title': '1 Introduction', 'number': '1'}"
"Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007).","{'title': '1 Introduction', 'number': '1'}"
We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars.,"{'title': '1 Introduction', 'number': '1'}"
The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence.,"{'title': '1 Introduction', 'number': '1'}"
"We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle & Mooney, 1996).","{'title': '1 Introduction', 'number': '1'}"
We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above.,"{'title': '1 Introduction', 'number': '1'}"
"We also compare performance to previous methods (Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detail in Section 6.","{'title': '1 Introduction', 'number': '1'}"
"Despite being the only approach that is general enough to run on all of the data sets, our algorithm achieves similar performance to the others, even outperforming them in several cases.","{'title': '1 Introduction', 'number': '1'}"
The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.,"{'title': '2 Overview of the Approach', 'number': '2'}"
"We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).","{'title': '2 Overview of the Approach', 'number': '2'}"
"The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.","{'title': '2 Overview of the Approach', 'number': '2'}"
We will present the approach in two parts.,"{'title': '2 Overview of the Approach', 'number': '2'}"
The lexical induction process (Section 4) uses a restricted form of higher order unification along with the CCG combinatory rules to propose new entries for A.,"{'title': '2 Overview of the Approach', 'number': '2'}"
The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.,"{'title': '2 Overview of the Approach', 'number': '2'}"
"Before presenting the details, we first review necessary background.","{'title': '2 Overview of the Approach', 'number': '2'}"
This section provides an introduction to the ways in which we will use lambda calculus and higher-order unification to construct meaning representations.,"{'title': '3 Background', 'number': '3'}"
"It also reviews the CCG grammar formalism and probabilistic extensions to it, including existing parsing and parameter estimation techniques.","{'title': '3 Background', 'number': '3'}"
"We assume that sentence meanings are represented as logical expressions, which we will construct from the meaning of individual words by using the operations defined in the lambda calculus.","{'title': '3 Background', 'number': '3'}"
We use a version of the typed lambda calculus (cf.,"{'title': '3 Background', 'number': '3'}"
"Carpenter (1997)), in which the basic types include e, for entities; t, for truth values; and i for numbers.","{'title': '3 Background', 'number': '3'}"
"There are also function types of the form (e, t) that are assigned to lambda expressions, such as Ax.state(x), which take entities and return truth values.","{'title': '3 Background', 'number': '3'}"
"We represent the meaning of words and phrases using lambda-calculus expressions that can contain constants, quantifiers, logical connectors, and lambda abstractions.","{'title': '3 Background', 'number': '3'}"
The advantage of using the lambda calculus lies in its generality.,"{'title': '3 Background', 'number': '3'}"
"The meanings of individual words and phrases can be arbitrary lambda expressions, while the final meaning for a sentence can take different forms.","{'title': '3 Background', 'number': '3'}"
"It can be a full lambdacalculus expression, a variable-free expression such as answer(state(borders(tex))), or any other logical expression that can be built from the primitive meanings via function application and composition.","{'title': '3 Background', 'number': '3'}"
"The higher-order unification problem (Huet, 1975) involves finding a substitution for the free variables in a pair of lambda-calculus expressions that, when applied, makes the expressions equal each other.","{'title': '3 Background', 'number': '3'}"
"This problem is notoriously complex; in the unrestricted form (Huet, 1973), it is undecidable.","{'title': '3 Background', 'number': '3'}"
"In this paper, we will guide the grammar induction process using a restricted version of higherorder unification that is tractable.","{'title': '3 Background', 'number': '3'}"
"For a given expression h, we will need to find expressions for f and g such that either h = f(g) or h = Ax.f(g(x)).","{'title': '3 Background', 'number': '3'}"
"This limited form of the unification problem will allow us to define the ways to split h into subparts that can be recombined with CCG parsing operations, which we will define in the next section, to reconstruct h. CCG (Steedman, 2000) is a linguistic formalism that tightly couples syntax and semantics, and can be used to model a wide range of language phenomena.","{'title': '3 Background', 'number': '3'}"
"For present purposes a CCG grammar includes a lexicon A with entries like the following: where each lexical item w �- X : h has words w, a syntactic category X, and a logical form h expressed as a lambda-calculus expression.","{'title': '3 Background', 'number': '3'}"
"For the first example, these are “New York,” NP, and ny.","{'title': '3 Background', 'number': '3'}"
"CCG syntactic categories may be atomic (such as S, NP) or complex (such as S\NP/NP).","{'title': '3 Background', 'number': '3'}"
CCG combines categories using a set of combinatory rules.,"{'title': '3 Background', 'number': '3'}"
"For example, the forward (>) and These rules apply to build syntactic and semantic derivations under the control of the word order information encoded in the slash directions of the lexical entries.","{'title': '3 Background', 'number': '3'}"
"For example, given the lexicon above, the sentence New York borders Vermont can be parsed to produce: where each step in the parse is labeled with the combinatory rule (− > or − <) that was used.","{'title': '3 Background', 'number': '3'}"
CCG also includes combinatory rules of forward (> B) and backward (< B) composition: These rules provide for a relaxed notion of constituency which will be useful during learning as we reason about possible refinements of the grammar.,"{'title': '3 Background', 'number': '3'}"
"We also allow vertical slashes in CCG categories, which act as wild cards.","{'title': '3 Background', 'number': '3'}"
"For example, with this extension the forward application combinator (>) could be used to combine the category S/(S�NP) with any of S\NP, S/NP, or SNP.","{'title': '3 Background', 'number': '3'}"
Figure 1 shows two parses where the composition combinators and vertical slashes are used.,"{'title': '3 Background', 'number': '3'}"
These parses closely resemble the types of analyses that will be possible under the grammars we learn in the experiments described in Section 8.,"{'title': '3 Background', 'number': '3'}"
"Given a CCG lexicon A, there will, in general, be many possible parses for each sentence.","{'title': '3 Background', 'number': '3'}"
"We select the most likely alternative using a log-linear model, which consists of a feature vector 0 and a parameter vector 0.","{'title': '3 Background', 'number': '3'}"
"The joint probability of a logical form z constructed with a parse y, given a sentence x is Section 7 defines the features used in the experiments, which include, for example, lexical features that indicate when specific lexical items in A are used in the parse y.","{'title': '3 Background', 'number': '3'}"
"For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below.","{'title': '3 Background', 'number': '3'}"
"The parsing, or inference, problem is to find the most likely logical form z given a sentence x, assuming the parameters 0 and lexicon A are known: where the probability of the logical form is found by summing over all parses that produce it: In this approach the distribution over parse trees y is modeled as a hidden variable.","{'title': '3 Background', 'number': '3'}"
The sum over parses in Eq.,"{'title': '3 Background', 'number': '3'}"
3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm.,"{'title': '3 Background', 'number': '3'}"
"To estimate the parameters themselves, we use stochastic gradient updates (LeCun et al., 1998).","{'title': '3 Background', 'number': '3'}"
"Given a set of n sentence-meaning pairs {(xi, zi) : i = 1...n}, we update the parameters 0 iteratively, for each example i, by following the local gradient of the conditional log-likelihood objective Oi = log P(zi|xi; 0, A).","{'title': '3 Background', 'number': '3'}"
"The local gradient of the individual parameter 0j associated with feature Oj and training instance (xi, zi) is given by: As with Eq.","{'title': '3 Background', 'number': '3'}"
"3, all of the expectations in Eq.","{'title': '3 Background', 'number': '3'}"
4 are calculated through the use of the inside-outside algorithm on a pruned parse chart.,"{'title': '3 Background', 'number': '3'}"
"In the experiments, each chart cell was pruned to the top 200 entries.","{'title': '3 Background', 'number': '3'}"
"Before presenting a complete learning algorithm, we first describe how to use higher-order unification to define a procedure for splitting CCG lexical entries.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
This splitting process is used to expand the lexicon during learning.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"We seed the lexical induction with a multi-word lexical item xi`S:zi for each training example (xi, zi), consisting of the entire sentence xi and its associated meaning representation zi.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, one initial lexical item might be: Although these initial, sentential lexical items can parse the training data, they will not generalize well to unseen data.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"To learn effectively, we will need to split overly specific entries of this type into pairs of new, smaller, entries that generalize better.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, one possible split of the lexical entry given in (5) would be the pair: New York borders ` S/NP : Ax.next to(ny, x), Vermont ` NP : vt where we broke the original logical expression into two new ones Ax.next to(ny, x) and vt, and paired them with syntactic categories that allow the new lexical entries to be recombined to produce the original analysis.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
The next three subsections define the set of possible splits for any given lexical item.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"The process is driven by solving a higher-order unification problem that defines all of the ways of splitting the logical expression into two parts, as described in Section 4.1.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
Section 4.2 describes how to construct syntactic categories that are consistent with the two new fragments of logical form and which will allow the new lexical items to recombine.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"Finally, Section 4.3 defines the full set of lexical entry pairs that can be created by splitting a lexical entry.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"As we will see, this splitting process is overly prolific for any single language and will yield many lexical items that do not generalize well.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, there is nothing in our original lexical entry above that provides evidence that the split should pair “Vermont” with the constant vt and not Ax.next to(ny, x).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
Section 5 describes how we estimate the parameters of a probabilistic parsing model and how this parsing model can be used to guide the selection of items to add to the lexicon.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
The set of possible splits for a logical expression h is defined as the solution to a pair of higherorder unification problems.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"We find pairs of logical expressions (f, g) such that either f(g) = h or Ax.f(g(x)) = h. Solving these problems creates new expressions f and g that can be recombined according to the CCG combinators, as defined in Section 3.2, to produce h. In the unrestricted case, there can be infinitely many solution pairs (f, g) for a given expression h. For example, when h = tex and f = Ax.tex, the expression g can be anything.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"Although it would be simple enough to forbid vacuous variables in f and g, the number of solutions would still be exponential in the size of h. For example, when h contains a conjunction, such as h = Ax.city(x) n major(x) n in(x, tex), any subset of the expressions in the conjunction can be assigned to f (or g).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"To limit the number of possible splits, we enforce the following restrictions on the possible higherorder solutions that will be used during learning: Together, these three restrictions guarantee that the number of splits is, in the worst case, an Ndegree polynomial of the number of constants in h. The constraints were designed to increase the efficiency of the splitting algorithm without impacting performance on the development data.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"We define the set of possible splits for a category X:h with syntax X and logical form h by enumerating the solution pairs (f, g) to the higher-order unification problems defined above and creating syntactic categories for the resulting expressions.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, given X :h = S\NP :Ax.in(x, tex), f = AyAx.in(x, y), and g = tex, we would produce the following two pairs of new categories: which were constructed by first choosing the syntactic category for g, in this case NP, and then enumerating the possible directions for the new slash in the category containing f. We consider each of these two steps in more detail below.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"The new syntactic category for g is determined based on its type, T(g).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, T(tex) = e and T(Ax.state(x)) = (e, t).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"Then, the function QT) takes an input type T and returns the syntactic category of T as follows: The basic types e and t are assigned syntactic categories NP and S, and all functional types are assigned categories recursively.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example Q(e, t)) = S|NP and Q(e, (e, t))) = S|NP|NP.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
This definition of CCG categories is unconventional in that it never assigns atomic categories to functional types.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, there is no distinct syntactic category N for nouns (which have semantic type he, ti).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"Instead, the more complex category S|NP is used.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"Now, we are ready to define the set of all category splits.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For a category A = X:h we can define which is a union of sets, each of which includes splits for a single CCG operator.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For example, FA(X:h) is the set of category pairs where each pair can be combined with the forward application combinator, described in Section 3.2, to reconstruct X:h. The remaining three sets are defined similarly, and are associated with the backward application and forward and backward composition operators, respectively: where the composition sets FC and BC only accept input categories with the appropriate outermost slash direction, for example FC(X/Y:h).","{'title': '4 Splitting Lexical Items', 'number': '4'}"
We can now define the lexical splits that will be used during learning.,"{'title': '4 Splitting Lexical Items', 'number': '4'}"
"For lexical entry w0:n ` A, with word sequence w0:n = hw0, ... , wni and CCG category A, define the set SL of splits to be: where we enumerate all ways of splitting the words sequence w0:n and aligning the subsequences with categories in SC(A), as defined in the last section.","{'title': '4 Splitting Lexical Items', 'number': '4'}"
The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.,"{'title': '5 Learning Algorithm', 'number': '5'}"
The space of possible lexical items supported by this splitting procedure is too large to explicitly enumerate.,"{'title': '5 Learning Algorithm', 'number': '5'}"
"Instead, we learn the parameters of a PCCG, which is used both to guide the splitting process, and also to select the best parse, given a learned lexicon.","{'title': '5 Learning Algorithm', 'number': '5'}"
"Figure 2 presents the unification-based learning algorithm, UBL.","{'title': '5 Learning Algorithm', 'number': '5'}"
This algorithm steps through the data incrementally and performs two steps for each training example.,"{'title': '5 Learning Algorithm', 'number': '5'}"
"First, new lexical items are induced for the training instance by splitting and merging nodes in the best correct parse, given the current parameters.","{'title': '5 Learning Algorithm', 'number': '5'}"
"Next, the parameters of the PCCG are updated by making a stochastic gradient update on the marginal likelihood, given the updated lexicon.","{'title': '5 Learning Algorithm', 'number': '5'}"
"Inputs and Initialization The algorithm takes as input the training set of n (sentence, logical form) pairs {(xi, zi) : i = 1...n} along with an NP list, ANP, of proper noun lexical items such as Texas ` NP:tex.","{'title': '5 Learning Algorithm', 'number': '5'}"
"The lexicon, A, is initialized with a single lexical item xi `S :zi for each of the training pairs along with the contents of the NP list.","{'title': '5 Learning Algorithm', 'number': '5'}"
"It is possible to run the algorithm without the initial NP list; we include it to allow direct comparisons with previous approaches, which also included NP lists.","{'title': '5 Learning Algorithm', 'number': '5'}"
Features and initial feature weights are described in Section 7.,"{'title': '5 Learning Algorithm', 'number': '5'}"
"Step 1: Updating the Lexicon In the lexical update step the algorithm first computes the best correct parse tree y* for the current training example and then uses y* as input to the procedure NEW-LEX, which determines which (if any) new lexical items to add to A. NEW-LEX begins by enumerating all pairs (C, wi:j), for i < j, where C is a category occurring at a node in y* and wi:j are the (two or more) words it spans.","{'title': '5 Learning Algorithm', 'number': '5'}"
"For example, in the left parse in Figure 1, there would be four pairs: one with the category C = NP\NP:Ax.border(x) and the phrase wi:j =“ye siniri vardir”, and one for each non-leaf node in the tree.","{'title': '5 Learning Algorithm', 'number': '5'}"
"For each pair (C, wi:j), NEW-LEX considers introducing a new lexical item wi:j `C, which allows for the possibility of a parse where the subtree rooted at C is replaced with this new entry.","{'title': '5 Learning Algorithm', 'number': '5'}"
"(If C is a leaf node, this item will already exist.)","{'title': '5 Learning Algorithm', 'number': '5'}"
"NEW-LEX also considers adding each pair of new lexical items that is obtained by splitting wi:j`C as described in Section 4, thereby considering many different ways of reanalyzing the node.","{'title': '5 Learning Algorithm', 'number': '5'}"
"This process creates a set of possible new lexicons, where each lexicon expands A in a different way by adding the items from either a single split or a single merge of a node in y*.","{'title': '5 Learning Algorithm', 'number': '5'}"
"For each potential new lexicon A', NEW-LEX computes the probability p(y*|xi, zi; B', A') of the original parse y* under A' and parameters B' that are the same as B but have weights for the new lexical items, as described in Section 7.","{'title': '5 Learning Algorithm', 'number': '5'}"
"It also finds the best new parse y' = arg maxy p(y|xi, zi; B', A').1 Finally, NEW-LEX selects the A' with the largest difference in log probability between y' and y*, and returns the new entries in A'.","{'title': '5 Learning Algorithm', 'number': '5'}"
"If y* is the best parse for every A', NEW-LEX returns the empty set; the lexicon will not change.","{'title': '5 Learning Algorithm', 'number': '5'}"
Step 2: Parameter Updates For each training example we update the parameters B using the stochastic gradient updates given by Eq.,"{'title': '5 Learning Algorithm', 'number': '5'}"
4.,"{'title': '5 Learning Algorithm', 'number': '5'}"
Discussion The alternation between refining the lexicon and updating the parameters drives the learning process.,"{'title': '5 Learning Algorithm', 'number': '5'}"
"The initial model assigns a conditional likelihood of one to each training example (there is a single lexical item for each sentence xi, and it contains the labeled logical form zi).","{'title': '5 Learning Algorithm', 'number': '5'}"
"Although the splitting step often decreases the probability of the data, the new entries it produces are less specific and should generalize better.","{'title': '5 Learning Algorithm', 'number': '5'}"
"Since we initially assign positive weights to the parameters for new lexical items, the overall approach prefers splitting; trees with many lexical items will initially be much more likely.","{'title': '5 Learning Algorithm', 'number': '5'}"
"However, if the learned lexical items are used in too many incorrect parses, the stochastic gradient updates will down weight them to the point where the lexical induction step can merge or re-split nodes in the trees that contain them.","{'title': '5 Learning Algorithm', 'number': '5'}"
"This allows the approach to correct the lexicon and, hopefully, improve future performance.","{'title': '5 Learning Algorithm', 'number': '5'}"
Previous work has focused on a variety of different meaning representations.,"{'title': '6 Related Work', 'number': '6'}"
Several approaches have been designed for the variable-free logical representations shown in examples throughout this paper.,"{'title': '6 Related Work', 'number': '6'}"
"For example, Kate & Mooney (2006) present a method (KRISP) that extends an existing SVM learning algorithm to recover logical representations.","{'title': '6 Related Work', 'number': '6'}"
The 1This computation can be performed efficiently by incrementally updating the parse chart used to find y*.,"{'title': '6 Related Work', 'number': '6'}"
"Inputs: Training set {(xi, zi) : i = 1... n} where each example is a sentence xi paired with a logical form zi.","{'title': '6 Related Work', 'number': '6'}"
Set of NP lexical items ANP.,"{'title': '6 Related Work', 'number': '6'}"
"Number of iterations T. Learning rate parameter α0 and cooling rate parameter c. Definitions: The function NEW-LEX(y) takes a parse y and returns a set of new lexical items found by splitting and merging categories in y, as described in Section 5.","{'title': '6 Related Work', 'number': '6'}"
"The distributions p(y|x, z; B, A) and p(y, z|x; B, A) are defined by the log-linear model, as described in Section 3.3.","{'title': '6 Related Work', 'number': '6'}"
"Initialization: WASP system (Wong & Mooney, 2006) uses statistical machine translation techniques to learn synchronous context free grammars containing both words and logic.","{'title': '6 Related Work', 'number': '6'}"
"Lu et al. (2008) (Lu08) developed a generative model that builds a single hybrid tree of words, syntax and meaning representation.","{'title': '6 Related Work', 'number': '6'}"
These algorithms are all language independent but representation specific.,"{'title': '6 Related Work', 'number': '6'}"
Other algorithms have been designed to recover lambda-calculus representations.,"{'title': '6 Related Work', 'number': '6'}"
"For example, Wong & Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation.","{'title': '6 Related Work', 'number': '6'}"
"Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates.","{'title': '6 Related Work', 'number': '6'}"
Our approach eliminates this need for manual effort.,"{'title': '6 Related Work', 'number': '6'}"
Another line of work has focused on recovering meaning representations that are not based on logic.,"{'title': '6 Related Work', 'number': '6'}"
"Examples include an early statistical method for learning to fill slot-value representations (Miller et al., 1996) and a more recent approach for recovering semantic parse trees (Ge & Mooney, 2006).","{'title': '6 Related Work', 'number': '6'}"
Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work.,"{'title': '6 Related Work', 'number': '6'}"
"Finally, there is work on using categorial grammars to solve other, related learning problems.","{'title': '6 Related Work', 'number': '6'}"
"For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition.","{'title': '6 Related Work', 'number': '6'}"
"Additionally, Bos et al. (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon.","{'title': '6 Related Work', 'number': '6'}"
Features We use two types of features in our model.,"{'title': '7 Experimental Setup', 'number': '7'}"
"First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used.","{'title': '7 Experimental Setup', 'number': '7'}"
"Second, we include semantic features that are functions of the output logical expression z.","{'title': '7 Experimental Setup', 'number': '7'}"
"Each time a predicate p in z takes an argument a with type T (a) in position i it triggers two binary indicator features: O(p,a,i) for the predicate-argument relation; and O(p,T(a),i) for the predicate argument-type relation.","{'title': '7 Experimental Setup', 'number': '7'}"
Initialization The weights for the semantic features are initialized to zero.,"{'title': '7 Experimental Setup', 'number': '7'}"
"The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1.","{'title': '7 Experimental Setup', 'number': '7'}"
"We compute translation scores for (word, constant) pairs that cooccur in examples in the training data.","{'title': '7 Experimental Setup', 'number': '7'}"
"The initial weight for each OL is set to ten times the average score over the (word, constant) pairs in L, except for the weights of seed lexical entries in ANP which are set to 10 (equivalent to the highest possible coocurrence score).","{'title': '7 Experimental Setup', 'number': '7'}"
"We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations.","{'title': '7 Experimental Setup', 'number': '7'}"
"These values were selected with cross validation on the Geo880 development set, described below.","{'title': '7 Experimental Setup', 'number': '7'}"
"Data and Evaluation We evaluate our system on the GeoQuery datasets, which contain naturallanguage queries of a geographical database paired with logical representations of each query’s meaning.","{'title': '7 Experimental Setup', 'number': '7'}"
"The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).","{'title': '7 Experimental Setup', 'number': '7'}"
"The Geo250 dataset is a subset of Geo880 containing 250 sentences that have been translated into Turkish, Spanish and Japanese as well as the original English.","{'title': '7 Experimental Setup', 'number': '7'}"
Due to the small size of this dataset we use 10-fold cross validation for evaluation.,"{'title': '7 Experimental Setup', 'number': '7'}"
"We use the same folds as Wong & Mooney (2006, 2007) and Lu et al. (2008), allowing a direct comparison.","{'title': '7 Experimental Setup', 'number': '7'}"
"The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.","{'title': '7 Experimental Setup', 'number': '7'}"
"We report results for both representations, using the standard measures of Recall (percentage of test sentences assigned correct logical forms), Precision (percentage of logical forms returned that are correct) and F1 (the harmonic mean of Precision and Recall).","{'title': '7 Experimental Setup', 'number': '7'}"
"Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strategy.","{'title': '7 Experimental Setup', 'number': '7'}"
"When the parser fails to return an analysis for a test sentence due to novel words or usage, we reparse the sentence and allow the parser to skip words, with a fixed cost.","{'title': '7 Experimental Setup', 'number': '7'}"
"Skipping words can potentially increase recall, if the ignored word is an unknown function word that does not contribute semantic content.","{'title': '7 Experimental Setup', 'number': '7'}"
"Tables 1, 2, and 3 present the results for all of the experiments.","{'title': '8 Results and Discussion', 'number': '8'}"
"In aggregate, they demonstrate that our algorithm, UBL, learns accurate models across languages and for both meaning representations.","{'title': '8 Results and Discussion', 'number': '8'}"
This is a new result; no previous system is as general.,"{'title': '8 Results and Discussion', 'number': '8'}"
"We also see the expected tradeoff between precision and recall that comes from the two-pass parsing approach, which is labeled UBL-s. With the ability to skip words, UBL-s achieves the highest recall of all reported systems for all evaluation conditions.","{'title': '8 Results and Discussion', 'number': '8'}"
"However, UBL achieves much higher precision and better overall F1 scores, which are generally comparable to the best performing systems.","{'title': '8 Results and Discussion', 'number': '8'}"
The comparison to the CCG induction techniques of ZC05 and ZC07 (Table 3) is particularly striking.,"{'title': '8 Results and Discussion', 'number': '8'}"
These approaches used language-specific templates to propose new lexical items and also required as input a set of hand-engineered lexical entries to model phenomena such as quantification and determiners.,"{'title': '8 Results and Discussion', 'number': '8'}"
"However, the use of higher-order unification allows UBL to achieve comparable performance while automatically inducing these types of entries.","{'title': '8 Results and Discussion', 'number': '8'}"
"For a more qualitative evaluation, Table 4 shows a selection of lexical items learned with high weights for the lambda-calculus meaning representations.","{'title': '8 Results and Discussion', 'number': '8'}"
"Nouns such as “state” or “estado” are consistently learned across languages with the category S|NP, which stands in for the more conventional N. The algorithm also learns language-specific constructions such as the Japanese case markers “no” and “wa”, which are treated as modifiers that do not add semantic content.","{'title': '8 Results and Discussion', 'number': '8'}"
"Language-specific word order is also encoded, using the slash directions of the CCG categories.","{'title': '8 Results and Discussion', 'number': '8'}"
"For example, “what” and “que” take their arguments to the right in the wh-initial English and Spanish.","{'title': '8 Results and Discussion', 'number': '8'}"
"However, the Turkish wh-word “nelerdir” and the Japanese question marker “nan desu ka” are sentence final, and therefore take their arguments to the left.","{'title': '8 Results and Discussion', 'number': '8'}"
Learning regularities of this type allows UBL to generalize well to unseen data.,"{'title': '8 Results and Discussion', 'number': '8'}"
There is less variation and complexity in the learned lexical items for the variable-free representation.,"{'title': '8 Results and Discussion', 'number': '8'}"
The fact that the meaning representation is deeply nested influences the form of the induced grammar.,"{'title': '8 Results and Discussion', 'number': '8'}"
"For example, recall that the sentence “what states border texas” would be paired with the meaning answer(state(borders(tex))).","{'title': '8 Results and Discussion', 'number': '8'}"
"For this representation, lexical items such as: can be used to construct the desired output.","{'title': '8 Results and Discussion', 'number': '8'}"
"In practice, UBL often learns entries with only a single slash, like those above, varying only in the direction, as required for the language.","{'title': '8 Results and Discussion', 'number': '8'}"
"Even the more complex items, such as those for quantifiers, are consistently simpler than those induced from the lambda-calculus meaning representations.","{'title': '8 Results and Discussion', 'number': '8'}"
"For example, one of the most complex entries learned in the experiments for English is the smallest ` NP\NP/(NP|NP):AfAx.smallest one(f(x)).","{'title': '8 Results and Discussion', 'number': '8'}"
There are also differences in the aggregate statistics of the learned lexicons.,"{'title': '8 Results and Discussion', 'number': '8'}"
"For example, the average length of a learned lexical item for the (lambdacalculus, variable-free) meaning representations is: (1.21,1.08) for Turkish, (1.34,1.19) for English, (1.43,1.25) for Spanish and (1.63,1.42) for Japanese.","{'title': '8 Results and Discussion', 'number': '8'}"
For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish.,"{'title': '8 Results and Discussion', 'number': '8'}"
"There are also variations in the average number of learned lexical items in the best parses during the final pass of training: 192 for Japanese, 206 for Spanish, 188 for English and 295 for Turkish.","{'title': '8 Results and Discussion', 'number': '8'}"
"As compared to the other languages, the morpologically rich Turkish requires significantly more lexical variation to explain the data.","{'title': '8 Results and Discussion', 'number': '8'}"
"Finally, there are a number of cases where the UBL algorithm could be improved in future work.","{'title': '8 Results and Discussion', 'number': '8'}"
"In cases where there are multiple allowable word orders, the UBL algorithm must learn individual entries for each possibility.","{'title': '8 Results and Discussion', 'number': '8'}"
"For example, the following two categories are often learned with high weight for the Japanese word “chiisai”: and are treated as distinct entries in the lexicon.","{'title': '8 Results and Discussion', 'number': '8'}"
"Similarly, the approach presented here does not model morphology, and must repeatedly learn the correct categories for the Turkish words “nehri,” “nehir,” “nehirler,” and “nehirlerin”, all of which correspond to the logical form Ax.river(x).","{'title': '8 Results and Discussion', 'number': '8'}"
This paper has presented a method for inducing probabilistic CCGs from sentences paired with logical forms.,"{'title': '9 Conclusions and Future Work', 'number': '9'}"
"The approach uses higher-order unification to define the space of possible grammars in a language- and representation-independent manner, paired with an algorithm that learns a probabilistic parsing model.","{'title': '9 Conclusions and Future Work', 'number': '9'}"
"We evaluated the approach on four languages with two meaning representations each, achieving high accuracy across all scenarios.","{'title': '9 Conclusions and Future Work', 'number': '9'}"
"For future work, we are interested in exploring the generality of the approach while extending it to new understanding problems.","{'title': '9 Conclusions and Future Work', 'number': '9'}"
One potential limitation is in the constraints we introduced to ensure the tractability of the higher-order unification procedure.,"{'title': '9 Conclusions and Future Work', 'number': '9'}"
"These restrictions will not allow the approach to induce lexical items that would be used with, among other things, many of the type-raised combinators commonly employed in CCG grammars.","{'title': '9 Conclusions and Future Work', 'number': '9'}"
"We are also interested in developing similar grammar induction techniques for context-dependent understanding problems, such as the one considered by Zettlemoyer & Collins (2009).","{'title': '9 Conclusions and Future Work', 'number': '9'}"
"Such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as VP ellipsis (Dalrymple et al., 1991).","{'title': '9 Conclusions and Future Work', 'number': '9'}"
We thank the reviewers for useful feedback.,"{'title': 'Acknowledgements', 'number': '10'}"
This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.,"{'title': 'Acknowledgements', 'number': '10'}"
Kwiatkowski was supported by an EPRSC studentship.,"{'title': 'Acknowledgements', 'number': '10'}"
Zettlemoyer was supported by a US NSF International Research Fellowship.,"{'title': 'Acknowledgements', 'number': '10'}"
