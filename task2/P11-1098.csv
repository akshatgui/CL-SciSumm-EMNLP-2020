col1,col2
"Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an the a template).",{}
This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance.,{}
"Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., include off, deassociated with semantic roles.",{}
"We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents.",{}
"We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.",{}
"A template defines a specific type of event (e.g., a bombing) with a set of semantic roles (or slots) for the typical entities involved in such an event (e.g., perpetrator, target, instrument).","{'title': '1 Introduction', 'number': '1'}"
"In contrast to work in relation discovery that focuses on learning atomic facts (Banko et al., 2007a; Carlson et al., 2010), templates can extract a richer representation of a particular domain.","{'title': '1 Introduction', 'number': '1'}"
"However, unlike relation discovery, most template-based IE approaches assume foreknowledge of the domain’s templates.","{'title': '1 Introduction', 'number': '1'}"
Very little work addresses how to learn the template structure itself.,"{'title': '1 Introduction', 'number': '1'}"
"Our goal in this paper is to perform the standard template filling task, but to first automatically induce the templates from an unlabeled corpus.","{'title': '1 Introduction', 'number': '1'}"
"There are many ways to represent events, ranging from role-based representations such as frames (Baker et al., 1998) to sequential events in scripts (Schank and Abelson, 1977) and narrative schemas (Chambers and Jurafsky, 2009; Kasch and Oates, 2010).","{'title': '1 Introduction', 'number': '1'}"
"Our approach learns narrative-like knowledge in the form of IE templates; we learn sets of related events and semantic roles, as shown in this sample output from our system: {detonate, blow up, plant, explode, defuse, destroy} Perpetrator: Person who detonates, plants, blows up Instrument: Object that is planted, detonated, defused Target: Object that is destroyed, is blown up A semantic role, such as target, is a cluster of syntactic functions of the template’s event words (e.g., the objects of detonate and explode).","{'title': '1 Introduction', 'number': '1'}"
Our goal is to characterize a domain by learning this template structure completely automatically.,"{'title': '1 Introduction', 'number': '1'}"
We learn templates by first clustering event words based on their proximity in a training corpus.,"{'title': '1 Introduction', 'number': '1'}"
We then use a novel approach to role induction that clusters the syntactic functions of these events based on selectional preferences and coreferring arguments.,"{'title': '1 Introduction', 'number': '1'}"
"The induced roles are template-specific (e.g., perpetrator), not universal (e.g., agent or patient) or verb-specific.","{'title': '1 Introduction', 'number': '1'}"
"After learning a domain’s template schemas, we perform the standard IE task of role filling from individual documents, for example: Perpetrator: guerrillas Instrument: dynamite Target: embassy This extraction stage identifies entities using the ing the same exact event (e.g.","{'title': '1 Introduction', 'number': '1'}"
"Hurricane Ivan), and learned syntactic functions of our roles.","{'title': '1 Introduction', 'number': '1'}"
We evalu- observing repeated word patterns across documents ate on the MUC-4 terrorism corpus with results ap- connecting the same proper nouns.,"{'title': '1 Introduction', 'number': '1'}"
"Learned patterns proaching those of supervised systems. represent binary relations, and they show how to The core of this paper focuses on how to char- construct tables of extracted entities for these relaacterize a domain-specific corpus by learning rich tions.","{'title': '1 Introduction', 'number': '1'}"
Our approach draws on this idea of using untemplate structure.,"{'title': '1 Introduction', 'number': '1'}"
"We describe how to first expand labeled documents to discover relations in text, and the small corpus’ size, how to cluster its events, and of defining semantic roles by sets of entities.","{'title': '1 Introduction', 'number': '1'}"
Howfinally how to induce semantic roles.,"{'title': '1 Introduction', 'number': '1'}"
"Section 5 then ever, the limitations to their approach are that (1) describes the extraction algorithm, followed by eval- redundant documents about specific events are reuations against previous work in section 6 and 7. quired, (2) relations are binary, and (3) only slots 2 Previous Work with named entities are learned.","{'title': '1 Introduction', 'number': '1'}"
"We will extend Many template extraction algorithms require full their work by showing how to learn without these knowledge of the templates and labeled corpora, assumptions, obviating the need for redundant docsuch as in rule-based systems (Chinchor et al., 1993; uments, and learning templates with any type and Rau et al., 1992) and modern supervised classi- any number of slots. fiers (Freitag, 1998; Chieu et al., 2003; Bunescu Large-scale learning of scripts and narrative and Mooney, 2004; Patwardhan and Riloff, 2009). schemas also captures template-like knowledge Classifiers rely on the labeled examples’ surround- from unlabeled text (Chambers and Jurafsky, 2008; ing context for features such as nearby tokens, doc- Kasch and Oates, 2010).","{'title': '1 Introduction', 'number': '1'}"
"Scripts are sets of reument position, syntax, named entities, semantic lated event words and semantic roles learned by classes, and discourse relations (Maslennikov and linking syntactic functions with coreferring arguChua, 2007).","{'title': '1 Introduction', 'number': '1'}"
Ji and Grishman (2008) also supple- ments.,"{'title': '1 Introduction', 'number': '1'}"
"While they learn interesting event structure, mented labeled with unlabeled data. the structures are limited to frequent topics in a large Weakly supervised approaches remove some of corpus.","{'title': '1 Introduction', 'number': '1'}"
"We borrow ideas from this work as well, but the need for fully labeled data.","{'title': '1 Introduction', 'number': '1'}"
Most still require the our goal is to instead characterize a specific domain templates and their slots.,"{'title': '1 Introduction', 'number': '1'}"
One common approach is with limited data.,"{'title': '1 Introduction', 'number': '1'}"
"Further, we are the first to apply to begin with unlabeled, but clustered event-specific this knowledge to the IE task of filling in template documents, and extract common word patterns as mentions in documents. extractors (Riloff and Schmelzenbach, 1998; Sudo In summary, our work extends previous work on et al., 2003; Riloff et al., 2005; Patwardhan and unsupervised IE in a number of ways.","{'title': '1 Introduction', 'number': '1'}"
"We are the Riloff, 2007).","{'title': '1 Introduction', 'number': '1'}"
"Filatova et al. (2006) integrate named first to learn MUC-4 templates, and we are the first entities into pattern learning (PERSON won) to ap- to extract entities without knowing how many temproximate unknown semantic roles.","{'title': '1 Introduction', 'number': '1'}"
"Bootstrapping plates exist, without examples of slot fillers, and with seed examples of known slot fillers has been without event-clustered documents. shown to be effective (Surdeanu et al., 2006; Yan- 3 The Domain and its Templates garber et al., 2000).","{'title': '1 Introduction', 'number': '1'}"
"In contrast, this paper removes Our goal is to learn the general event structure of these data assumptions, learning instead from a cor- a domain, and then extract the instances of each pus of unknown events and unclustered documents, learned event.","{'title': '1 Introduction', 'number': '1'}"
"In order to measure performance without seed examples. in both tasks (learning structure and extracting inShinyama and Sekine (2006) describe an ap- stances), we use the terrorism corpus of MUC-4 proach to template learning without labeled data.","{'title': '1 Introduction', 'number': '1'}"
"(Sundheim, 1991) as our target domain.","{'title': '1 Introduction', 'number': '1'}"
"This corThey present unrestricted relation discovery as a pus was chosen because it is annotated with temmeans of discovering relations in unlabeled docu- plates that describe all of the entities involved in ments, and extract their fillers.","{'title': '1 Introduction', 'number': '1'}"
Central to the al- each event.,"{'title': '1 Introduction', 'number': '1'}"
An example snippet from a bombing gorithm is collecting multiple documents describ- document is given here: 977 The terrorists used explosives against the town hall.,"{'title': '1 Introduction', 'number': '1'}"
"El Comercio reported that alleged Shining Path members also attacked public facilities in huarpacha, Ambo, tomayquichua, and kichki.","{'title': '1 Introduction', 'number': '1'}"
Municipal official Sergio Horna was seriously wounded in an explosion in Ambo.,"{'title': '1 Introduction', 'number': '1'}"
The entities from this document fill the following slots in a MUC-4 bombing template.,"{'title': '1 Introduction', 'number': '1'}"
"Target: public facilities Instrument: explosives We focus on these four string-based slots1 from the MUC-4 corpus, as is standard in this task.","{'title': '1 Introduction', 'number': '1'}"
"The corpus consists of 1300 documents, 733 of which are labeled with at least one template.","{'title': '1 Introduction', 'number': '1'}"
"There are six types of templates, but only four are modestly frequent: bombing (208 docs), kidnap (83 docs), attack (479 docs), and arson (40 docs).","{'title': '1 Introduction', 'number': '1'}"
567 documents do not have any templates.,"{'title': '1 Introduction', 'number': '1'}"
Our learning algorithm does not know which documents contain (or do not contain) which templates.,"{'title': '1 Introduction', 'number': '1'}"
"After learning event words that represent templates, we induce their slots, not knowing a priori how many there are, and then fill them in by extracting entities as in the standard task.","{'title': '1 Introduction', 'number': '1'}"
"In our example above, the three bold verbs (use, attack, wound) indicate the Bombing template, and their syntactic arguments fill its slots.","{'title': '1 Introduction', 'number': '1'}"
"Our goal is to learn templates that characterize a domain as described in unclustered, unlabeled documents.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"This presents a two-fold problem to the learner: it does not know how many events exist, and it does not know which documents describe which event (some may describe multiple events).","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"We approach this problem with a three step process: (1) cluster the domain’s event patterns to approximate the template topics, (2) build a new corpus specific to each cluster by retrieving documents from a larger unrelated corpus, (3) induce each template’s slots using its new (larger) corpus of documents.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
We cluster event patterns to create templates.,"{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"An event pattern is either (1) a verb, (2) a noun in WordNet under the Event synset, or (3) a verb and the head word of its syntactic object.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"Examples of each include (1) ‘explode’, (2) ‘explosion’, and (3) ‘explode:bomb’.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"We also tag the corpus with an NER system and allow patterns to include named entity types, e.g., ‘kidnap:PERSON’.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
These patterns are crucially needed later to learn a template’s slots.,"{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"However, we first need an algorithm to cluster these patterns to learn the domain’s core events.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
"We consider two unsupervised algorithms: Latent Dirichlet Allocation (LDA) (Blei et al., 2003), and agglomerative clustering based on word distance.","{'title': '4 Learning Templates from Raw Text', 'number': '2'}"
LDA is a probabilistic model that treats documents as mixtures of topics.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"It learns topics as discrete distributions (multinomials) over the event patterns, and thus meets our needs as it clusters patterns based on co-occurrence in documents.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The algorithm requires the number of topics to be known ahead of time, but in practice this number is set relatively high and the resulting topics are still useful.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Our best performing LDA model used 200 topics.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We had mixed success with LDA though, and ultimately found our next approach performed slightly better on the document classification evaluation.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Agglomerative clustering does not require foreknowledge of the templates, but its success relies on how event pattern similarity is determined.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Ideally, we want to learn that detonate and destroy belong in the same cluster representing a bombing.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Vector-based approaches are often adopted to represent words as feature vectors and compute their distance with cosine similarity.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Unfortunately, these approaches typically learn clusters of synonymous words that can miss detonate and destroy.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Our goal is to instead capture world knowledge of cooccuring events.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We thus adopt an assumption that closeness in the world is reflected by closeness in a text’s discourse.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We hypothesize that two patterns are related if they occur near each other in a document more often than chance.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Let g(wi, wj) be the distance between two events (1 if in the same sentence, 2 in neighboring, etc).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Let %dist(wi, wj) be the distance-weighted frequency of where d is a document in the set of all documents D. The base 4 logarithm discounts neighboring sentences by 0.5 and within the same sentence scores 1.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Using this definition of distance, pointwise mutual information measures our similarity of two events: We run agglomerative clustering with pmi over all event patterns.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Merging decisions use the average link score between all new links across two clusters.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"As with all clustering algorithms, a stopping criterion is needed.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We continue merging clusters until any single cluster grows beyond m patterns.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We briefly inspected the clustering process and chose m = 40 to prevent learned scenarios from intuitively growing too large and ambiguous.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Post-evaluation analysis shows that this value has wide flexibility.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For example, the Kidnap and Arson clusters are unchanged in 30 < m < 80, and Bombing unchanged in 30 < m < 50.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Figure 1 shows 3 clusters (of 77 learned) that characterize the main template types.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Learning a domain often suffers from a lack of training data.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The previous section clustered events from the MUC-4 corpus, but its 1300 documents do not provide enough examples of verbs and argument counts to further learn the semantic roles in each cluster.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Our solution is to assemble a larger IRcorpus of documents for each cluster.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For example, MUC-4 labels 83 documents with Kidnap, but our learned cluster (kidnap, abduct, release, ...) retrieved 3954 documents from a general corpus.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We use the Associated Press and New York Times sections of the Gigaword Corpus (Graff, 2002) as our general corpus.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
These sections include approximately 3.5 million news articles spanning 12 years.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Our retrieval algorithm retrieves documents that score highly with a cluster’s tokens.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The document score is defined by two common metrics: word match, and word coverage.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
A document’s match score is defined as the average number of times the words in cluster c appear in document d: We define word coverage as the number of seen cluster words.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Coverage penalizes documents that score highly by repeating a single cluster word a lot.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We only score a document if its coverage, cvg(d, c), is at least 3 words (or less for tiny clusters): ir(d, c) = r avgm(d, c) if cvg(d, c) > min(3, jcj/4) l 0 otherwise A document d is retrieved for a cluster c if ir(d, c) > 0.4.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Finally, we emphasize precision by pruning away 50% of a cluster’s retrieved documents that are farthest in distance from the mean document of the retrieved set.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Distance is the cosine similarity between bag-of-words vector representations.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
The confidence value of 0.4 was chosen from a manual inspection among a single cluster’s retrieved documents.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Pruning 50% was arbitrarily chosen to improve precision, and we did not experiment with other quantities.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
A search for optimum parameter values may lead to better results.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Having successfully clustered event words and retrieved an IR-corpus for each cluster, we now address the problem of inducing semantic roles.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Our learned roles will then extract entities in the next section and we will evaluate their per-role accuracy.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Most work on unsupervised role induction focuses on learning verb-specific roles, starting with seed examples (Swier and Stevenson, 2004; He and Gildea, 2006) and/or knowing the number of roles (Grenager and Manning, 2006; Lang and Lapata, 2010).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Our previous work (Chambers and Jurafsky, 2009) learned situation-specific roles over narrative schemas, similar to frame roles in FrameNet (Baker et al., 1998).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Schemas link the syntactic relations of verbs by clustering them based on observing coreferring arguments in those positions.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
This paper extends this intuition by introducing a new vectorbased approach to coreference similarity.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We learn the roles of cluster C by clustering the syntactic relations RC of its words.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Consider the following example: where verb:s is the verb’s subject, :o the object, and p in a preposition.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We ideally want to cluster RC as: We want to cluster all subjects, objects, and prepositions.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Passive voice is normalized to active2.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We adopt two views of relation similarity: coreferring arguments and selectional preferences.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Chambers and Jurafsky (2008) observed that coreferring arguments suggest a semantic relation between two predicates.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"In the sentence, he ran and then he fell, the subjects of run and fall corefer, and so they likely belong to the same scenario-specific semantic role.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We applied this idea to a new vector similarity framework.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We represent a relation as a vector of all relations with which their arguments coreferred.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For instance, arguments of the relation go off:s were seen coreferring with mentions in plant:o, set off:o and injure:s. We represent go off:s as a vector of these relation counts, calling this its coref vector representation.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Selectional preferences (SPs) are also useful in measuring similarity (Erk and Pado, 2008).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
A relation can be represented as a vector of its observed arguments during training.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The SPs for go off:s in our data include {bomb, device, charge, explosion}.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We measure similarity using cosine similarity between the vectors in both approaches.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"However, 2We use the Stanford Parser at nlp.stanford.edu/software coreference and SPs measure different types of similarity.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Coreference is a looser narrative similarity (bombings cause injuries), while SPs capture synonymy (plant and place have similar arguments).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We observed that many narrative relations are not synonymous, and vice versa.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We thus take the maximum of either cosine score as our final similarity metric between two relations.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We then back off to the average of the two cosine scores if the max is not confident (less than 0.7); the average penalizes the pair.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We chose the value of 0.7 from a grid search to optimize extraction results on the training set.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We use agglomerative clustering with the above pairwise similarity metric.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Cluster similarity is the average link score over all new links crossing two clusters.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We include the following sparsity penalty r(ca, cb) if there are too few links between clusters ca and cb.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
This penalizes clusters from merging when they share only a few high scoring edges.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Clustering stops when the merged cluster scores drop below a threshold optimized to extraction performance on the training data.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We also begin with two assumptions about syntactic functions and semantic roles.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
The first assumes that the subject and object of a verb carry different semantic roles.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For instance, the subject of sell fills a different role (Seller) than the object (Good).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
The second assumption is that each semantic role has a high-level entity type.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For instance, the subject of sell is a Person or Organization, and the object is a Physical Object.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We implement the first assumption as a constraint in the clustering algorithm, preventing two clusters from merging if their union contains the same verb’s subject and object.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We implement the second assumption by automatically labeling each syntactic function with a role type based on its observed arguments.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The role types are broad general classes: Person/Org, Physical Object, or Other.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"A syntactic function is labeled as a class if 20% of its arguments appear under the corresponding WordNet synset3, or if the NER system labels them as such.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Once labeled by type, we separately cluster the syntactic functions for each role type.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"For instance, Person functions are clustered separate from Physical Object functions.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Figure 2 shows some of the resulting roles.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Finally, since agglomerative clustering makes hard decisions, related events to a template may have been excluded in the initial event clustering stage.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"To address this problem, we identify the 200 nearby events to each event cluster.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
These are simply the top scoring event patterns with the cluster’s original events.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We add their syntactic functions to their best matching roles.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
This expands the coverage of each learned role.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
Varying the 200 amount does not lead to wide variation in extraction performance.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Once induced, the roles are evaluated by their entity extraction performance in Section 5.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We now compare our learned templates to those hand-created by human annotators for the MUC-4 terrorism corpus.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The corpus contains 6 template types, but two of them occur in only 4 and 14 of the 1300 training documents.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We thus only evaluate the 4 main templates (bombing, kidnapping, attack, and arson).","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
The gold slots are shown in figure 3.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We evaluate the four learned templates that score highest in the document classification evaluation (to be described in section 5.1), aligned with their MUC-4 types.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Figure 2 shows three of our four templates, and two brand new ones that our algorithm learned.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"Of the four templates, we learned 12 of the 13 semantic roles as created for MUC.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"In addition, we learned a new role not in MUC for bombings, kidnappings, and arson: the Police or Authorities role.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"The annotators chose not to include this in their labeling, but this knowledge is clearly relevant when understanding such events, so we consider it correct.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"There is one additional Bombing and one Arson role that does not align with MUC-4, marked incorrect.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We thus report 92% slot recall, and precision as 14 of 16 (88%) learned slots.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
"We only measure agreement with the MUC template schemas, but our system learns other events as well.","{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We show two such examples in figure 2: the Weapons Smuggling and Election Templates.,"{'title': '4.1.1 LDA for Unknown Data', 'number': '3'}"
We now present how to apply our learned templates to information extraction.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"This section will describe how to extract slot fillers using our templates, but without knowing which templates are correct.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"We could simply use a standard IE approach, for example, creating seed words for our new learned templates.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"But instead, we propose a new method that obviates the need for even a limited human labeling of seed sets.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"We consider each learned semantic role as a potential slot, and we extract slot fillers using the syntactic functions that were previously learned.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Thus, the learned syntactic patterns (e.g., the subject of release) serve the dual purpose of both inducing the template slots, and extracting appropriate slot fillers from text.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"A document is labeled for a template if two different conditions are met: (1) it contains at least one trigger phrase, and (2) its average per-token conditional probability meets a strict threshold.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
Both conditions require a definition of the conditional probability of a template given a token.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
The conditional is defined as the token’s importance relative to its uniqueness across all templates.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"This is not the usual conditional probability definition as IR-corpora are different sizes. where PIR,(w) is the probability of pattern w in the IR-corpus of template t. where Ct(w) is the number of times word w appears in the IR-corpus of template t. A template’s trigger words are defined as words satisfying P(tIw) > 0.2.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
Trigger phrases are thus template-specific patterns that are highly indicative of that template.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"After identifying triggers, we use the above definition to score a document with a template.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"A document is labeled with a template if it contains at least one trigger, and its average word probability is greater than a parameter optimized on the training set.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
A document can be (and often is) labeled with multiple templates.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Finally, we label the sentences that contain triggers and use them for extraction in section 5.2.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"The MUC-4 corpus links templates to documents, allowing us to evaluate our document labels.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"We treat each link as a gold label (kidnap, bomb, or attack) for that document, and documents can have multiple labels.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Our learned clusters naturally do not have MUC labels, so we report results on the four clusters that score highest with each label.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
Figure 4 shows the document classification scores.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
The bombing template performs best with an F1 score of .72.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Arson occurs very few times, and Attack is lower because it is essentially an agglomeration of diverse events (discussed later).","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Once documents are labeled with templates, we next extract entities into the template slots.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
Extraction occurs in the trigger sentences from the previous section.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
The extraction process is two-fold: Take the following MUC-4 sentence as an example: The two bombs were planted with the exclusive purpose of intimidating the owners of...,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"The verb plant is in our learned bombing cluster, so step (1) will extract its passive subject bombs and map it to the correct instrument role (see figure 2).","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"The human target, owners, is missed because intimidate was not learned.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"However, if owner is in the selectional preferences of the learned ‘human target’ role, step (2) correctly extracts it into that role.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"These are two different, but complementary, views of semantic roles.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
The first is that a role is defined by the set of syntactic relations that describe it.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Thus, we find all role relations and save their arguments (pattern extraction).","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
The second view is that a role is defined by the arguments that fill it.,"{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Thus, we extract all arguments that filled a role in training, regardless of their current syntactic environment.","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
"Finally, we filter extractions whose WordNet or named entity label does not match the learned slot’s type (e.g., a Location does not match a Person).","{'title': '5 Information Extraction: Slot Filling', 'number': '4'}"
We trained on the 1300 documents in the MUC-4 corpus and tested on the 200 document TST3 and TST4 test set.,"{'title': '6 Standard Evaluation', 'number': '5'}"
"We evaluate the four string-based slots: perpetrator, physical target, human target, and instrument.","{'title': '6 Standard Evaluation', 'number': '5'}"
We merge MUC’s two perpetrator slots (individuals and orgs) into one gold Perpetrator slot.,"{'title': '6 Standard Evaluation', 'number': '5'}"
"As in Patwardhan and Riloff (2007; 2009), we ignore missed optional slots in computing recall.","{'title': '6 Standard Evaluation', 'number': '5'}"
"We induced clusters in training, performed IR, and induced the slots.","{'title': '6 Standard Evaluation', 'number': '5'}"
We then extracted entities from the test documents as described in section 5.2.,"{'title': '6 Standard Evaluation', 'number': '5'}"
"The standard evaluation for this corpus is to report the F1 score for slot type accuracy, ignoring the template type.","{'title': '6 Standard Evaluation', 'number': '5'}"
"For instance, a perpetrator of a bombing and a perpetrator of an attack are treated the same.","{'title': '6 Standard Evaluation', 'number': '5'}"
"This allows supervised classifiers to train on all perpetrators at once, rather than template-specific learners.","{'title': '6 Standard Evaluation', 'number': '5'}"
"Although not ideal for our learning goals, we report it for comparison against previous work.","{'title': '6 Standard Evaluation', 'number': '5'}"
"Several supervised approaches have presented results on MUC-4, but unfortunately we cannot compare against them.","{'title': '6 Standard Evaluation', 'number': '5'}"
"Maslennikov and Chua (2006; 2007) evaluated a random subset of test (they report .60 and .63 F1), and Xiao et al. (2004) did not evaluate all slot types (they report .57 F1).","{'title': '6 Standard Evaluation', 'number': '5'}"
Figure 5 thus shows our results with previous work that is comparable: the fully supervised and weakly supervised approaches of Patwardhan and Riloff (2009; 2007).,"{'title': '6 Standard Evaluation', 'number': '5'}"
"We give two numbers for our system: mapping one learned template to Attack, and mapping five.","{'title': '6 Standard Evaluation', 'number': '5'}"
Our learned templates for Attack have a different granularity than MUC-4.,"{'title': '6 Standard Evaluation', 'number': '5'}"
"Rather than one broad Attack type, we learn several: Shooting, Murder, Coup, General Injury, and Pipeline Attack.","{'title': '6 Standard Evaluation', 'number': '5'}"
"We see these subtypes as strengths of our algorithm, but it misses the MUC-4 granularity of Attack.","{'title': '6 Standard Evaluation', 'number': '5'}"
"We thus show results when we apply the best five learned templates to Attack, rather than just one.","{'title': '6 Standard Evaluation', 'number': '5'}"
The final F1 with these Attack subtypes is .40.,"{'title': '6 Standard Evaluation', 'number': '5'}"
Our precision is as good as (and our F1 score near) two algorithms that require knowledge of the templates and/or labeled data.,"{'title': '6 Standard Evaluation', 'number': '5'}"
Our algorithm instead learned this knowledge without such supervision.,"{'title': '6 Standard Evaluation', 'number': '5'}"
"In order to more precisely evaluate each learned template, we also evaluated per-template performance.","{'title': '7 Specific Evaluation', 'number': '6'}"
"Instead of merging all slots across all template types, we score the slots within each template type.","{'title': '7 Specific Evaluation', 'number': '6'}"
"This is a stricter evaluation than Section 6; for example, bombing victims assigned to attacks were previously deemed correct4.","{'title': '7 Specific Evaluation', 'number': '6'}"
Figure 6 gives our results.,"{'title': '7 Specific Evaluation', 'number': '6'}"
"Three of the four templates score at or above .42 F1, showing that our lower score from the previous section is mainly due to the Attack template.","{'title': '7 Specific Evaluation', 'number': '6'}"
Arson also unexpectedly scored well.,"{'title': '7 Specific Evaluation', 'number': '6'}"
"It only occurs in 40 documents overall, suggesting our algorithm works with little evidence.","{'title': '7 Specific Evaluation', 'number': '6'}"
"Per-template performace is good, and our .40 overall score from the previous section illustrates that we perform quite well in comparison to the .44.53 range of weakly and fully supervised results.","{'title': '7 Specific Evaluation', 'number': '6'}"
"These evaluations use the standard TST3 and TST4 test sets, including the documents that are not labeled with any templates.","{'title': '7 Specific Evaluation', 'number': '6'}"
74 of the 200 test documents are unlabeled.,"{'title': '7 Specific Evaluation', 'number': '6'}"
"In order to determine where the system’s false positives originate, we also measure performance only on the 126 test documents that have at least one template.","{'title': '7 Specific Evaluation', 'number': '6'}"
Figure 7 presents the results on this subset.,"{'title': '7 Specific Evaluation', 'number': '6'}"
"Kidnap improves most significantly in F1 score (7 F1 points absolute), but the others only change slightly.","{'title': '7 Specific Evaluation', 'number': '6'}"
"Most of the false positives in the system thus do not originate from the unlabeled documents (the 74 unlabeled), but rather from extracting incorrect entities from correctly identified documents (the 126 labeled).","{'title': '7 Specific Evaluation', 'number': '6'}"
Template-based IE systems typically assume knowledge of the domain and its templates.,"{'title': '8 Discussion', 'number': '7'}"
"We began by showing that domain knowledge isn’t necessarily required; we learned the MUC-4 template structure with surprising accuracy, learning new semantic roles and several new template structures.","{'title': '8 Discussion', 'number': '7'}"
We are the first to our knowledge to automatically induce MUC-4 templates.,"{'title': '8 Discussion', 'number': '7'}"
"It is possible to take these learned slots and use a previous approach to IE (such as seed-based bootstrapping), but we presented an algorithm that instead uses our learned syntactic patterns.","{'title': '8 Discussion', 'number': '7'}"
"We achieved results with comparable precision, and an F1 score of .40 that approaches prior algorithms that rely on hand-crafted knowledge.","{'title': '8 Discussion', 'number': '7'}"
"The extraction results are encouraging, but the template induction itself is a central contribution of this work.","{'title': '8 Discussion', 'number': '7'}"
Knowledge induction plays an important role in moving to new domains and assisting users who may not know what a corpus contains.,"{'title': '8 Discussion', 'number': '7'}"
"Recent work in Open IE learns atomic relations (Banko et al., 2007b), but little work focuses on structured scenarios.","{'title': '8 Discussion', 'number': '7'}"
We learned more templates than just the main MUC-4 templates.,"{'title': '8 Discussion', 'number': '7'}"
"A user who seeks to know what information is in a body of text would instantly recognize these as key templates, and could then extract the central entities.","{'title': '8 Discussion', 'number': '7'}"
We hope to address in the future how the algorithm’s unsupervised nature hurts recall.,"{'title': '8 Discussion', 'number': '7'}"
"Without labeled or seed examples, it does not learn as many patterns or robust classifiers as supervised approaches.","{'title': '8 Discussion', 'number': '7'}"
We will investigate new text sources and algorithms to try and capture more knowledge.,"{'title': '8 Discussion', 'number': '7'}"
"The final experiment in figure 7 shows that perhaps new work should first focus on pattern learning and entity extraction, rather than document identification.","{'title': '8 Discussion', 'number': '7'}"
"Finally, while our pipelined approach (template induction with an IR stage followed by entity extraction) has the advantages of flexibility in development and efficiency, it does involve a number of parameters.","{'title': '8 Discussion', 'number': '7'}"
"We believe the IR parameters are quite robust, and did not heavily focus on improving this stage, but the two clustering steps during template induction require parameters to control stopping conditions and word filtering.","{'title': '8 Discussion', 'number': '7'}"
"While all learning algorithms require parameters, we think it is important for future work to focus on removing some of these to help the algorithm be even more robust to new domains and genres.","{'title': '8 Discussion', 'number': '7'}"
"This work was supported by the National Science Foundation IIS-0811974, and this material is also based upon work supported by the Air Force Research Laboratory (AFRL) under prime contract no.","{'title': 'Acknowledgments', 'number': '8'}"
FA8750-09-C-0181.,"{'title': 'Acknowledgments', 'number': '8'}"
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).","{'title': 'Acknowledgments', 'number': '8'}"
Thanks to the Stanford NLP Group and reviewers for helpful suggestions.,"{'title': 'Acknowledgments', 'number': '8'}"
