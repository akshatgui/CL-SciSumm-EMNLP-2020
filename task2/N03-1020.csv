col1,col2
"Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.",{}
"The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.",{}
Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years.,"{'title': '1 Introduction', 'number': '1'}"
"A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.","{'title': '1 Introduction', 'number': '1'}"
2002).,"{'title': '1 Introduction', 'number': '1'}"
"Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods.","{'title': '1 Introduction', 'number': '1'}"
The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization.,"{'title': '1 Introduction', 'number': '1'}"
"However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961).","{'title': '1 Introduction', 'number': '1'}"
"For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.","{'title': '1 Introduction', 'number': '1'}"
"To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.","{'title': '1 Introduction', 'number': '1'}"
"Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996).","{'title': '1 Introduction', 'number': '1'}"
Section 2 gives an overview of the evaluation procedure used in DUC.,"{'title': '1 Introduction', 'number': '1'}"
Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries.,"{'title': '1 Introduction', 'number': '1'}"
Section 4 compares n-gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction.,"{'title': '1 Introduction', 'number': '1'}"
Section 5 concludes this paper and discusses future directions.,"{'title': '1 Introduction', 'number': '1'}"
"The 2002 Document Understanding Conference2 included the follow two main tasks: given a set of documents about a single subject, participants were required to create 4 generic summaries of the entire set, containing 50, 100, 200, and 400 words respectively.","{'title': '2 Document Understanding Conference', 'number': '2'}"
The document sets were of four types: a single natural disaster event; a single event; multiple instances of a type of event; and information about an individual.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"The training set comprised 30 sets of approximately 10 documents, each provided with their 50, 100, 200, and 400-word human written summaries.","{'title': '2 Document Understanding Conference', 'number': '2'}"
The test set comprised 30 unseen sets.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
A total of 11 systems participated in the singledocument summarization task and 12 systems participated in the multi-document task.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"For each document or document set, one human summary was created as the `ideal' model summary at each specified length.","{'title': '2 Document Understanding Conference', 'number': '2'}"
Two other human summaries were also created at each length.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"In addition, baseline summaries were created automatically for each length as reference points.","{'title': '2 Document Understanding Conference', 'number': '2'}"
"For the multi-document summarization task, one baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection.","{'title': '2 Document Understanding Conference', 'number': '2'}"
"A second baseline, coverage baseline, took the first sentence in the first document, the first sentence in the second document and so on until it had a summary of 50, 100, 200, or 400 words.","{'title': '2 Document Understanding Conference', 'number': '2'}"
Only one baseline (baseline1) was created for the single document summarization task.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.","{'title': '2 Document Understanding Conference', 'number': '2'}"
They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).","{'title': '2 Document Understanding Conference', 'number': '2'}"
"As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows.","{'title': '2 Document Understanding Conference', 'number': '2'}"
SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (green/dark gray highlight in the model summary window), and specify that the marked system units express all, most, some, or hardly any of the content of the current model unit.","{'title': '2 Document Understanding Conference', 'number': '2'}"
"To measure quality, assessors rate grammaticality3, cohesion4, and coherence5 at five different levels: all, most, some, hardly any, or none6.","{'title': '2 Document Understanding Conference', 'number': '2'}"
"For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right).","{'title': '2 Document Understanding Conference', 'number': '2'}"
Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998).,"{'title': '2 Document Understanding Conference', 'number': '2'}"
"However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by Donaway et al. (2000).","{'title': '2 Document Understanding Conference', 'number': '2'}"
"Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none.","{'title': '2 Document Understanding Conference', 'number': '2'}"
"If we ignore E (set it to 1), we obtain simple sentence recall score.","{'title': '2 Document Understanding Conference', 'number': '2'}"
We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections.,"{'title': '2 Document Understanding Conference', 'number': '2'}"
To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001).,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
The NIST (NIST 2002) scoring metric is based on BLEU.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"BLEU, correlating highly with human assessments.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human summary, the better it is.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
The question is: &quot;Can we apply BLEU directly without any modifications to evaluate summaries as well?&quot;.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
We first ran IBM's BLEU evaluation script unmodified over the DUC 2001 model and peer summary set.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
These numbers indicate that they positively correlate at α = 0.018.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"According to Papineni et al. (2001), BLEU is essentially a precision metric.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"N-gram precision in BLEU is computed as follows: Where Countclip(n-gram) is the maximum number of ngrams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in the candidate translation.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: Where |c |is the length of the candidate translation and |r |is the length of the reference translation.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"The BLEU formula is then written as follows: N is set at 4 and wn, the weighting factor, is set at 1/N.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"For summaries by analogy, we can express equation (1) in terms of n-gram matches following equation (2): Where Countmatch(n-gram) is the maximum number of n-grams co-occurring in a peer summary and a model unit and Count(n-gram) is the number of n-grams in the model unit.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Notice that the average n-gram coverage score, Cn, as shown in equation 5 is a recall metric 8 The number of instances is 14 (11 systems, 2 humans, and 1 baseline) for the single document task and is 16 (12 systems, 2 humans, and 2 baselines) for the multi-document task. ings versus human ranking for the multidocument task data from DUC 2001.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"The same system is at each vertical line with ranking given by different Ngram(1,4)n scores.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
The straight line (AvgC) is the human ranking and n marks summaries of different sizes.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Ngram(1,4)all combines results from all sizes. instead of a precision one as pn.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
Since the denominator of equation 5 is the total sum of the number of n-grams occurring at the model summary side instead of the peer side and only one model summary is used for each evaluation; while there could be multiple references used in BLEU and Count�lip(n-gram) could come from matching different reference translations.,"{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Furthermore, instead of a brevity penalty that punishes overly short translations, a brevity bonus, BB, should be awarded to shorter summaries that contain equivalent content.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"In fact, a length adjusted average coverage score was used as an alternative performance metric in DUC 2002.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"However, we set the brevity bonus (or penalty) to 1 for all our experiments in this paper.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"In summary, the ngram co-occurrence statistics we use in the following sections are based on the following formula: Where j ≥ i, i and j range from 1 to 4, and wn is 1/(ji+1).","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"Ngram(1, 4) is a weighted variable length n-gram match score similar to the IBM BLEU score; while Ngram(k, k), i.e. i = j = k, is simply the average k-gram coverage score Ck.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"With these formulas, we describe how to evaluate them in the next section.","{'title': '3 BLEU and N-gram Co-Occurrence', 'number': '3'}"
"In order to evaluate the effectiveness of automatic evaluation metrics, we propose two criteria: cients of different DUC 2001 data between Ngram(1, 4)n rankings and human rankings including (S) and excluding (SX) stopwords.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"SD-100 is for single document summaries of 100 words and MD-50, 100, 200, and 400 are for multi-document summaries of 50, 100, 200, and 400 words.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
MD-All averages results from summaries of all sizes. should be a good predictor of the statistical significance of human assessments with high reliability.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"The first criterion ensures whenever a human recognizes a good summary/translation/system, an automatic evaluation will do the same with high probability.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"This enables us to use an automatic evaluation procedure in place of human assessments to compare system performance, as in the NIST MT evaluations (NIST 2002).","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The second criterion is critical in interpreting the significance of automatic evaluation results.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to &quot;real&quot; significance, i.e. the statistical significance in a human assessment of run A and run B?","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Ideally, we would like there to be a positive correlation between them.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"If this can be asserted with strong reliability (high recall and precision), then we can use the automatic evaluation to assist system development and to be reasonably sure that we have made progress.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"As stated in Section 3, direct application of BLEU on the DUC 2001 data showed promising results.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
We therefore prefer the metric given by equation 6 and use it in all our experiments.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"We then compare the Ngram(1,4) ranking with the human ranking.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Figure 2 shows the result of DUC 2001 multi-document data.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Stopwords are ignored during the computation of Ngram(1,4) scores and words are stemmed using a Porter stemmer (Porter 1980).","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"The x-axis is the human ranking and the y-axis gives the corresponding Ngram(1,4) rankings for summaries of difference sizes.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The straight line marked by AvgC is the ranking given by human assessment.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For example, a system at (5,8) means that human ranks its performance at the 5th rank while Ngram(1,4)400 ranks it at the 8th.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"If an automatic ranking fully matches the human ranking, its plot will coincide with the heavy diagonal.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
A line with less deviation from the heavy diagonal line indicates better correlation with the human assessment.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"To quantify the correlation, we compute the Spearman rank order correlation coefficient (p) for each Ngram(1,4)n run at different summary sizes (n).","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
We also test the effect of inclusion or exclusion of stopwords.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The results are summarized in Table 1.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Although these results are statistically significant (α _ 0.025) and are comparable to IBM BLEU's correlation figures shown in Section 3, they are not consistent across summary sizes and tasks.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For example, the correlations of the single document task are at the 60% level; while they range from 50% to 80% for the multidocument task.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The inclusion or exclusion of stopwords also shows mixed results.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"In order to meet the requirement of the first criterion stated in Section 3, we need better results.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"The Ngram(1,4)n score is a weighted average of variable length n-gram matches.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"By taking a log sum of the ngram matches, the Ngram(1,4)n favors match of longer n-grams.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For example, if &quot;United States of America&quot; occurs in a reference summary, while one peer summary, A, uses &quot;United States&quot; and another summary, B, uses the full phrase &quot;United States of America&quot;, summary B gets more contribution to its overall score simply due to the longer version of the name.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"However, intuitively one should prefer a short version of the name in summarization.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Therefore, we need to change the weighting scheme to not penalize or even reward shorter equivalents.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
We conduct experiments to understand the effect of individual n-gram co-occurrence scores in approximating human assessments.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Tables 2 and 3 show the results of these runs without and with stopwords respectively.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For each set of DUC 2001 data, single document 100word summarization task, multi-document 50, 100, 200, and 400 -word summarization tasks, we compute 4 different correlation statistics: Spearman rank order correlation coefficient (Spearman p), linear regression t-test (LRt, 11 degree of freedom for single document task and 13 degree of freedom for multi-document task), Pearson product moment coefficient of correlation (Pearson p), and coefficient of determination (CD) for each Ngram(i,�) evaluation metric.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Among them Spearman p is a nonparametric test, a higher number indicates higher correlation; while the other three tests are parametric tests.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Higher LRt, Pearson p, and CD also suggests higher linear correlation.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Analyzing all runs according to Tables 2 and 3, we make the following observations: outperform (0.99 ≥ Spearman p ≥ 0.75) the weighted average of n-gram of variable length Ngram(1, 4) (0.88 ≥ Spearman p ≥ 0.55) in single and multiple document tasks when stopwords are ignored.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Importantly, unigram performs especially well with Spearman p ranging from 0.88 to 0.99 that is better than the best case in which weighted average of variable length n-gram matches is used and is consistent across different data sets.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
(2) The performance of weighted average n-gram scores is in the range between bi-gram and tri-gram co-occurrence scores.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
This might suggest some summaries are over-penalized by the weighted average metric due to the lack of longer n-gram matches.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"For example, given a model string &quot;United States, Japan, and Taiwan&quot;, a candidate string &quot;United States, Taiwan, and Japan&quot; has a unigram score of 1, bi-gram score of 0.5, and trigram and 4-gram scores of 0 when the stopword &quot;and' is ignored.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The weighted average n-gram score for the candidate string is 0.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
(3) Excluding stopwords in computing n-gram cooccurrence statistics generally achieves better correlation than including stopwords.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"We have shown that simple unigram, Ngram(1,1), or bigram, Ngram(2,2), co-occurrence statistics based on equation 6 outperform the weighted average of n-gram matches, Ngram(1,4), in the previous section.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"To examine how well the statistical significance in the automatic Ngram(i,�) metrics translates to real significance when human assessments are involved, we set up the following test procedures: A good automatic metric should have high recall and precision.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
This implies that if a statistical test indicates a significant difference between two runs using the automatic metric then very probably there is also a significant difference in the manual evaluation.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
This would be very useful during the system development cycle to gauge if an improvement is really significant or not.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Figure 3 shows the recall and precision curves for the DUC 2001 single document task at different α levels and Figure 4 is for the multi-document task with differFigure 3.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Recall and precision curves of Ngram co-occurrence statistics versus human assessment for DUC 2001 single document task.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
The 5 points on each curve represent values for the 5 ❑ levels.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Figure 4.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Recall and precision curves of N-gram co-occurrence statistics versus human assessment for DUC 2001 multi-document task.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Dark (black) solid lines are for average of all summary sizes, light (red) solid lines are for 50-word summaries, dashed (green) lines are for 100-word summaries, dash-dot lines (blue) are for 200-word summaries, and dotted (magenta) lines are for 400-word summaries. ent summary sizes.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
Both of them exclude stopwords.,"{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"We use z-test in all the significance tests with ❑ level at 0.10, 0.05, 0.25, 0.01, and 0.005.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"From Figures 3 and 4, we can see Ngram(1,1) and Ngram(2,2) reside on the upper right corner of the recall and precision graphs.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"Ngram(1,1) has the best overall behavior.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"These graphs confirm Ngram(1,1) (simple unigram) is a good automatic scoring metric with good statistical significance prediction power.","{'title': '4 Evaluations of N-gram Co-Occurrence Metrics', 'number': '4'}"
"In this paper, we gave a brief introduction of the manual summary evaluation protocol used in the Document Understanding Conference.","{'title': '5 Conclusions', 'number': '5'}"
"We then discussed the IBM BLEU MT evaluation metric, its application to summary evaluation, and the difference between precisionbased BLEU translation evaluation and recall-based DUC summary evaluation.","{'title': '5 Conclusions', 'number': '5'}"
The discrepancy led us to examine the effectiveness of individual n-gram cooccurrence statistics as a substitute for expensive and error-prone manual evaluation of summaries.,"{'title': '5 Conclusions', 'number': '5'}"
"To evaluate the performance of automatic scoring metrics, we proposed two test criteria.","{'title': '5 Conclusions', 'number': '5'}"
One was to make sure system rankings produced by automatic scoring metrics were similar to human rankings.,"{'title': '5 Conclusions', 'number': '5'}"
This was quantified by Spearman's rank order correlation coefficient and three other parametric correlation coefficients.,"{'title': '5 Conclusions', 'number': '5'}"
Another was to compare the statistical significance test results between automatic scoring metrics and human assessments.,"{'title': '5 Conclusions', 'number': '5'}"
We used recall and precision of the agreement between the test statistics results to identify good automatic scoring metrics.,"{'title': '5 Conclusions', 'number': '5'}"
"According to our experiments, we found that unigram co-occurrence statistics is a good automatic scoring metric.","{'title': '5 Conclusions', 'number': '5'}"
It consistently correlated highly with human assessments and had high recall and precision in significance test with manual evaluation results.,"{'title': '5 Conclusions', 'number': '5'}"
"In contrast, the weighted average of variable length n-gram matches derived from IBM BLEU did not always give good correlation and high recall and precision.","{'title': '5 Conclusions', 'number': '5'}"
"We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do.","{'title': '5 Conclusions', 'number': '5'}"
Longer n-grams tend to score for grammaticality rather than content.,"{'title': '5 Conclusions', 'number': '5'}"
It is encouraging to know that the simple unigram cooccurrence metric works in the DUC 2001 setup.,"{'title': '5 Conclusions', 'number': '5'}"
The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction.,"{'title': '5 Conclusions', 'number': '5'}"
We plan to run similar experiments on DUC 2002 data to see if unigram does as well.,"{'title': '5 Conclusions', 'number': '5'}"
"If it does, we will make available our code available via a website to the summarization community.","{'title': '5 Conclusions', 'number': '5'}"
"Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assessment 100% of the time.","{'title': '5 Conclusions', 'number': '5'}"
There is more to be desired in the recall and precision of significance test agreement with manual evaluation.,"{'title': '5 Conclusions', 'number': '5'}"
We are starting to explore various metrics suggested in Donaway et al. (2000).,"{'title': '5 Conclusions', 'number': '5'}"
"For example, weight n-gram matches differently according to their information content measured by tf, tfidf, or SVD.","{'title': '5 Conclusions', 'number': '5'}"
"In fact, NIST MT automatic scoring metric (NIST 2002) already integrates such modifications.","{'title': '5 Conclusions', 'number': '5'}"
One future direction includes using an automatic question answer test as demonstrated in the pilot study in SUMMAC (Mani et al. 1998).,"{'title': '5 Conclusions', 'number': '5'}"
"In that study, an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations, although the experiment was only tested on a small set of 3 topics.","{'title': '5 Conclusions', 'number': '5'}"
"According to Over (2003), NIST spent about 3,000 man hours each in DUC 2001 and 2002 for topic and document selection, summary creation, and manual evaluation.","{'title': '5 Conclusions', 'number': '5'}"
"Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics.","{'title': '5 Conclusions', 'number': '5'}"
We would like to propose an annual automatic evaluation track in DUC that encourages participants to invent new automated evaluation metrics.,"{'title': '5 Conclusions', 'number': '5'}"
Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.,"{'title': '5 Conclusions', 'number': '5'}"
The best automatic metric will be posted at the DUC website and used as an alternative in-house and repeatable evaluation mechanism during the next year.,"{'title': '5 Conclusions', 'number': '5'}"
In this way the evaluation technologies can advance at the same pace as the summarization technologies improve.,"{'title': '5 Conclusions', 'number': '5'}"
