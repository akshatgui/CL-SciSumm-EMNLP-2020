col1,col2
A major engineering challenge in statistical machine translation systems is the efficient representation of extremely large translationrulesets.,{}
"In phrase-based models, this prob lem can be addressed by storing the training data in memory and using a suffix array asan efficient index to quickly lookup and extract rules on the fly.",{}
Hierarchical phrasebased translation introduces the added wrin kle of source phrases with gaps.,{}
"Lookup algorithms used for contiguous phrases nolonger apply and the best approximate pat tern matching algorithms are much too slow, taking several minutes per sentence.",{}
"Wedescribe new lookup algorithms for hierar chical phrase-based translation that reduce the empirical computation time by nearly two orders of magnitude, making on-the-fly lookup feasible for source phrases with gaps.",{}
Current statistical machine translation systems rely on very large rule sets.,"{'title': 'Introduction', 'number': '1'}"
"In phrase-based systems, rules are extracted from parallel corpora containingtens or hundreds of millions of words.","{'title': 'Introduction', 'number': '1'}"
This can result in millions of rules using even the most conser vative extraction heuristics.,"{'title': 'Introduction', 'number': '1'}"
Efficient algorithms for rule storage and access are necessary for practical decoding algorithms.,"{'title': 'Introduction', 'number': '1'}"
"They are crucial to keeping up with the ever-increasing size of parallel corpora, as well as the introduction of new data sources such as web-mined and comparable corpora.","{'title': 'Introduction', 'number': '1'}"
"Until recently, most approaches to this probleminvolved substantial tradeoffs.","{'title': 'Introduction', 'number': '1'}"
The common practice of test set filtering renders systems impracti cal for all but batch processing.,"{'title': 'Introduction', 'number': '1'}"
Tight restrictions on phrase length curtail the power of phrase-basedmodels.,"{'title': 'Introduction', 'number': '1'}"
"However, some promising engineering so lutions are emerging.","{'title': 'Introduction', 'number': '1'}"
"Zens and Ney (2007) use a disk-based prefix tree, enabling efficient access to phrase tables much too large to fit in main memory.","{'title': 'Introduction', 'number': '1'}"
"An alternative approach introduced independently by both Callison-Burch et al (2005) and Zhang and Vogel (2005) is to store the training data itself inmemory, and use a suffix array as an efficient in dex to look up, extract, and score phrase pairs on the fly.","{'title': 'Introduction', 'number': '1'}"
"We believe that the latter approach has several important applications (?7).So far, these techniques have focused on phrase based models using contiguous phrases (Koehn et al., 2003; Och and Ney, 2004).","{'title': 'Introduction', 'number': '1'}"
"Some recent models permit discontiguous phrases (Chiang, 2007; Quirket al, 2005; Simard et al, 2005).","{'title': 'Introduction', 'number': '1'}"
"Of particular in terest to us is the hierarchical phrase-based model ofChiang (2007), which has been shown to be supe rior to phrase-based models.","{'title': 'Introduction', 'number': '1'}"
"The ruleset extractedby this model is a superset of the ruleset in an equivalent phrase-based model, and it is an order of magnitude larger.","{'title': 'Introduction', 'number': '1'}"
This makes efficient rule representa tion even more critical.,"{'title': 'Introduction', 'number': '1'}"
We tackle the problem using the online rule extraction method of Callison-Burch et al (2005) and Zhang and Vogel (2005).,"{'title': 'Introduction', 'number': '1'}"
"The problem statement for our work is: Given an input sentence, efficiently find all hierarchical phrase-based translation rules for that sentence in the training corpus.","{'title': 'Introduction', 'number': '1'}"
976 We first review suffix arrays (?2) and hierarchicalphrase-based translation (?3).,"{'title': 'Introduction', 'number': '1'}"
We show that the obvious approach using state-of-the-art pattern match ing algorithms is hopelessly inefficient (?4).,"{'title': 'Introduction', 'number': '1'}"
We then describe a series of algorithms to address thisinefficiency (?5).,"{'title': 'Introduction', 'number': '1'}"
"Our algorithms reduce computa tion time by two orders of magnitude, making the approach feasible (?6).","{'title': 'Introduction', 'number': '1'}"
We close with a discussion that describes several applications of our work (?7).,"{'title': 'Introduction', 'number': '1'}"
"A suffix array is a data structure representing all suf fixes of a corpus in lexicographical order (Manber and Myers, 1993).","{'title': 'Suffix Arrays. ', 'number': '2'}"
"Formally, for a text T , the ith suffix of T is the substring of the text beginning atposition i and continuing to the end of T . This suf fix can be uniquely identified by the index i of itsfirst word.","{'title': 'Suffix Arrays. ', 'number': '2'}"
"The suffix array SAT of T is a permuta tion of [1, |T |] arranged by the lexicographical order of the corresponding suffixes.","{'title': 'Suffix Arrays. ', 'number': '2'}"
This representationenables fast lookup of any contiguous substring us ing binary search.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"Specifically, all occurrences of a length-m substring can be found in O(m + log |T |) time (Manber and Myers, 1993).","{'title': 'Suffix Arrays. ', 'number': '2'}"
1 Callison-Burch et al (2005) and Zhang and Vogel (2005) use suffix arrays as follows.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
1.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"Load the source training text F , the suffix array.","{'title': 'Suffix Arrays. ', 'number': '2'}"
"SAF , the target training text E, and the align ment A into memory.","{'title': 'Suffix Arrays. ', 'number': '2'}"
2.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"For each input sentence, look up each substring.","{'title': 'Suffix Arrays. ', 'number': '2'}"
(phrase) f?,"{'title': 'Suffix Arrays. ', 'number': '2'}"
of the sentence in the suffix array.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
aligned phrase e?,"{'title': 'Suffix Arrays. ', 'number': '2'}"
using the phrase extraction method of Koehn et al (2003).,"{'title': 'Suffix Arrays. ', 'number': '2'}"
4.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
Compute the relative frequency score p(e?|f?) of.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
each pair using the count of the extracted pair and the marginal count of f?,"{'title': 'Suffix Arrays. ', 'number': '2'}"
5.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
Compute the lexical weighting score of the.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
phrase pair using the alignment that gives the best score.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
1Abouelhoda et al (2004) show that lookup can be done in optimal O(m) time using some auxiliaray data structures.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"Forour purposes O(m + log |T |) is practical, since for the 27M word corpus used to carry out our experiments, log |T | ? 25.","{'title': 'Suffix Arrays. ', 'number': '2'}"
6.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
Use the scored rules to translate the input sen-.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
tence with a standard decoding algorithm.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"A difficulty with this approach is step 3, which canbe quite slow.","{'title': 'Suffix Arrays. ', 'number': '2'}"
Its complexity is linear in the num ber of occurrences of the source phrase f?,"{'title': 'Suffix Arrays. ', 'number': '2'}"
Both Callison-Burch et al (2005) and Zhang and Vogel (2005) solve this with sampling.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
"If a source phraseappears more than k times, they sample only k oc currences for rule extraction.","{'title': 'Suffix Arrays. ', 'number': '2'}"
Both papers reportthat translation performance is nearly identical to ex tracting all possible phrases when k = 100.,"{'title': 'Suffix Arrays. ', 'number': '2'}"
2,"{'title': 'Suffix Arrays. ', 'number': '2'}"
We consider the hierarchical translation model ofChiang (2007).,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"Formally, this model is a syn chronous context-free grammar.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"The lexicalizedtranslation rules of the grammar may contain a sin gle nonterminal symbol, denoted X . We will use a, b, c and d to denote terminal symbols, and u, v, andw to denote (possibly empty) sequences of these ter minals.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
We will additionally use ? and ? to denote(possibly empty) sequences containing both termi nals and nonterminals.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
A translation rule is written X ? ?/?.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
This rule states that a span of the input matching ? is replacedby ? in translation.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
We require that ? and ? con tain an equal number (possibly zero) of coindexed nonterminals.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
An example rule with coindexes is X ? uX 1 vX 2w/u ?X 2 v ?X 1w ?.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"When discussing only the source side of such rules, we will leave out the coindexes.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"For instance, the source side of the above rule will be written uXvXw.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"3 For the purposes of this paper, we adhere to therestrictions described by Chiang (2007) for rules ex tracted from the training data.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
Rules can contain at most two nonterminals.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
Rules can contain at most five terminals.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
Rules can span at most ten words.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"2A sample size of 100 is actually quite small for many phrases, some of which occur tens or hundreds of thousands of times.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
It is perhaps surprising that such a small sample size works as well as the full data.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"However, recent work by Och (2005) and Federico and Bertoldi (2006) has shown that the statistics used by phrase-based systems are not very precise.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"3In the canonical representation of the grammar, source-sidecoindexes are always in sorted order, making them unambigu ous.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
977 ? Nonterminals must span at least two words.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
Adjacent nonterminals are disallowed in the source side of a rule.,"{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"Expressed more economically, we say that our goal is to search for source phrases in the form u, uXv, or uXvXw, where 1 ? |uvw| ? 5, and |v| > 0 in the final case.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
"Note that the model also allows rules in the form Xu, uX , XuX , XuXv, and uXvX . However, these rules are lexically identical to other rules, and thus will match the same locations in the source text.","{'title': 'Hierarchical Phrase-Based Translation. ', 'number': '3'}"
On-the-fly lookup using suffix arrays involves an added complication when the rules are in form uXv or uXvXw.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
Binary search enables fast lookup of contiguous substrings.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"However, it cannot be used for discontiguous substrings.","{'title': 'The Collocation Problem. ', 'number': '4'}"
Consider the rule aXbXc.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"If we search for this rule in the followinglogical suffix array fragment, we will find the bold faced matches.","{'title': 'The Collocation Problem. ', 'number': '4'}"
... a c a c b a d c a d ... a c a d b a a d b d ... a d d b a a d a b c ... a d d b d a a b b a ... a d d b d d c a a a ...,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"Even though these suffixes are in lexicographicalorder, matching suffixes are interspersed with non matching suffixes.","{'title': 'The Collocation Problem. ', 'number': '4'}"
We will need another algorithmto find the source rules containing at least oneX surrounded by nonempty sequences of terminal sym bols.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
4.1 Baseline Approach.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"In the pattern-matching literature, words spanned by the nonterminal symbols of Chiang?s grammar are called don?t cares and a nonterminal symbol in a query pattern that matches a sequence of don?t caresis called a variable length gap.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"The search prob lem for patterns containing these gaps is a variant of approximate pattern matching, which has receivedsubstantial attention (Navarro, 2001).","{'title': 'The Collocation Problem. ', 'number': '4'}"
The best algo rithm for pattern matching with variable-length gaps in a suffix array is a recent algorithm by Rahman et al (2006).,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"It works on a pattern w1Xw2X...wI consisting of I contiguous substrings w1, w2, ...wI ,each separated by a gap.","{'title': 'The Collocation Problem. ', 'number': '4'}"
The algorithm is straight forward.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"After identifying all ni occurrences of each wi in O(|wi| + log |T |) time, collocations thatmeet the gap constraints are computed using an ef ficient data structure called a stratified tree (van Emde Boas et al, 1977).","{'title': 'The Collocation Problem. ', 'number': '4'}"
"4 Although we refer the reader to the source text for a full description of this data structure, its salient characteristic is that it implements priority queue operations insert and next-element in O(log log |T |) time.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"Therefore, thetotal running time for an algorithm to find all con tiguous subpatterns and compute their collocations is O( ?I i=1 [|wi|+ log|T |+ ni log log |T |]).","{'title': 'The Collocation Problem. ', 'number': '4'}"
We can improve on the algorithm of Rahman et al.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
(2006) using a variation on the idea of hashing.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"We exploit the fact that our large text is actually acollection of relatively short sentences, and that col located patterns must occur in the same sentence in order to be considered a rule.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"Therefore, we can use the sentence id of each subpattern occurrence as a kind of hash key.","{'title': 'The Collocation Problem. ', 'number': '4'}"
We create a hash table whosesize is exactly the number of sentences in our train ing corpus.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"Each location of the partially matched pattern w1X...Xwi is inserted into the hash bucket with the matching sentence id. To find collocated patterns wi+1, we probe the hash table with each of the ni+1 locations for that subpattern.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"When amatch is found, we compare the element with all el ements in the bucket to see if it is within the windowimposed by the phrase length constraints.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"Theoreti cally, the worst case for this algorithm occurs when all elements of both sets resolve to the same hash bucket, and we must compare all elements of one set with all elements of the other set.","{'title': 'The Collocation Problem. ', 'number': '4'}"
This leads to a worst case complexity of O( ?I i=1 [|wi|+ log|T |] +?Ii=1 ni).,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"However, for real language data the per formance for sets of any significant size will be O( ?I i=1 [|wi|+ log|T |+ ni]), since most patterns will occur once in any given sentence.","{'title': 'The Collocation Problem. ', 'number': '4'}"
4.2 Analysis.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
It is instructive to compare this with the complex ity for contiguous phrases.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"In that case, total lookup time is O(|w| + log|T |) for a contiguous pattern w. 4Often known in the literature as a van Emde Boas tree or van Emde Boas priority queue.","{'title': 'The Collocation Problem. ', 'number': '4'}"
978 The crucial difference between the contiguous and discontiguous case is the added term ?I i=1 ni.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
Foreven moderately frequent subpatterns this term dom inates complexity.,"{'title': 'The Collocation Problem. ', 'number': '4'}"
"To make matters concrete, consider the training corpus used in our experiments (?6), which contains27M source words.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"The three most frequent uni grams occur 1.48M, 1.16M and 688K times ? thefirst two occur on average more than once per sen tence.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"In the worst case, looking up a contiguous phrase containing any number and combination ofthese unigrams requires no more than 25 compari son operations.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"In contrast, the worst case scenario for a pattern with a single gap, bookended on either side by the most frequent word, requires over two million operations using our baseline algorithm and over thirteen million using the algorithm of Rahman et al (2006).","{'title': 'The Collocation Problem. ', 'number': '4'}"
"A single frequent word in an input sentence is enough to cause noticeable slowdowns, since it can appear in up to 530 hierarchical rules.To analyze the cost empirically, we ran our base line algorithm on the first 50 sentences of the NIST Chinese-English 2003 test set and measured the CPU time taken to compute collocations.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"We foundthat, on average, it took 2241.25 seconds (?37 min utes) per sentence just to compute all of the needed collocations.","{'title': 'The Collocation Problem. ', 'number': '4'}"
"By comparison, decoding time persentence is roughly 10 seconds with moderately ag gressive pruning, using the Python implementation of Chiang (2007).","{'title': 'The Collocation Problem. ', 'number': '4'}"
"Clearly, looking up patterns in this way is not prac tical.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"To analyze the problem, we measured the amount of CPU time per computation.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Cumulative lookup time was dominated by a very small fraction of the computations (Fig.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
1).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"As expected, further analysis showed that these expensive computations all involved one or more very frequent subpatterns.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
In the worst cases a single collocation took severalseconds to compute.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However, there is a silver lining.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Patterns follow a Zipf distribution, so the number of pattern types that cause the problem is actu ally quite small.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The vast majority of patterns arerare.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Therefore, our solution focuses on computa tions where one or more of the component patternsis frequent.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Assume that we are computing a collo Computations (ranked by time) C u m u l a t i v e T i m e ( s ) 300K 150K Figure 1: Ranked computations vs. cumulative time.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
A small fraction of all computations account for most of the computational time.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"cation of pattern w1X...Xwi and pattern wi+1, and we know all locations of each.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
There are three cases.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If both patterns are frequent, we resort to a precomputed intersection (?5.1).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"We were notaware of any algorithms to substantially im prove the efficiency of this computation when it is requested on the fly, but precomputation can be done in a single pass over the text at decoder startup.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If one pattern is frequent and the other is rare,we use an algorithm whose complexity is de pendent mainly on the frequency of the rare pattern (?5.2).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
It can also be used for pairs of rare patterns when one pattern is much rarer than the other.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If both patterns are rare, no special algorithms are needed.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Any linear algorithm will suffice.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However, for reasons described in ?5.3, our other collocation algorithms depend on sorted sets, so we use a merge algorithm.Finally, in order to cut down on the number of un necessary computations, we use an efficient method to enumerate the phrases to lookup (?5.4).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This method also forms the basis of various caching strategies for additional speedups.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We analyze the memory use of our algorithms in ?5.5.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.1 Precomputation.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Precomputation of the most expensive collocationscan be done in a single pass over the text.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"As in put, our algorithm requires the identities of the k 979 most frequent contiguous patterns.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5 It then iterates over the corpus.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Whenever a pattern from the list is seen, we push a tuple consisting of its identity and current location onto a queue.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Whenever the oldest item on the queue falls outside the maximum phrase length window with respect to the current position,we compute that item?s collocation with all succeed ing patterns (subject to pattern length constraints) and pop it from the queue.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We repeat this step for every item that falls outside the window.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"At the end of each sentence, we compute collocations for any remaining items in the queue and then empty it.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Our precomputation includes the most frequent n-gram subpatterns.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Most of these are unigrams, but in our experiments we found 5-grams among the 1000 most frequent patterns.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We precompute the locations of source phrase uXv for any pair u and v that both appear on this list.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
There is alsoa small number of patterns uXv that are very frequent.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"We cannot easily obtain a list of these in ad vance, but we observe that they always consist of apair u and v of patterns from near the top of the frequency list.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Therefore we also precompute the loca tions uXvXw of patterns in which both u and v are among these super-frequent patterns (all unigrams), treating this as the collocation of the frequent pattern uXv and frequent pattern w. We also compute the analagous case for u and vXw.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.2 Fast Intersection.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"For collocations of frequent and rare patterns, we use a fast set intersection method for sorted sets called double binary search (Baeza-Yates, 2004).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"6 It is based on the intuition that if one set in a pair of sorted sets is much smaller than the other, thenwe can compute their intersection efficiently by per forming a binary search in the larger data set D for each element of the smaller query set Q. Double binary search takes this idea a step further.It performs a binary search in D for the median ele ment of Q. Whether or not the element is found, the 5These can be identified using a single traversal over alongest common prefix (LCP) array, an auxiliary data struc ture of the suffix array, described by Manber and Myers (1993).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Since we don?t need the LCP array at runtime, we chose to do this computation once offline.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
6Minor modifications are required since we are computing collocation rather than intersection.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Due to space constraints, details and proof of correctness are available in Lopez (2007a).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
search divides both sets into two pairs of smaller sets that can be processed recursively.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Detailed analysis and empirical results on an information retrieval task are reported in Baeza-Yates (2004) and Baeza-Yates and Salinger (2005).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
If |Q| log |D| < |D| then theperformance is guaranteed to be sublinear.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
In practice it is often sublinear even if |Q| log |D| is somewhat larger than |D|.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
In our implementation we sim ply check for the condition ?|Q| log |D| < |D| to decide whether we should use double binary search or the merge algorithm.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This check is applied in the recursive cases as well as for the initial inputs.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Thevariable ? can be adjusted for performance.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We de termined experimentally that a good value for this parameter is 0.3.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.3 Obtaining Sorted Sets.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Double binary search requires that its input sets be in sorted order.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However, the suffix array returnsmatchings in lexicographical order, not numeric or der.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The algorithm of Rahman et al (2006) deals with this problem by inserting the unordered items into a stratified tree.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This requires O(n log log |T |) time for n items.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If we used the same strategy, our algorithm would no longer be sublinear.An alternative is to precompute all n-gram occur rences in order and store them in an inverted index.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This can be done in one pass over the data.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"7 This approach requires a separate inverted index for each n, up to the maximum n used by the model.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The memory cost is one length-|T | array per index.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"In order to avoid the full n|T | cost in memory, our implementation uses a mixed strategy.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"We keep a precomputed inverted index only for unigrams.For bigrams and larger n-grams, we generate the in dex on the fly using stratified trees.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This results in a superlinear algorithm for intersection.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However,we can exploit the fact that we must compute col locations multiple times for each input n-gram by caching the sorted set after we create it (The cachingstrategy is described in ?5.4).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Subsequent computations involving this n-gram can then be done in lin ear or sublinear time.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Therefore, the cost of building the inverted index on the fly is amortized over a large number of computations.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"7We combine this step with the other precomputations that require a pass over the data, thereby removing a redundant O(|T |) term from the startup cost.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
980 5.4 Efficient Enumeration.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
A major difference between contiguous phrase based models and hierarchical phrase-based models is the number of rules that potentially apply to an input sentence.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"To make this concrete, on our data, with an average of 29 words per sentence, there were on average 133 contiguous phrases of length 5 orless that applied.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"By comparison, there were on av erage 7557 hierarchical phrases containing up to 5words.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
These patterns are obviously highly overlap ping and we employ an algorithm to exploit this fact.We first describe a baseline algorithm used for con tiguous phrases (?5.4.1).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We then introduce some improvements (?5.4.2) and describe a data structureused by the algorithm (?5.4.3).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Finally, we dis cuss some special cases for discontiguous phrases (?5.4.4).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.4.1 The Zhang-Vogel AlgorithmZhang and Vogel (2005) present a clever algorithm for contiguous phrase searches in a suffix ar ray.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"It exploits the fact that for eachm-length source phrase that we want to look up, we will also want to look up its (m? 1)-length prefix.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"They observe that the region of the suffix array containing all suffixes prefixed by ua is a subset of the region containingthe suffixes prefixed by u. Therefore, if we enumer ate the phrases of our sentence in such a way that we always search for u before searching for ua, wecan restrict the binary search for ua to the range con taining the suffixes prefixed by u. If the search for u fails, we do not need to search for ua at all.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"They show that this approach leads to some time savings for phrase search, although the gains are relatively modest since the search for contiguous phrases is not very expensive to begin with.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However, the potential savings in the discontiguous case are much greater.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.4.2 Improvements and Extensions We can improve on the Zhang-Vogel algorithm.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"An m-length contiguous phrase aub depends not only on the existence of its prefix au, but also on the existence of its suffix ub.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"In the contiguous case, we cannot use this information to restrict the starting range of the binary search, but we can check for the existence of ub to decide whether we even need to search for aub at all.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This can help us avoid searches that are guaranteed to be fruitless.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Now consider the discontiguous case.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"As in the analogous contiguous case, a phrase a?b will onlyexist in the text if its maximal prefix a?","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
and maxi mal suffix ?b both exist in the corpus and overlap at specific positions.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"8 Searching for a?b is potentially very expensive, so we put all available information to work.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Before searching, we require that both a?and ?b exist.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Additionally, we compute the location of a?b using the locations of both maximal sub phrases.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"To see why the latter optimization is useful, consider a phrase abXcd.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"In our baseline algorithm, we would search for ab and cd, and then perform a computation to see whether these subphrases were collocated within an elastic window.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"However, if weinstead use abXc and bXcd as the basis of the com putation, we gain two advantages.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"First, the number elements of each set is likely to be smaller then in the former case.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Second, the computation becomes simpler, because we now only need to check to see whether the patterns exactly overlap with a starting offset of one, rather than checking within a window of locations.We can improve efficiency even further if we con sider cases where the same substring occurs morethan once within the same sentence, or even in mul tiple sentences.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If the computation required to look up a phrase is expensive, we would like to performthe lookup only once.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This requires some mecha nism for caching.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Depending on the situation, we might want to cache only certain subsets of phrases, based on their frequency or difficulty to compute.We would also like the flexibility to combine on the-fly lookups with a partially precomputed phrase table, as in the online/offline mixture of Zhang and Vogel (2005).We need a data structure that provides this flex ibility, in addition to providing fast access to both the maximal prefix and maximal suffix of any phrase that we might consider.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"5.4.3 Prefix Trees and Suffix Links Our search optimizations are easily captured in a prefix tree data structure augmented with suffix links.Formally, a prefix tree is an unminimized determin istic finite-state automaton that recognizes all of thepatterns in some set.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Each node in the tree repre8Except when ? = X , in which case a and b must be collo cated within a window defined by the phrase length constraints.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"981 ab b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d a b b c cX X (1)(2) (3) d (4) d X e a c d Case 1 Case 2 Figure 2: Illustration of prefix tree construction showing a partial prefix tree, including suffix links.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Suppose we are interested in pattern abXcd, represented by node (1).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Its prefix is represented by node (2), and node (2)?s suffix is represented by node (3).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Therefore, node (1)?s suffix is represented by the node pointed to by the d-edge from node (3), which is node (4).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
There are two cases.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"In case 1, node (4) is inactive, so we can mark node (1) inactive and stop.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"In case 2, node (4) is active, so we compute the collocation of abXc and bXcd with information stored at nodes (2) and (4), using either a precomputed intersection, double binary search, or merge, depending on the size of the sets.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If the result is empty, we mark the node inactive.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Otherwise, we store the results at node (1) and add its successor patterns to the frontier for the next iteration.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This includes all patterns containing exactly one more terminal symbol than the current pattern.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
sents the prefix of a unique pattern from the set that is specified by the concatenation of the edge labels along the path from the root to that node.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
A suffix link is a pointer from a node representing path a?,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
to the node representing path ?.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We will use this data structure to record the set of patterns that we have searched for and to cache information for those that were found successfully.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Our algorithm generates the tree breadth-search along a frontier.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
In the mth iteration we only searchfor patterns containingm terminal symbols.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Regardless of whether we find a particular pattern, we cre ate a node for it in the tree.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If the pattern was found in the corpus, its node is marked active.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Otherwise, it is marked inactive.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"For found patterns, we storeeither the endpoints of the suffix array range con taining the phrase (if it is contiguous), or the list oflocations at which the phrase is found (if it is dis contiguous).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
We can also store the extracted rules.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"9 Whenever a pattern is successfully found, we add all patterns with m + 1 terminals that are prefixed by it 9Conveniently, the implementation of Chiang (2007) uses aprefix tree grammar encoding, as described in Klein and Manning (2001).","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Our implementation decorates this tree with addi tional information required by our algorithms.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"to the frontier for processing in the next iteration.To search for a pattern, we use location infor mation from its parent node, which represents its maximal prefix.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Assuming that the node representsphrase ?b, we find the node representing its max imal suffix by following the b-edge from the node pointed to by its parent node?s suffix link.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If the node pointed to by this suffix link is inactive, we can mark the node inactive without running a search.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"When a node is marked inactive, we discontinue search for phrases that are prefixed by the path it represents.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The algorithm is illustrated in Figure 2.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
5.4.4 Special Cases for Phrases with GapsA few subtleties arise in the extraction of hierarchical patterns.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Gaps are allowed to occur at the be ginning or end of a phrase.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"For instance, we mayhave a source phrase Xu or uX or even XuX . Al though each of these phrases requires its own path in the prefix tree, they are lexically identical to phrase u. An analogous situation occurs with the patterns XuXv, uXvX , and uXv.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
There are two cases that we are concerned with.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The first case consists of all patterns prefixed with X . The paths to nodes representing these patterns 982 will all contain the X-edge originating at the rootnode.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
All of these paths form the shadow subtree.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Path construction in this subtree proceeds dif ferently.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"Because they are lexically identical to theirsuffixes, they are automatically extended if their suffix paths are active, and they inherit location infor mation of their suffixes.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"The second case consists of all patterns suffixedwith X . Whenever we successfully find a new pat tern ?, we automatically extend it with an X edge,provided that ?X is allowed by the model con straints.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The node pointed to by this edge inheritsits location information from its parent node (repre senting the maximal prefix ?).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Note that both special cases occur for patterns in the form XuX . 5.5 Memory Requirements.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"As shown in Callison-Burch et al (2005), we must keep an array for the source text F , its suffix array,the target text E, and alignment A in memory.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"As suming that A and E are roughly the size of F , thecost is 4|T |.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If we assume that all data use vocabu laries that can be represented using 32-bit integers, then our 27M word corpus can easily be represented in around 500MB of memory.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Adding the inverted index for unigrams increases this by 20%.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
The main additional cost in memory comes from the storage of the precomputed collocations.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
This is dependentboth on the corpus size and the number of colloca tions that we choose to precompute.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
Using detailed timing data from our experiments we were able to simulate the memory-speed tradeoff (Fig.,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
3).,"{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
"If we include a trigram model trained on our bitext and the Chinese Gigaword corpus, the overall storage costs for our system are approximately 2GB.","{'title': 'Solving the Collocation Problem. ', 'number': '5'}"
All of our experiments were performed on ChineseEnglish in the news domain.,"{'title': 'Experiments. ', 'number': '6'}"
We used a large train ing set consisting of over 1 million sentences from various newswire corpora.,"{'title': 'Experiments. ', 'number': '6'}"
This corpus is roughly the same as the one used for large-scale experiments by Chiang et al (2005).,"{'title': 'Experiments. ', 'number': '6'}"
"To generate alignments,we used GIZA++ (Och and Ney, 2003).","{'title': 'Experiments. ', 'number': '6'}"
"We symmetrized bidirectional alignments using the grow diag-final heuristic (Koehn et al, 2003).","{'title': 'Experiments. ', 'number': '6'}"
0 0 0 1000 0 Number of frequent subpatterns Insert text here 41 sec/sent 41 seconds 405 sec/sent 0 MB.,"{'title': 'Experiments. ', 'number': '6'}"
725MB Figure 3: Effect of precomputation on memory useand processing time.,"{'title': 'Experiments. ', 'number': '6'}"
Here we show only the mem ory requirements of the precomputed collocations.,"{'title': 'Experiments. ', 'number': '6'}"
We used the first 50 sentences of the NIST 2003test set to compute timing results.,"{'title': 'Experiments. ', 'number': '6'}"
All of our algo rithms were implemented in Python 2.4.,"{'title': 'Experiments. ', 'number': '6'}"
10 Timingresults are reported for machines with 8GB of mem ory and 4 3GHz Xeon processors running Red Hat linux 2.6.9.,"{'title': 'Experiments. ', 'number': '6'}"
"In order to understand the contributions of various improvements, we also ran the system with with various ablations.","{'title': 'Experiments. ', 'number': '6'}"
"In the default setting, the prefix tree is constructed for each sentence to guide phrase lookup, and then discarded.","{'title': 'Experiments. ', 'number': '6'}"
"To showthe effect of caching we also ran the algorithm without discarding the prefix tree between sentences, re sulting in full inter-sentence caching.","{'title': 'Experiments. ', 'number': '6'}"
The results are shown in Table 1.,"{'title': 'Experiments. ', 'number': '6'}"
11It is clear from the results that each of the op timizations is needed to sufficiently reduce lookuptime to practical levels.,"{'title': 'Experiments. ', 'number': '6'}"
"Although this is still rela tively slow, it is much closer to the decoding time of 10 seconds per sentence than the baseline.","{'title': 'Experiments. ', 'number': '6'}"
10Python is an interpreted language and our implementations do not use any optimization features.,"{'title': 'Experiments. ', 'number': '6'}"
It is therefore reasonable to think that a more efficient reimplementation would result in across-the-board speedups.11The results shown here do not include the startup time re quired to load the data structures into memory.,"{'title': 'Experiments. ', 'number': '6'}"
"In our Python implementation this takes several minutes, which in principle should be amortized over the cost for each sentence.","{'title': 'Experiments. ', 'number': '6'}"
"However,just as Zens and Ney (2007) do for phrase tables, we could com pile our data structures into binary memory-mapped files, whichcan be read into memory in a matter of seconds.","{'title': 'Experiments. ', 'number': '6'}"
We are cur rently investigating this option in a C reimplementation.,"{'title': 'Experiments. ', 'number': '6'}"
983 Algorithms Secs/Sent Collocations Baseline 2241.25 325548 Prefix Tree 1578.77 69994 Prefix Tree + precomputation 696.35 69994 Prefix Tree + double binary 405.02 69994 Prefix Tree + precomputation + double binary 40.77 69994 Prefix Tree with full caching + precomputation + double binary 30.70 67712 Table 1: Timing results and number of collocations computed for various combinations of algorithms.,"{'title': 'Experiments. ', 'number': '6'}"
The runs using precomputation use the 1000 most frequent patterns.,"{'title': 'Experiments. ', 'number': '6'}"
Our work solves a seemingly intractable problemand opens up a number of intriguing potential ap plications.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
Both Callison-Burch et al (2005) and Zhang and Vogel (2005) use suffix arrays to relax the length constraints on phrase-based models.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
Ourwork enables this in hierarchical phrase-based models.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"However, we are interested in additional appli cations.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Recent work in discriminative learning for manynatural language tasks, such as part-of-speech tagging and information extraction, has shown that feature engineering plays a critical role in these approaches.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"However, in machine translation most fea tures can still be traced back to the IBM Models of 15 years ago (Lopez, 2007b).","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Recently, Lopez and Resnik (2006) showed that most of the features used in standard phrase-based models do not help very much.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Our algorithms enable us to look up phrasepairs in context, which will allow us to compute interesting contextual features that can be used in discriminative learning algorithms to improve transla tion accuracy.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Essentially, we can use the training data itself as an indirect representation of whateverfeatures we might want to compute.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"This is not pos sible with table-based architectures.Most of the data structures and algorithms discussed in this paper are widely used in bioinformatics, including suffix arrays, prefix trees, and suf fix links (Gusfield, 1997).","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"As discussed in ?4.1, our problem is a variant of the approximate patternmatching problem.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"A major application of approx imate pattern matching in bioinformatics is queryprocessing in protein databases for purposes of se quencing, phylogeny, and motif identification.Current MT models, including hierarchical mod els, translate by breaking the input sentence intosmall pieces and translating them largely independently.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Using approximate pattern matching algo rithms, we imagine that machine translation could be treated very much like search in a protein database.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"In this scenario, the goal is to select training sentences that match the input sentence as closely as possible, under some evaluation function that accounts for both matching and mismatched sequences, as well as possibly other data features.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Once we have found the closest sentences we cantranslate the matched portions in their entirety, re placing mismatches with appropriate word, phrase, or hierarchical phrase translations as needed.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"This model would bring statistical machine translation closer to convergence with so-called example-based translation, following current trends (Marcu, 2001;Och, 2002).","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
We intend to explore these ideas in fu ture work.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"AcknowledgementsI would like to thank Philip Resnik for encour agement, thoughtful discussions and wise counsel; David Chiang for providing the source code for his translation system; and Nitin Madnani, Smaranda Muresan and the anonymous reviewers for very helpful comments on earlier drafts of this paper.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
Any errors are my own.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"This research was supported in part by ONR MURI Contract FCPO.810548265 and the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
HR0011 06-2-001.,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
"Any opinions, findings, conclusions or recommendations expressed in this paper are those of the author and do not necessarily reflect the view of DARPA.","{'title': 'Conclusions and Future Work. ', 'number': '7'}"
984,"{'title': 'Conclusions and Future Work. ', 'number': '7'}"
