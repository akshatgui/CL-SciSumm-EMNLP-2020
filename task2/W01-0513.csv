col1,col2
We seek a knowledge-free method for inducing multiword units from text corpora for use as machine-readable dictionary headwords.,{}
We provide two major evaluations of nine existing collocation-finders and illustrate the continuing need for improvement.,{}
"We use Latent Semantic Analysis to make modest gains in performance, but we show the significant challenges encountered in trying this approach.",{}
"A multiword unit (MWU) is a connected collocation: a sequence of neighboring words “whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components” (Choueka, 1988).","{'title': '1 Introduction', 'number': '1'}"
"In other words, MWUs are typically non-compositional at some linguistic level.","{'title': '1 Introduction', 'number': '1'}"
"For example, phonological non-compositionality has been observed (Finke & Weibel, 1997; Gregory, et al, 1999) where words like “got” [gat] and “to” [tu] change phonetically to “gotta” [gave] when combined.","{'title': '1 Introduction', 'number': '1'}"
"We have interest in inducing headwords for machine-readable dictionaries (MRDs), so our interest is in semantic rather than phonological non-compositionality.","{'title': '1 Introduction', 'number': '1'}"
"As an example of semantic non-compositionality, consider “compact disk”: one could not deduce that it was a music medium by only considering the semantics of “compact” and “disk.” MWUs may also be non-substitutable and/or non-modifiable (Manning and Schütze, 1999).","{'title': '1 Introduction', 'number': '1'}"
"Nonsubstitutability implies that substituting a word of the MWU with its synonym should no longer convey the same original content: “compact disk” does not readily imply “densely-packed disk.” Nonmodifiability, on the other hand, suggests one cannot modify the MWU’s structure and still convey the same content: “compact disk” does not signify “disk that is compact.” MWU dictionary headwords generally satisfy at least one of these constraints.","{'title': '1 Introduction', 'number': '1'}"
"For example, a compositional phrase would typically be excluded from a hard-copy dictionary since its constituent words would already be listed.","{'title': '1 Introduction', 'number': '1'}"
These strategies allow hard-copy dictionaries to remain compact.,"{'title': '1 Introduction', 'number': '1'}"
"As mentioned, we wish to find MWU headwords for machine-readable dictionaries (MRDs).","{'title': '1 Introduction', 'number': '1'}"
"Although space is not an issue in MRDs, we desire to follow the lexicographic practice of reducing redundancy.","{'title': '1 Introduction', 'number': '1'}"
"As Sproat indicated, &quot;simply expanding the dictionary to encompass every word one is ever likely to encounter is wrong: it fails to take advantage of regularities&quot; (1992, p. xiii).","{'title': '1 Introduction', 'number': '1'}"
"Our goal is to identify an automatic, knowledge-free algorithm that finds all and only those collocations where it is necessary to supply a definition.","{'title': '1 Introduction', 'number': '1'}"
"“Knowledge-free” means that the process should proceed without human input (other than, perhaps, indicating whitespace and punctuation).","{'title': '1 Introduction', 'number': '1'}"
This seems like a solved problem.,"{'title': '1 Introduction', 'number': '1'}"
"Many collocation-finders exist, so one might suspect that most could suffice for finding MWU dictionary headwords.","{'title': '1 Introduction', 'number': '1'}"
"To verify this, we evaluate nine existing collocation-finders to see which best identifies valid headwords.","{'title': '1 Introduction', 'number': '1'}"
We evaluate using two completely separate gold standards: (1) WordNet and (2) a compendium of Internet dictionaries.,"{'title': '1 Introduction', 'number': '1'}"
"Although web-based resources are dynamic and have better coverage than WordNet (especially for acronyms and names), we show that WordNet-based scores are comparable to those using Internet MRDs.","{'title': '1 Introduction', 'number': '1'}"
Yet the evaluations indicate that significant improvement is still needed in MWU-induction.,"{'title': '1 Introduction', 'number': '1'}"
"As an attempt to improve MWU headword induction, we introduce several algorithms using Latent Semantic Analysis (LSA).","{'title': '1 Introduction', 'number': '1'}"
LSA is a technique which automatically induces semantic relationships between words.,"{'title': '1 Introduction', 'number': '1'}"
We use LSA to try to eliminate proposed MWUs which are semantically compositional.,"{'title': '1 Introduction', 'number': '1'}"
"Unfortunately, this does not help.","{'title': '1 Introduction', 'number': '1'}"
Yet when we use LSA to identify substitutable delimiters.,"{'title': '1 Introduction', 'number': '1'}"
"This suggests that in a language with MWUs, we do show modest performance gains. whitespace, one might prefer to begin at the word","{'title': '1 Introduction', 'number': '1'}"
"For decades, researchers have explored various techniques for identifying interesting collocations.","{'title': '2 Previous Approaches', 'number': '2'}"
There have essentially been three separate kinds of approaches for accomplishing this task.,"{'title': '2 Previous Approaches', 'number': '2'}"
"These approaches could be broadly classified into (1) segmentation-based, (2) word-based and knowledgedriven, or (3) word-based and probabilistic.","{'title': '2 Previous Approaches', 'number': '2'}"
We will illustrate strategies that have been attempted in each of the approaches.,"{'title': '2 Previous Approaches', 'number': '2'}"
"Since we assume knowledge of whitespace, and since many of the first and all of the second categories rely upon human input, we will be most interested in the third category.","{'title': '2 Previous Approaches', 'number': '2'}"
Some researchers view MWU-finding as a natural by-product of segmentation.,"{'title': '2 Previous Approaches', 'number': '2'}"
One can regard text as a stream of symbols and segmentation as a means of placing delimiters in that stream so as to separate logical groupings of symbols from one another.,"{'title': '2 Previous Approaches', 'number': '2'}"
A segmentation process may find that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.,"{'title': '2 Previous Approaches', 'number': '2'}"
"In such cases, these larger units may be MWUs.","{'title': '2 Previous Approaches', 'number': '2'}"
"The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).","{'title': '2 Previous Approaches', 'number': '2'}"
"Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression.","{'title': '2 Previous Approaches', 'number': '2'}"
"Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not.","{'title': '2 Previous Approaches', 'number': '2'}"
These approaches could be applied to languages where word delimiters exist (such as in European languages delimited by the space character).,"{'title': '2 Previous Approaches', 'number': '2'}"
"However, in such languages, it seems more prudent to simply take advantage of delimiters rather than introducing potential errors by trying to find word boundaries while ignoring knowledge of the level and identify appropriate word combinations.","{'title': '2 Previous Approaches', 'number': '2'}"
"Some researchers start with words and propose MWU induction methods that make use of parts of speech, lexicons, syntax or other linguistic structure (Justeson and Katz, 1995; Jacquemin, et al., 1997; Daille, 1996).","{'title': '2 Previous Approaches', 'number': '2'}"
"For example, Justeson and Katz indicated that the patterns NOUN NOUN and ADJ NOUN are very typical of MWUs.","{'title': '2 Previous Approaches', 'number': '2'}"
"Daille also suggests that in French, technical MWUs follow patterns such as “NOUN de NOUN&quot; (1996, p. 50).","{'title': '2 Previous Approaches', 'number': '2'}"
To find word combinations that satisfy such patterns in both of these situations necessitates the use of a lexicon equipped with part of speech tags.,"{'title': '2 Previous Approaches', 'number': '2'}"
"Since we are interested in knowledge-free induction of MWUs, these approaches are less directly related to our work.","{'title': '2 Previous Approaches', 'number': '2'}"
"Furthermore, we are not really interested in identifying constructs such as general noun phrases as the above rules might generate, but rather, in finding only those collocations that one would typically need to define.","{'title': '2 Previous Approaches', 'number': '2'}"
The third category assumes at most whitespace and punctuation knowledge and attempts to infer MWUs using word combination probabilities.,"{'title': '2 Previous Approaches', 'number': '2'}"
Table 1 (see next page) shows nine commonly-used probabilistic MWU-induction approaches.,"{'title': '2 Previous Approaches', 'number': '2'}"
"In the table, fX and PX signify frequency and probability of a word X.","{'title': '2 Previous Approaches', 'number': '2'}"
A variable XY indicates a word bigram and 4XY indicates its expected frequency at random.,"{'title': '2 Previous Approaches', 'number': '2'}"
An overbar signifies a variable’s complement.,"{'title': '2 Previous Approaches', 'number': '2'}"
"For more details, one can consult the original sources as well as Ferreira and Pereira (1999) and Manning and Schütze (1999).","{'title': '2 Previous Approaches', 'number': '2'}"
"Prior to applying the algorithms, we lemmatize using a weakly-informed tokenizer that knows only that whitespace and punctuation separate words.","{'title': '3 Lexical Access', 'number': '3'}"
Punctuation can either be discarded or treated as words.,"{'title': '3 Lexical Access', 'number': '3'}"
Since we are equally interested in finding units like “Dr.” and “U.,"{'title': '3 Lexical Access', 'number': '3'}"
"S.,” we opt to treat punctuation as words.","{'title': '3 Lexical Access', 'number': '3'}"
"Once we tokenize, we use Church’s (1995) suffix array approach to identify word n-grams that occur at least T times (for T=10).","{'title': '3 Lexical Access', 'number': '3'}"
We then rank-order the n-gram list in accordance to each probabilistic algorithm.,"{'title': '3 Lexical Access', 'number': '3'}"
This task is non-trivial since most algorithms were originally suited for finding twoword collocations.,"{'title': '3 Lexical Access', 'number': '3'}"
"We must therefore decide how to expand the algorithms to identify general n-grams (say, C=w1w2 ...wn).","{'title': '3 Lexical Access', 'number': '3'}"
We can either generalize or approximate.,"{'title': '3 Lexical Access', 'number': '3'}"
"Since generalizing requires exponential compute time and memory for several of the algorithms, approximation is an attractive alternative.","{'title': '3 Lexical Access', 'number': '3'}"
"One approximation redefines X and Y to be, respectively, the word sequences w1w2 ...wi and wi+1wi+2...wn, where i is chosen to maximize PXPY.","{'title': '3 Lexical Access', 'number': '3'}"
This has a natural interpretation of being the expected probability of concatenating the two most probable substrings in order to form the larger unit.,"{'title': '3 Lexical Access', 'number': '3'}"
"Since it can be computed rapidly with low memory costs, we use this approximation.","{'title': '3 Lexical Access', 'number': '3'}"
Two additional issues need addressing before evaluation.,"{'title': '3 Lexical Access', 'number': '3'}"
The first regards document sourcing.,"{'title': '3 Lexical Access', 'number': '3'}"
"If an n-gram appears in multiple sources (eg., Congressional Record versus Associated Press), its likelihood of accuracy should increase.","{'title': '3 Lexical Access', 'number': '3'}"
This is particularly true if we are looking for MWU headwords for a general versus specialized dictionary.,"{'title': '3 Lexical Access', 'number': '3'}"
"Phrases that appear in one source may in fact be general MWUs, but frequently, they are text-specific units.","{'title': '3 Lexical Access', 'number': '3'}"
"Hence, precision gained by excluding single-source n-grams may be worth losses in recall.","{'title': '3 Lexical Access', 'number': '3'}"
We will measure this trade-off.,"{'title': '3 Lexical Access', 'number': '3'}"
"Second, evaluating with punctuation as words and applying no filtering mechanism may unfairly bias against some algorithms.","{'title': '3 Lexical Access', 'number': '3'}"
"Pre- or post-processing of n-grams with a linguistic filter has shown to improve some induction algorithms’ performance (Daille, 1996).","{'title': '3 Lexical Access', 'number': '3'}"
"Since we need knowledge-poor induction, we cannot use human-suggested filtering rules as in Section 2.2.","{'title': '3 Lexical Access', 'number': '3'}"
Yet we can filter by pruning n-grams whose beginning or ending word is among the top N most frequent words.,"{'title': '3 Lexical Access', 'number': '3'}"
This unfortunately eliminates acronyms like “U.,"{'title': '3 Lexical Access', 'number': '3'}"
"S.” and phrasal verbs like “throw up.” However, discarding some words may be worthwhile if the final list of n-grams is richer in terms of MRD headwords.","{'title': '3 Lexical Access', 'number': '3'}"
"We therefore evaluate with such an automatic filter, arbitrarily (and without optimization) choosing N=75.","{'title': '3 Lexical Access', 'number': '3'}"
A natural scoring standard is to select a language and evaluate against headwords from existing dictionaries in that language.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Others have used similar standards (Daille, 1996), but to our knowledge, none to the extent described here.","{'title': '4 Evaluating Performance', 'number': '4'}"
We evaluate thousands of hypothesized units from an unconstrained corpus.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Furthermore, we use two separate evaluation gold standards: (1) WordNet (Miller, et al, 1990) and (2) a collection of Internet MRDs.","{'title': '4 Evaluating Performance', 'number': '4'}"
Using two gold standards helps valid MWUs.,"{'title': '4 Evaluating Performance', 'number': '4'}"
It also provides evaluation using both static and dynamic resources.,"{'title': '4 Evaluating Performance', 'number': '4'}"
We choose to evaluate in English due to the wealth of linguistic resources.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"In particular, we use a randomly-selected corpus the first five columns as “information-like.” consisting of a 6.7 million word subset of the TREC Similarly, since the last four columns share databases (DARPA, 1993-1997). properties of the frequency approach, we will refer Table 2 illustrates a sample of rank-ordered output to them as “frequency-like.” from each of the different algorithms (following the One’s application may dictate which set of cross-source, filtered paradigm described in section algorithms to use.","{'title': '4 Evaluating Performance', 'number': '4'}"
Our gold standard selection 3).,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Note that algorithms in the first four columns reflects our interest in general word dictionaries, so produce results that are similar to each other as do results we obtain may differ from results we might those in the last four columns.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Although the mutual have obtained using terminology lexicons. information results seem to be almost in a class of If our gold standard contains K MWUs with their own, they actually are similar overall to the corpus frequencies satisfying threshold (T=10), our first four sets of results; therefore, we will refer to figure of merit (FOM) is given by where Pi (precision at i) equals i/Hi , and Hi is the number of hypothesized MWUs required to find the ith correct MWU.","{'title': '4 Evaluating Performance', 'number': '4'}"
This FOM corresponds to area under a precision-recall curve.,"{'title': '4 Evaluating Performance', 'number': '4'}"
WordNet has definite advantages as an evaluation resource.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"It has in excess of 50,000 MWUs, is freely accessible, widely used, and is in electronic form.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Yet, it obviously cannot contain every MWU.","{'title': '4 Evaluating Performance', 'number': '4'}"
"For instance, our corpus contains 177,331 n-grams (for 2<n<10) satisfying T>_10, but WordNet contains only 2610 of these.","{'title': '4 Evaluating Performance', 'number': '4'}"
"It is unclear, therefore, if algorithms are wrong when they propose MWUs that are not in WordNet.","{'title': '4 Evaluating Performance', 'number': '4'}"
We will assume they are wrong but with a special caveat for proper nouns.,"{'title': '4 Evaluating Performance', 'number': '4'}"
WordNet includes few proper noun MWUs.,"{'title': '4 Evaluating Performance', 'number': '4'}"
Yet several algorithms produce large numbers of proper nouns.,"{'title': '4 Evaluating Performance', 'number': '4'}"
This biases against them.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"One could contend that all proper nouns MWUs are valid, but we disagree.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Although such may be MWUs, they are not necessarily MRD headwords; one would not include every proper noun in a dictionary, but rather, those needing definitions.","{'title': '4 Evaluating Performance', 'number': '4'}"
"To overcome this, we will have two scoring modes.","{'title': '4 Evaluating Performance', 'number': '4'}"
"The first, “S” mode (standing for some) discards any proposed capitalized n-gram whose uncapitalized version is not in WordNet.","{'title': '4 Evaluating Performance', 'number': '4'}"
The second mode “N” (for none) disregards all capitalized n-grams.,"{'title': '4 Evaluating Performance', 'number': '4'}"
Table 3 illustrates algorithmic performance as compared to the 2610 MWUs from WordNet.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"The first double column illustrates “out-of-the-box” performance on all 177,331 possible n-grams.","{'title': '4 Evaluating Performance', 'number': '4'}"
"The second double column shows cross-sourcing: only hypothesizing MWUs that appear in at least two separate datasets (124,952 in all), but being evaluated against all of the 2610 valid units.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Double columns 3 and 4 show effects from high-frequency filtering the n-grams of the first and second columns (reporting only 29,716 and 17,720 n-grams) respectively.","{'title': '4 Evaluating Performance', 'number': '4'}"
"As Table 3 suggests, for every condition, the information-like algorithms seem to perform best at identifying valid, general MWU headwords.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Moreover, they are enhanced when cross-sourcing is considered; but since much of their strength comes from identifying proper nouns, filtering has little or even negative impact.","{'title': '4 Evaluating Performance', 'number': '4'}"
"On the other hand, data source.","{'title': '4 Evaluating Performance', 'number': '4'}"
They also improve significantly with filtering.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Overall, though, after the algorithms are judged, even the best score of 0.265 is far short of the maximum possible, namely 1.0.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Since WordNet is static and cannot report on all of a corpus’ n-grams, one may expect different performance by using a more all-encompassing, dynamic resource.","{'title': '4 Evaluating Performance', 'number': '4'}"
The Internet houses dynamic resources which can judge practically every induced n-gram.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"With permission and sufficient time, one can repeatedly query websites that host large collections of MRDs and evaluate each n-gram.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Having approval, we queried: (1) onelook.com, (2) acronymfinder.com, and (3) infoplease.com.","{'title': '4 Evaluating Performance', 'number': '4'}"
The first website interfaces with over 600 electronic dictionaries.,"{'title': '4 Evaluating Performance', 'number': '4'}"
The second is devoted to identifying proper acronyms.,"{'title': '4 Evaluating Performance', 'number': '4'}"
The third focuses on world facts such as historical figures and organization names.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"To minimize disruption to websites by reducing the total number of queries needed for evaluation, we use an evaluation approach from the information retrieval community (Sparck-Jones and van Rijsbergen, 1975).","{'title': '4 Evaluating Performance', 'number': '4'}"
Each algorithm reports its top 5000 MWU choices and the union of these choices (45192 possible n-grams) is looked up on the Internet.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Valid MWUs identified at any website are assumed to be the only valid units in the data. i, the frequency-like approaches are independent of Algorithms are then evaluated based on this showed how one could compute latent semantic collection.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Although this strategy for evaluation is vectors for any word in a corpus (Schone and not flawless, it is reasonable and makes dynamic Jurafsky, 2000).","{'title': '4 Evaluating Performance', 'number': '4'}"
"Using the same approach, we evaluation tractable.","{'title': '4 Evaluating Performance', 'number': '4'}"
"Table 4 shows the algorithms’ compute semantic vectors for every proposed word performance (including proper nouns). n-gram C=X X ...X Since LSA involves word Though Internet dictionaries and WordNet are counts, we can also compute semantic vectors completely separate “gold standards,” results are surprisingly consistent.","{'title': '4 Evaluating Performance', 'number': '4'}"
One can conclude that WordNet may safely be used as a gold standard in future MWU headword evaluations.,"{'title': '4 Evaluating Performance', 'number': '4'}"
"Also, SCP have virtually identical results and seem to best identify MWU headwords (particularly if proper nouns are desired).","{'title': '4 Evaluating Performance', 'number': '4'}"
Yet there is still significant room for improvement.,"{'title': '4 Evaluating Performance', 'number': '4'}"
Can performance be improved?,"{'title': '5 Improvement strategies', 'number': '5'}"
Numerous strategies could be explored.,"{'title': '5 Improvement strategies', 'number': '5'}"
"An idea we discuss here tries using induced semantics to rescore the output of the best algorithm (filtered, cross-sourced Zscore) and eliminate semantically compositional or modifiable MWU hypotheses.","{'title': '5 Improvement strategies', 'number': '5'}"
"Deerwester, et al (1990) introduced Latent Semantic Analysis (LSA) as a computational technique for inducing semantic relationships between words and documents.","{'title': '5 Improvement strategies', 'number': '5'}"
"It forms highdimensional vectors using word counts and uses singular value decomposition to project those vectors into an optimal k-dimensional, “semantic” subspace (see Landauer, et al, 1998).","{'title': '5 Improvement strategies', 'number': '5'}"
"Following an approach from Schütze (1993), we (denoted by SL) for C’s subcomponents.","{'title': '5 Improvement strategies', 'number': '5'}"
These can either include ({X } n) or exclude QX* } n ) C’s counts.,"{'title': '5 Improvement strategies', 'number': '5'}"
We seek to see if induced sema4ics can help eliminate incorrectly-chosen MWUs.,"{'title': '5 Improvement strategies', 'number': '5'}"
"As will be shown, the effort using semantics in this nature has a very small payoff for the expended cost.","{'title': '5 Improvement strategies', 'number': '5'}"
"Non-compositionality is a key component of valid MWUs, so we may desire to emphasize n-grams that are semantically non-compositional.","{'title': '5 Improvement strategies', 'number': '5'}"
Suppose we wanted to determine if C (defined above) were noncompositional.,"{'title': '5 Improvement strategies', 'number': '5'}"
"Then given some meaning function, `I', C should satisfy an equation like: where h combines the semantics of C’s subcomponents and g measures semantic differences.","{'title': '5 Improvement strategies', 'number': '5'}"
"If C were a bigram, then if g(a,b) is defined to be |a-b|, if h(c,d) is the sum of c and d, and if T(e) is set to -log Pe, then equation (1) would become the pointwise mutual information of the bigram.","{'title': '5 Improvement strategies', 'number': '5'}"
"If g(a,b) were defined to be (a-b)/b , and if h(a,b)=ab/N and `I'(X)=fX , we essentially get Zscores.","{'title': '5 Improvement strategies', 'number': '5'}"
These formulations suggest that several of the probabilistic algorithms we have seen include non-compositionality measures already.,"{'title': '5 Improvement strategies', 'number': '5'}"
"However, since the probabilistic algorithms rely only on distributional information obtained by considering juxtaposed words, they tend to incorporate a significant amount of non-semantic information such as syntax.","{'title': '5 Improvement strategies', 'number': '5'}"
Can semantic-only rescoring help?,"{'title': '5 Improvement strategies', 'number': '5'}"
"To find out, we must select g, h, and T. Since we want to eliminate MWUs that are compositional, we want h’s output to correlate well with C when there is compositionality and correlate poorly otherwise.","{'title': '5 Improvement strategies', 'number': '5'}"
"Frequently, LSA vectors are correlated using the cosine between them: A large cosine indicates strong correlation, so large values for g(a,b)=1-|cos(a,b) |should signal weak correlation or non-compositionality. h could represent a weighted vector sum of the components’ required for this task.","{'title': '5 Improvement strategies', 'number': '5'}"
This seems to be a significant semantic vectors with weights (wi) set to either 1.0 component.,"{'title': '5 Improvement strategies', 'number': '5'}"
Yet there is still another: maybe or the reciprocal of the words’ frequencies. semantic compositionality is not always bad.,"{'title': '5 Improvement strategies', 'number': '5'}"
"Table 5 indicates several results using these Interestingly, this is often the case.","{'title': '5 Improvement strategies', 'number': '5'}"
Consider settings.,"{'title': '5 Improvement strategies', 'number': '5'}"
"As the first four rows indicate and as vice_president, organized crime, and desired, non-compositionality is more apparent for Marine_Corps.","{'title': '5 Improvement strategies', 'number': '5'}"
"Although these are MWUs, one 52X* (i.e., the vectors derived from excluding C’s counts) than for 52X.","{'title': '5 Improvement strategies', 'number': '5'}"
"Yet, performance overall is horrible, particularly considering we are rescoring Z-score output whose score was 0.269.","{'title': '5 Improvement strategies', 'number': '5'}"
Rescoring caused five-fold degradation!,"{'title': '5 Improvement strategies', 'number': '5'}"
What happens if we instead emphasize compositionality?,"{'title': '5 Improvement strategies', 'number': '5'}"
Rows 5-8 illustrate the effect: there is a significant recovery in performance.,"{'title': '5 Improvement strategies', 'number': '5'}"
"The most reasonable explanation for this is that if MWUs and their components are strongly correlated, the components may rarely occur except in context with the MWU.","{'title': '5 Improvement strategies', 'number': '5'}"
It takes about 20 hours to compute the 52X* for each possible n-gram combination.,"{'title': '5 Improvement strategies', 'number': '5'}"
"Since the probabilistic algorithms already identify n-grams that share strong distributional properties with their components, it seems imprudent to exhaust resources on this LSAbased strategy for non-compositionality.","{'title': '5 Improvement strategies', 'number': '5'}"
These findings warrant some discussion.,"{'title': '5 Improvement strategies', 'number': '5'}"
Why did non-compositionality fail?,"{'title': '5 Improvement strategies', 'number': '5'}"
"Certainly there is the possibility that better choices for g, h, and T could yield improvements.","{'title': '5 Improvement strategies', 'number': '5'}"
"We actually spent months trying to find an optimal combination as well as a strategy for coupling LSA-based scores with the Zscores, but without avail.","{'title': '5 Improvement strategies', 'number': '5'}"
"Another possibility: although LSA can find semantic relationships, it may not make semantic decisions at the level would still expect that the first is related to president, the second relates to crime, and the last relates to Marine.","{'title': '5 Improvement strategies', 'number': '5'}"
"Similarly, tokens such as Johns_Hopkins and Elvis are anaphors for Johns_Hopkins_University and Elvis_Presley, so they should have similar meanings.","{'title': '5 Improvement strategies', 'number': '5'}"
This begs the question: can induced semantics help at all?,"{'title': '5 Improvement strategies', 'number': '5'}"
The answer is “yes.” The key is using LSA where it does best: finding things that are similar — or substitutable.,"{'title': '5 Improvement strategies', 'number': '5'}"
"For every collocation C=X1X2..Xi-1XiXi+1..Xn, we attempt to find other similar patterns in the data, X1X2..Xi-1YXi+1..Xn.","{'title': '5 Improvement strategies', 'number': '5'}"
"If Xi and Y are semantically related, chances are that C is substitutable.","{'title': '5 Improvement strategies', 'number': '5'}"
"Since LSA excels at finding semantic correlations, we can compare 52Xi and SLY to see if C is substitutable.","{'title': '5 Improvement strategies', 'number': '5'}"
"We use our earlier approach (Schone and Jurafsky, 2000) for performing the comparison; namely, for every word W, we compute cos(52w, SLR) for 200 randomly chosen words, R. This allows for computation of a correlaton mean (µW) and standard deviation (6W) between W and other words.","{'title': '5 Improvement strategies', 'number': '5'}"
"As before, we then compute a normalized cosine score ( ) between words of interest, defined by With this set-up, we now look for substitutivity.","{'title': '5 Improvement strategies', 'number': '5'}"
Note that phrases may be substitutable and still be headword if their substitute phrases are themselves MWUs.,"{'title': '5 Improvement strategies', 'number': '5'}"
"For example, dioxide in carbon_dioxide is semantically similar to monoxide in carbon_monoxide.","{'title': '5 Improvement strategies', 'number': '5'}"
"Moreover, there are other important instances of valid substitutivity: However, guilty and innocent are semantically related, but pleaded_guilty and pleaded_innocent are not MWUs.","{'title': '5 Improvement strategies', 'number': '5'}"
We would like to emphasize only ngrams whose substitutes are valid MWUs.,"{'title': '5 Improvement strategies', 'number': '5'}"
"To show how we do this using LSA, suppose we want to rescore a list L whose entries are potential MWUs.","{'title': '5 Improvement strategies', 'number': '5'}"
"For every entry X in L, we seek out all other entries whose sorted order is less than some maximum value (such as 5000) that have all but one word in common.","{'title': '5 Improvement strategies', 'number': '5'}"
"For example, suppose X is “bachelor_’_s_degree.” The only other entry that matches in all but one word is “master_’_s_degree.” If the semantic vectors for “bachelor” and “master” have a normalized cosine score greater than a threshold of 2.0, we then say that the two MWUs are in each others substitution set.","{'title': '5 Improvement strategies', 'number': '5'}"
"To rescore, we assign a new score to each entry in substitution set.","{'title': '5 Improvement strategies', 'number': '5'}"
Each element in the substitution set gets the same score.,"{'title': '5 Improvement strategies', 'number': '5'}"
The score is derived using a combination of the previous Z-scores for each element in the substitution set.,"{'title': '5 Improvement strategies', 'number': '5'}"
"The combining function may be an averaging, or a computation of the median, the maximum, or something else.","{'title': '5 Improvement strategies', 'number': '5'}"
The maximum outperforms the average and the median on our data.,"{'title': '5 Improvement strategies', 'number': '5'}"
"By applying in to our data, we observe a small but visible improvement of 1.3% absolute to .282 (see Fig.","{'title': '5 Improvement strategies', 'number': '5'}"
1).,"{'title': '5 Improvement strategies', 'number': '5'}"
It is also possible that other improvements could be gained using other combining strategies.,"{'title': '5 Improvement strategies', 'number': '5'}"
This paper identifies several new results in the area of MWU-finding.,"{'title': '6 Conclusions', 'number': '6'}"
We saw that MWU headword evaluations using WordNet provide similar results to those obtained from far more extensive webbased resources.,"{'title': '6 Conclusions', 'number': '6'}"
"Thus, one could safely use WordNet as a gold standard for future evaluations.","{'title': '6 Conclusions', 'number': '6'}"
"We also noted that information-like algorithms, particularly Z-scores, SCP, and x2, seem to perform best at finding MRD headwords regardless of filtering mechanism, but that improvements are still needed.","{'title': '6 Conclusions', 'number': '6'}"
We proposed two new LSA-based approaches which attempted to address issues of non-compositionality and non-substitutivity.,"{'title': '6 Conclusions', 'number': '6'}"
"Apparently, either current algorithms already capture much non-compositionality or LSA-based models of non-compositionality are of little help.","{'title': '6 Conclusions', 'number': '6'}"
LSA does help somewhat as a model of substitutivity.,"{'title': '6 Conclusions', 'number': '6'}"
"However, LSA-based gains are small compared to the effort required to obtain them.","{'title': '6 Conclusions', 'number': '6'}"
The authors would like to thank the anonymous reviewers for their comments and insights.,"{'title': 'Acknowledgments', 'number': '7'}"
