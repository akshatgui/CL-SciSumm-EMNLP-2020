col1,col2
The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.,{}
"Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.",{}
"In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.",{}
"We are fortunate that for this particular application, correctly labeled training data is free.",{}
"Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.",{}
"Machine learning techniques, which automatically learn linguistic information from online text corpora, have been applied to a number of natural language problems throughout the last decade.","{'title': '1 Introduction', 'number': '1'}"
A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.,"{'title': '1 Introduction', 'number': '1'}"
"While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.","{'title': '1 Introduction', 'number': '1'}"
"In part, this is due to the standardization of data sets used within the field, as well as the potentially large cost of annotating data for those learning methods that rely on labeled text.","{'title': '1 Introduction', 'number': '1'}"
"The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets.","{'title': '1 Introduction', 'number': '1'}"
"Yet since we now have access to significantly more data, one has to wonder what conclusions that have been drawn on small data sets may carry over when these learning methods are trained using much larger corpora.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation.","{'title': '1 Introduction', 'number': '1'}"
"In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem.","{'title': '1 Introduction', 'number': '1'}"
First we show learning curves for four different machine learning algorithms.,"{'title': '1 Introduction', 'number': '1'}"
"Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.","{'title': '1 Introduction', 'number': '1'}"
"Confusion set disambiguation is the problem of choosing the correct use of a word, given a set of words with which it is commonly confused.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
Numerous methods have been presented for confusable disambiguation.,"{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"The more recent set of techniques includes multiplicative weightupdate algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"{to,two,too}), all occurrences of confusion set members in the test set are replaced by a marker; everywhere the system sees this marker, it must decide which member of the confusion set to choose.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.,"{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"Other such problems include word sense disambiguation, part of speech tagging and some formulations of phrasal chunking.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
"One advantageous aspect of confusion set disambiguation, which allows us to study the effects of large data sets on performance, is that labeled training data is essentially free, since the correct answer is surface apparent in any collection of reasonably well-edited text.","{'title': '2 Confusion Set Disambiguation', 'number': '2'}"
This work was partially motivated by the desire to develop an improved grammar checker.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Given a fixed amount of time, we considered what would be the most effective way to focus our efforts in order to attain the greatest performance improvement.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Some possibilities included modifying standard learning algorithms, exploring new learning techniques, and using more sophisticated features.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Before exploring these somewhat expensive paths, we decided to first see what happened if we simply trained an existing method with much more data.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"This led to the exploration of learning curves for various machine learning algorithms: winnow1, perceptron, na√Øve Bayes, and a very simple memory-based learner.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"For the first three learners, we used the standard collection of features employed for this problem: the set of words within a window of the target word, and collocations containing words and/or parts of speech.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
The memory-based learner used only the word before and word after as features.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"We collected a 1-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
This training corpus is three orders of magnitude greater than the largest training corpus previously used for this problem.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"We used 1 million words of Wall Street Journal text as our test set, and no data from the Wall Street Journal was used when constructing the training corpus.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Each learner was trained at several cutoff points in the training corpus, i.e. the first one million words, the first five million words, and so on, until all one billion words were used for training.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"In order to avoid training biases that may result from merely concatenating the different data sources to form a larger training corpus, we constructed each consecutive training corpus by probabilistically sampling sentences from the different sources weighted by the size of each source.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"In Figure 1, we show learning curves for each learner, up to one billion words of training data.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
Each point in the graph is the average performance over ten confusion sets for that size training corpus.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
Note that the curves appear to be log-linear even out to one billion words.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Of course for many problems, additional training data has a non-zero cost.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"However, these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"At least for the problem of confusable disambiguation, none of the learners tested is close to asymptoting in performance at the training corpus size commonly employed by the field.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Such gains in accuracy, however, do not come for free.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
Figure 2 shows the size of learned representations as a function of training data size.,"{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"For some applications, this is not necessarily a concern.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"But for others, where space comes at a premium, obtaining the gains that come with a billion words of training data may not be viable without an effort made to compress information.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"In such cases, one could look at numerous methods for compressing data (e.g.","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Dagan and Engleson, 1995, Weng, et al, 1998).","{'title': '3 Learning Curve Expe riments', 'number': '3'}"
"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al, 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000).","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner.,"{'title': '4 The Efficacy of Voting', 'number': '4'}"
"When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
But does voting still offer performance gains when classifiers are trained on much larger corpora?,"{'title': '4 The Efficacy of Voting', 'number': '4'}"
"The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"As training size increases significantly, we would expect complementarity between classifiers to decrease.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
This is due in part to the fact that a larger training corpus will reduce the data set variance and any bias arising from this.,"{'title': '4 The Efficacy of Voting', 'number': '4'}"
"Also, some of the differences between classifiers might be due to how they handle a sparse training set.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"As a result of comparing a sample of two learners as a function of increasingly large training sets, we see in Table 1 that complementarity does indeed decrease as training size increases.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
Next we tested whether this decrease in complementarity meant that voting loses its effectiveness as the training set increases.,"{'title': '4 The Efficacy of Voting', 'number': '4'}"
"To examine the impact of voting when using a significantly larger training corpus, we ran 3 out of the 4 learners on our set of 10 confusable pairs, excluding the memory-based learner.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
Voting was done by combining the normalized score each learner assigned to a classification choice.,"{'title': '4 The Efficacy of Voting', 'number': '4'}"
"In Figure 3, we show the accuracy obtained from voting, along with the single best learner accuracy at each training set size.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"We see that for very small corpora, voting is beneficial, resulting in better performance than any single classifier.","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"Beyond 1 million words, little is gained by voting, and indeed on the","{'title': '4 The Efficacy of Voting', 'number': '4'}"
"While the observation that learning curves are not asymptoting even with orders of magnitude more training data than is currently used is very exciting, this result may have somewhat limited ramifications.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Very few problems exist for which annotated data of this size is available for free.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Surely we cannot reasonably expect that the manual annotation of one billion words along with corresponding parse trees will occur any time soon (but see (Banko and Brill 2001) for a discussion that this might not be completely infeasible).,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Despite this pitfall, there are techniques one can use to try to obtain the benefits of considerably larger training corpora without incurring significant additional costs.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"In the sections that follow, we study two such solutions: active learning and unsupervised learning.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Active learning involves intelligently selecting a portion of samples for annotation from a pool of as-yet unannotated training samples.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Not all samples in a training set are equally useful.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"By concentrating human annotation efforts on the samples of greatest utility to the machine learning algorithm, it may be possible to attain better performance for a fixed annotation cost than if samples were chosen randomly for human annotation.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Most active learning approaches work by first training a seed learner (or family of learners) and then running the learner(s) over a set of unlabeled samples.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
A sample is presumed to be more useful for training the more uncertain its classification label is.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Uncertainty can be judged by the relative weights assigned to different labels by a single classifier (Lewis and Catlett, 1994).","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Another approach, committee-based sampling, first creates a committee of classifiers and then judges classification uncertainty according to how much the learners differ among label assignments.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"For example, Dagan and Engelson (1995) describe a committee-based sampling technique where a part of speech tagger is trained using an annotated seed corpus.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"A family of taggers is then generated by randomly permuting the tagger probabilities, and the disparity among tags output by the committee members is used as a measure of classification uncertainty.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Sentences for human annotation are drawn, biased to prefer those containing high uncertainty instances.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"While active learning has been shown to work for a number of tasks, the majority of active learning experiments in natural language processing have been conducted using very small seed corpora and sets of unlabeled examples.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Therefore, we wish to explore situations where we have, or can afford, a nonnegligible sized training corpus (such as for part-of-speech tagging) and have access to very large amounts of unlabeled data.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We can use bagging (Breiman, 1996), a technique for generating a committee of classifiers, to assess the label uncertainty of a potential training instance.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"With bagging, a variant of the original training set is constructed by randomly sampling sentences with replacement from the source training set in order to produce N new training sets of size equal to the original.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"After the N models have been trained and run on the same test set, their classifications for each test sentence can be compared for classification agreement.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"The higher the disagreement between classifiers, the more useful it would be to have an instance manually labeled.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We used the na√Øve Bayes classifier, creating 10 classifiers each trained on bags generated from an initial one million words of labeled training data.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
We present the active learning algorithm we used below.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Initialize: Training data consists of X words correctly labeled We initially tried selecting the M most uncertain examples, but this resulted in a sample too biased toward the difficult instances.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Instead we pick half of our samples for annotation randomly and the other half from those whose labels we are most uncertain of, as judged by the entropy of the votes assigned to the instance by the committee.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"This is, in effect, biasing our sample toward instances the classifiers are most uncertain of.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
We show the results from sample selection for confusion set disambiguation in Figure 4.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"The line labeled &quot;sequential&quot; shows test set accuracy achieved for different percentages of the one billion word training set, where training instances are taken at random.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We ran three active learning experiments, increasing the size of the total unlabeled training corpus from which we can pick samples to be annotated.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"In all three cases, sample selection outperforms sequential sampling.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"At the endpoint of each training run in the graph, the same number of samples has been annotated for training.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"However, we see that the larger the pool of candidate instances for annotation is, the better the resulting accuracy.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"By increasing the pool of unlabeled training instances for active learning, we can improve accuracy with only a fixed additional annotation cost.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Thus it is possible to benefit from the availability of extremely large corpora without incurring the full costs of annotation, training time, and representation size.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"While the previous section shows that we can benefit from substantially larger training corpora without needing significant additional manual annotation, it would be ideal if we could improve classification accuracy using only our seed annotated corpus and the large unlabeled corpus, without requiring any additional hand labeling.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
In this section we turn to unsupervised learning in an attempt to achieve this goal.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated, e.g.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Marialdo's HMM part-of-speech tagger training (1994), Charniak's parser retraining experiment (1996), Yarowsky's seeds for word sense disambiguation (1995) and Nigam et al's (1998) topic classifier learned in part from unlabelled documents.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
A nice discussion of this general problem can be found in Mitchell (1999).,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
The question we want to answer is whether there is something to be gained by combining unsupervised and supervised learning when we scale up both the seed corpus and the unlabeled corpus significantly.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We can again use a committee of bagged classifiers, this time for unsupervised learning.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Whereas with active learning we want to choose the most uncertain instances for human annotation, with unsupervised learning we want to choose the instances that have the highest probability of being correct for automatic labeling and inclusion in our labeled training data.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"In Table 2, we show the test set accuracy (averaged over the four most frequently occurring confusion pairs) as a function of the number of classifiers that agree upon the label of an instance.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"For this experiment, we trained a collection of 10 na√Øve Bayes classifiers, using bagging on a 1-millionword seed corpus.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"As can be seen, the greater the classifier agreement, the more likely it is that a test sample has been correctly labeled.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Since the instances in which all bags agree have the highest probability of being correct, we attempted to automatically grow our labeled training set using the 1-million-word labeled seed corpus along with the collection of na√Øve Bayes classifiers described above.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"All instances from the remainder of the corpus on which all 10 classifiers agreed were selected, trusting the agreed-upon label.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
The classifiers were then retrained using the labeled seed corpus plus the new training material collected automatically during the previous step.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"In both cases we gain from unsupervised training compared to using only the seed corpus, but only up to a point.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"At this point, test set accuracy begins to decline as additional training instances are automatically harvested.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We are able to attain improvements in accuracy for free using unsupervised learning, but unlike our learning curve experiments using correctly labeled data, accuracy does not continue to improve with additional data.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"Charniak (1996) ran an experiment in which he trained a parser on one million words of parsed data, ran the parser over an additional 30 million words, and used the resulting parses to reestimate model probabilities.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Doing so gave a small improvement over just using the manually parsed data.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"We repeated this experiment with our data, and show the outcome in Table 4.","{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
Choosing only the labeled instances most likely to be correct as judged by a committee of classifiers results in higher accuracy than using all instances classified by a model trained with the labeled seed corpus.,"{'title': '5 When Annotated Data Is Not Free', 'number': '5'}"
"In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.","{'title': 'Methods', 'number': '6'}"
This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.,"{'title': 'Methods', 'number': '6'}"
It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.,"{'title': 'Methods', 'number': '6'}"
"In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.","{'title': '6 Conclusions', 'number': '7'}"
"We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.","{'title': '6 Conclusions', 'number': '7'}"
"We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.","{'title': '6 Conclusions', 'number': '7'}"
"We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.","{'title': '6 Conclusions', 'number': '7'}"
"While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.","{'title': '6 Conclusions', 'number': '7'}"
