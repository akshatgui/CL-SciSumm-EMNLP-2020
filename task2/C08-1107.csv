col1,col2
"Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.",{}
In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.,{}
The results show that the learned unary rule-sets outperform the binary rule-set.,{}
"In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.",{}
"In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.","{'title': 'Introduction', 'number': '1'}"
"For example, a QA system has to deduce that ?SCO sued IBM?","{'title': 'Introduction', 'number': '1'}"
is inferred from ?SCO won a lawsuit against IBM?,"{'title': 'Introduction', 'number': '1'}"
to answer ?Whom did SCO sue??.,"{'title': 'Introduction', 'number': '1'}"
"This type of reasoning has been identified as a core semanticinference paradigm by the generic Textual Entail ment framework (Giampiccolo et al, 2007).","{'title': 'Introduction', 'number': '1'}"
An important type of knowledge needed for such inference is entailment rules.,"{'title': 'Introduction', 'number': '1'}"
"An entailmentrule specifies a directional inference relation be tween two templates, text patterns with variables, such as ?X win lawsuit against Y ? X sue Y ?.","{'title': 'Introduction', 'number': '1'}"
Applying this rule by matching ?X win lawsuit against Y ? in the above text allows a QA system to c ? 2008.,"{'title': 'Introduction', 'number': '1'}"
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).,"{'title': 'Introduction', 'number': '1'}"
"Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question.","{'title': 'Introduction', 'number': '1'}"
"Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.","{'title': 'Introduction', 'number': '1'}"
"(Romano et al, 2006).","{'title': 'Introduction', 'number': '1'}"
One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet).,"{'title': 'Introduction', 'number': '1'}"
Supervised learning of broad coverage rule-sets is an arduous task.,"{'title': 'Introduction', 'number': '1'}"
This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.,"{'title': 'Introduction', 'number': '1'}"
"(Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).","{'title': 'Introduction', 'number': '1'}"
"Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).","{'title': 'Introduction', 'number': '1'}"
"However, a predicate quite of ten appears in the text with just a single variable(e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ?X take a nap?X sleep?","{'title': 'Introduction', 'number': '1'}"
(further motivations in Section 3.1).In this paper we focus on unsupervised learning of unary entailment rules.,"{'title': 'Introduction', 'number': '1'}"
Two learning ap proaches are proposed.,"{'title': 'Introduction', 'number': '1'}"
"In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpusare.","{'title': 'Introduction', 'number': '1'}"
"In addition to adapting state-of-the-art similar ity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity.","{'title': 'Introduction', 'number': '1'}"
"In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods.","{'title': 'Introduction', 'number': '1'}"
"We tested the various unsupervised unary rule 849learning methods, as well as a binary rule learn ing method, on a test set derived from a standard IE benchmark.","{'title': 'Introduction', 'number': '1'}"
This provides the first comparisonbetween the performance of unary and binary rule sets.,"{'title': 'Introduction', 'number': '1'}"
"Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods interms of F1 measure.","{'title': 'Introduction', 'number': '1'}"
"Moreover, only BalancedInclusion improved F1 score over a baseline infer ence that does not use entailment rules at all .","{'title': 'Introduction', 'number': '1'}"
"This section reviews relevant distributional simi larity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning.","{'title': 'Background. ', 'number': '2'}"
"Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954).","{'title': 'Background. ', 'number': '2'}"
"Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ].","{'title': 'Background. ', 'number': '2'}"
Weeds and Weir (2003) proposed to measure thesymmetric similarity between two words by av eraging two directional (asymmetric) scores: the coverage of each word?s features by the other.,"{'title': 'Background. ', 'number': '2'}"
"The coverage of u by v is measured by: Cover(u, v) = ? f?F u ?F v w u (f) ? f?F u w u (f) The average can be arithmetic or harmonic: WeedsA(u, v) = 1 2 [Cover(u, v) + Cover(v, u)] WeedsH(u, v) = 2 ? Cover(u, v) ? Cover(v, u) Cover(u, v) + Cover(v, u) Weeds et al also used pmi for feature weights.","{'title': 'Background. ', 'number': '2'}"
"Binary rule learning algorithms adopted suchlexical similarity approaches for learning rules between templates, where the features of each tem plate are its variable instantiations in a corpus, such as {X=?SCO?, Y =?IBM?}","{'title': 'Background. ', 'number': '2'}"
for the example in Section 1.,"{'title': 'Background. ', 'number': '2'}"
"Some works focused on learningrules from comparable corpora, containing com parable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al, 2003).","{'title': 'Background. ', 'number': '2'}"
"Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variableinstantiations are shared across comparable docu ments the same predicates are described.","{'title': 'Background. ', 'number': '2'}"
"However,it is hard to collect broad-scale comparable cor pora, as the majority of texts are non-comparable.","{'title': 'Background. ', 'number': '2'}"
"A complementary approach is learning from the abundant regular, non-comparable, corpora.","{'title': 'Background. ', 'number': '2'}"
"Yet,in such corpora it is harder to recognize varia tions of the same predicate.","{'title': 'Background. ', 'number': '2'}"
"The DIRT algorithm(Lin and Pantel, 2001) learns non-directional binary rules for templates that are paths in a depen dency parse-tree between two noun variables X and Y . The similarity between two templates t and t ? is the geometric average: DIRT (t, t ? ) = ? Lin x (t, t ? ) ? Lin y (t, t ? ) where Lin xis the Lin similarity between X?s in stantiations of t and X?s instantiations of t ? in a corpus (equivalently for Lin y ).","{'title': 'Background. ', 'number': '2'}"
"Some workstake the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =?SCO-IBM?}, and com pare between these complex features of t and t ?","{'title': 'Background. ', 'number': '2'}"
"(Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother.","{'title': 'Background. ', 'number': '2'}"
"However, entailment is in general a direc tional relation.","{'title': 'Background. ', 'number': '2'}"
"For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.","{'title': 'Background. ', 'number': '2'}"
"(Weeds and Weir, 2003) propose a directional measure for learning hyponymy between twowords, ?l? r?, by giving more weight to the cov erage of the features of l by r (with ? > 1 2 ): WeedsD(l, r)=?Cover(l, r)+(1??)Cover(r, l) When ?=1, this measure degenerates into Cover(l, r), termed Precision(l, r).","{'title': 'Background. ', 'number': '2'}"
"With 850 Precision(l, r) we obtain a ?soft?","{'title': 'Background. ', 'number': '2'}"
"version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the ?important?","{'title': 'Background. ', 'number': '2'}"
"features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al, 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem plates.","{'title': 'Background. ', 'number': '2'}"
"However, (Shinyama et al, 2002) relies on comparable corpora for identifying paraphrasesand simply takes any two templates from comparable sentences that share a named entity instan tiation to be paraphrases.","{'title': 'Background. ', 'number': '2'}"
Such approach is notfeasible for non-comparable corpora where statis tical measurement is required.,"{'title': 'Background. ', 'number': '2'}"
"(Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored).","{'title': 'Background. ', 'number': '2'}"
"In addition, their template structure islimited to only verbs and their direct syntactic ar guments, which may yield incorrect rules, e.g. forlight verbs (see Section 5.2).","{'title': 'Background. ', 'number': '2'}"
"To overcome this limitation, we use a more expressive template struc ture.","{'title': 'Background. ', 'number': '2'}"
3.1 Motivations.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
Most unsupervised rule learning algorithms focused on learning binary entailment rules.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"How ever, using binary rules for inference is not enough.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"First, a predicate that can have multiple arguments may still occur with only one of its arguments.For example, in ?The acquisition of TCA was successful?, ?TCA?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
is the only argument of ?acqui sition?.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Second, some predicate expressions are unary by nature.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"For example, modifiers, such as ?the elected X?, or intransitive verbs.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.For these reasons, it seems that unary rule learn ing should be addressed in addition to binary rule learning.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
We first describe the structure of unarytemplates and then explore two conceivable approaches for learning unary rules.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
The first ap proach directly assesses the relation between twogiven templates based on the similarity of their in stantiations in the corpus.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"The second approach,which was also mentioned in (Iftene and Balahur Dobrescu, 2007), derives unary rules from learned binary rules.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
3.2 Unary Template Structure.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
To learn unary rules we first need to define theirstructure.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
In this paper we work at the syntac tic representation level.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodesas variables (Sudo et al, 2003).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"However, the num ber of possible templates is exponential in the sizeof the sentence.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"In the binary rule learning litera ture, the main solution for exhaustively learning allrules between any pair of templates in a given corpus is to restrict the structure of templates.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Typi cally, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al, 2003).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"How ever, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"For example, the combination of ?X call indictable?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
and ?call Y indictable?,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"is the template ?X call Y indictable?, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
Traverse the path from v towards the root of.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
the parse tree.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Whenever a candidate pred icate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"We stop when the first verb orclause boundary (e.g. a relative clause) is encountered, which typically represent the syn tactic boundary of a specific predicate.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
851 2.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
To enable templates with control verbs and.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"light verbs, e.g. ?X help preventing?, ?Xmake noise?, whenever a verb is encountered we generate templates that are paths between v and the verb?s modifiers, either ob jects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
3.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
To capture noun modifiers that act as predi-.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"cates, e.g. ?the losingX?, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"We use the Catvar database to identify verb derivations (Habash and Dorr, 2003).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"As an example for the procedure, the templates extracted from the sentence ?The losing party played it safe?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
with ?party?,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"as the variable are: ?losing X?, ?X play?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
and ?X play safe?.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
3.3 Direct Learning of Unary Rules.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
We applied the lexical similarity measures pre sented in Section 2 for unary rule learning.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature?s weight.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"We first adaptedDIRT for unary templates (unary-DIRT, apply ing Lin-similarity to the single feature vector), as well as its output filtering by LEDIR.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"The various Weeds measures were also applied 1 : symmetric arithmetic average, symmetric harmonic average, weighted arithmetic average and Precision.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
This behav ior generates high-score incorrect rules.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Based on this analysis, we propose a new measure that balances the two behaviors, termed 1We applied the best performing parameter values pre sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"BInc identifies entail ing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BInc(l, r) = ? Lin(l, r) ? Precision(l, r) 3.4 Deriving Unary Rules From Binary Rules.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
An alternative way to learn unary rules is to first learn binary entailment rules and then derive unary rules from them.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
We derive unary rules from a given binary rule-base in two steps.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"For example, from ?X find solu tion to Y ? X solve Y ? we generate the unary rules ?X find?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"X solve?, ?X find solution?","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
The score of each generated rule is set to be the score of the original binary rule.The same unary rule can be derived from different binary rules.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"For example, ?hire Y ? employ Y ? is derived both from ?X hire Y ? X em ploy Y ? and ?hire Y for Z ? employ Y for Z?, having a different score from each original binary rule.","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
The second step of the algorithm aggregates the different scores yielded for each derived rule to produce the final rule score.,"{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"Three aggregationfunctions were tested: sum (Derived-Sum), aver age (Derived-Avg) and maximum (Derived-Max).","{'title': 'Learning Unary Entailment Rules. ', 'number': '3'}"
"We want to evaluate learned unary and binary rule bases by their utility for NLP applications throughassessing the validity of inferences that are per formed in practice using the rule base.To perform such experiments, we need a test set of seed templates, which correspond to a set of target predicates, and a corpus annotated with allargument mentions of each predicate.","{'title': 'Experimental Setup. ', 'number': '4'}"
"The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application).","{'title': 'Experimental Setup. ', 'number': '4'}"
"Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.","{'title': 'Experimental Setup. ', 'number': '4'}"
"This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce.","{'title': 'Experimental Setup. ', 'number': '4'}"
"2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate.","{'title': 'Experimental Setup. ', 'number': '4'}"
"ACE guidelines specify for each event its possible arguments, each associated with a semantic role.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For instance, some of the Injure event arguments are Agent, Victim and Time.To utilize the ACE dataset for evaluating entail ment rule applications, we manually represented each ACE event predicate by unary seed templates.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For example, the seed templates for Injure are ?A injure?, ?injure V ? and ?injure in T ?.","{'title': 'Experimental Setup. ', 'number': '4'}"
"We mapped each event role annotation to the corresponding seed template variable, e.g. ?Agent?","{'title': 'Experimental Setup. ', 'number': '4'}"
to A and ?Victim?,"{'title': 'Experimental Setup. ', 'number': '4'}"
to V in the above example.,"{'title': 'Experimental Setup. ', 'number': '4'}"
"Templatesare matched using a syntactic matcher that han dles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007).","{'title': 'Experimental Setup. ', 'number': '4'}"
A rule application is considered correct if the matched argument is annotated by the corresponding ACE role.,"{'title': 'Experimental Setup. ', 'number': '4'}"
"For testing binary rule-bases, we automatically generated binary seed templates from any twounary seeds that share the same predicate.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For ex ample, for Injure the binary seeds ?A injure V ?, ?A injure in T ? and ?injure V in T ? were automatically generated from the above unary seeds.","{'title': 'Experimental Setup. ', 'number': '4'}"
We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.,"{'title': 'Experimental Setup. ', 'number': '4'}"
"First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.","{'title': 'Experimental Setup. ', 'number': '4'}"
"Thus, four events that correspond ambiguously tomultiple distinct predicates were ignored.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For instance, the Transfer-Money event refers to both do nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem plate.","{'title': 'Experimental Setup. ', 'number': '4'}"
"We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu ment mentions).","{'title': 'Experimental Setup. ', 'number': '4'}"
"Additionally, we regard all entailing mentions under the textual entailment definition as correct.","{'title': 'Experimental Setup. ', 'number': '4'}"
"However, event mentions are annotated as correct in ACE only if they explicitly describe the targetevent.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For instance, a Divorce mention does entail a preceding marriage event but it does not ex plicitly describe it, and thus it is not annotated as a Marry event.","{'title': 'Experimental Setup. ', 'number': '4'}"
"To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.We note that each argument was considered sep arately.","{'title': 'Experimental Setup. ', 'number': '4'}"
"For example, we marked a mention of a divorced person as entailing the marriage of that person, but did not consider the place and time of the divorce act to be those of the marriage .","{'title': 'Experimental Setup. ', 'number': '4'}"
"We implemented the unary rule learning algo rithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001).","{'title': 'Results and Analysis. ', 'number': '5'}"
"We executed each method over the Reuters RCV1 corpus 3 , learning for each template r in the corpus the top100 rules in which r is entailed by another tem plate l, ?l? r?.","{'title': 'Results and Analysis. ', 'number': '5'}"
"All rules were learned in canonical form (Szpektor and Dagan, 2007).","{'title': 'Results and Analysis. ', 'number': '5'}"
The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules.,"{'title': 'Results and Analysis. ', 'number': '5'}"
The performance of each acquired rule-base was measured for each ACE event.,"{'title': 'Results and Analysis. ', 'number': '5'}"
We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentionsextracted for the event (precision).,"{'title': 'Results and Analysis. ', 'number': '5'}"
"We also mea sured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types.","{'title': 'Results and Analysis. ', 'number': '5'}"
"No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex tracted only by direct matching of seed templates.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching).","{'title': 'Results and Analysis. ', 'number': '5'}"
"It is thus useful for comparing different rule-bases,though the absolute extraction figures do not re flect the full potential of the rules.","{'title': 'Results and Analysis. ', 'number': '5'}"
In Secion 5.2 we analyze the full-system?s errors to isolate the rules?,"{'title': 'Results and Analysis. ', 'number': '5'}"
contribution to overall system performance.,"{'title': 'Results and Analysis. ', 'number': '5'}"
5.1 Results.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg.","{'title': 'Results and Analysis. ', 'number': '5'}"
"We omitted the results of methods that were clearly inferior to others: (a) WeedsA, WeedsD and Weeds-Precision did not increase 3 http://about.reuters.com/researchandstandards/corpus/ 853Recall over not using rules because rules with in frequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points.","{'title': 'Results and Analysis. ', 'number': '5'}"
"First, we observe that even when matching only the seed tem plates (K=0), unary seeds outperform the binary seeds in terms of both Precision and Recall.","{'title': 'Results and Analysis. ', 'number': '5'}"
This surprising behavior is consistent through all rulecutoff points: all unary learning algorithms per form better than binary DIRT in all parameters.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"The inferior behavior of binary DIRT is analyzed in Section 5.2.The graphs show that symmetric unary approaches substantially increase recall, but dramati cally decrease precision already at the top 10 rules.","{'title': 'Results and Analysis. ', 'number': '5'}"
"As a result, F1 only decreases for these methods.","{'title': 'Results and Analysis. ', 'number': '5'}"
Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors.,"{'title': 'Results and Analysis. ', 'number': '5'}"
They consistently outperform Derived-Avg.,"{'title': 'Results and Analysis. ', 'number': '5'}"
One reason for this is that incorrectunary rules may be derived even from correct bi nary rules.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"For example, from ?X gain seat on Y ? elect X to Y ? the incorrect unary rule ?X gain?","{'title': 'Results and Analysis. ', 'number': '5'}"
electX?,"{'title': 'Results and Analysis. ', 'number': '5'}"
is also generated.,"{'title': 'Results and Analysis. ', 'number': '5'}"
This problem is less frequent when unary rules are directly scored based on their corpus statistics.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms.","{'title': 'Results and Analysis. ', 'number': '5'}"
"As a result, it is the only algo rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20.","{'title': 'Results and Analysis. ', 'number': '5'}"
"BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10).","{'title': 'Results and Analysis. ', 'number': '5'}"
"We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2).","{'title': 'Results and Analysis. ', 'number': '5'}"
This is reflected in the very low precision of the other methods.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue?","{'title': 'Results and Analysis. ', 'number': '5'}"
"and ?X take wife ? X marry?.When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated arFigure 1: Average Precision, Recall and F1 at dif ferent top K rule cutoff points.","{'title': 'Results and Analysis. ', 'number': '5'}"
"guments), binary rules either miss that mention, orextract both the correct argument and another in correct one.","{'title': 'Results and Analysis. ', 'number': '5'}"
"To neutralize this bias, we also testedthe various methods only on event mentions an notated with two or more arguments and obtained similar results to those presented for all mentions.","{'title': 'Results and Analysis. ', 'number': '5'}"
This further emphasizes the general advantage of using unary rules over binary rules.,"{'title': 'Results and Analysis. ', 'number': '5'}"
854 5.2 Analysis.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions.","{'title': 'Results and Analysis. ', 'number': '5'}"
"We manually classified each rule ?l ? r? as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam ple, learning ?X decide?","{'title': 'Results and Analysis. ', 'number': '5'}"
X meet?,"{'title': 'Results and Analysis. ', 'number': '5'}"
"instead of ?X decide to meet ? X meet?; (e) Incorrect - other incorrect rules, e.g. ?charge X ? convict X?.Table 1 summarizes the analysis and demonstrates two problems of binary-DIRT.","{'title': 'Results and Analysis. ', 'number': '5'}"
"First, rela tive to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extractedmany more incorrect mentions for the same num ber of incorrect rules.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side.","{'title': 'Results and Analysis. ', 'number': '5'}"
Such rules are leaned because many binary templates have a more complex structure than paths between arguments.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules.","{'title': 'Results and Analysis. ', 'number': '5'}"
"For example, BInc learned?take Y into custody ? arrest Y ? while binary DIRT learned ?X take Y ? X arrest Y ?.","{'title': 'Results and Analysis. ', 'number': '5'}"
"System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac tions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K=20), by sampling 200 extractions of each type.","{'title': 'Results and Analysis. ', 'number': '5'}"
From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules.,"{'title': 'Results and Analysis. ', 'number': '5'}"
The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates.,"{'title': 'Results and Analysis. ', 'number': '5'}"
All learning methods suffer from these issues.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"As wasshown by our results, BInc provides a first step to wards reducing these problems.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Yet, these issues require further research.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of theerrors.","{'title': 'Results and Analysis. ', 'number': '5'}"
Context mismatches occur when the entail ing template is matched in inappropriate contexts.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"For example, ?slam X ? attack X?","{'title': 'Results and Analysis. ', 'number': '5'}"
"should not be applied when X is a ball, only when it is a person.","{'title': 'Results and Analysis. ', 'number': '5'}"
"The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision.","{'title': 'Results and Analysis. ', 'number': '5'}"
Binary DIRT Balanced Inclusion Correct 16 (70) 38 (91) Partial Template 27 (2665) 6 (81) Incorrect 157 (2584) 156 (787) Total 200 (5319) 200 (959) Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions.,"{'title': 'Results and Analysis. ', 'number': '5'}"
The corre sponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses.,"{'title': 'Results and Analysis. ', 'number': '5'}"
Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20.,"{'title': 'Results and Analysis. ', 'number': '5'}"
Reason % mentions Rule not learned 61.5 Match error 25.0 Discourse analysis needed 12.0 Argument is predicative 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K=20.,"{'title': 'Results and Analysis. ', 'number': '5'}"
Table 3 presents the analysis of false negatives.,"{'title': 'Results and Analysis. ', 'number': '5'}"
"First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary discourse analysis.","{'title': 'Results and Analysis. ', 'number': '5'}"
"Thus, a recall upper bound for en tailment rules is 88%.","{'title': 'Results and Analysis. ', 'number': '5'}"
Many missed extractions aredue to rules that were not learned (61.5%).,"{'title': 'Results and Analysis. ', 'number': '5'}"
"How ever, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules.","{'title': 'Results and Analysis. ', 'number': '5'}"
"By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.","{'title': 'Results and Analysis. ', 'number': '5'}"
We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.,"{'title': 'Conclusions. ', 'number': '6'}"
"In the first approach, rules are directly learned based on distributionalsimilarity measures.","{'title': 'Conclusions. ', 'number': '6'}"
The second approach de rives unary rules from a given rule-base of binary rules.,"{'title': 'Conclusions. ', 'number': '6'}"
"Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.","{'title': 'Conclusions. ', 'number': '6'}"
We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.,"{'title': 'Conclusions. ', 'number': '6'}"
Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.,"{'title': 'Conclusions. ', 'number': '6'}"
"In addition, the Balanced-Inclusion measure outperformed all other tested methods.","{'title': 'Conclusions. ', 'number': '6'}"
"In future work, we plan to explore additional unary template structures and similarity scores, and to improve rule application utilizing context matching methods such as (Szpektor et al, 2008).","{'title': 'Conclusions. ', 'number': '6'}"
"Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).","{'title': 'Conclusions. ', 'number': '6'}"
