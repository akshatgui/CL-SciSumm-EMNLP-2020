col1,col2
This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.,{}
"Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.",{}
The resulting parser is shown to be the bestperforming system so far in a database query domain.,{}
"Originally developed as a theory of compiling programming languages (Aho and Ullman, 1972), synchronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL).","{'title': '1 Introduction', 'number': '1'}"
"In generating multiple parse trees in a single derivation, synchronous grammars are ideal for modeling syntax-based translation because they describe not only the hierarchical structures of a sentence and its translation, but also the exact correspondence between their sub-parts.","{'title': '1 Introduction', 'number': '1'}"
"Among the grammar formalisms successfully put into use in syntaxbased SMT are synchronous context-free grammars (SCFG) (Wu, 1997) and synchronous treesubstitution grammars (STSG) (Yamada and Knight, 2001).","{'title': '1 Introduction', 'number': '1'}"
"Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006).","{'title': '1 Introduction', 'number': '1'}"
"Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence.","{'title': '1 Introduction', 'number': '1'}"
"In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL).","{'title': '1 Introduction', 'number': '1'}"
"Our algorithm, WASP, uses statistical models developed for syntax-based SMT for lexical learning and parse disambiguation.","{'title': '1 Introduction', 'number': '1'}"
The result is a robust semantic parser that gives good performance in various domains.,"{'title': '1 Introduction', 'number': '1'}"
"More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007).","{'title': '1 Introduction', 'number': '1'}"
"Currently, the use of learned synchronous grammars in semantic parsing and NL generation is limited to simple MRLs that are free of logical variables.","{'title': '1 Introduction', 'number': '1'}"
This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables.,"{'title': '1 Introduction', 'number': '1'}"
"This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005).","{'title': '1 Introduction', 'number': '1'}"
"For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)).","{'title': '1 Introduction', 'number': '1'}"
"However, development of such a functional language is non-trivial, and as we will see, logical languages can be more appropriate for certain domains.","{'title': '1 Introduction', 'number': '1'}"
"On the other hand, most existing methods for mapping NL sentences to logical forms involve substantial hand-written components that are difficult to maintain (Joshi and Vijay-Shanker, 2001; Bayer et al., 2004; Bos, 2005).","{'title': '1 Introduction', 'number': '1'}"
"Zettlemoyer and Collins (2005) present a statistical method that is considerably more robust, but it still relies on hand-written rules for lexical acquisition, which can create a performance bottleneck.","{'title': '1 Introduction', 'number': '1'}"
"In this work, we show that methods developed for SMT can be brought to bear on tasks where logical forms are involved, such as semantic parsing.","{'title': '1 Introduction', 'number': '1'}"
"In particular, we extend the WASP semantic parsing algorithm by adding variable-binding λ-operators to the underlying SCFG.","{'title': '1 Introduction', 'number': '1'}"
"The resulting synchronous grammar generates logical forms using λ-calculus (Montague, 1970).","{'title': '1 Introduction', 'number': '1'}"
A semantic parser is learned given a set of sentences and their correct logical forms using SMT methods.,"{'title': '1 Introduction', 'number': '1'}"
"The new algorithm is called λWASP, and is shown to be the best-performing system so far in the GEOQUERY domain.","{'title': '1 Introduction', 'number': '1'}"
"In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996).","{'title': '2 Test Domain', 'number': '2'}"
The query language consists of logical forms augmented with meta-predicates for concepts such as smallest and count.,"{'title': '2 Test Domain', 'number': '2'}"
Figure 1 shows two sample logical forms and their English glosses.,"{'title': '2 Test Domain', 'number': '2'}"
"Throughout this paper, we use the notation x1, x2,... for logical variables.","{'title': '2 Test Domain', 'number': '2'}"
"Although Prolog logical forms are the main focus of this paper, our algorithm makes minimal assumptions about the target MRL.","{'title': '2 Test Domain', 'number': '2'}"
The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms).,"{'title': '2 Test Domain', 'number': '2'}"
"Figure 2(a) shows a sample parse tree of a logical form, where each CFG production corresponds to a subformula.","{'title': '2 Test Domain', 'number': '2'}"
"Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"In WASP, each SCFG production has the following form: where α is an NL phrase and β is the MR translation of α.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Both α and β are strings of terminal and nonterminal symbols.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Each non-terminal in α appears in β exactly once.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
We use indices to show the correspondence between non-terminals in α and β.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"All derivations start with a pair of co-indexed start symbols, (S1 , S1 ).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Each step of a derivation involves the rewriting of a pair of co-indexed non-terminals by the same SCFG production.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"The yield of a derivation is a pair of terminal strings, (e, f), where e is an NL sentence and f is the MR translation of e. For convenience, we call an SCFG production a rule throughout this paper.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
The problem is that WASP lacks a principled mechanism for handling logical variables.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"In this work, we extend the WASP algorithm by adding a variable-binding mechanism based on λ-calculus, which allows for compositional semantics for logical forms.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"This work is based on an extended version of SCFG, which we call λ-SCFG, where each rule has the following form: where α is an NL phrase and β is the MR translation of α.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Unlike (1), β is a string of terminals, non-terminals, and logical variables.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"The variable-binding operator λ binds occurrences of the logical variables x1, ... , xk in β, which makes λx1 ... λxk.β a λ-function of arity k. When applied to a list of arguments, (xi1, ... , xik), the λfunction gives βσ, where σ is a substitution operator, {x1/xi1,..., xk/xik}, that replaces all bound occurrences of xj in β with xij.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"If any of the arguments xij appear in β as a free variable (i.e. not bound by any λ), then those free variables in β must be renamed before function application takes place.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Each non-terminal Aj in β is followed by a list of arguments, xj _ (xj1, ... , xjkj ).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"During parsing, Aj must be rewritten by a λ-function fj of arity kj.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Like SCFG, a derivation starts with a pair of co-indexed start symbols and ends when all nonterminals have been rewritten.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"To compute the yield of a derivation, each fj is applied to its corresponding arguments xj to obtain an MR string free of λoperators with logical variables properly named.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"As a concrete example, Figure 2(b) shows an MR parse tree that corresponds to the English parse, [What is the [smallest [state] [by area]]], based on the A-SCFG rules in Figure 3.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"To compute the yield of this MR parse tree, we start from the leaf nodes: apply Ax1.state(x1) to the argument (x1), and Ax1.Ax2.area(x1,x2) to the arguments (x1, x2).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"This results in two MR strings: state(x1) and area(x1,x2).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Substituting these MR strings for the FORM nonterminals in the parent node gives the A-function Ax1.smallest(x2,(state(x1),area(x1,x2))).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Applying this A-function to (x1) gives the MR string smallest(x2,(state(x1),area(x1,x2))).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Substituting this MR string for the FORM nonterminal in the grandparent node in turn gives the logical form in Figure 1(a).,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"This is the yield of the MR parse tree, since the root node of the parse tree is reached.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Given a set of training sentences paired with their correct logical forms, {(ei, fi)}, the main learning task is to find a A-SCFG, G, that covers the training data.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"We use the K = 5 most probable word alignments for the training set given by GIZA++ (Och and Ney, 2003), with variable names ignored to reduce sparsity.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Rules are then extracted from each word alignment as follows.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"To ground our discussion, we use the word alignment in Figure 4 as an example.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"To represent the logical form in Figure 4, we use its linearized parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Figure 2(a)).,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Since the MRL grammar is unambiguous, every logical form has a unique linearized parse.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"We assume the alignment to be n-to-1, where each word is linked to at most one MRL production.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Rules are extracted in a bottom-up manner, starting with MRL productions at the leaves of the MR parse tree, e.g.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
FORM —* state(x1) in Figure 2(a).,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Given an MRL production, A —* Q, a rule A —* (α, Axi1 ... Axik.Q) is extracted such that: (1) α is the NL phrase linked to the MRL production; (2) xi1, ... , xik are the logical variables that appear in Q and outside the current leaf node in the MR parse tree.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"If xi1, ... , xik were not bound by A, they would become free variables in Q, subject to renaming during function application (and therefore, invisible to the rest of the logical form).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"For example, since x1 is an argument of the state predicate as well as answer and area, x1 must be bound (cf. the corresponding tree node in Figure 2(b)).","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
The rule extracted for the state predicate is shown in Figure 3.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
The case for the internal nodes of the MR parse tree is similar.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Given an MRL production, A —* Q, where Q contains non-terminals A1, ... , An, a rule A —* (α, Axi1 ... Axik.Q′) is extracted such that: (1) α is the NL phrase linked to the MRL production, with non-terminals A1, ... , An showing the positions of the argument strings; (2) Q′ is Q with each non-terminal Aj replaced with Aj(xj1, ... , xjkj ), where xj1, ... , xjkj are the bound variables in the A-function used to rewrite Aj; (3) xi1, ... , xik are the logical variables that appear in Q′ and outside the current MR sub-parse.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"For example, see the rule extracted for the smallest predicate in Figure 3, where x2 is an argument of smallest, but it does not appear outside the formula smallest(...), so x2 need not be bound by A.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"On the other hand, x1 appears in Q′, and it appears outside smallest(...) (as an argument of answer), so x1 must be bound.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Rule extraction continues in this manner until the root of the MR parse tree is reached.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"Figure 3 shows all the rules extracted from Figure 4.1 Since the learned A-SCFG can be ambiguous, a probabilistic model is needed for parse disambiguation.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
The output MR is the yield of the most probable derivation according to this model.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
Parameter estimation involves maximizing the conditional log-likelihood of the training set.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
"For each rule, r, there is a feature that returns the number of times r is used in a derivation.","{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
More features will be introduced in Section 5.,"{'title': '3 The Semantic Parsing Algorithm', 'number': '3'}"
We have described the A-WASP algorithm which generates logical forms based on A-calculus.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"While reasonably effective, it can be improved in several ways.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"In this section, we focus on improving lexical acquisition.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"To see why the current lexical acquisition algorithm can be problematic, consider the word alignment in Figure 5 (for the sentence pair in Figure 1(b)).","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"No rules can be extracted for the state predicate, because the shortest NL substring that covers the word states and the argument string Texas, i.e. states bordering Texas, contains the word bordering, which is linked to an MRL production outside the MR sub-parse rooted at state.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Rule extraction is forbidden in this case because it would destroy the link between bordering and next to.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"In other words, the NL and MR parse trees are not isomorphic.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
This problem can be ameliorated by transforming the logical form of each training sentence so that the NL and MR parse trees are maximally isomorphic.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"This is possible because some of the operators used in the logical forms, notably the conjunction operator (,), are both associative (a,(b,c) = (a,b),c = a,b,c) and commutative (a,b = b,a).","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"Hence, conjuncts can be reordered and regrouped without changing the meaning of a conjunction.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"For example, rule extraction would be possible if the positions of the next to and state conjuncts were switched.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"We present a method for regrouping conjuncts to promote isomorphism between NL and MR parse trees.2 Given a conjunction, it does the following: (See Figure 6 for the pseudocode, and Figure 5 for an illustration.)","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Step 1.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"Identify the MRL productions that correspond to the conjuncts and the meta-predicate that takes the conjunction as an argument (count in Figure 5), and figure them as vertices in an undi2This method also applies to any operators that are associative and commutative, e.g. disjunction.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"For concreteness, however, we use conjunction as an example. rected graph, F. An edge (pi7 pj) is in F if and only if pi and pj contain occurrences of the same logical variables.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Each edge in F indicates a possible edge in the transformed MR parse tree.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"Intuitively, two concepts are closely related if they involve the same logical variables, and therefore, should be placed close together in the MR parse tree.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"By keeping occurrences of a logical variable in close proximity in the MR parse tree, we also avoid unnecessary variable bindings in the extracted rules.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Step 2.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Remove edges from F whose inclusion in the MR parse tree would prevent the NL and MR parse trees from being isomorphic.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Step 3.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Add edges to F to make sure that a spanning tree for F exists.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Steps 4–6.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"Assign edge weights based on word distance, find a minimum spanning tree, T, for F, then regroup the conjuncts based on T. The choice of T reflects the intuition that words that occur close together in a sentence tend to be semantically related.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
This procedure is repeated for all conjunctions that appear in a logical form.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
Rules are then extracted from the same input alignment used to regroup conjuncts.,"{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"Of course, the regrouping of conjuncts requires a good alignment to begin with, and that requires a reasonable ordering of conjuncts in the training data, since the alignment model is sensitive to word order.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"This suggests an iterative algorithm in which a better grouping of conjuncts leads to a better alignment model, which guides further regrouping until convergence.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"We did not pursue this, as it is not needed in our experiments so far.","{'title': '4 Promoting NL/MRL Isomorphism', 'number': '4'}"
"In this section, we propose two methods for modeling the target MRL.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
This is motivated by the fact that many of the errors made by the A-WASP parser can be detected by inspecting the MR translations alone.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"Figure 7 shows some typical errors, which can be classified into two broad categories: 1.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
Type mismatch errors.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"For example, a state cannot possibly be a river (Figure 7(a)).","{'title': '5 Modeling the Target MRL', 'number': '5'}"
Also it is awkward to talk about the population density of a state’s highest point (Figure 7(b)).,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
2.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
Errors that do not involve type mismatch.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"For example, a query can be overly trivial (Figure 7(c)), or involve aggregate functions on a known singleton (Figure 7(d)).","{'title': '5 Modeling the Target MRL', 'number': '5'}"
The first type of errors can be fixed by type checking.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
Each m-place predicate is associated with a list of m-tuples showing all valid combinations of entity types that the m arguments can refer to: These m-tuples of entity types are given as domain knowledge.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible).,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"If there is a logical variable that cannot refer to any types of entities (i.e. the set of entity types is empty), then the partial derivation is considered invalid.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
"For example, based on the tuples shown above, point(x1) and density(x1, ) cannot be both true, because {POINT} n {COUNTRY, STATE, CITY} = ∅.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
"The use of type checking is to exploit the fact that people tend not to ask questions that obviously have no valid answers (Grice, 1975).","{'title': '5 Modeling the Target MRL', 'number': '5'}"
It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
Errors that do not involve type mismatch are handled by adding new features to the maximumentropy model (Section 3.2).,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"We only consider features that are based on the MR translations, and therefore, these features can be seen as an implicit language model of the target MRL (Papineni et al., 1997).","{'title': '5 Modeling the Target MRL', 'number': '5'}"
"Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
We use only the MRL part of the rules.,"{'title': '5 Modeling the Target MRL', 'number': '5'}"
"For example, a negative weight for the combination of QUERY → answer(x1,FORM(x1)) and FORM → Ax1.equal(x1, ) would discourage any parse that yields Figure 7(c).","{'title': '5 Modeling the Target MRL', 'number': '5'}"
"The two-level rules features, along with the features described in Section 3.2, are used in the final version of A-WASP.","{'title': '5 Modeling the Target MRL', 'number': '5'}"
We evaluated the A-WASP algorithm in the GEOQUERY domain.,"{'title': '6 Experiments', 'number': '6'}"
"The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006).","{'title': '6 Experiments', 'number': '6'}"
The questions were manually translated into Prolog logical forms.,"{'title': '6 Experiments', 'number': '6'}"
The average length of a sentence is 7.57 words.,"{'title': '6 Experiments', 'number': '6'}"
"We performed a single run of 10-fold cross validation, and measured the performance of the learned parsers using precision (percentage of translations that were correct), recall (percentage of test sentences that were correctly translated), and Fmeasure (harmonic mean of precision and recall).","{'title': '6 Experiments', 'number': '6'}"
A translation is considered correct if it retrieves the same answer as the correct logical form.,"{'title': '6 Experiments', 'number': '6'}"
"Figure 8 shows the learning curves for the AWASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&C), a CCG-based algorithm which uses Prolog logical forms.","{'title': '6 Experiments', 'number': '6'}"
"Table 1 summarizes the results at the end of the learning curves (792 training examples for AWASP, WASP and SCISSOR, 600 for Z&C).","{'title': '6 Experiments', 'number': '6'}"
A few observations can be made.,"{'title': '6 Experiments', 'number': '6'}"
"First, algorithms that use Prolog logical forms as the target MRL generally show better recall than those using FunQL.","{'title': '6 Experiments', 'number': '6'}"
"In particular, A-WASP has the best recall by far.","{'title': '6 Experiments', 'number': '6'}"
"One reason is that it allows lexical items to be combined in ways not allowed by FunQL or the hand-written templates in Z&C, e.g.","{'title': '6 Experiments', 'number': '6'}"
[smallest [state] [by area]] in Figure 3.,"{'title': '6 Experiments', 'number': '6'}"
"Second, Z&C has the best precision, although their results are based on 280 test examples only, whereas our results are based on 10-fold cross validation.","{'title': '6 Experiments', 'number': '6'}"
"Third, A-WASP has the best F-measure.","{'title': '6 Experiments', 'number': '6'}"
"To see the relative importance of each component of the A-WASP algorithm, we performed two ablation studies.","{'title': '6 Experiments', 'number': '6'}"
"First, we compared the performance of A-WASP with and without conjunct regrouping (Section 4).","{'title': '6 Experiments', 'number': '6'}"
"Second, we compared the performance of A-WASP with and without language modeling for the MRL (Section 5).","{'title': '6 Experiments', 'number': '6'}"
Table 2 shows the results.,"{'title': '6 Experiments', 'number': '6'}"
"It is found that conjunct regrouping improves recall (p < 0.01 based on the paired t-test), and the use of two-level rules in the maximum-entropy model improves precision and recall (p < 0.05).","{'title': '6 Experiments', 'number': '6'}"
Type checking also significantly improves precision and recall.,"{'title': '6 Experiments', 'number': '6'}"
A major advantage of A-WASP over SCISSOR and Z&C is that it does not require any prior knowledge of the NL syntax.,"{'title': '6 Experiments', 'number': '6'}"
Figure 9 shows the performance of A-WASP on the multilingual GEOQUERY data set.,"{'title': '6 Experiments', 'number': '6'}"
The 250-example data set is a subset of the larger GEOQUERY corpus.,"{'title': '6 Experiments', 'number': '6'}"
"All English questions in this data set were manually translated into Spanish, Japanese and Turkish, while the corresponding Prolog queries remain unchanged.","{'title': '6 Experiments', 'number': '6'}"
Figure 9 shows that A-WASP performed comparably for all NLs.,"{'title': '6 Experiments', 'number': '6'}"
"In contrast, SCISSOR cannot be used directly on the nonEnglish data, because syntactic annotations are only available in English.","{'title': '6 Experiments', 'number': '6'}"
"Z&C cannot be used directly either, because it requires NL-specific templates for building CCG grammars.","{'title': '6 Experiments', 'number': '6'}"
"We have presented A-WASP, a semantic parsing algorithm based on a A-SCFG that generates logical forms using A-calculus.","{'title': '7 Conclusions', 'number': '7'}"
A semantic parser is learned given a set of training sentences and their correct logical forms using standard SMT techniques.,"{'title': '7 Conclusions', 'number': '7'}"
"The result is a robust semantic parser for predicate logic, and it is the best-performing system so far in the GEOQUERY domain.","{'title': '7 Conclusions', 'number': '7'}"
This work shows that it is possible to use standard SMT methods in tasks where logical forms are involved.,"{'title': '7 Conclusions', 'number': '7'}"
"For example, it should be straightforward to adapt A-WASP to the NL generation task—all one needs is a decoder that can handle input logical forms.","{'title': '7 Conclusions', 'number': '7'}"
Other tasks that can potentially benefit from this include question answering and interlingual MT.,"{'title': '7 Conclusions', 'number': '7'}"
"In future work, we plan to further generalize the synchronous parsing framework to allow different combinations of grammar formalisms.","{'title': '7 Conclusions', 'number': '7'}"
"For example, to handle long-distance dependencies that occur in open-domain text, CCG and TAG would be more appropriate than CFG.","{'title': '7 Conclusions', 'number': '7'}"
"Certain applications may require different meaning representations, e.g. frame semantics.","{'title': '7 Conclusions', 'number': '7'}"
"Acknowledgments: We thank Rohit Kate, Razvan Bunescu and the anonymous reviewers for their valuable comments.","{'title': '7 Conclusions', 'number': '7'}"
This work was supported by a gift from Google Inc.,"{'title': '7 Conclusions', 'number': '7'}"
