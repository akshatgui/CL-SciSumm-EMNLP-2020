col1,col2
Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.,Introduction
The current state of the art discovers many semantic classes but fails to label their concepts.,Introduction
We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.,Introduction
The natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977).,Experiment/Discussion
"However, WordNet (Miller 1990) and Cyc (Lenat 1995) aside, the community has had little success in actually building large semantic repositories.",Experiment/Discussion
"Such broad-coverage lexical resources are extremely useful in applications such as word sense disambiguation (Leacock, Chodorow and Miller 1998) and question answering (Pasca and Harabagiu 2001).",Experiment/Discussion
Current manually constructed ontologies such as WordNet and Cyc have important limitations.,Experiment/Discussion
"First, they often contain rare senses.",Experiment/Discussion
"For example, WordNet includes a rare sense of computer that means `the person who computes'.",Experiment/Discussion
"Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.",Experiment/Discussion
"Also, the words dog, computer and company all have a sense that is a hyponym of person.",Experiment/Discussion
Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person.,Experiment/Discussion
The second problem with these lexicons is that they miss many domain specific senses.,Experiment/Discussion
"For example, WordNet misses the user-interface-object sense of the word dialog (as often used in software manuals).",Experiment/Discussion
WordNet also contains a very poor coverage of proper nouns.,Experiment/Discussion
There is a need for (semi-) automatic approaches to building and extending ontologies as well as for validating the structure and content of existing ones.,Experiment/Discussion
"With the advent of the Web, we have access to enormous amounts of text.",Experiment/Discussion
The future of ontology growing lies in leveraging this data by harvesting it for concepts and semantic relationships.,Experiment/Discussion
"Moreover, once such knowledge is discovered, mechanisms must be in place to enrich current ontologies with this new knowledge.",Experiment/Discussion
"To address some of the coverage and specificity problems in WordNet and Cyc, Pantel and Lin (2002) proposed and algorithm, called CBC, for automatically extracting semantic classes.",Experiment/Discussion
Their classes consist of clustered instances like the three shown below: A limitation of these concepts is that CBC does not discover their actual names.,Experiment/Discussion
"That is, CBC discovers a semantic class of Canadian provinces such as Manitoba, Alberta, and Ontario, but stops short of labeling the concept as Canadian Provinces.",Experiment/Discussion
Some applications such as question answering would benefit from class labels.,Experiment/Discussion
"For example, given the concept list (B) and a label goalie/goaltender, a QA system could look for answers to the question &quot;Which goaltender won the most Hart Trophys?&quot; in the concept.",Experiment/Discussion
"In this paper, we propose an algorithm for automatically inducing names for semantic classes and for finding instance/concept (is-a) relationships.",Experiment/Discussion
"Using concept signatures (templates describing the prototypical syntactic behavior of instances of a concept), we extract concept names by searching for simple syntactic patterns such as &quot;concept apposition-of instance&quot;.",Experiment/Discussion
Searching concept signatures is more robust than searching the syntactic features of individual instances since many instances suffer from sparse features or multiple senses.,Experiment/Discussion
"Once labels are assigned to concepts, we can extract a hyponym relationship between each instance of a concept and its label.",Experiment/Discussion
"For example, once our system labels list (C) as color, we may extract relationships such as: pink is a color, red is a color, turquoise is a color, etc.",Experiment/Discussion
"Our results show that of the 159,000 hyponyms we extract using this simple method, 68% are correct.",Experiment/Discussion
"Of the 65,000 proper name hyponyms we discover, 81.5% are correct.",Experiment/Discussion
The remainder of this paper is organized as follows.,Experiment/Discussion
"In the next section, we review previous algorithms for extracting semantic classes and hyponym relationships.",Experiment/Discussion
Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships.,Experiment/Discussion
"Experimental results are presented in Section 4 and finally, we conclude with a discussion and future work.",Experiment/Discussion
There have been several approaches to automatically discovering lexico-semantic information from text (Hearst 1992; Riloff and Shepherd 1997; Riloff and Jones 1999; Berland and Charniak 1999; Pantel and Lin 2002; Fleischman et al. 2003; Girju et al.,Experiment/Discussion
2003).,Experiment/Discussion
One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).,Experiment/Discussion
The output of these programs is a ranked list of similar words to each word.,Experiment/Discussion
"For example, Lin's approach outputs the following top-20 similar words of orange: (D) peach, grapefruit, yellow, lemon, pink, avocado, tangerine, banana, purple, Santa Ana, strawberry, tomato, red, pineapple, pear, Apricot, apple, green, citrus, mango A common problem of such lists is that they do not discriminate between the senses of polysemous words.",Experiment/Discussion
"For example, in (D), the color and fruit senses of orange are mixed up.",Experiment/Discussion
"Lin and Pantel (2001) proposed a clustering algorithm, UNICON, which generates similar lists but discriminates between senses of words.",Experiment/Discussion
"Later, Pantel and Lin (2002) improved the precision and recall of UNICON clusters with CBC (Clustering by Committee).",Experiment/Discussion
"Using sets of representative elements called committees, CBC discovers cluster centroids that unambiguously describe the members of a possible class.",Experiment/Discussion
The algorithm initially discovers committees that are well scattered in the similarity space.,Experiment/Discussion
It then proceeds by assigning elements to their most similar committees.,Experiment/Discussion
"After assigning an element to a cluster, CBC removes their overlapping features from the element before assigning it to another cluster.",Experiment/Discussion
This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses.,Experiment/Discussion
"CBC discovered both the color sense of orange, as shown in list (C) of Section 1, and the fruit sense shown below: (E) peach, pear, apricot, strawberry, banana, mango, melon, apple, pineapple, cherry, plum, lemon, grapefruit, orange, berry, raspberry, blueberry, kiwi, ...",Experiment/Discussion
There have also been several approaches to discovering hyponym (is-a) relationships from text.,Experiment/Discussion
"Hearst (1992) used seven lexico-syntactic patterns, for example &quot;such NP as {NP,}*{(or|and)} NP&quot; and &quot;NP {, NP}*{,} or other NP&quot;.",Experiment/Discussion
Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations.,Experiment/Discussion
"They reported an accuracy of about 55% precision on a corpus of 100,000 words.",Experiment/Discussion
"Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter.",Experiment/Discussion
Mann (2002) and Fleischman et al. (2003) used part of speech patterns to extract a subset of hyponym relations involing proper nouns.,Experiment/Discussion
The research discussed above on discovering hyponym relationships all take a bottom up approach.,Experiment/Discussion
"That is, they use patterns to independently discover semantic relationships of words.",Experiment/Discussion
"However, for infrequent words, these patterns do not match or, worse yet, generate incorrect relationships.",Experiment/Discussion
Ours is a top down approach.,Experiment/Discussion
We make use of cooccurrence statistics of semantic classes discovered by algorithms like CBC to label their concepts.,Experiment/Discussion
Hyponym relationships may then be extracted easily: one hyponym per instance/concept label pair.,Experiment/Discussion
"For example, if we labeled concept (A) from Section 1 with disease, then we could extract is-a relationships such as: diabetes is a disease, cancer is a disease, and lupus is a disease.",Experiment/Discussion
"A concept instance such as lupus is assigned a hypernym disease not because it necessarily occurs in any particular syntactic relationship with disease, but because it belongs to the class of instances that does.",Experiment/Discussion
"The input to our labeling algorithm is a list of semantic classes, in the form of clusters of words, which may be generated from any source.",Experiment/Discussion
"In our experiments, we used the clustering outputs of CBC (Pantel and Lin 2002).",Experiment/Discussion
The output of the system is a ranked list of concept names for each semantic class.,Experiment/Discussion
"In the first phase of the algorithm, we extract feature vectors for each word that occurs in a semantic class.",Experiment/Discussion
Phase II then uses these features to compute grammatical signatures of concepts using the CBC algorithm.,Experiment/Discussion
"Finally, we use simple syntactic patterns to discover class names from each class' signature.",Experiment/Discussion
"Below, we describe these phases in detail.",Experiment/Discussion
We represent each word (concept instance) by a feature vector.,Experiment/Discussion
Each feature corresponds to a context in which the word occurs.,Experiment/Discussion
"For example, &quot;catch _&quot; is a verbobject context.",Experiment/Discussion
"If the word wave occurred in this context, then the context is a feature of wave.",Experiment/Discussion
"We first construct a frequency count vector C(e) = (ce1, ce2, �, cem), where m is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in a grammatical context f. For example, if the word wave occurred 217 times as the object of the verb catch, then the feature vector for wave will have value 217 for its &quot;object-of catch&quot; feature.",Experiment/Discussion
"In Section 4.1, we describe how we obtain these features.",Experiment/Discussion
"We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miem) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: Following (Pantel and Lin 2002), we construct a committee for each semantic class.",Experiment/Discussion
A committee is a set of representative elements that unambiguously describe the members of a possible class.,Experiment/Discussion
"For each class c, we construct a matrix containing the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c' according to the following metric: where |c' |is the number of elements in c' and avgsim(c') is the average pairwise similarity between words in c'.",Experiment/Discussion
The assumption is that the best representative for a concept is a large set of very similar instances.,Experiment/Discussion
"The committee for class c is then the highest scoring candidate committee containing only words from c. For example, below are the committee members discovered for the semantic classes (A), (B), and (C) from Section 1: equency count of all features of all words.",Experiment/Discussion
Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).,Experiment/Discussion
A well-known problem is that mutual information is biased towards infrequent elements/features.,Experiment/Discussion
"We therefore multiply mief with the following discounting factor: n m where n is the number of words and N = cef × By averaging the feature vectors of the committee members of a particular semantic class, we obtain a grammatical template, or signature, for that class.",Experiment/Discussion
"For example, Figure 1 shows an excerpt of the grammatical signature for concept (B) in Section 1.",Experiment/Discussion
"The vector is obtained by averaging the feature vectors for the words Curtis Joseph, John Vanbiesbrouck, Mike Richter, and Tommy Salo (the committee of this concept).",Experiment/Discussion
The &quot;-V:subj:N:sprawl� feature indicates a subject-verb relationship between the concept and the verb sprawl while &quot;N:appo:N:goaltender&quot; indicates an apposition relationship between the concept and the noun goaltender.,Experiment/Discussion
The (-) in a relationship means that the right hand side of the relationship is the head (e.g. sprawl is the head of the subject-verb relationship).,Experiment/Discussion
The two columns of numbers indicate the frequency and mutual information score for each feature respectively.,Experiment/Discussion
"In order to discover the characteristics of human naming conventions, we manually named 50 concepts discovered by CBC.",Experiment/Discussion
"For each concept, we extracted the relationships between the concept committee and the assigned label.",Experiment/Discussion
We then added the mutual information scores for each extracted relationship among the 50 concepts.,Experiment/Discussion
"The top-4 highest scoring relationships are: To name a class, we simply search for these syntactic relationships in the signature of a concept.",Experiment/Discussion
We sum up the mutual information scores for each term that occurs in these relationships with a committee of a class.,Experiment/Discussion
The highest scoring term is the name of the class.,Experiment/Discussion
"For example, the top-5 scoring terms that occurred in these relationships with the signature of the concept represented by the committee {Curtis Joseph, John Vanbiesbrouck, Mike Richter, Tommy Salo} are: goalie 40.37 goaltender 33.64 goalkeeper 19.22 player 14.55 backup 9.40 The numbers are the total mutual information scores of each name in the four syntactic relationships.",Experiment/Discussion
"In this section, we present an evaluation of the class labeling algorithm and of the hyponym relationships discovered by our system.",Experiment/Discussion
"We used Minipar (Lin 1994), a broad coverage parser, to parse 3GB of newspaper text from the Aquaint (TREC-9) collection.",Experiment/Discussion
We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1.,Experiment/Discussion
We used the 1432 noun clusters extracted by CBC1 as the list of concepts to name.,Experiment/Discussion
"For each concept, we then used our algorithm described in Section 3 to extract the top-20 names for each concept.",Experiment/Discussion
"Out of the 1432 noun concepts, we were unable to name 21 (1.5%) of them.",Experiment/Discussion
This occurs when a concept's committee members do not occur in any of the four syntactic relationships described in Section 0.,Experiment/Discussion
We performed a manual evaluation of the remaining 1411 concepts.,Experiment/Discussion
We randomly selected 125 concepts and their top-5 highest ranking names according to our algorithm.,Experiment/Discussion
Table 1 shows the first 10 randomly selected concepts (each concept is represented by three of its committee members).,Experiment/Discussion
"For each concept, we added to the list of names a human generated name (obtained from an annotator looking at only the concept instances).",Experiment/Discussion
We also appended concept names extracted from WordNet.,Experiment/Discussion
"For each concept that contains at least five instances in the WordNet hierarchy, we named the concept with the most frequent common ancestor of each pair of instances.",Experiment/Discussion
Up to five names were generated by WordNet for each concept.,Experiment/Discussion
"Because of the low coverage of proper nouns in WordNet, only 33 of the 125 concepts we evaluated had WordNet generated labels.",Experiment/Discussion
"We presented to three human judges the 125 randomly selected concepts together with the system, human, and WordNet generated names randomly ordered.",Experiment/Discussion
"That way, there was no way for a judge to know the source of a label nor the system's ranking of the labels.",Experiment/Discussion
"For each name, we asked the judges to assign a score of correct, partially correct, or incorrect.",Experiment/Discussion
"We then computed the mean reciprocal rank (MRR) of the system, human, and WordNet labels.",Experiment/Discussion
"For each concept, a naming scheme receives a score of 1 / M where M is the rank of the first name judged correct.",Experiment/Discussion
Table 2 shows the results.,Experiment/Discussion
Table 3 shows similar results for a more lenient evaluation where M is the rank of the first name judged correct or partially correct.,Experiment/Discussion
Our system achieved an overall MRR score of 77.1%.,Experiment/Discussion
We performed much better than the baseline WordNet (19.9%) because of the lack of coverage (mostly proper nouns) in the hierarchy.,Experiment/Discussion
"For the 33 concepts that WordNet named, it achieved a score of 75.3% and a lenient score of 82.7%, which is high considering the simple algorithm we used to extract labels using WordNet.",Experiment/Discussion
The Kappa statistic (Siegel and Castellan Jr. 1988) measures the agreements between a set of judges' assessments correcting for chance agreements: where P(A) is the probability of agreement between the judges and P(E) is the probability that the judges agree by chance on an assessment.,Experiment/Discussion
An experiment with K ≥ 0.8 is generally viewed as reliable and 0.67 < K < 0.8 allows tentative conclusions.,Experiment/Discussion
The Kappa statistic for our experiment is K = 0.72.,Experiment/Discussion
The human labeling is at a disadvantage since only one label was generated per concept.,Experiment/Discussion
"Therefore, the human scores either 1 or 0 for each concept.",Experiment/Discussion
Our system's highest ranking name was correct 72% of the time.,Experiment/Discussion
Table 4 shows the percentage of semantic classes with a correct label in the top 1-5 ranks returned by our system.,Experiment/Discussion
"Overall, 41.8% of the top-5 names extracted by our system were judged correct.",Experiment/Discussion
"The overall accuracy for the top-4, top-3, top-2, and top-1 names are 44.4%, 48.8%, 58.5%, and 72% respectively.",Experiment/Discussion
"Hence, the name ranking of our algorithm is effective.",Experiment/Discussion
"The 1432 CBC concepts contain 18,000 unique words.",Experiment/Discussion
"For each concept to which a word belongs, we extracted up to 3 hyponyms, one for each of the top-3 labels for the concept.",Experiment/Discussion
"The result was 159,000 hyponym relationships.",Experiment/Discussion
24 are shown in the Appendix.,Experiment/Discussion
"Two judges annotated two random samples of 100 relationships: one from all 159,000 hyponyms and one from the subset of 65,000 proper nouns.",Experiment/Discussion
"For each instance, the judges were asked to decide whether the hyponym relationship was correct, partially correct or incorrect.",Experiment/Discussion
Table 5 shows the results.,Experiment/Discussion
The strict measure counts a score of 1 for each correctly judged instance and 0 otherwise.,Experiment/Discussion
The lenient measure also gives a score of 0.5 for each instance judged partially correct.,Experiment/Discussion
Many of the CBC concepts contain noise.,Experiment/Discussion
"For example, the wine cluster: Zinfandel, merlot, Pinot noir, Chardonnay, Cabernet Sauvignon, cabernet, riesling, Sauvignon blanc, Chenin blanc, sangiovese, syrah, Grape, Chianti ... contains some incorrect instances such as grape, appelation, and milk chocolate.",Experiment/Discussion
Each of these instances will generate incorrect hyponyms such as grape is wine and milk chocolate is wine.,Experiment/Discussion
This hyponym extraction task would likely serve well for evaluating the accuracy of lists of semantic classes.,Experiment/Discussion
Table 5 shows that the hyponyms involving proper nouns are much more reliable than common nouns.,Experiment/Discussion
"Since WordNet contains poor coverage of proper nouns, these relationships could be useful to enrich it.",Experiment/Discussion
Semantic extraction tasks are notoriously difficult to evaluate for recall.,Experiment/Discussion
"To approximate recall, we conducted two question answering (QA) tasks: answering definition questions and performing QA information retrieval.",Experiment/Discussion
"We chose the 50 definition questions that appeared in the QA track of TREC2003 (Voorhees, 2003).",Experiment/Discussion
For example: &quot;Who is Aaron Copland?&quot; and &quot;What is the Kama Sutra?&quot; For each question we looked for at most five corresponding concepts in our hyponym list.,Experiment/Discussion
"For example, for Aaron Copland, we found the following hypernyms: composer, music, and gift.",Experiment/Discussion
We compared our system with the concepts in WordNet and Fleischman et al. 's instance/concept relations (Fleischman et al.,Experiment/Discussion
2003).,Experiment/Discussion
Table 6 shows the percentage of correct answers in the top-1 and top-5 returned answers from each system.,Experiment/Discussion
"All systems seem to have similar performance on the top-1 answers, but our system has many more answers in the top-5.",Experiment/Discussion
This shows that our system has comparatively higher recall for this task.,Experiment/Discussion
Passage retrieval is used in QA to supply relevant information to an answer pinpointing module.,Experiment/Discussion
"The higher the performance of the passage retrieval module, the higher will be the performance of the answer pinpointing module.",Experiment/Discussion
The passage retrieval module can make use of the hyponym relationships that are discovered by our system.,Experiment/Discussion
"Given a question such as &quot;What color ...&quot;, the likelihood of a correct answer being present in a retrieved passage is greatly increased if we know the set of all possible colors and index them in the document collection appropriately.",Experiment/Discussion
We used the hyponym relations learned by our system to perform semantic indexing on a QA passage retrieval task.,Experiment/Discussion
We selected the 179 questions from the QA track of TREC-2003 that had an explicit semantic answer type (e.g.,Experiment/Discussion
&quot;What band was Jerry Garcia with?&quot; and &quot;What color is the top stripe on the U.S. flag?&quot;).,Experiment/Discussion
"For each expected semantic answer type corresponding to a given question (e.g. band and color), we indexed the entire TREC-2002 IR collection with our system's hyponyms.",Experiment/Discussion
We compared the passages returned by the passage retrieval module with and without the semantic indexing.,Experiment/Discussion
We counted how many of the 179 questions had a correct answer returned in the top-1 and top-100 passages.,Experiment/Discussion
Table 7 shows the results.,Experiment/Discussion
Our system shows small gains in the performance of the IR output.,Experiment/Discussion
"In the top-1 category, the performance improved by 20%.",Experiment/Discussion
This may lead to better answer selections.,Experiment/Discussion
Current state of the art concept discovery algorithms generate lists of instances of semantic classes but stop short of labeling the classes with concept names.,Results/Conclusion
Class labels would serve useful in applications such as question answering to map a question concept into a semantic class and then search for answers within that class.,Results/Conclusion
We propose here an algorithm for automatically labeling concepts that searches for syntactic patterns within a grammatical template for a class.,Results/Conclusion
"Of the 1432 noun concepts discovered by CBC, our system labelled 98.5% of them with an NRR score of 77.1% in a human evaluation.",Results/Conclusion
"Hyponym relationships were then easily extracted, one for each instance/concept label pair.",Results/Conclusion
"We extracted 159,000 hyponyms and achieved a precision of 68%.",Results/Conclusion
"On a subset of 65,000 proper names, our performance was 81.5%.",Results/Conclusion
This work forms an important attempt to building large-scale semantic knowledge bases.,Results/Conclusion
"Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.",Results/Conclusion
"Of course, it is a serious open question how many names each cluster (concept) should have, and how good each name is.",Results/Conclusion
Our method begins to address this thorny issue by quantifying the name assigned to a class and by simultaneously assigning a number that can be interpreted to reflect the strength of membership of each element to the class.,Results/Conclusion
This is potentially a significant step away from traditional all-or-nothing semantic/ontology representations to a concept representation scheme that is more nuanced and admits multiple names and graded set memberships.,Results/Conclusion
The authors wish to thank the reviewers for their helpful comments.,Results/Conclusion
This research was partly supported by NSF grant #EIA-0205111.,Results/Conclusion
