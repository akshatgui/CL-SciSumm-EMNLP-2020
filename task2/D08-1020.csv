col1,col2
"We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability.",{}
This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.,{}
We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.,{}
We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.,{}
Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.,{}
The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition.,"{'title': '1 Introduction', 'number': '1'}"
"Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952).","{'title': '1 Introduction', 'number': '1'}"
"In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006).","{'title': '1 Introduction', 'number': '1'}"
"Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions.","{'title': '1 Introduction', 'number': '1'}"
"Still, we do not have unified computational models that capture the interplay between various aspects of readability.","{'title': '1 Introduction', 'number': '1'}"
Most studies focus on a single factor contributing to readability for a given intended audience.,"{'title': '1 Introduction', 'number': '1'}"
"The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007).","{'title': '1 Introduction', 'number': '1'}"
"Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability.","{'title': '1 Introduction', 'number': '1'}"
"Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability.","{'title': '1 Introduction', 'number': '1'}"
But we know little about the relative importance of each factor and how they combine in determining perceived text quality.,"{'title': '1 Introduction', 'number': '1'}"
"In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence and discourse.","{'title': '1 Introduction', 'number': '1'}"
"We study the association between these features and reader assigned readability ratings, showing that discourse and vocabulary are the factors most strongly linked to text quality.","{'title': '1 Introduction', 'number': '1'}"
"In the easier task of text quality ranking, entity coherence and syntax features also become significant and the combination of features allows for ranking prediction accuracy of 88%.","{'title': '1 Introduction', 'number': '1'}"
Our study is novel in the use of gold-standard discourse features for predicting readability and the simultaneous analysis of various readability factors.,"{'title': '1 Introduction', 'number': '1'}"
"The definition of what one might consider to be a well-written and readable text heavily depends on the intended audience (Schriver, 1989).","{'title': '2 Related work', 'number': '2'}"
"Obviously, even a superbly written scientific paper will not be perceived as very readable by a lay person and a great novel might not be appreciated by a third grader.","{'title': '2 Related work', 'number': '2'}"
"As a result, the vast majority of prior work on readability deals with labeling texts with the appropriate school grade level.","{'title': '2 Related work', 'number': '2'}"
A key observation in even the oldest work in this area is that the vocabulary used in a text largely determines its readability.,"{'title': '2 Related work', 'number': '2'}"
"More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language.","{'title': '2 Related work', 'number': '2'}"
"It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list.","{'title': '2 Related work', 'number': '2'}"
"Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975).","{'title': '2 Related work', 'number': '2'}"
They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability.,"{'title': '2 Related work', 'number': '2'}"
"For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty.","{'title': '2 Related work', 'number': '2'}"
"In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public.","{'title': '2 Related work', 'number': '2'}"
"(Elhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora.","{'title': '2 Related work', 'number': '2'}"
"When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled.","{'title': '2 Related work', 'number': '2'}"
A more general and principled approach to using vocabulary information for readability decisions has been the use of language models.,"{'title': '2 Related work', 'number': '2'}"
"For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level.","{'title': '2 Related work', 'number': '2'}"
"(Si and Callan, 2001), (Collins-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level.","{'title': '2 Related work', 'number': '2'}"
But even for this type of task other factors besides vocabulary use are at play in determining readability.,"{'title': '2 Related work', 'number': '2'}"
"Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.","{'title': '2 Related work', 'number': '2'}"
"For the task of deciding whether a text is written for an adult or child reader, (Barzilay and Lapata, 2008) found that adding entity coherence to (Schwarm and Ostendorf, 2005)’s list of features improves classification accuracy by 10%.","{'title': '2 Related work', 'number': '2'}"
"In linguistics and natural language processing, the text properties rather than those of the reader are emphasized.","{'title': '2 Related work', 'number': '2'}"
Text coherence is defined as the ease with which a person (tacitly assumed to be a competent language user) understands a text.,"{'title': '2 Related work', 'number': '2'}"
"Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976).","{'title': '2 Related work', 'number': '2'}"
"In recent work, considerable attention has been devoted to entity coherence in text quality, especially in relation to information ordering.","{'title': '2 Related work', 'number': '2'}"
"In many applications such as text generation and summarization, systems need to decide the order in which selected sentences or generated clauses should be presented to the user.","{'title': '2 Related work', 'number': '2'}"
"Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al., 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference.","{'title': '2 Related work', 'number': '2'}"
"In a detailed study of information ordering in three very different corpora, (Karamanis et al., to appear) assessed the performance of various formulations of centering.","{'title': '2 Related work', 'number': '2'}"
"Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoiding rough shifts, that is, sequences of sentences that share no entities in common.","{'title': '2 Related work', 'number': '2'}"
"This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features (Miltsakaki and Kukich, 2000).","{'title': '2 Related work', 'number': '2'}"
"In a more powerful generalization of centering, Barzilay and Lapata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion.","{'title': '2 Related work', 'number': '2'}"
"Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level.","{'title': '2 Related work', 'number': '2'}"
Form of reference is also important in wellwritten text and appropriate choices lead to improved readability.,"{'title': '2 Related work', 'number': '2'}"
"Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002).","{'title': '2 Related work', 'number': '2'}"
"The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008).","{'title': '2 Related work', 'number': '2'}"
"The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality.","{'title': '3 Data', 'number': '3'}"
"In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005).","{'title': '3 Data', 'number': '3'}"
For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives.,"{'title': '3 Data', 'number': '3'}"
"The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994).","{'title': '3 Data', 'number': '3'}"
All explicit relations (those marked with a discourse connective) are annotated.,"{'title': '3 Data', 'number': '3'}"
"In addition, each adjacent pair of sentences within a paragraph is annotated.","{'title': '3 Data', 'number': '3'}"
"If there is a discourse relation, then it is marked implicit and annotated with one or more connectives.","{'title': '3 Data', 'number': '3'}"
"If there is a relation between the sentences but adding a connective would be inappropriate, it is marked AltLex.","{'title': '3 Data', 'number': '3'}"
"If the consecutive sentences are only related by entity-based coherence (Knott et al., 2001) they are annotated with EntRel.","{'title': '3 Data', 'number': '3'}"
"Otherwise, they are annotated with NoRel.","{'title': '3 Data', 'number': '3'}"
"Besides labeling the connective, the PDTB also annotates the sense of each relation.","{'title': '3 Data', 'number': '3'}"
The relations are organized into a hierarchy.,"{'title': '3 Data', 'number': '3'}"
"The top level relations are Expansion, Comparison, Contingency, and Temporal.","{'title': '3 Data', 'number': '3'}"
"Briefly, an expansion relation means that the second clause continues the theme of the first clause, a comparison relation indicates that something in the two clauses is being compared, contingency means that there is a causal relation between the clauses, and temporal means they occur either at the same time or sequentially.","{'title': '3 Data', 'number': '3'}"
"We randomly selected thirty articles from the Wall Street Journal corpus that was used in both the Penn Treebank and the Penn Discourse Treebank.1 Each article was read by at least three college students, each of whom was given unlimited time to read the texts and perform the ratings.2 Subjects were asked the following questions: For each question, they provided a rating between 1 and 5, with 5 being the best and 1 being the worst.","{'title': '3 Data', 'number': '3'}"
1One of the selected articles was missing from the Penn Treebank.,"{'title': '3 Data', 'number': '3'}"
"Thus, results that do not require syntactic information (Tables 1, 2, 4, and 6) are over all thirty articles, while Tables 3, 5, and 7 report results for the twenty-nine articles with Treebank parse trees.","{'title': '3 Data', 'number': '3'}"
"2(Lapata, 2006) found that human ratings are significantly correlated with self-paced reading times, a more direct measure of processing effort which we plan to explore in future work.","{'title': '3 Data', 'number': '3'}"
"After collecting the data, it turned out that most of the time subjects gave the same rating to all questions.","{'title': '3 Data', 'number': '3'}"
"For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written.","{'title': '3 Data', 'number': '3'}"
"Thus for all subsequent analysis, we will use only the first question (“On a scale of 1 to 5, how well written is this text?”).","{'title': '3 Data', 'number': '3'}"
The score of an article was then the average of all the ratings it received.,"{'title': '3 Data', 'number': '3'}"
"The article scores ranged from 1.5 to 4.33, with a mean of 3.2008 and a standard deviation of .7242.","{'title': '3 Data', 'number': '3'}"
The median score was 3.286.,"{'title': '3 Data', 'number': '3'}"
We define our task as predicting this average rating for each article.,"{'title': '3 Data', 'number': '3'}"
"Note that this task may be more difficult than predicting reading level, as each of these articles appeared in the Wall Street Journal and thus is aimed at the same target audience.","{'title': '3 Data', 'number': '3'}"
"We suspected that in classifying adult text, more subtle features might be necessary.","{'title': '3 Data', 'number': '3'}"
We first computed the Pearson correlation coefficients between the simple metrics that most traditional readability formulas use and the average human ratings.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
These results are shown in Table 1.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We tested the average number of characters per word, average number of words per sentence, maximum number of words per sentence, and article length (F7).3 Article length (F7) was the only significant baseline factor, with correlation of -0.37.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Longer articles are perceived as less well-written and harder to read than shorter ones.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
None of the other baseline metrics were close to being significant predictors of readability.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We use a unigram language model, where the probability of an article is: P(wIM) is the probability of word-type w according to a background corpus M, and C(w) is the number of times w appears in the article.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The log likelihood of an article is then: Note that this model will be biased in favor of shorter articles.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Since each word has probability less than 1, the log probability of each word is less than 0, and hence including additional words decreases the log likelihood.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
We compensate for this by performing linear regressions with the unigram log likelihood and with the number of words in the article as an additional variable.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The question then arises as to what to use as a background corpus.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We chose to experiment with two corpora: the entire Wall Street Journal corpus and a collection of general AP news, which is generally more diverse than the financial news found in the WSJ.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
We predicted that the NEWS vocabulary would be more representative of the types of words our readers would be familiar with.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
In both cases we used Laplace smoothing over the word frequencies and a stoplist.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The vocabulary features we used are article likelihood estimated from a language model from WSJ (F5), and article likelihood according to a unigram language model from NEWS (F6).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We also combine the two likelihood features with article length, in order to get a better estimate of the language model’s influence on readability independent of the length of the article.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Both vocabulary-based features (F5 and F6) are significantly correlated with the readability judgments, with p-values smaller than 0.05 (see Table 2).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The correlations are positive: the more probable an article was based on its vocabulary, the higher it was generally rated.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"As expected, the NEWS model that included more general news stories had a higher correlation with people’s judgments.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"When combined with the length of the article, the unigram language model from the NEWS corpus becomes very predictive of readability, with the correlation between the two as high as 0.63.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Syntactic constructions affect processing difficulty and so might also affect readability judgments.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We examined the four syntactic features used in (Schwarm and Ostendorf, 2005): average parse tree height (F1), average number of noun phrases per sentence (F2), average number of verb phrases per sentence (F3), and average number of subordinate clauses per sentence(SBARs in the Penn Treebank tagset) (F4).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The sentence “We’re talking about years ago [SBAR before anyone heard of asbestos having any questionable properties].” contains an example of an SBAR clause.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Having multiple noun phrases (entities) in each sentence requires the reader to remember more items, but may make the article more interesting.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"(Barzilay and Lapata, 2008) found that articles written for adults tended to contain many more entities than articles written for children.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"While including more verb phrases in each sentence increases the sentence complexity, adults might prefer to have related clauses explicitly grouped together.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The correlations between readability and syntactic features is shown in Table 3.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The strongest correlation is that between readability and number of verb phrases (0.42).,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"This finding is in line with prescriptive clear writing advice (Gunning, 1952; Spandel, 2004), but is to our knowledge novel in the computational linguistics literature.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"As (Bailin and Grafstein, 2001) point out, the sentences in (1) are easier to comprehend than the sentences in (2), even though they are longer.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Multiple verb phrases in one sentence may be indicative of explicit discourse relations, which we will discuss further in section 4.6.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Surprisingly, the use of clauses introduced by a (possibly empty) subordinating conjunction (SBAR), are actually positively correlated (and almost approaching significance) with readability.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"So while for children or less educated adults these constructions might pose difficulties, they were favored by our assessors.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"On the other hand, the average parse tree height negatively correlated with readability as expected, but surprisingly the correlation is very weak (-0.06).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In their classic study of cohesion in English, (Halliday and Hasan, 1976) discuss the various aspects of well written discourse, including the use of cohesive devices such as pronouns, definite descriptions and topic continuity from sentence to sentence.4 To measure the association between these features and readability rankings, we compute the number ofpronouns per sentence (F11) and the number of definite articles per sentence (F12).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In order to qualify topic continuity from sentence to sentence in the articles, we compute average cosine similarity (F8), word overlap (F9) and word overlap over just nouns and pronouns (F10) between pairs of adjacent sentences5.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Each sentence is turned into a vector of word-types, where each type’s value is its tf-idf (where document frequency is computed over all the articles in the WSJ corpus).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The cosine similarity metric is then: None of these features correlate significantly with readability as can be seen from the results in Table 4.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The overlap features are particularly bad predictors of readability, with average word/cosine overlap in fact being negatively correlated with readability.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The form of reference—use of pronouns and definite descriptions—exhibit a higher correlation with readability (0.23), but these values are not significant for the size of our corpus.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We use the Brown Coherence Toolkit6 to compute entity grids (Barzilay and Lapata, 2008) for each article.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In each sentence, an entity is identified as the subject (S), object (O), other (X) (for example, part of a prepositional phrase), or not present (N).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The probability of each transition type is computed.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"For example, an S-O transition occurs when an entity is the subject in one sentence then an object in the next; X-N transition occurs when an entity appears in non-subject or object position in one sentence and not present in the next, etc.7 The entity coherence features are the probability of each of these pairs of transitions, for a total of 16 features (F17_32; see complete results in Table 5).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
None of the entity grid features are significantly correlated with the readability ratings.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"One very interesting result is that the proportion of S-S transitions in which the same entity was mentioned in subject position in two adjacent sentences, is negatively correlated with readability.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In centering theory, this is considered the most coherent type of transition, keeping the same center of attention.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Moreover, the feature most strongly correlated with readability is the S-N transition (0.31) in which the subject of one sentence does not appear at all in the following sentence.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Of course, it is difficult to interpret the entity grid features one by one, since they are interdependent and probably it is the interaction of features (relative proportions of transitions) that capture overall readability patterns.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Discourse relations are believed to be a major factor in text coherence.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
We computed another language model which is over discourse relations instead of words.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
We treat each text as a bag of relations rather than a bag of words.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Each relation is annotated for both its sense and how it is realized (implicit or explicit).,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"For example, one text might contain {Implicit Comparison, Explicit Temporal, NoRel}.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"We computed the probability of each of our articles according to a multinomial model, where the probability of a text with n relation tokens and k relation types is: P(n) is the probability of an article having length n, xi is the number of times relation i appeared, and pi is the probability of relation i based on the Penn Discourse Treebank.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"P(n) is the maximum likelihood estimation of an article having n discourse relations based on the entire Penn Discourse Treebank (the number of articles with exactly n discourse relations, divided by the total number of articles).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The log likelihood of an article based on its discourse relations (F13) feature is defined as: The multinomial distribution is particularly suitable, because it directly incorporates length, which significantly affects readability as we discussed earlier.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"It also captures patterns of relative frequency of relations, unlike the simpler unigram model.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Note also that this equation has an advantage over the unigram model that was not present for vocabulary.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"While every article contains at least one word, some articles do not contain any discourse relations.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Since the PDTB annotated all explicit relations and relations between adjacent sentences in a paragraph, an article with no discourse connectives and only single sentence paragraphs would not contain any annotated discourse relations.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Under the unigram model, these articles’ probabilities cannot be computed.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"Under the multinomial model, the probability of an article with zero relations is estimated as Pr(N = 0), which can be calculated from the corpus.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"As in the case of vocabulary features, the presence of more relations will lead to overall lower probabilities so we also consider the number of discourse relations (F14) and the log likelihood combined with the number of relations as features.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In order to isolate the effect of the type of discourse relation (explicitly expressed by a discourse connective such as “because” or “however” versus implicitly expressed by adjacency), we also compute multinomial model features for the explicit discourse relations (F15) and over just the implicit discourse relations (F16).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"The likelihood of discourse relations in the text under a multinomial model is very highly and significantly correlated with readability ratings, especially after text length is taken into account.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Correlations are 0.48 and 0.54 respectively.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The probability of the explicit relations alone is not a sufficiently strong indicator of readability.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008).","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Note that the sequence of just the implicit relations is also not sufficient.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
This observation implies that the proportion of explicit and implicit relations may be meaningful but we leave the exploration of this issue for later work.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"So far, we introduced six classes of factors that have been discussed in the literature as readability correlates.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Through statistical tests of associations we identified the individual factors significantly correlated with readability ratings.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"These are, in decreasing order of association strength: Vocabulary and discourse relations are the strongest predictors of readability, followed by average number of verb phrases and length of the text.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
This empirical confirmation of the significance of discourse relations as a readability factor is novel for the computational linguistics literature.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
Note though that for our work we use oracle discourse annotations directly from the PDTB and no robust systems for automatic discourse annotation exist today.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
The significance of the average number of verb phrases as a readability predictor is somewhat surprising but intriguing.,"{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"It would lead to reexamination of the role of verbs/predicates in written text, which we also plan to address in future work.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"None of the other factors showed significant association with readability ratings, even though some correlations had relatively large positive values.","{'title': '4 Identifying correlates of text quality', 'number': '4'}"
"In this section, we turn to the question of how the combination of various factors improves the prediction of readability.","{'title': '5 Combining readability factors', 'number': '5'}"
"We use the leaps package in R to find the best subset of features for linear regression, for subsets of size one to eight.","{'title': '5 Combining readability factors', 'number': '5'}"
We use the squared multiple correlation coefficient (R2) to assess the effectiveness of predictions.,"{'title': '5 Combining readability factors', 'number': '5'}"
R2 is the proportion of variance in readability ratings explained by the model.,"{'title': '5 Combining readability factors', 'number': '5'}"
"If the model predicts readability perfectly, R2 = 1, and if the model has no predictive capability, R2 = 0.","{'title': '5 Combining readability factors', 'number': '5'}"
The linear regression results confirm the expectation that the combination of different factors is a rather complex issue.,"{'title': '5 Combining readability factors', 'number': '5'}"
"As expected, discourse, vocabulary and length which were the significant individual factors appear in the best model for each feature set size.","{'title': '5 Combining readability factors', 'number': '5'}"
"Their combination gives the best result for regression with three predictors, and they explain half of the variance in readability ratings, R2 = 0.5029.","{'title': '5 Combining readability factors', 'number': '5'}"
"But the other individually significant feature, average number of verb phrases per sentence (F3) never appears in the best models.","{'title': '5 Combining readability factors', 'number': '5'}"
"Instead, F1—the depth of the parse tree—appears in the best model with more than four features.","{'title': '5 Combining readability factors', 'number': '5'}"
"Also unexpectedly, two of the superficial cohesion features appear in the larger models: F10 is the average word overlap over nouns and pronouns and F11 is the average number of pronouns per sentence.","{'title': '5 Combining readability factors', 'number': '5'}"
"Entity grid features also make their way into the best models when more features are used for prediction: S-X, O-O, O-X, N-O transitions (F19, F22, F23, F30).","{'title': '5 Combining readability factors', 'number': '5'}"
In this section we consider the problem of pairwise ranking of text readability.,"{'title': '6 Readability as ranking', 'number': '6'}"
"That is, rather than trying to predict the readability of a single document, we consider pairs of documents and predict which one is better.","{'title': '6 Readability as ranking', 'number': '6'}"
"This task may in fact be the more natural one, since in most applications the main concern is with the relative quality of articles rather than their absolute scores.","{'title': '6 Readability as ranking', 'number': '6'}"
"This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task.","{'title': '6 Readability as ranking', 'number': '6'}"
"We thus create a classification problem: given two articles, is article 1 more readable than article 2?","{'title': '6 Readability as ranking', 'number': '6'}"
"For each pair of texts whose readability ratings on the 1 to 5 scale differed by at least 0.5, we form one data point for the ranking problem, resulting in 243 examples.","{'title': '6 Readability as ranking', 'number': '6'}"
The predictors are the differences between the two articles’ features.,"{'title': '6 Readability as ranking', 'number': '6'}"
"For classification, we used WEKA’s linear support vector implementation (SMO) and performance was evaluated using 10-fold cross-validation.","{'title': '6 Readability as ranking', 'number': '6'}"
The classification results are shown in Table 7.,"{'title': '6 Readability as ranking', 'number': '6'}"
"When all features are used for prediction, the accuracy is high, 88.88%.","{'title': '6 Readability as ranking', 'number': '6'}"
"The length of the article can serve as a baseline feature—longer articles are ranked lower by the assessors, so this feature can be taken as baseline indicator of readability.","{'title': '6 Readability as ranking', 'number': '6'}"
Only six features used by themselves lead to accuracies higher than the length baseline.,"{'title': '6 Readability as ranking', 'number': '6'}"
"These results indicate that the most important individual factors in the readability ranking task, in decreasing order of importance, are log likelihood of discourse relations, number of discourse relations, N-O transitions, O-N transitions, average number of VPs per sentence and text probability under a general language model.","{'title': '6 Readability as ranking', 'number': '6'}"
"In terms of classes of features, the 16 entity grid features perform the best, leading to an accuracy of 79.41%, followed by the combination of the four discourse features (77.36%), and syntax features (74.07%).","{'title': '6 Readability as ranking', 'number': '6'}"
"This is evidence for the fact that there is a complex interplay between readability factors: the entity grid factors which individually have very weak correlation with readability combine well, while adding the three additional discourse features to the likelihood of discourses relations actually worsens performance slightly.","{'title': '6 Readability as ranking', 'number': '6'}"
"Similar indication for interplay between features is provided by the class ablation classification results, in which classes of features are removed.","{'title': '6 Readability as ranking', 'number': '6'}"
"Surprisingly, removing syntactic features causes the biggest deterioration in performance, a drop in accuracy from 88.88% to 82.71%.","{'title': '6 Readability as ranking', 'number': '6'}"
"The removal of vocabulary, length, or discourse features has a minimal negative impact on performance, while removing the cohesion features actually boosts performance.","{'title': '6 Readability as ranking', 'number': '6'}"
We have investigated which linguistic features correlate best with readability judgments.,"{'title': '7 Conclusion', 'number': '7'}"
"While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.","{'title': '7 Conclusion', 'number': '7'}"
"The average number of verb phrases in each sentence, the number of words in the article, the likelihood of the vocabulary, and the likelihood of the discourse relations all are highly correlated with humans’ judgments of how well an article is written.","{'title': '7 Conclusion', 'number': '7'}"
"While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.","{'title': '7 Conclusion', 'number': '7'}"
This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.,"{'title': '8 Acknowledgments', 'number': '8'}"
"We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.","{'title': '8 Acknowledgments', 'number': '8'}"
