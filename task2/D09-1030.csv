col1,col2
Manual evaluation of translation quality is generally thought to be excessively time consuming and expensive.,{}
We explore a fast and inexpensive way of doing it using Amazon’s Mechanical Turk to pay small sums to a large number of non-expert annotators.,{}
For $10 we redundantly recreate judgments from a WMT08 translation task.,{}
"We find that when combined non-expert judgments have a high-level of agreement with the existing gold-standard judgments of machine translation quality, and correlate more strongly with expert judgments than Bleu does.",{}
"We go on to show that Mechanical Turk can be used to calculate human-mediated translation edit rate (HTER), to conduct reading comprehension experiments with machine translation, and to create high quality reference translations.",{}
Conventional wisdom holds that manual evaluation of machine translation is too time-consuming and expensive to conduct.,"{'title': '1 Introduction', 'number': '1'}"
"Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality.","{'title': '1 Introduction', 'number': '1'}"
"Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments.","{'title': '1 Introduction', 'number': '1'}"
"Therefore, having people evaluate translation output would be preferable, if it were more practical.","{'title': '1 Introduction', 'number': '1'}"
In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought.,"{'title': '1 Introduction', 'number': '1'}"
"We use Amazon’s Mechanical Turk, an online labor market that is designed to pay people small sums of money to complete human intelligence tests – tasks that are difficult for computers but easy for people.","{'title': '1 Introduction', 'number': '1'}"
We show that:,"{'title': '1 Introduction', 'number': '1'}"
Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.,"{'title': '2 Related work', 'number': '2'}"
"These tasks included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.","{'title': '2 Related work', 'number': '2'}"
Snow et al. measured the quality of non-expert annotations by comparing them against labels that had been previously created by expert annotators.,"{'title': '2 Related work', 'number': '2'}"
"They report inter-annotator agreement between expert and non-expert annotators, and show that the average of many non-experts converges on performance of a single expert for many of their tasks.","{'title': '2 Related work', 'number': '2'}"
"Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.","{'title': '2 Related work', 'number': '2'}"
"These include public forums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program.","{'title': '2 Related work', 'number': '2'}"
Various types of human judgments are used.,"{'title': '2 Related work', 'number': '2'}"
"NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and WMT collect relative rankings (Callison-Burch et al., 2008; Paul, 2006), and DARPA evaluates using HTER (Snover et al., 2006).","{'title': '2 Related work', 'number': '2'}"
The details of these are provided later in the paper.,"{'title': '2 Related work', 'number': '2'}"
Public evaluation campaigns provide a ready source of goldstandard data that non-expert annotations can be compared to.,"{'title': '2 Related work', 'number': '2'}"
Amazon describes its Mechanical Turk web service1 as artificial artificial intelligence.,"{'title': '3 Mechanical Turk', 'number': '3'}"
"The name and tag line refer to a historical hoax from the 18th century where an automaton appeared to be able to beat human opponents at chess using a clockwork mechanism, but was, in fact, controlled by a person hiding inside the machine.","{'title': '3 Mechanical Turk', 'number': '3'}"
The Mechanical Turk web site provides a way to pay people small amounts of money to perform tasks that are simple for humans but difficult for computers.,"{'title': '3 Mechanical Turk', 'number': '3'}"
Examples of these Human Intelligence Tasks (or HITs) range from labeling images to moderating blog comments to providing feedback on relevance of results for a search query.,"{'title': '3 Mechanical Turk', 'number': '3'}"
Anyone with an Amazon account can either submit HITs or work on HITs that were submitted by others.,"{'title': '3 Mechanical Turk', 'number': '3'}"
Workers are sometimes referred to as “Turkers” and people designing the HITs are “Requesters.” Requesters can specify the amount that they will pay for each item that is completed.,"{'title': '3 Mechanical Turk', 'number': '3'}"
Payments are frequently as low as $0.01.,"{'title': '3 Mechanical Turk', 'number': '3'}"
Turkers are free to select whichever HITs interest them.,"{'title': '3 Mechanical Turk', 'number': '3'}"
"Amazon provides three mechanisms to help ensure quality: First, Requesters can have each HIT be completed by multiple Turkers, which allows higher quality labels to be selected, for instance, by taking the majority label.","{'title': '3 Mechanical Turk', 'number': '3'}"
"Second, the Requester can require that all workers meet a particular set of qualications, such as sufficient accuracy on a small test set or a minimum percentage of previously accepted submissions.","{'title': '3 Mechanical Turk', 'number': '3'}"
"Finally, the Requester has the option of rejecting the work of individual workers, in which case they are not paid.","{'title': '3 Mechanical Turk', 'number': '3'}"
"The level of good-faith participation by Turkers is surprisingly high, given the generally small nature of the payment.2 For complex undertakings like creating data for NLP tasks, Turkers do not have a specialized background in the subject, so there is an obvious tradeoff between hiring individuals from this non-expert labor pool and seeking out annotators who have a particular expertise.","{'title': '3 Mechanical Turk', 'number': '3'}"
We use Mechanical Turk as an inexpensive way of evaluating machine translation.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"In this section, we measure the level of agreement between expert and non-expert judgments of translation quality.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"To do so, we recreate an existing set of goldstandard judgments of machine translation quality taken from the Workshop on Statistical Machine Translation (WMT), which conducts an annual large-scale human evaluation of machine translation quality.","{'title': '4 Experts versus non-experts', 'number': '4'}"
The experts who produced the goldstandard judgments are computational linguists who develop machine translation systems.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
We recreated all judgments from the WMT08 German-English News translation task.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
The output of the 11 different machine translation systems that participated in this task was scored by ranking translated sentences relative to each other.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"To collect judgements, we reproduced the WMT08 web interface in Mechanical Turk and provided these instructions: Evaluate machine translation quality Rank each translation from Best to Worst relative to the other choices (ties are allowed).","{'title': '4 Experts versus non-experts', 'number': '4'}"
"If you do not know the source language then you can read the reference translation, which was created by a professional human translator.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"The web interface displaced 5 different machine translations of the same source sentence, and had radio buttons to rate them.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"Turkers were paid a grand total of $9.75 to complete nearly 1,000 HITs.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"These HITs exactly replicated the 200 screens worth of expert judgments that were collected for the WMT08 German-English News translation task, with each screen being completed by five different Turkers.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"The Turkers were shown a source sentence, a reference translation, and translations from five MT systems.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"They were asked to rank the translations relative to each other, assigning scores from best to worst and allowing ties.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"We evaluate non-expert Turker judges by measuring their inter-annotator agreement with the WMT08 expert judges, and by comparing the correlation coefficient across the rankings of the machine translation systems produced by the two sets of judges. equal.","{'title': '4 Experts versus non-experts', 'number': '4'}"
The quality of their works varies.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"Figure 2 shows the agreement of individual Turkers with expert annotators, plotted against the number of HITs they completed.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"The figure shows that their agreement varies considerably, and that Turker who completed the most judgments was among the worst performing.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"To avoid letting careless annotators drag down results, we experimented with weighted voting.","{'title': '4 Experts versus non-experts', 'number': '4'}"
We weighted votes in two ways: Turker agreed with the rest of the Turkers over the whole data set.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
This does not require any gold standard calibration data.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"It goes beyond simple voting, because it looks at a Turker’s performance over the entire set, rather than on an item-by-item basis.","{'title': '4 Experts versus non-experts', 'number': '4'}"
Figure 1 shows that these weighting mechanisms perform similarly well.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"For this task, deriving weights from agreement with other non-experts is as effective as deriving weights from experts.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"Moreover, by weighting the votes of five Turkers, non-expert judgments perform at the upper bound of expert-expert correlation.","{'title': '4 Experts versus non-experts', 'number': '4'}"
All correlate more strongly than Bleu. we are able to achieve the same rate of agreement with experts as they achieve with each other.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"Correlation when ranking systems In addition to measuring agreement with experts at the sentence-level, we also compare non-expert system-level rankings with experts.","{'title': '4 Experts versus non-experts', 'number': '4'}"
"Following Callison-Burch et al. (2008), we assigned a score to each of the 11 MT systems based on how often its translations were judged to be better than or equal to any other system.","{'title': '4 Experts versus non-experts', 'number': '4'}"
These scores were used to rank systems and we measured Spearman’s ρ against the system-level ranking produced by experts.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
Figure 3 shows how well the non-expert rankings correlate with expert rankings.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
An upper bound is indicated by the expert-expert bar.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
This was created using a five-fold cross validation where we used 20% of the expert judgments to rank the systems and measured the correlation against the rankings produced by the other 80% of the judgments.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
This gave a ρ of 0.78.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"All ways of combining the non-expert judgments resulted in nearly identical correlation, and all produced correlation within the range of with what we would experts to.","{'title': '4 Experts versus non-experts', 'number': '4'}"
The rankings produced using Mechanical Turk had a much stronger correlation with the WMT08 expert rankings than the Blue score did.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
It should be noted that the WMT08 data set does not have multiple reference translations.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
If multiple references were used that Bleu would likely have stronger correlation.,"{'title': '4 Experts versus non-experts', 'number': '4'}"
"However, it is clear that the cost of hiring professional translators to create multiple references for the 2000 sentence test set would be much greater than the $10 cost of collecting manual judgments on Mechanical Turk.","{'title': '4 Experts versus non-experts', 'number': '4'}"
In this section we report on a number of creative uses of Mechanical Turk to do more sophisticated tasks.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We give evidence that Turkers can create high quality translations for some languages, which would make creating multiple reference translations for Bleu less costly than using professional translators.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We report on experiments evaluating translation quality with HTER and with reading comprehension tests.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"In addition to evaluating machine translation quality, we also investigated the possibility of using Mechanical Turk to create additional reference translations for use with automatic metrics like Bleu.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Before trying this, we were skeptical that Turkers would have sufficient language skills to produce translations.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Our translation HIT had the following instructions: We solicited translations for 50 sentences in French, German, Spanish, Chinese and Urdu, and designed the HIT so that five Turkers would translate each sentence.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Filtering machine translation Upon inspecting the Turker’s translations it became clear that many had ignored the instructions, and had simply cutand-paste machine translation rather then translating the text themselves.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We therefore set up a second HIT to filter these out.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
After receiving the score when one LDC translator is compared against the other 10 translators (or the other 2 translators in the case of Urdu).,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
This gives an upper bound on the expected quality.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The Turkers’ translation quality falls within a standard deviation of LDC translators for Spanish, German and Chinese.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"For all languages, Turkers produce significantly better translations than an online machine translation system. translations, we had a second group of Turkers clean the results.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We automatically excluded Turkers whose translations were flagged 30% of the time or more.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Quality of Turkers’ translations Our 50 sentence test sets were selected so that we could compare the translations created by Turkers to translations commissioned by the Linguistics Data Consortium.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"For the Chinese, French, Spanish, and German translations we used the the MultipleTranslation Chinese Corpus.3 This corpus has 11 reference human translations for each Chinese source sentence.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We had bilingual graduate students translate the first 50 English sentences of that corpus into French, German and Spanish, so that we could re-use the multiple English reference translations.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The Urdu sentences were taken from the NIST MT Eval 2008 Urdu-English Test Set4 which includes three distinct English translations for every Urdu source sentence.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Figure 4 shows the Turker’s translation quality in terms of the Bleu metric.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"To establish an upper bound on expected quality, we determined what the Bleu score would be for a professional translator when measured against other professionals.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We calculated a Bleu score for each of the 11 LDC translators using the other 10 translators as the reference set.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The average Bleu score for LDC2002T01 was 0.54, with a standard deviation of 0.07.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The average Bleu for the Urdu test set is lower because it has fewer reference translations.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"To measure the Turkers’ translation quality, we randomly selected translations of each sentence from Turkers who passed the Detect MT HIT, and compared them against the same sets of 10 reference translations that the LDC translators were compared against.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We randomly sampled the Turkers 10 times, and calculated averages and standard deviations for each source language.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Figure 4 the Bleu scores for the Turkers’ translations of Spanish, German and Chinese are within the range of the LDC translators.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"For all languages, the quality is significantly higher than an online machine translation system.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We used Yahoo’s Babelfish for Spanish, German, French and Chinese,5 was likely and Babylon for Urdu.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Demographics We collected demographic information about the Turkers who completed the translation task.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We asked how long they had spoken the source language, how long they had spostatistics on the left are for people who appeared to do the task honestly.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The statistics on the right are for people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT). ken English, what their native language was, and where they lived.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Table 1 gives their replies.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Cost and speed We paid Turkers $0.10 to translate each sentence, and $0.006 to detect whether a sentence was machine translated.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The cost is low enough that we could create a multiple reference set quite cheaply; it would cost less than $1,000 to create 4 reference translations for 2000 sentences.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The time it took for the 250 translations to be completed for each language varied.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"It took less than 4 hours for Spanish, 20 hours for French, 22.5 hours for German, 2 days for Chinese, and nearly 4 days for Urdu.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Human-mediated translation edit rate (HTER) is the official evaluation metric of the DARPA GALE program.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The evaluation is conducted annually by the Linguistics Data Consortium, and it is used to determine whether the teams participating the program have met that year’s benchmarks.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
These evaluations are used as a “Go / No Go” determinant of whether teams will continue to receive funding.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Thus, each team have a strong incentive to get as good a result as possible under the metric.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Each of the three GALE teams encompasses multiple sites and each has a collection of machine translation systems.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"A general strategy employed by all teams is to perform system combination over these systems to produce a synthetic translation that is better than the sum of its parts (Matusov et al., 2006; Rosti et al., 2007).","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The contribution of each component system is weighted by the expectation that it will produce good output.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"To our knowledge, none of the teams perform their own HTER evaluations in order to set these weights.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We evaluated the feasibility of using Mechanical Turk to perform HTER.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We simplified the official GALE post-editing guidelines (NIST and LDC, 2007).","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We provided these instructions: Edit Machine Translation Your task is to edit the machine translation making as few changes as possible so that it matches the meaning of the human translation and is good English.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Please follow these guidelines: edit rate decreases as the number of editors increases from zero (where HTER is simply the TER score between the MT output and the reference translation) and five.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We displayed 10 sentences from a news article.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"In one column was the reference English translation, in the other column were text boxes containing the MT output to be edited.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"To minimize the edit rate, we collected edits from five different Turkers for every machine translated segment.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We verified these with a second HIT were we prompted Turkers to: For the final score, we choose the edited segment which passed the criteria and which minimized the edit distance to the unedited machine translation output.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"If none of the five edits was deemed to be acceptable, then we used the edit distance between the MT and the reference.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Setup We evaluated five machine translation systems using HTER.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"These systems were selected from WMT09 (Callison-Burch et al., 2009).","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We wanted a spread in quality, so we took the top two and bottom two systems from the GermanEnglish task, and the top system from the FrenchEnglish task (which significantly outperformed everything else).","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Based on the results of the WMT09 evaluation we would expect the see the following ranking from the least edits to the most edits: google.fr-en, google.de-en, rbmt5.de-en, geneva.de-en and tromble.de-en.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Results Table 2 gives the HTER scores for the five systems.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Their ranking is as predicted, indicating that the editing is working as expected.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The table reports averaged scores when the five annotators are subsampled.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
This gives a sense of how much each additional editor is able to minimize the score for each system.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The difference between the TER score with zero editors, and the HTER five editors is greatest for the rmbt5 system, which has a delta of .29 and is smallest for jhu-tromble with .07.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
One interesting technique for evaluating machine translation quality is through reading comprehension questions about automatically translated text.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The quality of machine translation systems can be quantified based on how many questions are answered correctly.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Jones et al. (2005) evaluated translation quality using a reading comprehension test the Defense Language Proficiency Test (DLPT), which is administered to military translators.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The DLPT contains a collection of foreign articles of varying levels of difficulties, and a set of short answer questions.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Jones et al used the Arabic DLPT to do a study of machine translation quality, by automatically translating the Arabic documents into English and seeing how many human subjects could successfully pass the exam.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
The advantage of this type of evaluation is that the results have a natural interpretation.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"They indicate how understandable the output of a machine translation system is better than Bleu does, and better than other manual evaluation like the relative ranking.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Despite this advantage, evaluating MT through reading comprehension hasn’t caught on, due to the difficulty of administering it and due to the fact that the DLPT or similar tests are not publicly available.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We conducted a reading comprehension evaluation using Mechanical Turk.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Instead of simply administering the test on Mechanical Turk, we used it for all aspects from test creation to answer grading.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Our procedure was as follows: Test creation We posted human translations of foreign news articles, and ask Tukers to write three questions and provide sample answers.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We gave simple instructions on what qualifies as a good reading comprehension question.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"System google.fr-en google.de-en rbmt5.de-en geneva.de-en tromble.de-en Question selection We posted the questions for each article back to Mechanical Turk, and asked other Turkers to vote on whether each question was a good and to indicate if it was redundant with any other questions in the set.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We sorted questions to maximize the votes and minimized redundancies using a simple perl script, which discarded questions below a threshold, and eliminated all redundancies.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Taking the test We posted machine translated versions of the foreign articles along with the questions, and had Turkers answer them.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
We ensured that no one would see multiple translations of the same article.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Grading the answers We aggregated the answers and used Mechanical Turk to grade them.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"We showed the human translation of the article, one question, the sample answer, and displayed all answers to it.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"After the Turkers graded the answers, we calculated the percentage of questions that were answered correctly for each system.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"Turkers created 90 questions for 10 articles, which were subsequently filtered down to 47 good questions, ranging from 3–6 questions per article.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
25 Turkers answered questions about each translated article.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"To avoid them answering the questions multiple times, we randomly selected which system’s translation was shown to them.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Each system’s translation was displayed an average of 5 reference 0.94 google.fr-en 0.85 google.de-en 0.80 rbmt5.de-en 0.77 geneva.de-en 0.63 jhu-tromble.de-en 0.50 times per article.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"As a control, we had three Turkers answer the reading comprehension questions using the reference translation.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Table 3 gives the percent of questions that were correctly answered using each of the different systems’ outputs and using the reference translation.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The ranking is exactly what we would expect, based on the HTER scores and on the human evaluation of the systems in WMT09.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
This again helps to validate that the reading comprehension methodology.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
"The scores are more interpretable than Blue scores and than the WMT09 relative rankings, since it gives an indication of how understandable the MT output is.","{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Appendix A shows some sample questions and answers for an article.,"{'title': '5 Feasibility of more complex evaluations', 'number': '5'}"
Mechanical Turk is an inexpensive way of gathering human judgments and annotations for a wide variety of tasks.,"{'title': '6 Conclusions', 'number': '6'}"
In this paper we demonstrate that it is feasible to perform manual evaluations of machine translation quality using the web service.,"{'title': '6 Conclusions', 'number': '6'}"
"The low cost of the non-expert labor found on Mechanical Turk is cheap enough to collect redundant annotations, which can be utilized to ensure translation quality.","{'title': '6 Conclusions', 'number': '6'}"
By combining the judgments of many non-experts we are able to achieve the equivalent quality of experts.,"{'title': '6 Conclusions', 'number': '6'}"
"The suggests that manual evaluation of translation quality could be straightforwardly done to validate performance improvements reported in conference papers, or even for mundane tasks like tracking incremental system updates.","{'title': '6 Conclusions', 'number': '6'}"
This challenges the conventional wisdom which has long held that automatic metrics must be used since manual evaluation is too costly and timeconsuming.,"{'title': '6 Conclusions', 'number': '6'}"
We have shown that Mechanical Turk can be used creatively to produce quite interesting things.,"{'title': '6 Conclusions', 'number': '6'}"
"We showed how a reading comprehension test could be created, administered, and graded, with only very minimal intervention.","{'title': '6 Conclusions', 'number': '6'}"
"We believe that it is feasible to use Mechanical Turk for a wide variety of other machine translated tasks like creating word alignments for sentence pairs, verifying the accuracy of document- and sentence-alignments, performing non-simulated active learning experiments for statistical machine translation, even collecting training data for low resource languages like Urdu.","{'title': '6 Conclusions', 'number': '6'}"
"The cost of using Mechanical Turk is low enough that we might consider attempting quixotic things like human-in-the-loop minimum error rate training (Zaidan and Callison-Burch, 2009), or doubling the amount of training data available for Urdu.","{'title': '6 Conclusions', 'number': '6'}"
"This research was supported by the EuroMatrixPlus project funded by the European Commission, and by the US National Science Foundation under grant IIS-0713448.","{'title': 'Acknowledgments', 'number': '7'}"
The views and findings are the author’s alone.,"{'title': 'Acknowledgments', 'number': '7'}"
"The actress Heather Locklear, Amanda on the popular series Melrose Place, was arrested this weekend in Santa Barbara (California) after driving under the influence of drugs.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"A witness saw her performing inappropriate maneuvers while trying to take her car out of a parking space in Montecito, as revealed to People magazine by a spokesman for the Californian Highway Police.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"The witness stated that around 4.30pm Ms. Locklear “hit the accelerator very roughly, making excessive noise and trying to take the car out from the parking space with abrupt back and forth maneuvers.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"While reversing, she passed several times in front of his sunglasses.” Shortly after, the witness, who at first, apparently had not recognized the actress, saw Ms. Locklear stopping in a nearby street and leaving the vehicle.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"It was this person who alerted the emergency services, because “he was concerned about Ms. Locklear’s life.” When the patrol arrived, the police found the actress sitting inside her car, which was partially blocking the road.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"“She seemed confused,” so the policemen took her to a specialized centre for drugs and alcohol and submitted her a test.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"According to a spokesman for the police, the actress was cooperative and excessive alcohol was ruled out from the beginning, even if “as the officers initially observed, we believe Ms. Locklear was under the influences drugs.” Ms. Locklear was arrested under suspicion of driving under the influence of some - unspecified substance, and imprisoned in the local jail at 7.00pm, to be released some hours later.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
"Two months ago, Ms. Locklear was released from a specialist clinic in Arizona where she was treated after an episode of anxiety and depression.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
4 questions were selected She was arested on suspicion of driving under the influence of drugs.,"{'title': 'A Example reading comprehension questions', 'number': '8'}"
She was cured for anxiety and depression.,"{'title': 'A Example reading comprehension questions', 'number': '8'}"
"Answers to Where was Ms. Locklear two months ago? that were judged to be correct: Arizona hospital for treatment of depression; at a treatmend clinic in Arizona; in the Arizona clinic being treated for nervous breakdown; a clinic in Arizona; Arizona, under treatment for depression; She was a patient in a clinic in Arizona undergoing treatment for anxiety and depression; In an Arizona mental health facility ; A clinic in Arizona.","{'title': 'A Example reading comprehension questions', 'number': '8'}"
; In a clinic being treated for anxiety and depression.,"{'title': 'A Example reading comprehension questions', 'number': '8'}"
; at an Arizona clinic These answers were judged to be incorrect: Locklear was retired in Arizona; Arizona; Arizona; in Arizona; Ms.Locklaer were laid off after a treatment out of the clinic in Arizona.,"{'title': 'A Example reading comprehension questions', 'number': '8'}"
