col1,col2
"In addition to a high accuracy, short parsing and training times are the most important properties of a parser.",{}
"However, pars ing and training times are still relatively long.",{}
"To determine why, we analyzed thetime usage of a dependency parser.",{}
We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity.,{}
"To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel.",{}
The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.,{}
This has lead to a higher accuracy.,{}
We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm.,{}
"We are convinced that the HashKernel and the parallelization can be ap plied successful to other NLP applicationsas well such as transition based depen dency parsers, phrase structrue parsers, and machine translation.",{}
Highly accurate dependency parsers have high de mands on resources and long parsing times.,"{'title': 'Introduction', 'number': '1'}"
The training of a parser frequently takes several days and the parsing of a sentence can take on averageup to a minute.,"{'title': 'Introduction', 'number': '1'}"
The parsing time usage is impor tant for many applications.,"{'title': 'Introduction', 'number': '1'}"
"For instance, dialog systems only have a few hundred milliseconds toanalyze a sentence and machine translation sys tems, have to consider in that time some thousandtranslation alternatives for the translation of a sen tence.","{'title': 'Introduction', 'number': '1'}"
"Parsing and training times can be improved by methods that maintain the accuracy level, or methods that trade accuracy against better parsing times.","{'title': 'Introduction', 'number': '1'}"
Software developers and researchers areusually unwilling to reduce the quality of their ap plications.,"{'title': 'Introduction', 'number': '1'}"
"Consequently, we have to consider atfirst methods to improve a parser, which do not in volve an accuracy loss, such as faster algorithms,faster implementation of algorithms, parallel al gorithms that use several CPU cores, and feature selection that eliminates the features that do not improve accuracy.","{'title': 'Introduction', 'number': '1'}"
"We employ, as a basis for our parser, the secondorder maximum spanning tree dependency pars ing algorithm of Carreras (2007).","{'title': 'Introduction', 'number': '1'}"
"This algorithmfrequently reaches very good, or even the best la beled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajic?","{'title': 'Introduction', 'number': '1'}"
"et al, 2009).","{'title': 'Introduction', 'number': '1'}"
"We combined thisparsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al, 2003; McDon ald et al, 2005; Crammer et al, 2006).","{'title': 'Introduction', 'number': '1'}"
A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times.,"{'title': 'Introduction', 'number': '1'}"
The rest of the paper is structured as follows.,"{'title': 'Introduction', 'number': '1'}"
"In Section 2, we describe related work.","{'title': 'Introduction', 'number': '1'}"
"In section 3, we analyze the time usage of the components of 89the parser.","{'title': 'Introduction', 'number': '1'}"
"In Section 4, we introduce a new Kernel that resolves some of the bottlenecks and im proves the performance.","{'title': 'Introduction', 'number': '1'}"
"In Section 5, we describethe parallel parsing algorithms which nearly allowed us to divide the parsing times by the number of cores.","{'title': 'Introduction', 'number': '1'}"
"In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm.","{'title': 'Introduction', 'number': '1'}"
"In Section 7, we conclude with a sum mary and an outline of further research.","{'title': 'Introduction', 'number': '1'}"
"The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006).","{'title': 'Related Work. ', 'number': '2'}"
"Transition based parsers typically have a linear or quadratic complexity (Nivre et al, 2004; Attardi, 2006).Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars ing time.","{'title': 'Related Work. ', 'number': '2'}"
"Titov and Henderson (2007) combined a transition based parsing algorithm, which used abeam search with a latent variable machine learn ing technique.","{'title': 'Related Work. ', 'number': '2'}"
Maximum spanning tree dependency based parsers decomposes a dependency structure into parts known as ?factors?.,"{'title': 'Related Work. ', 'number': '2'}"
"The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.","{'title': 'Related Work. ', 'number': '2'}"
This algorithm has a quadratic complexity.,"{'title': 'Related Work. ', 'number': '2'}"
The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.,"{'title': 'Related Work. ', 'number': '2'}"
This algo rithm uses in addition to the first order factors: theedges to those children which are closest to the de pendent.,"{'title': 'Related Work. ', 'number': '2'}"
"The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild.","{'title': 'Related Work. ', 'number': '2'}"
The edge labeling is an integral part of the algorithm which requires an additional loop over the labels.,"{'title': 'Related Work. ', 'number': '2'}"
This algorithm therefore has a complexity of O(n4).,"{'title': 'Related Work. ', 'number': '2'}"
Johansson and Nugues (2008) reduced the needed number of loops over the edge labels by using only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination.The transition based parsers have a lower com plexity.,"{'title': 'Related Work. ', 'number': '2'}"
"Nevertheless, the reported run times inthe last shared tasks were similar to the maxi mum spanning tree parsers.","{'title': 'Related Work. ', 'number': '2'}"
"For a transition based parser, Gesmundo et al (2009) reported run times between 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se mantic dependencies.","{'title': 'Related Work. ', 'number': '2'}"
"The parsing times were about one word per second, which speeds upquickly with a smaller beam-size, although the ac curacy of the parser degrades a bit.","{'title': 'Related Work. ', 'number': '2'}"
Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007).,"{'title': 'Related Work. ', 'number': '2'}"
We built a baseline parser to measure the time usage.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The baseline parser resembles the architec ture of McDonald and Pereira (2006).,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"It consists of the second order parsing algorithm of Carreras(2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The features are listed in Table 4.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"As in McDonald et al (2005), the parser stores the features of each training example in a file.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"In each epoch of the training, the feature file is read, and the weights are calculated and stored in an array.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
This procedure is up to 5 times faster than computing the features each time anew.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
But the parser has to maintain large arrays: for the weights of the sentence and the training file.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Therefore, the parser needs 3GB of main memoryfor English and 100GB of disc space for the train ing file.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The parsing time is approximately 20% faster, since some of the values did not have to be recalculated.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
Algorithm 1 illustrates the training algorithm in pseudo code.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"is the set of training examples where an example is a pair (xi, yi) of a sentence and the corresponding dependency structure.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
??wand ??v are weight vectors.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The first loop ex tracts features from the sentence xi and maps the features to numbers.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The numbers are grouped into three vectors for the features of all possible edges ?h,d, possible edges in combination withsiblings ?h,d,s and in combination with grandchil 90 te+s tr tp ta rest total te pars.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
train.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
sent.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
feat.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"LAS UAS Chinese 4582 748 95 - 3 846 3298 3262 84h 22277 8.76M 76.88 81.27 English 1509 168 12.5 20 1.5 202 1223 1258 38.5h 39279 8.47M 90.14 92.45 German 945 139 7.7 17.8 1.5 166 419 429 26.7h 36020 9.16M 87.64 90.03 Spanish 3329 779 36 - 2 816 2518 2550 16.9h 14329 5.51M 86.02 89.54 Table 1: te+s is the elapsed time in milliseconds to extract and store the features, tr to read the features and to calculate the weight arrays, tp to predict the projective parse tree, ta to apply the non-projective approximation algorithm, rest is the time to conduct the other parts such as the update function, train.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"is the total training time per instance (tr + tp + ta+rest ), and te is the elapsed time to extract the features.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The next columns illustrate the parsing time in milliseconds per sentence for the test set, training time in hours, the number of sentences in the training set, the total number of features in million, the labeled attachment score of the test set, and the unlabeled attachment score.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Algorithm 1: Training ? baseline algorithm ? = {(xi, yi)}Ii=1 // Training data??w = 0,??v = 0 ? = E ? I // passive-aggresive update weight for i = 1 to I tss+e; extract-and-store-features(xi); tes+e; for n = 1 to E // iteration over the training epochs for i = 1 to I // iteration over the training examples k ?","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"(n? 1) ? I + i ? = E ? I ? k + 2 // passive-aggressive weight tsr,k; A = read-features-and-calc-arrays(i,??w ) ; ter,k tsp,k; yp = predicte-projective-parse-tree(A);tep,k tsa,k; ya = non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average dren ?h,d,g where h, d, g, and s are the indexes of the words included in xi.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Finally, the method stores the feature vectors on the hard disc.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The next two loops build the main part of the training algorithm.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The outer loop iterates over the number of training epochs, while the innerloop iterates over all training examples.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The on line training algorithm considers a single training example in each iteration.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The first function in the loop reads the features and computes the weights A for the factors in the sentence xi.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
A is a set of weight arrays.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"A = {??w ? ??f h,d,??w ? ??f h,d,s,??w ? ??f h,d,g} The parsing algorithm uses the weight arrays to predict a projective dependency structure yp.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The non-projective approximation algorithm has as input the dependency structure and the weightarrays.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
It rearranges the edges and tries to in crease the total score of the dependency structure.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"This algorithm builds a dependency structure ya,which might be non-projective.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The training al gorithm updates ??w according to the difference between the predicted dependency structures ya and the reference structure yi.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"It updates ??v as well, whereby the algorithm additionally weights the updates by ?.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Since the algorithm decreases ? in each round, the algorithm adapts the weights more aggressively at the beginning (Crammer etal., 2006).","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"After all iterations, the algorithm com putes the average of ??v , which reduces the effect of overfitting (Collins, 2002).","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"We have inserted into the training algorithm functions to measure the start times ts and the end times te for the procedures to compute andstore the features, to read the features, to predict the projective parse, and to calculate the nonprojective approximation.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"We calculate the aver age elapsed time per instance, as the average over all training examples and epochs: tx = ?E?I k=1 t e x,k?tsx,k E?I . We use the training set and the test set of theCoNLL shared task 2009 for our experiments.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
Ta ble 1 shows the elapsed times in 11000 seconds (milliseconds) of the selected languages for the procedure calls in the loops of Algorithm 1.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"We had to measure the times for the feature extractionin the parsing algorithm, since in the training al gorithm, the time can only be measured together with the time for storing the features.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The table contains additional figures for the total training time and parsing scores.1 The parsing algorithm itself only required, to our surprise, 12.5 ms (tp) for a English sentence 1We use a Intel Nehalem i7 CPU 3.33 Ghz.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"With turbo mode on, the clock speed was 3.46 Ghz.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"91 on average, while the feature extraction needs 1223 ms. To extract the features takes about100 times longer than to build a projective dependency tree.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The feature extraction is already implemented efficiently.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
It uses only numbers to rep resent features which it combines to a long integer number and then maps by a hash table2 to a 32bit integer number.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The parsing algorithm uses the integer number as an index to access the weights in the vectors ??w and ??v .The complexity of the parsing algorithm is usu ally considered the reason for long parsing times.However, it is not the most time consuming component as proven by the above analysis.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"There fore, we investigated the question further, askingwhat causes the high time consumption of the fea ture extraction?","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"In our next experiment, we left out the mapping of the features to the index of the weight vectors.The feature extraction takes 88 ms/sentence with out the mapping and 1223 ms/sentence with the mapping.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The feature?index mapping needs 93% of the time to extract the features and 91% of thetotal parsing time.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"What causes the high time con sumption of the feature?index mapping?The mapping has to provide a number as an in dex for the features in the training examples and to filter out the features of examples built, while theparser predicts the dependency structures.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The al gorithm filters out negative features to reduce the memory requirement, even if they could improve the parsing result.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
We will call the features built due to the training examples positive features and the rest negative features.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
We counted 5.8 timesmore access to negative features than positive fea tures.We now look more into the implementation details of the used hash table to answer the pre viously asked question.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The hash table for the feature?index mapping uses three arrays: one for the keys, one for the values and a status array to indicate the deleted elements.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
If a program storesa value then the hash function uses the key to cal culate the location of the value.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Since the hashfunction is a heuristic function, the predicted lo cation might be wrong, which leads to so-called 2We use the hash tables of the trove library: http://sourceforge.net/projects/trove4j.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
hash misses.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
In such cases the hash algorithm has to retry to find the value.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
We counted 87% hash misses including misses where the hash had to retry several times.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"The number of hash misseswas high, because of the additional negative fea tures.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
The CPU cache can only store a small amount of the data from the hash table.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Therefore, the memory controller has frequently to transfer data from the main memory into the CPU.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
This procedure is relatively slow.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
We traced down the high time consumption to the access of the key and the access of the value.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Successive accessesto the arrays are fast, but the relative random ac cesses via the hash function are very slow.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"Thelarge number of accesses to the three arrays, be cause of the negative features, positive features and because of the hash misses multiplied by the time needed to transfer the data into the CPU are the reason for the high time consumption.We tried to solve this problem with Bloom filters, larger hash tables and customized hash func tions to reduce the hash misses.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
These techniquesdid not help much.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"However, a substantial im provement did result when we eliminated the hash table completely, and directly accessed the weight vectors ??w and ??v with a hash function.","{'title': 'Analysis of Time Usage. ', 'number': '3'}"
This led us to the use of Hash Kernels.,"{'title': 'Analysis of Time Usage. ', 'number': '3'}"
"A Hash Kernel for structured data uses a hash function h : J ? {1...n} to index ?, cf.","{'title': 'Hash Kernel. ', 'number': '4'}"
Shi etal.,"{'title': 'Hash Kernel. ', 'number': '4'}"
(2009).,"{'title': 'Hash Kernel. ', 'number': '4'}"
maps the observations X to a feature space.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"We define ?(x, y) as the numeric fea ture representation indexed by J . Let ?k(x, y) = ?j(x, y) the hash based feature?index mapping,where h(j) = k. The process of parsing a sen tence xi is to find a parse tree yp that maximizes a scoring function argmaxyF (xi, y).","{'title': 'Hash Kernel. ', 'number': '4'}"
The learning problem is to fit the function F so that the errors of the predicted parse tree y are as low as possible.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"The scoring function of the Hash Kernel is F (x, y) = ??w ? ?(x, y) where ??w is the weight vector and the size of ??w is n. Algorithm 2 shows the update function of the Hash Kernel.","{'title': 'Hash Kernel. ', 'number': '4'}"
"We derived the update function from the update function of MIRA (Crammer et 92 Algorithm 2: Update of the Hash Kernel // yp = arg maxyF (xi, y) update(??w,??v , xi, yi, yp, ?) ?","{'title': 'Hash Kernel. ', 'number': '4'}"
"= ?(yi, yp) // number of wrong labeled edges if ? > 0 then ??u ?","{'title': 'Hash Kernel. ', 'number': '4'}"
"(?(xi, yi)?","{'title': 'Hash Kernel. ', 'number': '4'}"
"?(xi, yp)) ? = ??(F (xt,yi)?F (xi,yp))||??u ||2??w ? ??w + ? ?","{'title': 'Hash Kernel. ', 'number': '4'}"
??u ??v ? ~v + ? ?,"{'title': 'Hash Kernel. ', 'number': '4'}"
"??u return ??w , ??v al., 2006).","{'title': 'Hash Kernel. ', 'number': '4'}"
"The parameters of the function are the weight vectors ??w and ??v , the sentence xi, the gold dependency structure yi, the predicted dependency structure yp, and the update weight ?.","{'title': 'Hash Kernel. ', 'number': '4'}"
The function ? calculates the number ofwrong labeled edges.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"The update function updates the weight vectors, if at least one edge is la beled wrong.","{'title': 'Hash Kernel. ', 'number': '4'}"
"It calculates the difference ??u of the feature vectors of the gold dependency structure ?(xi, yi) and the predicted dependency structure?(xi, yp).","{'title': 'Hash Kernel. ', 'number': '4'}"
"Each time, we use the feature represen tation ?, the hash function h maps the features to integer numbers between 1 and |??w |.","{'title': 'Hash Kernel. ', 'number': '4'}"
After that the update function calculates the margin ? and updates ??w and ??v respectively.,"{'title': 'Hash Kernel. ', 'number': '4'}"
Algorithm 3 shows the training algorithm forthe Hash Kernel in pseudo code.,"{'title': 'Hash Kernel. ', 'number': '4'}"
A main dif ference to the baseline algorithm is that it does not store the features because of the required time which is needed to store the additional negative features.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"Accordingly, the algorithm first extracts the features for each training instance, then maps the features to indexes for the weight vector with the hash function and calculates the weight arrays.","{'title': 'Hash Kernel. ', 'number': '4'}"
Algorithm 3: Training ? Hash Kernel for n?,"{'title': 'Hash Kernel. ', 'number': '4'}"
1 to E // iteration over the training epochs for i?,"{'title': 'Hash Kernel. ', 'number': '4'}"
1 to I // iteration over the training exmaples k ?,"{'title': 'Hash Kernel. ', 'number': '4'}"
(n? 1) ? I + i ? ?,"{'title': 'Hash Kernel. ', 'number': '4'}"
"E ? I ? k + 2 // passive-aggressive weight tse,k; A?","{'title': 'Hash Kernel. ', 'number': '4'}"
"extr.-features-&-calc-arrays(i,??w ) ; tee,k tsp,k; yp?","{'title': 'Hash Kernel. ', 'number': '4'}"
"predicte-projective-parse-tree(A);tep,k tsa,k; ya?","{'title': 'Hash Kernel. ', 'number': '4'}"
"non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average For different j, the hash function h(j) might generate the same value k. This means that the hash function maps more than one feature to thesame weight.","{'title': 'Hash Kernel. ', 'number': '4'}"
We call such cases collisions.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"Col lisions can reduce the accuracy, since the weights are changed arbitrarily.","{'title': 'Hash Kernel. ', 'number': '4'}"
"This procedure is similar to randomization of weights (features), which aims to save space by sharing values in the weight vector (Blum., 2006; Rahimi and Recht, 2008).","{'title': 'Hash Kernel. ', 'number': '4'}"
"The Hash Kernel shares values when collisions occur that can be considered as an approximation of the kernel function, because a weight might be adapted due to more than one feature.","{'title': 'Hash Kernel. ', 'number': '4'}"
If the approximation works well then we would need only a relatively small weight vector otherwise we need a larger weight vector to reduce the chance of collisions.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"In an experiments, we compared two hash functions and different hash sizes.","{'title': 'Hash Kernel. ', 'number': '4'}"
We selected for the comparison a standard hash function (h1) and a custom hash function (h2).,"{'title': 'Hash Kernel. ', 'number': '4'}"
The idea for the custom hash function h2 is not to overlap the values of the feature sequence number and the edge label with other values.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"These values are stored at the beginning of a long number, which represents a feature.","{'title': 'Hash Kernel. ', 'number': '4'}"
h1 ? |(l xor(l ? 0xffffffff00000000 >> 32))% size|3 h2 ? |(l xor ((l >> 13) ? 0xffffffffffffe000) xor ((l >> 24) ? 0xffffffffffff0000) xor ((l >> 33) ? 0xfffffffffffc0000) xor ((l >> 40) ? 0xfffffffffff00000)) % size | vector size h1 #(h1) h2 #(h2) 411527 85.67 0.41 85.74 0.41 3292489 87.82 3.27 87.97 3.28 10503061 88.26 8.83 88.35 8.77 21006137 88.19 12.58 88.41 12.53 42012281 88.32 12.45 88.34 15.27 115911564?,"{'title': 'Hash Kernel. ', 'number': '4'}"
88.32 17.58 88.39 17.34 179669557 88.34 17.65 88.28 17.84Table 2: The labeled attachment scores for differ ent weight vector sizes and the number of nonzero values in the feature vectors in millions.,"{'title': 'Hash Kernel. ', 'number': '4'}"
Not a prime number.,"{'title': 'Hash Kernel. ', 'number': '4'}"
Table 2 shows the labeled attachment scores for selected weight vector sizes and the number of nonzero weights.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"Most of the numbers in Table2 are primes, since they are frequently used to obtain a better distribution of the content in hash ta 3>> n shifts n bits right, and % is the modulo operation.","{'title': 'Hash Kernel. ', 'number': '4'}"
93bles.,"{'title': 'Hash Kernel. ', 'number': '4'}"
h2 has more nonzero weights than h1.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"Nevertheless, we did not observe any clear improve ment of the accuracy scores.","{'title': 'Hash Kernel. ', 'number': '4'}"
The values do not change significantly for a weight vector size of 10 million and more elements.,"{'title': 'Hash Kernel. ', 'number': '4'}"
We choose a weightvector size of 115911564 values for further exper iments since we get more non zero weights and therefore fewer collisions.,"{'title': 'Hash Kernel. ', 'number': '4'}"
te tp ta r total par.,"{'title': 'Hash Kernel. ', 'number': '4'}"
trai.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"Chinese 1308 - 200 3 1511 1184 93h English 379 21.3 18.2 1.5 420 354 46h German 209 12 15.3 1.7 238 126 24h Spanish 1056 - 39 2 1097 1044 44h Table 3: The time in milliseconds for the featureextraction, projective parsing, non-projective ap proximation, rest (r), the total training time perinstance, the average parsing (par.)","{'title': 'Hash Kernel. ', 'number': '4'}"
time in mil liseconds for the test set and the training time in hours 0 1 2 3 0 5000 10000 15000 Spanish Figure 1: The difference of the labeled attachment score between the baseline parser and the parser with the Hash Kernel (y-axis) for increasing large training sets (x-axis).,"{'title': 'Hash Kernel. ', 'number': '4'}"
Table 3 contains the measured times for the Hash Kernel as used in Algorithm 2.,"{'title': 'Hash Kernel. ', 'number': '4'}"
The parserneeds 0.354 seconds in average to parse a sen tence of the English test set.,"{'title': 'Hash Kernel. ', 'number': '4'}"
This is 3.5 times faster than the baseline parser.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"The reason for that is the faster feature mapping of the Hash Kernel.Therefore, the measured time te for the feature ex traction and the calculation of the weight arrays are much lower than for the baseline parser.","{'title': 'Hash Kernel. ', 'number': '4'}"
The training is about 19% slower since we could no longer use a file to store the feature indexes of the training examples because of the large number of negative features.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"We counted about twice the number of nonzero weights in the weight vector of the Hash Kernel compared to the baseline parser.For instance, we counted for English 17.34 Mil lions nonzero weights in the Hash Kernel and 8.47 Millions in baseline parser and for Chinese 18.28 Millions nonzero weights in the Hash Kernel and 8.76 Millions in the baseline parser.","{'title': 'Hash Kernel. ', 'number': '4'}"
Table 6 shows.,"{'title': 'Hash Kernel. ', 'number': '4'}"
the scores for all languages of the shared task2009.,"{'title': 'Hash Kernel. ', 'number': '4'}"
The attachment scores increased for all languages.,"{'title': 'Hash Kernel. ', 'number': '4'}"
It increased most for Catalan and Span ish.,"{'title': 'Hash Kernel. ', 'number': '4'}"
These two corpora have the smallest training sets.,"{'title': 'Hash Kernel. ', 'number': '4'}"
We searched for the reason and found thatthe Hash Kernel provides an overproportional ac curacy gain with less training data compared to MIRA.,"{'title': 'Hash Kernel. ', 'number': '4'}"
Figure 1 shows the difference between the labeled attachment score of the parser with MIRA and the Hash Kernel for Spanish.,"{'title': 'Hash Kernel. ', 'number': '4'}"
The decreasing curve shows clearly that the Hash Kernel providesan overproportional accuracy gain with less train ing data compared to the baseline.,"{'title': 'Hash Kernel. ', 'number': '4'}"
"This provides an advantage for small training corpora.However, this is probably not the main rea son for the high improvement, since for languageswith only slightly larger training sets such as Chinese the improvement is much lower and the gra dient at the end of the curve is so that a huge amount of training data would be needed to make the curve reach zero.","{'title': 'Hash Kernel. ', 'number': '4'}"
Current CPUs have up to 12 cores and we will see soon CPUs with more cores.,"{'title': 'Parallelization. ', 'number': '5'}"
Also graphiccards provide many simple cores.,"{'title': 'Parallelization. ', 'number': '5'}"
Parsing algo rithms can use several cores.,"{'title': 'Parallelization. ', 'number': '5'}"
"Especially, the tasks to extract the features and to calculate the weightarrays can be well implemented as parallel algo rithm.","{'title': 'Parallelization. ', 'number': '5'}"
We could also successful parallelize theprojective parsing and the non-projective approximation algorithm.,"{'title': 'Parallelization. ', 'number': '5'}"
Algorithm 4 shows the paral lel feature extraction in pseudo code.,"{'title': 'Parallelization. ', 'number': '5'}"
The mainmethod prepares a list of tasks which can be per formed in parallel and afterwards it creates thethreads that perform the tasks.,"{'title': 'Parallelization. ', 'number': '5'}"
"Each thread re moves from the task list an element, carries out the task and stores the result.","{'title': 'Parallelization. ', 'number': '5'}"
This procedure is repeated until the list is empty.,"{'title': 'Parallelization. ', 'number': '5'}"
The main method waits until all threads are completed and returns the result.,"{'title': 'Parallelization. ', 'number': '5'}"
"For the parallel algorithms, Table 5 shows the elapsed times depend on the number of 94 # Standard Features # Linear Features Linear G. Features Sibling Features 1 l,hf ,hp,d(h,d) 14 l,hp,h+1p,dp,d(h,d) 44 l,gp,dp,d+1p,d(h,d) 99 l,sl,hp,d(h,d)?r(h,d) 2 l,hf ,d(h,d) 15 l,hp,d-1p,dp,d(h,d) 45 l,gp,dp,d-1p,d(h,d) 100 l,sl,dp,d(h,d)?r(h,d) 3 l,hp,d(h,d) 16 l,hp,dp,d+1p,d(h,d) 46 l,gp,g+1p,d-1p,dp,d(h,d) 101 l,hl,dp,d(h,d)?r(h,d) 4 l,df ,dp,d(h,d) 17 l,hp,h+1p,d-1p,dp,d(h,d) 47 l,g-1p,gp,d-1p,dp,d(h,d) 102 l,dl,sp,d(h,d)?r(h,d) 5 l,hp,d(h,d) 18 l,h-1p,h+1p,d-1p,dp,d(h,d) 48 l,gp,g+1p,dp,d+1p,d(h,d) 75 l,?dm,?sm,d(h,d) 6 l,dp,d(h,d) 19 l,hp,h+1p,dp,d+1p,d(h,d) 49 l,g-1p,gp,dp,d+1p,d(h,d) 76 l,?hm,?sm,d(h,s) 7 l,hf ,hp,df ,dp,d(h,d) 20 l,h-1p,hp,dp,d-1p,d(h,d) 50 l,gp,g+1p,hp,d(h,d) Linear S. Features 8 l,hp,df ,dp,d(h,d) Grandchild Features 51 l,gp,g-1p,hp,d(h,d) 58 l,sp,s+1p,hp,d(h,d) 9 l,hf ,df ,dp,d(h,d) 21 l,hp,dp,gp,d(h,d,g) 52 l,gp,hp,h+1p,d(h,d) 59 l,sp,s-1p,hp,d(h,d) 10 l,hf ,hp,df ,d(h,d) 22 l,hp,gp,d(h,d,g) 53 l,gp,hp,h-1p,d(h,d) 60 l,sp,hp,h+1p,d(h,d) 11 l,hf ,df ,hp,d(h,d) 23 l,dp,gp,d(h,d,g) 54 l,gp,g+1p,h-1p,hp,d(h,d) 61 l,sp,hp,h-1p,d(h,d) 12 l,hf ,df ,d(h,d) 24 l,hf ,gf ,d(h,d,g) 55 l,g-1p,gp,h-1p,hp,d(h,d) 62 l,sp,s+1p,h-1p,d(h,d) 13 l,hp,dp,d(h,d) 25 l,df ,gf ,d(h,d,g) 56 l,gp,g+1p,hp,h+1p,d(h,d) 63 l,s-1p,sp,h-1p,d(h,d) 77 l,hl,hp,d(h,d) 26 l,gf ,hp,d(h,d,g) 57 l,g-1p,gp,hp,h+1p,d(h,d) 64 l,sp,s+1p,hp,d(h,d) 78 l,hl,d(h,d) 27 l,gf ,dp,d(h,d,g) Sibling Features 65 l,s-1p,sp,hp,h+1p,d(h,d) 79 l,hp,d(h,d) 28 l,hf ,gp,d(h,d,g) 30 l,hp,dp,sp,d(h,d) ?r(h,d) 66 l,sp,s+1p,dp,d(h,d) 80 l,dl,dp,d(h,d) 29 l,df ,gp,d(h,d,g) 31 l,hp,sp,d(h,d)?r(h,d) 67 l,sp,s-1p,dp,d(h,d) 81 l,dl,d(h,d) 91 l,hl,gl,d(h,d,g) 32 l,dp,sp,d(h,d)?r(h,d) 68 sp,dp,d+1p,d(h,d) 82 l,dp,d(h,d) 92 l,dp,gp,d(h,d,g) 33 l,pf ,sf ,d(h,d)?r(h,d) 69 sp,dp,d-1p,d(h,d) 83 l,dl,hp,dp,hl,d(h,d) 93 l,gl,hp,d(h,d,g) 34 l,pp,sf ,d(h,d)?r(h,d) 70 sp,s+1p,d-1p,dp,d(h,d) 84 l,dl,hp,dp,d(h,d) 94 l,gl,dp,d(h,d,g) 35 l,sf ,pp,d(h,d)?r(h,d) 71 s-1p,sp,d-1p,dp,d(h,d) 85 l,hl,dl,dp,d(h,d) 95 l,hl,gp,d(h,d,g) 36 l,sf ,dp,d(h,d)?r(h,d) 72 sp,s+1p,dp,d+1p,d(h,d) 86 l,hl,hp,dp,d(h,d) 96 l,dl,gp,d(h,d,g) 37 l,sf ,dp,d(h,d)?r(h,d) 73 s-1p,sp,dp,d+1p,d(h,d) 87 l,hl,dl,hp,d(h,d) 74 l,?dm,?gm,d(h,d) 38 l,df ,sp,d(h,d)?r(h,d) Special Feature 88 l,hl,dl,d(h,d) Linear G. Features 97 l,hl,sl,d(h,d)?r(h,d) 39 ?l,hp,dp,xpbetween h,d 89 l,hp,dp,d(h,d) 42 l,gp,g+1p,dp,d(h,d) 98 l,dl,sl,d(h,d)?r(h,d) 41 l,?hm,?dm,d(h,d) 43 l,gp,g-1p,dp,d(h,d) Table 4: Features Groups.","{'title': 'Parallelization. ', 'number': '5'}"
"l represents the label, h the head, d the dependent, s a sibling, and g a grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.","{'title': 'Parallelization. ', 'number': '5'}"
used cores.,"{'title': 'Parallelization. ', 'number': '5'}"
The parsing time is 1.9 times fasteron two cores and 3.4 times faster on 4 cores.,"{'title': 'Parallelization. ', 'number': '5'}"
Hy per threading can improve the parsing times again and we get with hyper threading 4.6 faster parsingtimes.,"{'title': 'Parallelization. ', 'number': '5'}"
"Hyper threading possibly reduces the over head of threads, which contains already our single core version.","{'title': 'Parallelization. ', 'number': '5'}"
Algorithm 4: Parallel Feature Extraction A // weight arrays extract-features-and-calc-arrays(xi) data-list?,"{'title': 'Parallelization. ', 'number': '5'}"
{} // thread-save data list for w1 ? 1 to |xi| for w2 ? 1 to |xi| data-list?,"{'title': 'Parallelization. ', 'number': '5'}"
"data-list ?{(w1, w2)} c?","{'title': 'Parallelization. ', 'number': '5'}"
number of CPU cores for t?,"{'title': 'Parallelization. ', 'number': '5'}"
"1 to c Tt ? create-array-thread(t, xi,data-list) start array-thread Tt// start thread t for t?","{'title': 'Parallelization. ', 'number': '5'}"
1 to c join Tt// wait until thread t is finished A?,"{'title': 'Parallelization. ', 'number': '5'}"
A ? collect-result(Tt) return A // array-thread T d?,"{'title': 'Parallelization. ', 'number': '5'}"
remove-first-element(data-list) if d is empty then end-thread ...,"{'title': 'Parallelization. ', 'number': '5'}"
// extract features and calculate part d of A Cores te tp ta rest total pars.,"{'title': 'Parallelization. ', 'number': '5'}"
train.,"{'title': 'Parallelization. ', 'number': '5'}"
1 379 21.3 18.2 1.5 420 354 45.8h 2 196 11.7 9.2 2.1 219 187 23.9h 3 138 8.9 6.5 1.6 155 126 16.6h 4 106 8.2 5.2 1.6 121 105 13.2h 4+4h 73.3 8.8 4.8 1.3 88.2 77 9.6hTable 5: Elapsed times in milliseconds for differ ent numbers of cores.,"{'title': 'Parallelization. ', 'number': '5'}"
The parsing time (pars.),"{'title': 'Parallelization. ', 'number': '5'}"
are expressed in milliseconds per sentence and the training (train.),"{'title': 'Parallelization. ', 'number': '5'}"
time in hours.,"{'title': 'Parallelization. ', 'number': '5'}"
The last row shows the times for 8 threads on a 4 core CPU with Hyper-threading.,"{'title': 'Parallelization. ', 'number': '5'}"
"For these experiment, we set the clock speed to 3.46 Ghz in order to have the same clock speed for all experiments.","{'title': 'Parallelization. ', 'number': '5'}"
"ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon ald and Pereira (2006).","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
The algorithm rearranges edges in a dependency tree when they improve the score.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
Bohnet (2009) extended the algorithm by a threshold which biases the rearrangement of the edges.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"With a threshold, it is possible to gain a higher percentage of correct dependency links.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"We determined a threshold in experiments for Czech, English and German.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"In the experiment,we use the Hash Kernel and increase the thresh 95 System Average Catalan Chinese Czech English German Japanese Spanish Top CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1) Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12 this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13 Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che et al.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
The numbers in bold face mark the top scores.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
old at the beginning in small steps by 0.1 and later in larger steps by 0.5 and 1.0.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"Figure 2 showsthe labeled attachment scores for the Czech, En glish and German development set in relation to the rearrangement threshold.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
The curves for all languages are a bit volatile.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
The English curve is rather flat.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
It increases a bit until about 0.3and remains relative stable before it slightly decreases.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
The labeled attachment score for Ger man and Czech increases until 0.3 as well and thenboth scores start to decrease.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
For English a thresh old between 0.3 and about 2.0 would work well.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"For German and Czech, a threshold of about 0.3is the best choice.","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
We selected for all three lan guages a threshold of 0.3.,"{'title': 'Non-Projective Approximation. ', 'number': '6'}"
"74 76 78 80 82 84 86 88 0 1 2 3 4 5 Czech English GermanFigure 2: English, German, and Czech labeled at tachment score (y-axis) for the development set in relation to the rearrangement threshold (x-axis).","{'title': 'Non-Projective Approximation. ', 'number': '6'}"
We have developed a very fast parser with ex cellent attachment scores.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"For the languages of the 2009 CoNLL Shared Task, the parser could reach higher accuracy scores on average than the top performing systems.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"The scores for Catalan, Chinese and Japanese are still lower than the top scores.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"However, the parser would have ranked second for these languages.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"For Catalan and Chinese, the top results obtained transition-basedparsers.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"Therefore, the integration of both tech niques as in Nivre and McDonald (2008) seems to be very promising.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"For instance, to improvethe accuracy further, more global constrains capturing the subcategorization correct could be inte grated as in Riedel and Clarke (2006).","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
Our fasteralgorithms may make it feasible to consider fur ther higher order factors.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"In this paper, we have investigated possibilities for increasing parsing speed without any accuracyloss.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
The parsing time is 3.5 times faster on a sin gle CPU core than the baseline parser which has an typical architecture for a maximum spanning tree parser.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
The improvement is due solely to theHash Kernel.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
The Hash Kernel was also a prereq uisite for the parallelization of the parser because it requires much less memory bandwidth which is nowadays a bottleneck of parsers and many other applications.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"By using parallel algorithms, we could further increase the parsing time by a factor of 3.4 on a 4 core CPU and including hyper threading by a factor of 4.6.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
The parsing speed is 16 times fasterfor the English test set than the conventional ap proach.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
The parser needs only 77 millisecond in average to parse a sentence and the speed willscale with the number of cores that become avail able in future.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"To gain even faster parsing times, it may be possible to trade accuracy against speed.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"In a pilot experiment, we have shown that it is possible to reduce the parsing time in this way to as little as 9 milliseconds.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"We are convinced thatthe Hash Kernel can be applied successful to tran sition based dependency parsers, phrase structure parsers and many other NLP applications.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
4 4We provide the Parser and Hash Kernel as open source for download from http://code.google.com/p/mate-tools.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
96,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
