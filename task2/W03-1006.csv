col1,col2
"We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.",{}
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.,{}
Syntax mediates between surface word order and meaning.,"{'title': '1 Introduction', 'number': '1'}"
The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giving a semantic interpretation of a string of words.,"{'title': '1 Introduction', 'number': '1'}"
"So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available.","{'title': '1 Introduction', 'number': '1'}"
"The completion of the first phase of the PropBank (Kingsbury et al., 2002) represents an important step.","{'title': '1 Introduction', 'number': '1'}"
"The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994).","{'title': '1 Introduction', 'number': '1'}"
"The arc labels chosen for the arguments are specific to the predicate, not universal.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)).","{'title': '1 Introduction', 'number': '1'}"
"Specifically, we show that the syntactic dependency structure that results load from the extraction of a Tree Adjoining Grammar (TAG) from the PTB, and the features that accompany this structure, form a better basis for determining semantic role labels.","{'title': '1 Introduction', 'number': '1'}"
"Crucially, the same structure is also produced when parsing with TAG.","{'title': '1 Introduction', 'number': '1'}"
"We suggest that the syntactic representation chosen in the PTB is less well suited for semantic processing than the other, deeper syntactic representations.","{'title': '1 Introduction', 'number': '1'}"
"In fact, this deeper representation expresses syntactic notions that have achieved a wide acceptance across linguistic frameworks, unlike the very particular surface-syntactic choices made by the linguists who created the PTB syntactic annotation rules.","{'title': '1 Introduction', 'number': '1'}"
The outline of this paper is as follows.,"{'title': '1 Introduction', 'number': '1'}"
In Section 2 we introduce the PropBank and describe the problem of predicting semantic tags.,"{'title': '1 Introduction', 'number': '1'}"
Section 3 presents an overview of our work and distinguishes it from previous work.,"{'title': '1 Introduction', 'number': '1'}"
Section 4 describes the method used to produce the TAGs that are the basis of our experiments.,"{'title': '1 Introduction', 'number': '1'}"
Section 5 specifies how training and test data that are used in our experiments are derived from the PropBank.,"{'title': '1 Introduction', 'number': '1'}"
"Next, we give results on two sets of experiments.","{'title': '1 Introduction', 'number': '1'}"
Those that predict semantic tags given gold-standard linguistic information are described in Section 6.,"{'title': '1 Introduction', 'number': '1'}"
Those that do prediction from raw text are described in Section 7.,"{'title': '1 Introduction', 'number': '1'}"
"Finally, in Section 8 we present concluding remarks.","{'title': '1 Introduction', 'number': '1'}"
"The PropBank (Kingsbury et al., 2002) annotates the PTB with dependency structures (or ‘predicateargument’ structures), using sense tags for each word and local semantic labels for each argument and adjunct.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"Argument labels are numbered and used consistently across syntactic alternations for the same verb meaning, as shown in Figure 1.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"Adjuncts are given special tags such as TMP (for temporal), or LOC (for locatives) derived from the original annotation of the Penn Treebank.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"In addition to the annotated corpus, PropBank provides a lexicon which lists, for each meaning of each annotated verb, its roleset, i.e., the possible arguments in the predicate and their labels.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"As an example, the entry for the verb kick, is given in Figure 2.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"The notion of “meaning” used is fairly coarse-grained, typically motivated from differing syntactic behavior.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"Since each verb meaning corresponds to exactly one roleset, these terms are often used interchangeably.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"The roleset also includes a “descriptor” field which is intended for use during annotation and as documentation, but which does not have any theoretical standing.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
Each entry also includes examples.,"{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"Currently there are frames for about 1600 verbs in the corpus, with a total of 2402 rolesets.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"Since we did not yet have access to a corpus annotated with rolesets, we concentrate in this paper on predicting the role labels for the arguments.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"It is only once we have both that we can interpret the relation between predicate and argument at a very fine level (for example, truck in he kicked the truck withhay as the destination of the loading action).","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
We will turn to the problem of assigning rolesets to predicates once the data is available.,"{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"We note though that preliminary investigations have shown that for about 65% of predicates (tokens) in the WSJ, there is only one roleset.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
"In a further 7% of predicates (tokens), the set of semantic labels on the arguments of that predicate completely disambiguates the roleset.","{'title': '2 The PropBank and the Labeling of Semantic Roles', 'number': '2'}"
Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy.,"{'title': '3 Overview', 'number': '3'}"
"Furthermore, they show that this method can be used in conjunction with a parser to produce parses annotated with semantic labels, and that the parser outperforms a chunker.","{'title': '3 Overview', 'number': '3'}"
The features they use in their experiments can be listed as follows.,"{'title': '3 Overview', 'number': '3'}"
Head Word (HW.),"{'title': '3 Overview', 'number': '3'}"
The predicate’s head word as well as the argument’s head word is used.,"{'title': '3 Overview', 'number': '3'}"
Phrase Type.,"{'title': '3 Overview', 'number': '3'}"
This feature represents the type of phrase expressing the semantic role.,"{'title': '3 Overview', 'number': '3'}"
In Figure 3 phrase type for the argument prices is NP.,"{'title': '3 Overview', 'number': '3'}"
Path.,"{'title': '3 Overview', 'number': '3'}"
This feature captures the surface syntactic relation between the argument’s constituent and the predicate.,"{'title': '3 Overview', 'number': '3'}"
See Figure 3 for an example.,"{'title': '3 Overview', 'number': '3'}"
Position.,"{'title': '3 Overview', 'number': '3'}"
This binary feature represents whether the argument occurs before or after the predicate in the sentence.,"{'title': '3 Overview', 'number': '3'}"
Voice.,"{'title': '3 Overview', 'number': '3'}"
This binary feature represents whether the predicate is syntactically realized in either passive or active voice.,"{'title': '3 Overview', 'number': '3'}"
"Notice that for the exception of voice, the features solely represent surface syntax aspects of the input parse tree.","{'title': '3 Overview', 'number': '3'}"
This should not be taken to mean that deep syntax features are not important.,"{'title': '3 Overview', 'number': '3'}"
"For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function.","{'title': '3 Overview', 'number': '3'}"
"Aside from voice, we posit that other deep linguistic features may be useful to predict semantic role.","{'title': '3 Overview', 'number': '3'}"
"In this work, we explore the use of more general, deeper syntax features.","{'title': '3 Overview', 'number': '3'}"
We also experiment with semantic features derived from the PropBank.,"{'title': '3 Overview', 'number': '3'}"
Our methodology is as follows.,"{'title': '3 Overview', 'number': '3'}"
The first stage entails generating features representing different levels of linguistic analysis.,"{'title': '3 Overview', 'number': '3'}"
This is done by first automatically extracting several kinds of TAG from the PropBank.,"{'title': '3 Overview', 'number': '3'}"
This may in itself generate useful features because TAG structures typically relate closely syntactic arguments with their corresponding predicate.,"{'title': '3 Overview', 'number': '3'}"
"Beyond this, our TAG extraction procedure produces a set of features that relate TAG structures on both the surface-syntax as well as the deep-syntax level.","{'title': '3 Overview', 'number': '3'}"
"Finally, because a TAG is extracted from the PropBank, we have a set of semantic features derived indirectly from the PropBank through TAG.","{'title': '3 Overview', 'number': '3'}"
The second stage of our methodology entails using these features to predict semantic roles.,"{'title': '3 Overview', 'number': '3'}"
We first experiment with prediction of semantic roles given gold-standard parses from the test corpus.,"{'title': '3 Overview', 'number': '3'}"
We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser.,"{'title': '3 Overview', 'number': '3'}"
Our experiments depend upon automatically extracting TAGs from the PropBank.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002).","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features from the resulting TAG.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"A TAG is defined to be a set of lexicalized elementary trees (Joshi and Schabes, 1991).","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
They may be composed by several well-defined operations to form parse trees.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
A lexicalized elementary tree where the lexical item is removed is called a tree frame or a supertag.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The lexical item in the tree is called an anchor.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"Although the TAG formalism allows wide latitude in how elementary trees may be defined, various linguistic principles generally guide their formation.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"An important principle is that dependencies, including long-distance dependencies, are typically localized the same elementary tree by appropriate grouping of syntactically or semantically related elements.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The extraction procedure fragments a parse tree from the PTB that is provided as input into elementary trees.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
See Figure 4.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
These elementary trees can be composed by TAG operations to form the original parse tree.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"Salient heuristics include the use of a head percolation table (Magerman, 1995), and another table that distinguishes between complements and adjunct nodes in the tree.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For our current work, we use the head percolation table to determine heads of phrases.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"Also, we treat a PropBank argument (ARG0 ... ARG9) as a complement and a PropBank adjunct (ARGM’s) as an adjunct when such annotation is available.1 Otherwise, we basically follow the approach of (Chen, 2001).2 Besides introducing one kind of TAG extraction procedure, (Chen, 2001) introduces the notion of grouping linguistically-related extracted tree frames together.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"In one approach, each tree frame is decomposed into a feature vector.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
Each element of this vector describes a single linguistically-motivated characteristic of the tree.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The elements comprising a feature vector are listed in Table 1.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
Each elementary tree is decomposed into a feature vector in a relatively straightforward manner.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For example, the POS feature is obtained from the preterminal node of the elementary tree.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
There are also features that specify the syntactic transformations that an elementary tree exhibits.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
Each such transformation is recognized by structural pattern matching the elementary tree against a pattern that identifies the transformation’s existence.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For more details, see (Chen, 2001).","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"Given a set of elementary trees which compose a TAG, and also the feature vector corresponding to each tree, it is possible to annotate each node representing an argument in the tree with role information.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
These are syntactic roles including for example subject and direct object.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
Each argument node is labeled with two kinds of roles: a surface syntactic role and a deep syntactic role.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The former is obtained through determining the position of the node with respect to the anchor of the tree using the usually positional rules for determining argument status in English.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The latter is obtained from the former and also from knowledge of the syntactic transformations that have been applied to the tree.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For example, we determine the deep syntactic role of a whmoved element by “undoing” the wh-movement by using the trace information in the PTB.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
The PropBank contains all of the notation of the Penn Treebank as well as semantic notation.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For our current work, we extract two kinds of TAG from the PropBank.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"One grammar, SEM-TAG, has elementary trees annotated with the aforementioned syntactic information as well as semantic information.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
Semantic information includes semantic role as well as semantic subcategorization information.,"{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"The other grammar, SYNT-TAG, differs from SEM-TAG only by the absence of any semantic role information.","{'title': '4 Extraction of TAGs from the PropBank', 'number': '4'}"
"For our experiments, we use a version of the PropBank where the most commonly appearing predicates have been annotated, not all.","{'title': '5 Corpora', 'number': '5'}"
Our extracted TAGs are derived from Sections 02-21 of the PTB.,"{'title': '5 Corpora', 'number': '5'}"
"Furthermore, training data for our experiments are always derived from these sections.","{'title': '5 Corpora', 'number': '5'}"
Section 23 is used for test data.,"{'title': '5 Corpora', 'number': '5'}"
The entire set of semantic roles that are found in the PropBank are not used in our experiments.,"{'title': '5 Corpora', 'number': '5'}"
"In particular, we only include as semantic roles those instances in the propbank such that in the extracted TAG they are localized in the same elementary tree.","{'title': '5 Corpora', 'number': '5'}"
"As a consequence, adjunct semantic roles (ARGM’s) are basically absent from our test corpus.","{'title': '5 Corpora', 'number': '5'}"
"Furthermore, not all of the complement semantic roles are found in our test corpus.","{'title': '5 Corpora', 'number': '5'}"
"For example, cases of subject-control PRO are ignored because the surface subject is found in a different tree frame than the predicate.","{'title': '5 Corpora', 'number': '5'}"
"Still, a large majority of complement semantic roles are found in our test corpus (more than 87%).","{'title': '5 Corpora', 'number': '5'}"
This section is devoted towards evaluating different features obtained from a gold-standard corpus in the task of determining semantic role.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
We use the feature set mentioned in Section 3 as well as features derived from TAGs mentioned in Section 4.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"In this section, we detail the latter set of features.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
We then describe the results of using different feature sets.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
These experiments are performed using the C4.5 decision tree machine learning algorithm.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
The standard settings are used.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Furthermore, results are always given using unpruned decision trees because we find that these are the ones that performed the best on a development set.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
These features are determined during the extraction of a TAG: Supertag Path.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is a path in a tree frame from its preterminal to a particular argument node in a tree frame.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
The supertag path of the subject of the rightmost tree frame in Figure 4 is VBGVPSNP.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Supertag.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This can be the tree frame corresponding to either the predicate or the argument.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Srole.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the surface-syntactic role of an argument.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Example of values include 0 (subject) and 1 (direct object).,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Ssubcat.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the surface-syntactic subcategorization frame.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the ssubcat corresponding to a transitive tree frame would be NP0 NP1.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
PPs as arguments are always annotated with the preposition.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the ssubcat for the passive version of hit would be NP1 NP2(by).","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Drole.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the deep-syntactic role of an argument.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Example of values include 0 (subject) and 1 (direct object).,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Dsubcat.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the deep-syntactic subcategorization frame.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the dsubcat corresponding to a transitive tree frame would be NP0 NP1.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Generally, PPs as arguments are annotated with the preposition.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the dsubcat for load is NP0 NP1 NP2(into).","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
The exception is when the argument is not realized as a PP when the predicate is realized in a non-syntactically transformed way.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the dsubcat for the passive version of hit would be NP0 NP1.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Semsubcat.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the semantic subcategorization frame.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Call this feature set GP0.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is presumably because our training and test data has been assembled in a different manner as mentioned in Section 5.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Our next experiment is on the same set of features, with the exception that Path has been replaced with Supertag Path.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
(Feature set GP1).,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
The error rate is reduced from 10.0% to 9.7%.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"This is statistically significant (t-test, p < 0.05), albeit a small improvement.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
One explanation for the improvement is that Path does not generalize as well as Supertag path does.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"For example, the path feature value VBGVPVPSNP reflects surface subject position in the sentence Prices are falling but so does VBGVPSNP in the sentence Sellers regret prices falling.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Because TAG localizes dependencies, the corresponding values for Supertag path in these sentences would be identical.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"We now experiment with our surface syntax features: Pred HW, Arg HW, Ssubcat, and Srole.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
(Feature set SURFACE.),"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Its performance on SEMTAG is 8.2% whereas its performance on SYNTTAG is 7.6%, a tangible improvement over previous models.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"One reason for the improvement could be that this model is assigning semantic labels with knowledge of the other roles the predicate assigns, unlike previous models.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Our next experiment involves using deep syntax features: Pred HW, Arg HW, Dsubcat, and Drole.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
(Feature set DEEP.),"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Its performance on both SEMTAG and SYNT-TAG is 6.5%, better than previous models.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Its performance is better than SURFACE presumably because syntactic transformations are taken to account by deep syntax features.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002).,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"This experiment considers use of semantic features: Pred HW, Arg HW, Semsubcat, and Drole.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
(Feature set SEMANTIC.),"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Of course, there are only results for SEM-TAG, which turns out to be 1.9%.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This is the best performance yet.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"In our final experiment, we use supertag features: pertag, Drole.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
(Feature set SUPERTAG.),"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
The error rates are 2.8% for SEM-TAG and 7.4% for SYNTTAG.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Considering SEM-TAG only, this model performs better than its corresponding DEEP model, probably because supertag for SEM-TAG include crucial semantic information.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"Considering SYNTTAG only, this model performs worse than its corresponding DEEP model, presumably because of sparse data problems when modeling supertags.","{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
This sparse data problem is also apparent by comparing the model based on SEM-TAG with the corresponding SEM-TAG SEMANTIC model.,"{'title': '6 Semantic Roles from Gold-Standard Linguistic Information', 'number': '6'}"
"In this section, we are concerned with the problem of finding semantic arguments and labeling them with their correct semantic role given raw text as input.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"In order to perform this task, we parse this raw text using a combination of supertagging and LDA, which is a method that yields partial dependency parses annotated with TAG structures.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
We perform this task using both SEM-TAG and SYNT-TAG.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"For the former, after supertagging and LDA, the task is accomplished because the TAG structures are already annotated with semantic role information.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"For the latter, we use the best performing model from Section 6 in order to find semantic roles given syntactic features from the parse.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Supertagging (Bangalore and Joshi (1999)) is the task of assigning a single supertag to each word given raw text as input.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"For example, given the sentence Prices are falling, a supertagger might return the supertagged sentence in Figure 4.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Supertagging returns an almost-parse in the sense that it is performing much parsing disambiguation.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"The typical technique to perform supertagging is the trigram model, akin to models of the same name for partof-speech tagging.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
This is the technique that we use here.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Data sparseness is a significant issue when supertagging with extracted grammar (Chen and Vijay-Shanker (2000)).,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"For this reason, we smooth the emit probabilities P(w1t) in the trigram model using distributional similarity following Chen (2001).","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"In particular, we use Jaccard’s coefficient as the similarity metric with a similarity threshold of 0.04 and a radius of 25 because these were found to attain optimal results in Chen (2001).","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Training data for supertagging is Sections 02-21 of the PropBank.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
A supertagging model based on SEM-TAG performs with 76.32% accuracy on Section 23.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The corresponding model for SYNT-TAG performs with 80.34% accuracy.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Accuracy is measured for all words in the sentence including punctuation.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"The SYNT-TAG model performs better than the SEM-TAG model, understandably, because SYNT-TAG is the simpler grammar.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
LDA is an acronym for Lightweight Dependency Analyzer (Srinivas (1997)).,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"Given as input a supertagged sequence of words, it outputs a partial dependency parse.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"It takes advantage of the fact that supertagging provides an almost-parse in order to dependency parse the sentence in a simple, deterministic fashion.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Basic LDA is a two step procedure.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The first step involves linking each word serving as a modifier with the word that it modifies.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The second step involves linking each word serving as an argument with its predicate.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Linking always only occurs so that grammatical requirements as stipulated by the supertags are satisfied.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"The version of LDA that is used in this work differs from Srinivas (1997) in that there are other constraints on the linking process.3 In particular, a link is not established if its existence would create crossing brackets or cycles in the dependency tree for the sentence.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"We perform LDA on two versions of Section 23, one supertagged with SEM-TAG and the other with SYNT-TAG.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The results are shown in Table 3.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Evaluation is performed on dependencies excluding leafnode punctuation.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Each dependency is evaluated according to both whether the correct head and dependent is related as well as whether they both receive the correct part of speech tag.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"The F-measure scores, in the 70% range, are relatively low compared to Collins (1999) which has a corresponding score of around 90%.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
This is perhaps to be expected because Collins (1999) is based on a full parser.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Note also that the accuracy of LDA is highly dependent on the accuracy of the supertagged input.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"This explains, for example, the fact that the accuracy on SEM-TAG supertagged input is lower than the accuracy with SYNT-TAG supertagged input.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The output of LDA is a partial dependency parse annotated with TAG structures.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
We can use this output to predict semantic roles of arguments.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The manner in which this is done depends on the kind of grammar that is used.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
The LDA output using SEM-TAG is already annotated with semantic role information because it is encoded in the grammar itself.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"On the other hand, the LDA output using SYNT-TAG contains strictly syntactic information.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"In this case, we use the highest performing model from Section 6 in order to label arguments with semantic roles.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Evaluation of prediction of semantic roles takes the following form.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Each argument labeled by a semantic role in the test corpus is treated as one trial.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Certain aspects of this trial are always checked for correctness.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
These include checking that the semantic role and the dependency-link are correct.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"There are other aspects which may or may not be checked, depending on the type of evaluation.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"One aspect, “bnd,” is whether or not the argument’s bracketing as specified in the dependency tree is correct.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"Another aspect, “arg,” is whether or not the headword of the argument is chosen to be correct.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Table 4 show the results when we use SEM-TAG in order to supertag the input and perform LDA.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"When the boundaries are found, finding the head word additionally does not result in a decrease of performance.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"However, correctly identifying the head word instead of the boundaries leads to an important increase in performance.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"Furthermore, note the low recall and high precision of the “base + arg” evaluation.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
In part this is due to the nature of the PropBank corpus that we are using.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"In particular, because not all predicates in our version of the PropBank are annotated with semantic roles, the supertagger for SEM-TAG will sometimes annotate text without semantic roles when in fact it should contain them.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
Table 5 shows the results of first supertagging the input with SYNT-TAG and then using a model trained on the DEEP feature set to annotate the resulting syntactic structure with semantic roles.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
This two-step approach greatly increases performance over the corresponding SEM-TAG based approach.,"{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999).","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
"They also experiment with using a chunker which yields a recall of 0.35, a precision of 0.50, and an F-measure of 0.41.","{'title': '7 Semantic Roles from Raw Text', 'number': '7'}"
We have presented various alternative approaches to predicting PropBank role labels using forms of linguistic information that are deeper than the PTB’s surface-syntax labels.,"{'title': '8 Conclusions', 'number': '8'}"
"These features may either be directly derived from a TAG, such as Supertag path, or indirectly via aspects of supertags, such Task: determine Recall Precision F base + arg 0.65 0.75 0.70 base + bnd 0.48 0.55 0.51 base + bnd + arg 0.48 0.55 0.51 as deep syntactic features like Drole.","{'title': '8 Conclusions', 'number': '8'}"
These are found to produce substantial improvements in accuracy.,"{'title': '8 Conclusions', 'number': '8'}"
We believe that such improvement is due to these features better capturing the syntactic information that is relevant for the task of semantic labeling.,"{'title': '8 Conclusions', 'number': '8'}"
"Also, these features represent syntactic categories about which there is a broad consensus in the literature.","{'title': '8 Conclusions', 'number': '8'}"
"Therefore, we believe that our results are portable to other frameworks and differently annotated corpora such as dependency corpora.","{'title': '8 Conclusions', 'number': '8'}"
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.,"{'title': '8 Conclusions', 'number': '8'}"
"Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example.","{'title': '8 Conclusions', 'number': '8'}"
This paper is based upon work supported by the National Science Foundation under the KDD program through a supplement to Grant No.,"{'title': 'Acknowledgments', 'number': '9'}"
IIS-98-17434.,"{'title': 'Acknowledgments', 'number': '9'}"
"Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation.","{'title': 'Acknowledgments', 'number': '9'}"
