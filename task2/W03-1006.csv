col1,col2
"We use deep linguistic features to predict semantic roles on syntactic arguments, and show that these perform considerably better than surface-oriented features.",Introduction
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.,Introduction
Syntax mediates between surface word order and meaning.,Experiment/Discussion
The goal of parsing (syntactic analysis) is ultimately to provide the first step towards giving a semantic interpretation of a string of words.,Experiment/Discussion
"So far, attention has focused on parsing, because the semantically annotated corpora required for learning semantic interpretation have not been available.",Experiment/Discussion
"The completion of the first phase of the PropBank (Kingsbury et al., 2002) represents an important step.",Experiment/Discussion
"The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994).",Experiment/Discussion
"The arc labels chosen for the arguments are specific to the predicate, not universal.",Experiment/Discussion
"In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)).",Experiment/Discussion
"Specifically, we show that the syntactic dependency structure that results load from the extraction of a Tree Adjoining Grammar (TAG) from the PTB, and the features that accompany this structure, form a better basis for determining semantic role labels.",Experiment/Discussion
"Crucially, the same structure is also produced when parsing with TAG.",Experiment/Discussion
"We suggest that the syntactic representation chosen in the PTB is less well suited for semantic processing than the other, deeper syntactic representations.",Experiment/Discussion
"In fact, this deeper representation expresses syntactic notions that have achieved a wide acceptance across linguistic frameworks, unlike the very particular surface-syntactic choices made by the linguists who created the PTB syntactic annotation rules.",Experiment/Discussion
The outline of this paper is as follows.,Experiment/Discussion
In Section 2 we introduce the PropBank and describe the problem of predicting semantic tags.,Experiment/Discussion
Section 3 presents an overview of our work and distinguishes it from previous work.,Experiment/Discussion
Section 4 describes the method used to produce the TAGs that are the basis of our experiments.,Experiment/Discussion
Section 5 specifies how training and test data that are used in our experiments are derived from the PropBank.,Experiment/Discussion
"Next, we give results on two sets of experiments.",Experiment/Discussion
Those that predict semantic tags given gold-standard linguistic information are described in Section 6.,Experiment/Discussion
Those that do prediction from raw text are described in Section 7.,Experiment/Discussion
"Finally, in Section 8 we present concluding remarks.",Experiment/Discussion
"The PropBank (Kingsbury et al., 2002) annotates the PTB with dependency structures (or ‘predicateargument’ structures), using sense tags for each word and local semantic labels for each argument and adjunct.",Experiment/Discussion
"Argument labels are numbered and used consistently across syntactic alternations for the same verb meaning, as shown in Figure 1.",Experiment/Discussion
"Adjuncts are given special tags such as TMP (for temporal), or LOC (for locatives) derived from the original annotation of the Penn Treebank.",Experiment/Discussion
"In addition to the annotated corpus, PropBank provides a lexicon which lists, for each meaning of each annotated verb, its roleset, i.e., the possible arguments in the predicate and their labels.",Experiment/Discussion
"As an example, the entry for the verb kick, is given in Figure 2.",Experiment/Discussion
"The notion of “meaning” used is fairly coarse-grained, typically motivated from differing syntactic behavior.",Experiment/Discussion
"Since each verb meaning corresponds to exactly one roleset, these terms are often used interchangeably.",Experiment/Discussion
"The roleset also includes a “descriptor” field which is intended for use during annotation and as documentation, but which does not have any theoretical standing.",Experiment/Discussion
Each entry also includes examples.,Experiment/Discussion
"Currently there are frames for about 1600 verbs in the corpus, with a total of 2402 rolesets.",Experiment/Discussion
"Since we did not yet have access to a corpus annotated with rolesets, we concentrate in this paper on predicting the role labels for the arguments.",Experiment/Discussion
"It is only once we have both that we can interpret the relation between predicate and argument at a very fine level (for example, truck in he kicked the truck withhay as the destination of the loading action).",Experiment/Discussion
We will turn to the problem of assigning rolesets to predicates once the data is available.,Experiment/Discussion
"We note though that preliminary investigations have shown that for about 65% of predicates (tokens) in the WSJ, there is only one roleset.",Experiment/Discussion
"In a further 7% of predicates (tokens), the set of semantic labels on the arguments of that predicate completely disambiguates the roleset.",Experiment/Discussion
Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy.,Experiment/Discussion
"Furthermore, they show that this method can be used in conjunction with a parser to produce parses annotated with semantic labels, and that the parser outperforms a chunker.",Experiment/Discussion
The features they use in their experiments can be listed as follows.,Experiment/Discussion
Head Word (HW.),Experiment/Discussion
The predicate’s head word as well as the argument’s head word is used.,Experiment/Discussion
Phrase Type.,Experiment/Discussion
This feature represents the type of phrase expressing the semantic role.,Experiment/Discussion
In Figure 3 phrase type for the argument prices is NP.,Experiment/Discussion
Path.,Experiment/Discussion
This feature captures the surface syntactic relation between the argument’s constituent and the predicate.,Experiment/Discussion
See Figure 3 for an example.,Experiment/Discussion
Position.,Experiment/Discussion
This binary feature represents whether the argument occurs before or after the predicate in the sentence.,Experiment/Discussion
Voice.,Experiment/Discussion
This binary feature represents whether the predicate is syntactically realized in either passive or active voice.,Experiment/Discussion
"Notice that for the exception of voice, the features solely represent surface syntax aspects of the input parse tree.",Experiment/Discussion
This should not be taken to mean that deep syntax features are not important.,Experiment/Discussion
"For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function.",Experiment/Discussion
"Aside from voice, we posit that other deep linguistic features may be useful to predict semantic role.",Experiment/Discussion
"In this work, we explore the use of more general, deeper syntax features.",Experiment/Discussion
We also experiment with semantic features derived from the PropBank.,Experiment/Discussion
Our methodology is as follows.,Experiment/Discussion
The first stage entails generating features representing different levels of linguistic analysis.,Experiment/Discussion
This is done by first automatically extracting several kinds of TAG from the PropBank.,Experiment/Discussion
This may in itself generate useful features because TAG structures typically relate closely syntactic arguments with their corresponding predicate.,Experiment/Discussion
"Beyond this, our TAG extraction procedure produces a set of features that relate TAG structures on both the surface-syntax as well as the deep-syntax level.",Experiment/Discussion
"Finally, because a TAG is extracted from the PropBank, we have a set of semantic features derived indirectly from the PropBank through TAG.",Experiment/Discussion
The second stage of our methodology entails using these features to predict semantic roles.,Experiment/Discussion
We first experiment with prediction of semantic roles given gold-standard parses from the test corpus.,Experiment/Discussion
We subsequently experiment with their prediction given raw text fed through a deterministic dependency parser.,Experiment/Discussion
Our experiments depend upon automatically extracting TAGs from the PropBank.,Experiment/Discussion
"In doing so, we follow the work of others in extracting grammars of various kinds from the PTB, whether it be TAG (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000), combinatory categorial grammar (Hockenmaier and Steedman, 2002), or constraint dependency grammar (Wang and Harper, 2002).",Experiment/Discussion
"We will discuss TAGs and an important principle guiding their formation, the extraction procedure from the PTB that is described in (Chen, 2001) including extensions to extract a TAG from the PropBank, and finally the extraction of deeper linguistic features from the resulting TAG.",Experiment/Discussion
"A TAG is defined to be a set of lexicalized elementary trees (Joshi and Schabes, 1991).",Experiment/Discussion
They may be composed by several well-defined operations to form parse trees.,Experiment/Discussion
A lexicalized elementary tree where the lexical item is removed is called a tree frame or a supertag.,Experiment/Discussion
The lexical item in the tree is called an anchor.,Experiment/Discussion
"Although the TAG formalism allows wide latitude in how elementary trees may be defined, various linguistic principles generally guide their formation.",Experiment/Discussion
"An important principle is that dependencies, including long-distance dependencies, are typically localized the same elementary tree by appropriate grouping of syntactically or semantically related elements.",Experiment/Discussion
The extraction procedure fragments a parse tree from the PTB that is provided as input into elementary trees.,Experiment/Discussion
See Figure 4.,Experiment/Discussion
These elementary trees can be composed by TAG operations to form the original parse tree.,Experiment/Discussion
The extraction procedure determines the structure of each elementary tree by localizing dependencies through the use of heuristics.,Experiment/Discussion
"Salient heuristics include the use of a head percolation table (Magerman, 1995), and another table that distinguishes between complements and adjunct nodes in the tree.",Experiment/Discussion
"For our current work, we use the head percolation table to determine heads of phrases.",Experiment/Discussion
"Also, we treat a PropBank argument (ARG0 ... ARG9) as a complement and a PropBank adjunct (ARGM’s) as an adjunct when such annotation is available.1 Otherwise, we basically follow the approach of (Chen, 2001).2 Besides introducing one kind of TAG extraction procedure, (Chen, 2001) introduces the notion of grouping linguistically-related extracted tree frames together.",Experiment/Discussion
"In one approach, each tree frame is decomposed into a feature vector.",Experiment/Discussion
Each element of this vector describes a single linguistically-motivated characteristic of the tree.,Experiment/Discussion
The elements comprising a feature vector are listed in Table 1.,Experiment/Discussion
Each elementary tree is decomposed into a feature vector in a relatively straightforward manner.,Experiment/Discussion
"For example, the POS feature is obtained from the preterminal node of the elementary tree.",Experiment/Discussion
There are also features that specify the syntactic transformations that an elementary tree exhibits.,Experiment/Discussion
Each such transformation is recognized by structural pattern matching the elementary tree against a pattern that identifies the transformation’s existence.,Experiment/Discussion
"For more details, see (Chen, 2001).",Experiment/Discussion
"Given a set of elementary trees which compose a TAG, and also the feature vector corresponding to each tree, it is possible to annotate each node representing an argument in the tree with role information.",Experiment/Discussion
These are syntactic roles including for example subject and direct object.,Experiment/Discussion
Each argument node is labeled with two kinds of roles: a surface syntactic role and a deep syntactic role.,Experiment/Discussion
The former is obtained through determining the position of the node with respect to the anchor of the tree using the usually positional rules for determining argument status in English.,Experiment/Discussion
The latter is obtained from the former and also from knowledge of the syntactic transformations that have been applied to the tree.,Experiment/Discussion
"For example, we determine the deep syntactic role of a whmoved element by “undoing” the wh-movement by using the trace information in the PTB.",Experiment/Discussion
The PropBank contains all of the notation of the Penn Treebank as well as semantic notation.,Experiment/Discussion
"For our current work, we extract two kinds of TAG from the PropBank.",Experiment/Discussion
"One grammar, SEM-TAG, has elementary trees annotated with the aforementioned syntactic information as well as semantic information.",Experiment/Discussion
Semantic information includes semantic role as well as semantic subcategorization information.,Experiment/Discussion
"The other grammar, SYNT-TAG, differs from SEM-TAG only by the absence of any semantic role information.",Experiment/Discussion
"For our experiments, we use a version of the PropBank where the most commonly appearing predicates have been annotated, not all.",Experiment/Discussion
Our extracted TAGs are derived from Sections 02-21 of the PTB.,Experiment/Discussion
"Furthermore, training data for our experiments are always derived from these sections.",Experiment/Discussion
Section 23 is used for test data.,Experiment/Discussion
The entire set of semantic roles that are found in the PropBank are not used in our experiments.,Experiment/Discussion
"In particular, we only include as semantic roles those instances in the propbank such that in the extracted TAG they are localized in the same elementary tree.",Experiment/Discussion
"As a consequence, adjunct semantic roles (ARGM’s) are basically absent from our test corpus.",Experiment/Discussion
"Furthermore, not all of the complement semantic roles are found in our test corpus.",Experiment/Discussion
"For example, cases of subject-control PRO are ignored because the surface subject is found in a different tree frame than the predicate.",Experiment/Discussion
"Still, a large majority of complement semantic roles are found in our test corpus (more than 87%).",Experiment/Discussion
This section is devoted towards evaluating different features obtained from a gold-standard corpus in the task of determining semantic role.,Experiment/Discussion
We use the feature set mentioned in Section 3 as well as features derived from TAGs mentioned in Section 4.,Experiment/Discussion
"In this section, we detail the latter set of features.",Experiment/Discussion
We then describe the results of using different feature sets.,Experiment/Discussion
These experiments are performed using the C4.5 decision tree machine learning algorithm.,Experiment/Discussion
The standard settings are used.,Experiment/Discussion
"Furthermore, results are always given using unpruned decision trees because we find that these are the ones that performed the best on a development set.",Experiment/Discussion
These features are determined during the extraction of a TAG: Supertag Path.,Experiment/Discussion
This is a path in a tree frame from its preterminal to a particular argument node in a tree frame.,Experiment/Discussion
The supertag path of the subject of the rightmost tree frame in Figure 4 is VBGVPSNP.,Experiment/Discussion
Supertag.,Experiment/Discussion
This can be the tree frame corresponding to either the predicate or the argument.,Experiment/Discussion
Srole.,Experiment/Discussion
This is the surface-syntactic role of an argument.,Experiment/Discussion
Example of values include 0 (subject) and 1 (direct object).,Experiment/Discussion
Ssubcat.,Experiment/Discussion
This is the surface-syntactic subcategorization frame.,Experiment/Discussion
"For example, the ssubcat corresponding to a transitive tree frame would be NP0 NP1.",Experiment/Discussion
PPs as arguments are always annotated with the preposition.,Experiment/Discussion
"For example, the ssubcat for the passive version of hit would be NP1 NP2(by).",Experiment/Discussion
Drole.,Experiment/Discussion
This is the deep-syntactic role of an argument.,Experiment/Discussion
Example of values include 0 (subject) and 1 (direct object).,Experiment/Discussion
Dsubcat.,Experiment/Discussion
This is the deep-syntactic subcategorization frame.,Experiment/Discussion
"For example, the dsubcat corresponding to a transitive tree frame would be NP0 NP1.",Experiment/Discussion
"Generally, PPs as arguments are annotated with the preposition.",Experiment/Discussion
"For example, the dsubcat for load is NP0 NP1 NP2(into).",Experiment/Discussion
The exception is when the argument is not realized as a PP when the predicate is realized in a non-syntactically transformed way.,Experiment/Discussion
"For example, the dsubcat for the passive version of hit would be NP0 NP1.",Experiment/Discussion
Semsubcat.,Experiment/Discussion
This is the semantic subcategorization frame.,Experiment/Discussion
"We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice.",Experiment/Discussion
Call this feature set GP0.,Experiment/Discussion
"The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%.",Experiment/Discussion
This is presumably because our training and test data has been assembled in a different manner as mentioned in Section 5.,Experiment/Discussion
"Our next experiment is on the same set of features, with the exception that Path has been replaced with Supertag Path.",Experiment/Discussion
(Feature set GP1).,Experiment/Discussion
The error rate is reduced from 10.0% to 9.7%.,Experiment/Discussion
"This is statistically significant (t-test, p < 0.05), albeit a small improvement.",Experiment/Discussion
One explanation for the improvement is that Path does not generalize as well as Supertag path does.,Experiment/Discussion
"For example, the path feature value VBGVPVPSNP reflects surface subject position in the sentence Prices are falling but so does VBGVPSNP in the sentence Sellers regret prices falling.",Experiment/Discussion
"Because TAG localizes dependencies, the corresponding values for Supertag path in these sentences would be identical.",Experiment/Discussion
"We now experiment with our surface syntax features: Pred HW, Arg HW, Ssubcat, and Srole.",Experiment/Discussion
(Feature set SURFACE.),Experiment/Discussion
"Its performance on SEMTAG is 8.2% whereas its performance on SYNTTAG is 7.6%, a tangible improvement over previous models.",Experiment/Discussion
"One reason for the improvement could be that this model is assigning semantic labels with knowledge of the other roles the predicate assigns, unlike previous models.",Experiment/Discussion
"Our next experiment involves using deep syntax features: Pred HW, Arg HW, Dsubcat, and Drole.",Experiment/Discussion
(Feature set DEEP.),Experiment/Discussion
"Its performance on both SEMTAG and SYNT-TAG is 6.5%, better than previous models.",Experiment/Discussion
Its performance is better than SURFACE presumably because syntactic transformations are taken to account by deep syntax features.,Experiment/Discussion
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002).,Experiment/Discussion
"This experiment considers use of semantic features: Pred HW, Arg HW, Semsubcat, and Drole.",Experiment/Discussion
(Feature set SEMANTIC.),Experiment/Discussion
"Of course, there are only results for SEM-TAG, which turns out to be 1.9%.",Experiment/Discussion
This is the best performance yet.,Experiment/Discussion
"In our final experiment, we use supertag features: pertag, Drole.",Experiment/Discussion
(Feature set SUPERTAG.),Experiment/Discussion
The error rates are 2.8% for SEM-TAG and 7.4% for SYNTTAG.,Experiment/Discussion
"Considering SEM-TAG only, this model performs better than its corresponding DEEP model, probably because supertag for SEM-TAG include crucial semantic information.",Experiment/Discussion
"Considering SYNTTAG only, this model performs worse than its corresponding DEEP model, presumably because of sparse data problems when modeling supertags.",Experiment/Discussion
This sparse data problem is also apparent by comparing the model based on SEM-TAG with the corresponding SEM-TAG SEMANTIC model.,Experiment/Discussion
"In this section, we are concerned with the problem of finding semantic arguments and labeling them with their correct semantic role given raw text as input.",Experiment/Discussion
"In order to perform this task, we parse this raw text using a combination of supertagging and LDA, which is a method that yields partial dependency parses annotated with TAG structures.",Experiment/Discussion
We perform this task using both SEM-TAG and SYNT-TAG.,Experiment/Discussion
"For the former, after supertagging and LDA, the task is accomplished because the TAG structures are already annotated with semantic role information.",Experiment/Discussion
"For the latter, we use the best performing model from Section 6 in order to find semantic roles given syntactic features from the parse.",Experiment/Discussion
Supertagging (Bangalore and Joshi (1999)) is the task of assigning a single supertag to each word given raw text as input.,Experiment/Discussion
"For example, given the sentence Prices are falling, a supertagger might return the supertagged sentence in Figure 4.",Experiment/Discussion
Supertagging returns an almost-parse in the sense that it is performing much parsing disambiguation.,Experiment/Discussion
"The typical technique to perform supertagging is the trigram model, akin to models of the same name for partof-speech tagging.",Experiment/Discussion
This is the technique that we use here.,Experiment/Discussion
Data sparseness is a significant issue when supertagging with extracted grammar (Chen and Vijay-Shanker (2000)).,Experiment/Discussion
"For this reason, we smooth the emit probabilities P(w1t) in the trigram model using distributional similarity following Chen (2001).",Experiment/Discussion
"In particular, we use Jaccard’s coefficient as the similarity metric with a similarity threshold of 0.04 and a radius of 25 because these were found to attain optimal results in Chen (2001).",Experiment/Discussion
Training data for supertagging is Sections 02-21 of the PropBank.,Experiment/Discussion
A supertagging model based on SEM-TAG performs with 76.32% accuracy on Section 23.,Experiment/Discussion
The corresponding model for SYNT-TAG performs with 80.34% accuracy.,Experiment/Discussion
Accuracy is measured for all words in the sentence including punctuation.,Experiment/Discussion
"The SYNT-TAG model performs better than the SEM-TAG model, understandably, because SYNT-TAG is the simpler grammar.",Experiment/Discussion
LDA is an acronym for Lightweight Dependency Analyzer (Srinivas (1997)).,Experiment/Discussion
"Given as input a supertagged sequence of words, it outputs a partial dependency parse.",Experiment/Discussion
"It takes advantage of the fact that supertagging provides an almost-parse in order to dependency parse the sentence in a simple, deterministic fashion.",Experiment/Discussion
Basic LDA is a two step procedure.,Experiment/Discussion
The first step involves linking each word serving as a modifier with the word that it modifies.,Experiment/Discussion
The second step involves linking each word serving as an argument with its predicate.,Experiment/Discussion
Linking always only occurs so that grammatical requirements as stipulated by the supertags are satisfied.,Experiment/Discussion
"The version of LDA that is used in this work differs from Srinivas (1997) in that there are other constraints on the linking process.3 In particular, a link is not established if its existence would create crossing brackets or cycles in the dependency tree for the sentence.",Experiment/Discussion
"We perform LDA on two versions of Section 23, one supertagged with SEM-TAG and the other with SYNT-TAG.",Experiment/Discussion
The results are shown in Table 3.,Experiment/Discussion
Evaluation is performed on dependencies excluding leafnode punctuation.,Experiment/Discussion
Each dependency is evaluated according to both whether the correct head and dependent is related as well as whether they both receive the correct part of speech tag.,Experiment/Discussion
"The F-measure scores, in the 70% range, are relatively low compared to Collins (1999) which has a corresponding score of around 90%.",Experiment/Discussion
This is perhaps to be expected because Collins (1999) is based on a full parser.,Experiment/Discussion
Note also that the accuracy of LDA is highly dependent on the accuracy of the supertagged input.,Experiment/Discussion
"This explains, for example, the fact that the accuracy on SEM-TAG supertagged input is lower than the accuracy with SYNT-TAG supertagged input.",Experiment/Discussion
The output of LDA is a partial dependency parse annotated with TAG structures.,Experiment/Discussion
We can use this output to predict semantic roles of arguments.,Experiment/Discussion
The manner in which this is done depends on the kind of grammar that is used.,Experiment/Discussion
The LDA output using SEM-TAG is already annotated with semantic role information because it is encoded in the grammar itself.,Experiment/Discussion
"On the other hand, the LDA output using SYNT-TAG contains strictly syntactic information.",Experiment/Discussion
"In this case, we use the highest performing model from Section 6 in order to label arguments with semantic roles.",Experiment/Discussion
Evaluation of prediction of semantic roles takes the following form.,Experiment/Discussion
Each argument labeled by a semantic role in the test corpus is treated as one trial.,Experiment/Discussion
Certain aspects of this trial are always checked for correctness.,Experiment/Discussion
These include checking that the semantic role and the dependency-link are correct.,Experiment/Discussion
"There are other aspects which may or may not be checked, depending on the type of evaluation.",Experiment/Discussion
"One aspect, “bnd,” is whether or not the argument’s bracketing as specified in the dependency tree is correct.",Experiment/Discussion
"Another aspect, “arg,” is whether or not the headword of the argument is chosen to be correct.",Experiment/Discussion
Table 4 show the results when we use SEM-TAG in order to supertag the input and perform LDA.,Experiment/Discussion
"When the boundaries are found, finding the head word additionally does not result in a decrease of performance.",Experiment/Discussion
"However, correctly identifying the head word instead of the boundaries leads to an important increase in performance.",Experiment/Discussion
"Furthermore, note the low recall and high precision of the “base + arg” evaluation.",Experiment/Discussion
In part this is due to the nature of the PropBank corpus that we are using.,Experiment/Discussion
"In particular, because not all predicates in our version of the PropBank are annotated with semantic roles, the supertagger for SEM-TAG will sometimes annotate text without semantic roles when in fact it should contain them.",Experiment/Discussion
Table 5 shows the results of first supertagging the input with SYNT-TAG and then using a model trained on the DEEP feature set to annotate the resulting syntactic structure with semantic roles.,Experiment/Discussion
This two-step approach greatly increases performance over the corresponding SEM-TAG based approach.,Experiment/Discussion
"These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora.",Experiment/Discussion
"Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999).",Experiment/Discussion
"They also experiment with using a chunker which yields a recall of 0.35, a precision of 0.50, and an F-measure of 0.41.",Experiment/Discussion
We have presented various alternative approaches to predicting PropBank role labels using forms of linguistic information that are deeper than the PTB’s surface-syntax labels.,Results/Conclusion
"These features may either be directly derived from a TAG, such as Supertag path, or indirectly via aspects of supertags, such Task: determine Recall Precision F base + arg 0.65 0.75 0.70 base + bnd 0.48 0.55 0.51 base + bnd + arg 0.48 0.55 0.51 as deep syntactic features like Drole.",Results/Conclusion
These are found to produce substantial improvements in accuracy.,Results/Conclusion
We believe that such improvement is due to these features better capturing the syntactic information that is relevant for the task of semantic labeling.,Results/Conclusion
"Also, these features represent syntactic categories about which there is a broad consensus in the literature.",Results/Conclusion
"Therefore, we believe that our results are portable to other frameworks and differently annotated corpora such as dependency corpora.",Results/Conclusion
We also show that predicting labels from a “lightweight” parser that generates deep syntactic features performs comparably to using a full parser that generates only surface syntactic features.,Results/Conclusion
"Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example.",Results/Conclusion
This paper is based upon work supported by the National Science Foundation under the KDD program through a supplement to Grant No.,Results/Conclusion
IIS-98-17434.,Results/Conclusion
"Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Results/Conclusion
