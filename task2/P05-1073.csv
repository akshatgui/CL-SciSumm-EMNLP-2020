col1,col2
"Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding.",{}
This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments.,{}
"We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.",{}
This system achieves an reduction of all arguments core arguments over a stateof-the art independent classifier for goldstandard parse trees on PropBank.,{}
"The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004).","{'title': '1 Introduction', 'number': '1'}"
Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1).,"{'title': '1 Introduction', 'number': '1'}"
These features usually characterize aspects of individual arguments and the predicate.,"{'title': '1 Introduction', 'number': '1'}"
It is evident that the labels and the features of arguments are highly correlated.,"{'title': '1 Introduction', 'number': '1'}"
"For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument.","{'title': '1 Introduction', 'number': '1'}"
"Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004).","{'title': '1 Introduction', 'number': '1'}"
"However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure.","{'title': '1 Introduction', 'number': '1'}"
"We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models.","{'title': '1 Introduction', 'number': '1'}"
"To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches.","{'title': '1 Introduction', 'number': '1'}"
"We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000).","{'title': '1 Introduction', 'number': '1'}"
"Consider the pair of sentences, Despite the different syntactic positions of the labeled phrases, we recognize that each plays the same role – indicated by the label – in the meaning of this sense of the verb give.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"We call such phrases fillers of semantic roles and our task is, given a sentence and a target verb, to return all such phrases along with their correct labels.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
Therefore one subtask is to group the words of a sentence into phrases or constituents.,"{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"As in most previous work on semantic role labeling, we assume the existence of a separate parsing model that can assign a parse tree t to each sentence, and the task then is to label each node in the parse tree with the semantic role of the phrase it dominates, or NONE, if the phrase does not fill any role.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"We do stress however that the joint framework and features proposed here can also be used when only a shallow parse (chunked) representation is available as in the CoNLL-2004 shared task (Carreras and M`arquez, 2004).","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al., 1993).","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"Possible labels of arguments in this corpus are the core argument labels ARG[0-5], and the modifier argument labels.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
The core arguments ARG[3-5] do not have consistent global roles and tend to be verb specific.,"{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"There are about 14 modifier labels such as ARGM-LOC and ARGM-TMP, for location and temporal modifiers respectively.'","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
Figure 1 shows an example parse tree annotated with semantic roles.,"{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"We distinguish between models that learn to label nodes in the parse tree independently, called local models, and models that incorporate dependencies among the labels of multiple nodes, called joint models.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"We build both local and joint models for semantic role labeling, and evaluate the gains achievable by incorporating joint information.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"We start by introducing our local models, and later build on them to define joint models.","{'title': '2 Semantic Role Labeling: Task Definition and Architectures', 'number': '2'}"
"In the context of role labeling, we call a classifier local if it assigns a probability (or score) to the label of an individual parse tree node ni independently of the labels of other nodes.","{'title': '3 Local Classifiers', 'number': '3'}"
"We use the standard separation of the task of semantic role labeling into identification and classifi'For a full listing of PropBank argument labels see (Palmer et al., 2003) cation phases.","{'title': '3 Local Classifiers', 'number': '3'}"
"In identification, our task is to classify nodes of t as either ARG, an argument (including modifiers), or NONE, a non-argument.","{'title': '3 Local Classifiers', 'number': '3'}"
"In classification, we are given a set of arguments in t and must label each one with its appropriate semantic role.","{'title': '3 Local Classifiers', 'number': '3'}"
"Formally, let L denote a mapping of the nodes in t to a label set of semantic roles (including NONE) and let Id(L) be the mapping which collapses L’s non-NONE values into ARG.","{'title': '3 Local Classifiers', 'number': '3'}"
Then we can decompose the probability of a labeling L into probabilities according to an identification model PID and a classification model PCLS.,"{'title': '3 Local Classifiers', 'number': '3'}"
"This decomposition does not encode any independence assumptions, but is a useful way of thinking about the problem.","{'title': '3 Local Classifiers', 'number': '3'}"
Our local models for semantic role labeling use this decomposition.,"{'title': '3 Local Classifiers', 'number': '3'}"
"Previous work has also made this distinction because, for example, different features have been found to be more effective for the two tasks, and it has been a good way to make training and search during testing more efficient.","{'title': '3 Local Classifiers', 'number': '3'}"
"Here we use the same features for local identification and classification models, but use the decomposition for efficiency of training.","{'title': '3 Local Classifiers', 'number': '3'}"
"The identification models are trained to classify each node in a parse tree as ARG or NONE, and the classification models are trained to label each argument node in the training set with its specific label.","{'title': '3 Local Classifiers', 'number': '3'}"
In this way the training set for the classification models is smaller.,"{'title': '3 Local Classifiers', 'number': '3'}"
"Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1.","{'title': '3 Local Classifiers', 'number': '3'}"
"Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005).","{'title': '3 Local Classifiers', 'number': '3'}"
"In previous work, various machine learning methods have been used to learn local classifiers for role labeling.","{'title': '3 Local Classifiers', 'number': '3'}"
"Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004).","{'title': '3 Local Classifiers', 'number': '3'}"
In this work we use log-linear models for multi-class classification.,"{'title': '3 Local Classifiers', 'number': '3'}"
One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other.,"{'title': '3 Local Classifiers', 'number': '3'}"
"Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint.","{'title': '3 Local Classifiers', 'number': '3'}"
"Here we describe a fast exact dynamic programming algorithm to find the most likely non-overlapping (consistent) labeling of all nodes in the parse tree, according to a product of probabilities from local models, as in Equation 2.","{'title': '3 Local Classifiers', 'number': '3'}"
"For simplicity, we describe the dynamic program for the case where only two classes are possible – ARG and NONE.","{'title': '3 Local Classifiers', 'number': '3'}"
The generalization to more classes is straightforward.,"{'title': '3 Local Classifiers', 'number': '3'}"
"Intuitively, the algorithm is similar to the Viterbi algorithm for context-free grammars, because we can describe the non-overlapping constraint by a “grammar” that disallows ARG nodes to have ARG descendants.","{'title': '3 Local Classifiers', 'number': '3'}"
"Below we will talk about maximizing the sum of the logs of local probabilities rather than the product of local probabilities, which is equivalent.","{'title': '3 Local Classifiers', 'number': '3'}"
"The dynamic program works from the leaves of the tree up and finds a best assignment for each tree, using already computed assignments for its children.","{'title': '3 Local Classifiers', 'number': '3'}"
"Suppose we want the most likely consistent assignment for subtree t with children trees t1, ... , tk each storing the most likely consistent assignment of nodes it dominates as well as the log-probability of the assignment of all nodes it dominates to NONE.","{'title': '3 Local Classifiers', 'number': '3'}"
"The most likely assignment for t is the one that corresponds to the maximum of: Propagating this procedure from the leaves to the root of t, we have our most likely non-overlapping assignment.","{'title': '3 Local Classifiers', 'number': '3'}"
"By slightly modifying this procedure, we obtain the most likely assignment according to and classification models can be chained in a principled way, as in Equation 1.","{'title': '3 Local Classifiers', 'number': '3'}"
The features we used for local identification and classification models are outlined in Table 1.,"{'title': '3 Local Classifiers', 'number': '3'}"
These features are a subset of features used in previous work.,"{'title': '3 Local Classifiers', 'number': '3'}"
"The standard features at the top of the table were defined by (Gildea and Jurafsky, 2002), and the rest are other useful lexical and structural features identified in more recent work (Pradhan et al., 2004; Surdeanu et al., 2003; Xue and Palmer, 2004).","{'title': '3 Local Classifiers', 'number': '3'}"
The most direct way to use trained local identification and classification models in testing is to select a labeling L of the parse tree that maximizes the product of the probabilities according to the two models as in Equation 1.,"{'title': '3 Local Classifiers', 'number': '3'}"
"Since these models are local, this is equivalent to independently maximizing the product of the probabilities of the two models for the label li of each parse tree node ni as shown below in Equation 2. a product of local identification and classification models.","{'title': '3 Local Classifiers', 'number': '3'}"
We use the local models in conjunction with this search procedure to select a most likely labeling in testing.,"{'title': '3 Local Classifiers', 'number': '3'}"
Test set results for our local model PsRL are given in Table 2.,"{'title': '3 Local Classifiers', 'number': '3'}"
"As discussed in previous work, there are strong dependencies among the labels of the semantic argument nodes of a verb.","{'title': '4 Joint Classifiers', 'number': '4'}"
"A drawback of local models is that, when they decide the label of a parse tree node, they cannot use information about the labels and features of other nodes in the tree.","{'title': '4 Joint Classifiers', 'number': '4'}"
"Furthermore, these dependencies are highly nonlocal.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For instance, to avoid repeating argument labels in a frame, we need to add a dependency from each node label to the labels of all other nodes.","{'title': '4 Joint Classifiers', 'number': '4'}"
"A factorized sequence model that assumes a finite Markov horizon, such as a chain Conditional Random Field (Lafferty et al., 2001), would not be able to encode such dependencies.","{'title': '4 Joint Classifiers', 'number': '4'}"
"The need for Re-ranking For argument identification, the number of possible assignments for a parse tree with n nodes is 2n.","{'title': '4 Joint Classifiers', 'number': '4'}"
This number can run into the hundreds of billions for a normal-sized tree.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"For argument labeling, the number of possible assignments is Pt� 201, if m is the number of arguments of a verb (typically between 2 and 5), and 20 is the approximate number of possible labels if considering both core and modifying arguments.","{'title': '4 Joint Classifiers', 'number': '4'}"
Training a model which has such huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Therefore, in order to be able to incorporate long-range dependencies in our models, we chose to adopt a re-ranking approach (Collins, 2000), which selects from likely assignments generated by a model which makes stronger independence assumptions.","{'title': '4 Joint Classifiers', 'number': '4'}"
We utilize the top N assignments of our local semantic role labeling model PSRL to generate likely assignments.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"As can be seen from Table 3, for relatively small values of N, our re-ranking approach does not present a serious bottleneck to performance.","{'title': '4 Joint Classifiers', 'number': '4'}"
We used a value of N = 20 for training.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"In Table 3 we can see that if we could pick, using an oracle, the best assignment out for the top 20 assignments according to the local model, we would achieve an F-Measure of 98.8 on all arguments.","{'title': '4 Joint Classifiers', 'number': '4'}"
Increasing the number of N to 30 results in a very small gain in the upper bound on performance and a large increase in memory requirements.,"{'title': '4 Joint Classifiers', 'number': '4'}"
We therefore selected N = 20 as a good compromise.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Generation of top N most likely joint assignments We generate the top N most likely nonoverlapping joint assignments of labels to nodes in a parse tree according to a local model PsRL, by an exact dynamic programming algorithm, which is a generalization of the algorithm for finding the top non-overlapping assignment described in section 3.1.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We learn log-linear re-ranking models for joint semantic role labeling, which use feature maps from a parse tree and label sequence to a vector space.","{'title': '4 Joint Classifiers', 'number': '4'}"
The form of the models is as follows.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Let 4b(t, v, L) E R' denote a feature map from a tree t, target verb v, and joint assignment L of the nodes of the tree, to the vector space R'.","{'title': '4 Joint Classifiers', 'number': '4'}"
"Let L1, L2, · · · , LN denote top N possible joint assignments.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We learn a loglinear model with a parameter vector W, with one weight for each of the s dimensions of the feature vector.","{'title': '4 Joint Classifiers', 'number': '4'}"
The probability (or score) of an assignment L according to this re-ranking model is defined as: The score of an assignment L not in the top N is zero.,"{'title': '4 Joint Classifiers', 'number': '4'}"
We train the model to maximize the sum of log-likelihoods of the best assignments minus a quadratic regularization term.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"In this framework, we can define arbitrary features of labeled trees that capture general properties of predicate-argument structure.","{'title': '4 Joint Classifiers', 'number': '4'}"
We will introduce the features of the joint reranking model in the context of the example parse tree shown in Figure 1.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"We model dependencies not only between the label of a node and the labels of other nodes, but also dependencies between the label of a node and input features of other argument nodes.","{'title': '4 Joint Classifiers', 'number': '4'}"
The features are specified by instantiation of templates and the value of a feature is the number of times a particular pattern occurs in the labeled tree.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"For a tree t, predicate v, and joint assignment L of labels to the nodes of the tree, we define the candidate argument sequence as the sequence of nonNONE labeled nodes [n1, l1, ... , vPRED, nm, lm] (li is the label of node ni).","{'title': '4 Joint Classifiers', 'number': '4'}"
"A reasonable candidate argument sequence usually contains very few of the nodes in the tree – about 2 to 7 nodes, as this is the typical number of arguments for a verb.","{'title': '4 Joint Classifiers', 'number': '4'}"
"To make it more convenient to express our feature templates, we include the predicate node v in the sequence.","{'title': '4 Joint Classifiers', 'number': '4'}"
This sequence of labeled nodes is defined with respect to the left-to-right order of constituents in the parse tree.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Since non-NONE labeled nodes do not overlap, there is a strict left-to-right order among these nodes.","{'title': '4 Joint Classifiers', 'number': '4'}"
"The candidate argument sequence that corresponds to the correct assignment in Figure 1 will be: [NP1-ARG1,VBD1-PRED,PP1-ARG4,NP3-ARGM-TMP] Features from Local Models: All features included in the local models are also included in our joint models.","{'title': '4 Joint Classifiers', 'number': '4'}"
"In particular, each template for local features is included as a joint template that concatenates the local template and the node label.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, for the local feature PATH, we define a joint feature template, that extracts PATH from every node in the candidate argument sequence and concatenates it with the label of the node.","{'title': '4 Joint Classifiers', 'number': '4'}"
Both a feature with the specific argument label is created and a feature with the generic back-off ARG label.,"{'title': '4 Joint Classifiers', 'number': '4'}"
This is similar to adding features from identification and classification models.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"In the case of the example candidate argument sequence above, for the node NP1 we have When comparing a local and a joint model, we use the same set of local feature templates in the two models.","{'title': '4 Joint Classifiers', 'number': '4'}"
"Whole Label Sequence: As observed in previous work (Gildea and Jurafsky, 2002; Pradhan et al., 2004), including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, including such information will make the model less likely to pick multiple fillers for the same role or to come up with a labeling that does not contain an obligatory argument.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We added a whole label sequence feature template that extracts the labels of all argument nodes, and preserves information about the position of the predicate.","{'title': '4 Joint Classifiers', 'number': '4'}"
The template also includes information about the voice of the predicate.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, this template will be instantiated as follows for the example candidate argument sequence: [ voice:active ARG1,PRED,ARG4,ARGM-TMP] We also add a variant of this feature which uses a generic ARG label instead of specific labels.","{'title': '4 Joint Classifiers', 'number': '4'}"
"This feature template has the effect of counting the number of arguments to the left and right of the predicate, which provides useful global information about argument structure.","{'title': '4 Joint Classifiers', 'number': '4'}"
"As previously observed (Pradhan et al., 2004), including modifying arguments in sequence features is not helpful.","{'title': '4 Joint Classifiers', 'number': '4'}"
This was confirmed in our experiments and we redefined the whole label sequence features to exclude modifying arguments.,"{'title': '4 Joint Classifiers', 'number': '4'}"
One important variation of this feature uses the actual predicate lemma in addition to “voice:active”.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Additionally, we define variations of these feature templates that concatenate the label sequence with features of individual nodes.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We experimented with variations, and found that including the phrase type and the head of a directly dominating PP – if one exists – was most helpful.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We also add a feature that detects repetitions of the same label in a candidate argument sequence, together with the phrase types of the nodes labeled with that label.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, (NP-ARG0,WHNP-ARG0) is a common pattern of this form.","{'title': '4 Joint Classifiers', 'number': '4'}"
Frame Features: Another very effective class of features we defined are features that look at the label of a single argument node and internal features of other argument nodes.,"{'title': '4 Joint Classifiers', 'number': '4'}"
The idea of these features is to capture knowledge about the label of a constituent given the syntactic realization of all arguments of the verb.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"This is helpful to capture syntactic alternations, such as the dative alternation.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, consider the sentence (i) “[Shaw Publishing]ARG0 offered [Mr. Smith]ARG2 [a reimbursement]ARG, ” and the alternative realization (ii) “[Shaw Publishing]ARG0 offered [a reimbursement]ARG, [to Mr. Smith]ARG2”.","{'title': '4 Joint Classifiers', 'number': '4'}"
"When classifying the NP in object position, it is useful to know whether the following argument is a PP.","{'title': '4 Joint Classifiers', 'number': '4'}"
"If yes, the NP will more likely be an ARG1, and if not, it will more likely be an ARG2.","{'title': '4 Joint Classifiers', 'number': '4'}"
"A feature template that captures such information extracts, for each argument node, its phrase type and label in the context of the phrase types for all other arguments.","{'title': '4 Joint Classifiers', 'number': '4'}"
"For example, the instantiation of such a template for [a reimbursement] in (ii) would be [ voice:active NP,PRED,NP-ARG1,PP] We also add a template that concatenates the identity of the predicate lemma itself.","{'title': '4 Joint Classifiers', 'number': '4'}"
"We should note that Xue and Palmer (2004) define a similar feature template, called syntactic frame, which often captures similar information.","{'title': '4 Joint Classifiers', 'number': '4'}"
"The important difference is that their template extracts contextual information from noun phrases surrounding the predicate, rather than from the sequence of argument nodes.","{'title': '4 Joint Classifiers', 'number': '4'}"
"Because our model is joint, we are able to use information about other argument nodes when labeling a node.","{'title': '4 Joint Classifiers', 'number': '4'}"
"Here we describe the application in testing of a joint model for semantic role labeling, using a local model P`SRL, and a joint re-ranking model PrSRL.","{'title': '4 Joint Classifiers', 'number': '4'}"
"P`SRL is used to generate top N non-overlapping joint assignments L1, ... , LN.","{'title': '4 Joint Classifiers', 'number': '4'}"
"One option is to select the best Li according to PrSRL, as in Equation 3, ignoring the score from the local model.","{'title': '4 Joint Classifiers', 'number': '4'}"
"In our experiments, we noticed that for larger values of N, the performance of our reranking model PrSRL decreased.","{'title': '4 Joint Classifiers', 'number': '4'}"
"This was probably due to the fact that at test time the local classifier produces very poor argument frames near the bottom of the top N for large N. Since the re-ranking model is trained on relatively few good argument frames, it cannot easily rule out very bad frames.","{'title': '4 Joint Classifiers', 'number': '4'}"
It makes sense then to incorporate the local model into our final score.,"{'title': '4 Joint Classifiers', 'number': '4'}"
Our final score is given by: where α is a tunable parameter 2 for how much influence the local score has in the final score.,"{'title': '4 Joint Classifiers', 'number': '4'}"
"Such interpolation with a score from a first-pass model was also used for parse re-ranking in (Collins, 2000).","{'title': '4 Joint Classifiers', 'number': '4'}"
"Given this score, at test time we choose among the top N local assignments L1, ... , LN according to: arg max α log P`SRL(L|t, v) + log PrSRL(L|t, v) LE{L1,...,LN}","{'title': '4 Joint Classifiers', 'number': '4'}"
For our experiments we used the February 2004 release of PropBank.,"{'title': '5 Experiments and Results', 'number': '5'}"
"3 As is standard, we used the annotations from sections 02–21 for training, 24 for development, and 23 for testing.","{'title': '5 Experiments and Results', 'number': '5'}"
"As is done in some previous work on semantic role labeling, we discard the relatively infrequent discontinuous arguments from both the training and test sets.","{'title': '5 Experiments and Results', 'number': '5'}"
"In addition to reporting the standard results on individual argument F-Measure, we also report Frame Accuracy (Acc.","{'title': '5 Experiments and Results', 'number': '5'}"
"), the fraction of sentences for which we successfully label all nodes.","{'title': '5 Experiments and Results', 'number': '5'}"
There are reasons to prefer Frame Accuracy as a measure of performance over individual-argument statistics.,"{'title': '5 Experiments and Results', 'number': '5'}"
"Foremost, potential applications of role labeling may require correct labeling of all (or at least the core) arguments in a sentence in order to be effective, and partially correct labelings may not be very useful.","{'title': '5 Experiments and Results', 'number': '5'}"
We report results for two variations of the semantic role labeling task.,"{'title': '5 Experiments and Results', 'number': '5'}"
"For CORE, we identify and label only core arguments.","{'title': '5 Experiments and Results', 'number': '5'}"
"For ARGM, we identify and label core as well as modifier arguments.","{'title': '5 Experiments and Results', 'number': '5'}"
"We report results for local and joint models on argument identification, argument classification, and the complete identification and classification pipeline.","{'title': '5 Experiments and Results', 'number': '5'}"
Our local models use the features listed in Table 1 and the technique for enforcing the non-overlapping constraint discussed in Section 3.1.,"{'title': '5 Experiments and Results', 'number': '5'}"
The labeling of the tree in Figure 1 is a specific example of the kind of errors fixed by the joint models.,"{'title': '5 Experiments and Results', 'number': '5'}"
"The local classifier labeled the first argument in the tree as ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject position.","{'title': '5 Experiments and Results', 'number': '5'}"
All joint models for these experiments used the whole sequence and frame features.,"{'title': '5 Experiments and Results', 'number': '5'}"
"As can be seen from Table 4, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively.","{'title': '5 Experiments and Results', 'number': '5'}"
"With respect to the Frame Accuracy metric, the joint error reduction is 38% and 26% for CORE and ARGM respectively.","{'title': '5 Experiments and Results', 'number': '5'}"
We also report results on automatic parses (see Table 5).,"{'title': '5 Experiments and Results', 'number': '5'}"
"We trained and tested on automatic parse trees from Charniak’s parser (Charniak, 2000).","{'title': '5 Experiments and Results', 'number': '5'}"
"For approximately 5.6% of the argument constituents in the test set, we could not find exact matches in the automatic parses.","{'title': '5 Experiments and Results', 'number': '5'}"
"Instead of discarding these arguments, we took the largest constituent in the automatic parse having the same head-word as the gold-standard argument constituent.","{'title': '5 Experiments and Results', 'number': '5'}"
"Also, 19 of the propositions in the test set were discarded because Charniak’s parser altered the tokenization of the input sentence and tokens could not be aligned.","{'title': '5 Experiments and Results', 'number': '5'}"
"As our results show, the error reduction of our joint model with respect to the local model is more modest in this setting.","{'title': '5 Experiments and Results', 'number': '5'}"
"One reason for this is the lower upper bound, due largely to the the much poorer performance of the identification model on automatic parses.","{'title': '5 Experiments and Results', 'number': '5'}"
"For ARGM, the local identification model achieves 85.9 F-Measure and 59.4 Frame Accuracy; the local classification model achieves 92.3 F-Measure and 83.1 Frame Accuracy.","{'title': '5 Experiments and Results', 'number': '5'}"
"It seems that the largest boost would come from features that can identify arguments in the presence of parser errors, rather than the features of our joint model, which ensure global coherence of the argument frame.","{'title': '5 Experiments and Results', 'number': '5'}"
We still achieve 10.7% and 18.5% error reduction for CORE arguments in F-Measure and Frame Accuracy respectively.,"{'title': '5 Experiments and Results', 'number': '5'}"
Several semantic role labeling systems have successfully utilized joint information.,"{'title': '6 Related Work', 'number': '6'}"
"(Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution.","{'title': '6 Related Work', 'number': '6'}"
"(Pradhan et al., 2004) train a language model over label sequences.","{'title': '6 Related Work', 'number': '6'}"
"(Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels.","{'title': '6 Related Work', 'number': '6'}"
"The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies.","{'title': '6 Related Work', 'number': '6'}"
"Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.","{'title': '7 Conclusions', 'number': '7'}"
This is especially true when we model the dependencies with discriminative models capable of incorporating long-distance features.,"{'title': '7 Conclusions', 'number': '7'}"
The authors would like to thank the reviewers for their helpful comments and Dan Jurafsky for his insightful suggestions and useful discussions.,"{'title': '8 Acknowledgements', 'number': '8'}"
This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.,"{'title': '8 Acknowledgements', 'number': '8'}"
