col1,col2
A good decoding algorithm is critical to the success of any statistical machine translation system.,{}
The decoder’s job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).,{}
"Since the space of possible translations is extremely large, typical decoding algorithms are only able to examine a portion of it, thus risking to miss good solutions.",{}
"In this paper, we compare the speed and output quality of a traditional stack-based decoding algorithm with two new decoders: a fast greedy decoder and a slow but optimal decoder that treats decoding as an integer-programming optimization problem.",{}
"A statistical MT system that translates (say) French sentences into English, is divided into three parts: (1) a language model (LM) that assigns a probability P(e) to any English string, (2) a translation model (TM) that assigns a probability P(fe) to any pair of English and French strings, and (3) a decoder.","{'title': '1 Introduction', 'number': '1'}"
"The decoder takes a previously unseen sentenceand tries to find the that maximizes P(ef), or equivalently maximizes P(e)P(fe).","{'title': '1 Introduction', 'number': '1'}"
"Brown et al. (1993) introduced a series of TMs based on word-for-word substitution and reordering, but did not include a decoding algorithm.","{'title': '1 Introduction', 'number': '1'}"
"If the source and target languages are constrained to have the same word order (by choice or through suitable pre-processing), then the linear Viterbi algorithm can be applied (Tillmann et al., 1997).","{'title': '1 Introduction', 'number': '1'}"
"If re-ordering is limited to rotations around nodes in a binary tree, then optimal decoding can be carried out by a high-polynomial algorithm (Wu, 1996).","{'title': '1 Introduction', 'number': '1'}"
"For arbitrary word-reordering, the decoding problem is NP-complete (Knight, 1999).","{'title': '1 Introduction', 'number': '1'}"
"A sensible strategy (Brown et al., 1995; Wang and Waibel, 1997) is to examine a large subset of likely decodings and choose just from that.","{'title': '1 Introduction', 'number': '1'}"
"Of course, it is possible to miss a good translation this way.","{'title': '1 Introduction', 'number': '1'}"
"If the decoder returns ebut there exists some e for which P(ef) P(ef), this is called a search error.","{'title': '1 Introduction', 'number': '1'}"
"As Wang and Waibel (1997) remark, it is hard to know whether a search error has occurred—the only way to show that a decoding is sub-optimal is to actually produce a higherscoring one.","{'title': '1 Introduction', 'number': '1'}"
"Thus, while decoding is a clear-cut optimization task in which every problem instance has a right answer, it is hard to come up with good answers quickly.","{'title': '1 Introduction', 'number': '1'}"
"This paper reports on measurements of speed, search errors, and translation quality in the context of a traditional stack decoder (Jelinek, 1969; Brown et al., 1995) and two new decoders.","{'title': '1 Introduction', 'number': '1'}"
"The first is a fast greedy decoder, and the second is a slow optimal decoder based on generic mathematical programming techniques.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we work with IBM Model 4, which revolves around the notion of a word alignment over a pair of sentences (see Figure 1).","{'title': '2 IBM Model 4', 'number': '2'}"
A word alignment assigns a single home (English string position) to each French word.,"{'title': '2 IBM Model 4', 'number': '2'}"
"If two French words align to the same English word, then that English word is said to have a fertility of two.","{'title': '2 IBM Model 4', 'number': '2'}"
"Likewise, if an English word remains unalignedto, then it has fertility zero.","{'title': '2 IBM Model 4', 'number': '2'}"
The word alignment in Figure 1 is shorthand for a hypothetical stochastic process by which an English string gets converted into a French string.,"{'title': '2 IBM Model 4', 'number': '2'}"
There are several sets of decisions to be made.,"{'title': '2 IBM Model 4', 'number': '2'}"
"First, every English word is assigned a fertility.","{'title': '2 IBM Model 4', 'number': '2'}"
These assignments are made stochastically according to a table n( e).,"{'title': '2 IBM Model 4', 'number': '2'}"
"We delete from the string any word with fertility zero, we duplicate any word with fertility two, etc.","{'title': '2 IBM Model 4', 'number': '2'}"
"If a word has fertility greater than zero, we call it fertile.","{'title': '2 IBM Model 4', 'number': '2'}"
"If its fertility is greater than one, we call it very fertile.","{'title': '2 IBM Model 4', 'number': '2'}"
"After each English word in the new string, we may increment the fertility of an invisible English NULL element with probability p(typically about 0.02).","{'title': '2 IBM Model 4', 'number': '2'}"
The NULL element ultimately produces “spurious” French words.,"{'title': '2 IBM Model 4', 'number': '2'}"
"Next, we perform a word-for-word replacement of English words (including NULL) by French words, according to the table t(f e).","{'title': '2 IBM Model 4', 'number': '2'}"
"Finally, we permute the French words.","{'title': '2 IBM Model 4', 'number': '2'}"
"In permuting, Model 4 distinguishes between French words that are heads (the leftmost French word generated from a particular English word), nonheads (non-leftmost, generated only by very fertile English words), and NULL-generated.","{'title': '2 IBM Model 4', 'number': '2'}"
Heads.,"{'title': '2 IBM Model 4', 'number': '2'}"
The head of one English word is assigned a French string position based on the position assigned to the previous English word.,"{'title': '2 IBM Model 4', 'number': '2'}"
"If an English word e translates into something at French position j, then the French head word of eis stochastically placed in French position k with distortion probability d(k–jclass(e ), class(f)), where “class” refers to automatically determined word classes for French and English vocabulary items.","{'title': '2 IBM Model 4', 'number': '2'}"
This relative offset k–j encourages adjacent English words to translate into adjacent French words.,"{'title': '2 IBM Model 4', 'number': '2'}"
"If e is infertile, then j is taken from e , etc.","{'title': '2 IBM Model 4', 'number': '2'}"
"If e is very fertile, then j is the average of the positions of its French translations.","{'title': '2 IBM Model 4', 'number': '2'}"
Non-heads.,"{'title': '2 IBM Model 4', 'number': '2'}"
"If the head of English word e is placed in French position j, then its first nonhead is placed in French position k ( j) according to another table d (k–jclass(f)).","{'title': '2 IBM Model 4', 'number': '2'}"
"The next non-head is placed at position q with probability d (q–kclass(f)), and so forth.","{'title': '2 IBM Model 4', 'number': '2'}"
NULL-generated.,"{'title': '2 IBM Model 4', 'number': '2'}"
"After heads and non-heads are placed, NULL-generated words are permuted into the remaining vacant slots randomly.","{'title': '2 IBM Model 4', 'number': '2'}"
"If there are NULL-generated words, then any placement scheme is chosen with probability 1/ .","{'title': '2 IBM Model 4', 'number': '2'}"
"These stochastic decisions, starting with e, result in different choices of f and an alignment of f with e. We map an e onto a particulara,f✠pair with probability: d e d NULL where the factors separated bysymbols denote fertility, translation, head permutation, non-head permutation, null-fertility, and null-translation probabilities.1","{'title': '2 IBM Model 4', 'number': '2'}"
"If we observe a new sentence f, then an optimal decoder will search for an e that maximizes P(ef) P(e)P(fe).","{'title': '3 Definition of the Problem', 'number': '3'}"
"Here, P(fe) is the sum of P(a,fe) over all possible alignments a.","{'title': '3 Definition of the Problem', 'number': '3'}"
"Because this sum involves significant computation, we typically avoid it by instead searching for ane,a pair that maximizes P(e,af) P(e)P(a,fe).","{'title': '3 Definition of the Problem', 'number': '3'}"
We take the language model P(e) to be a smoothed n-gram model of English.,"{'title': '3 Definition of the Problem', 'number': '3'}"
"The stack (also called A*) decoding algorithm is a kind of best-first search which was first introduced in the domain of speech recognition (Jelinek, 1969).","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"By building solutions incrementally and storing partial solutions, or hypotheses, in a “stack” (in modern terminology, a priority queue), the decoder conducts an ordered search of the solution space.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"In the ideal case (unlimited stack size and exhaustive search time), a stack decoder is guaranteed to find an optimal solution; our hope is to do almost as well under real-world constraints of limited space and time.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
The generic stack decoding algorithm follows: Initialize the stack with an empty hypothesis.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Pop h, the best hypothesis, off the stack.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"If h is a complete sentence, output h and terminate.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"For each possible next word w, extend h by adding w and push the resulting hypothesis onto the stack.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
Return to the second step (pop).,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
One crucial difference between the decoding process in speech recognition (SR) and machine translation (MT) is that speech is always produced in the same order as its transcription.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Consequently, in SR decoding there is always a simple left-to-right correspondence between input and output sequences.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"By contrast, in MT the leftto-right relation rarely holds even for language pairs as similar as French and English.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"We address this problem by building the solution from left to right, but allowing the decoder to consume its input in any order.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"This change makes decoding significantly more complex in MT; instead of knowing the order of the input in advance, we must consider allpermutations of an-word input sentence.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
Another important difference between SR and MT decoding is the lack of reliable heuristics in MT.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
A heuristic is used in A* search to estimate the cost of completing a partial hypothesis.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"A good heuristic makes it possible to accurately compare the value of different partial hypotheses, and thus to focus the search in the most promising direction.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
The left-to-right restriction in SR makes it possible to use a simple yet reliable class of heuristics which estimate cost based on the amount of input left to decode.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Partly because of the absence of left-to-right correspondence, MT heuristics are significantly more difficult to develop (Wang and Waibel, 1997).","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Without a heuristic, a classic stack decoder is ineffective because shorter hypotheses will almost always look more attractive than longer ones, since as we add words to a hypothesis, we end up multiplying more and more terms to find the probability.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Because of this, longer hypotheses will be pushed off the end of the stack by shorter ones even if they are in reality better decodings.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Fortunately, by using more than one stack, we can eliminate this effect.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"In a multistack decoder, we employ more than one stack to force hypotheses to compete fairly.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"More specifically, we have one stack for each subset of input words.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"This way, a hypothesis can only be pruned if there are other, better, hypotheses that represent the same portion of the input.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"With more than one stack, however, how does a multistack decoder choose which hypothesis to extend during each iteration?","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"We address this issue by simply taking one hypothesis from each stack, but a better solution would be to somehow compare hypotheses from different stacks and extend only the best ones.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"The multistack decoder we describe is closely patterned on the Model 3 decoder described in the (Brown et al., 1995) patent.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
We build solutions incrementally by applying operations to hypotheses.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
There are four operations: Add adds a new English word and aligns a single French word to it.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
AddZfert adds two new English words.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"The first has fertility zero, while the second is aligned to a single French word.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Extend aligns an additional French word to the most recent English word, increasing its fertility.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
AddNull aligns a French word to the English NULL element.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"AddZfert is by far the most expensive operation, as we must consider inserting a zero-fertility English word before each translation of each unaligned French word.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"With an English vocabulary size of 40,000, AddZfert is 400,000 times more expensive than AddNull!","{'title': '4 Stack-Based Decoding', 'number': '4'}"
We can reduce the cost of AddZfert in two ways.,"{'title': '4 Stack-Based Decoding', 'number': '4'}"
"First, we can consider only certain English words as candidates for zero-fertility, namely words which both occur frequently and have a high probability of being assigned frequency zero.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Second, we can only insert a zero-fertility word if it will increase the probability of a hypothesis.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"According to the definition of the decoding problem, a zero-fertility English word can only make a decoding more likely by increasing P(e) more than it decreases P(a,fe).2 By only considering helpful zero-fertility insertions, we save ourselves significant overhead in the AddZfert operation, in many cases eliminating all possibilities and reducing its cost to less than that of AddNull.","{'title': '4 Stack-Based Decoding', 'number': '4'}"
"Over the last decade, many instances of NPcomplete problems have been shown to be solvable in reasonable/polynomial time using greedy methods (Selman et al., 1992; Monasson et al., 1999).","{'title': '5 Greedy Decoding', 'number': '5'}"
"Instead of deeply probing the search space, such greedy methods typically start out with a random, approximate solution and then try to improve it incrementally until a satisfactory solution is reached.","{'title': '5 Greedy Decoding', 'number': '5'}"
"In many cases, greedy methods quickly yield surprisingly good solutions.","{'title': '5 Greedy Decoding', 'number': '5'}"
We conjectured that such greedy methods may prove to be helpful in the context of MT decoding.,"{'title': '5 Greedy Decoding', 'number': '5'}"
The greedy decoder that we describe starts the translation process from an English gloss of the French sentence given as input.,"{'title': '5 Greedy Decoding', 'number': '5'}"
The gloss is constructed by aligning each French word f with its most likely English translation ef(ef argmaxt(ef)).,"{'title': '5 Greedy Decoding', 'number': '5'}"
"For example, in translating the French sentence “Bien entendu , il parle de une belle victoire .”, the greedy decoder initially assumes that a good translation of it is “Well heard , it talking a beautiful victory” because the best translation of “bien” is “well”, the best translation of “entendu” is “heard”, and so on.","{'title': '5 Greedy Decoding', 'number': '5'}"
The alignment corresponding to this translation is shown at the top of Figure 2.,"{'title': '5 Greedy Decoding', 'number': '5'}"
"Once the initial alignment is created, the greedy decoder tries to improve it, i.e., tries to find an alignment (and implicitly translation) of higher probability, by applying one of the following operations: translateOneOrTwoWords( ,e, ,e) changes the translation of one or two French words, those located at positions and , from e and e into eand e. If eis a word of fertility 1 and eis NULL, then eis deleted from the translation.","{'title': '5 Greedy Decoding', 'number': '5'}"
"If eis the NULL word, the word eis inserted into the translation at the position that yields the alignment of highest probability.","{'title': '5 Greedy Decoding', 'number': '5'}"
"If e eor e e, this operation amounts to changing the translation of a single word. translateAndInsert( ,e,e) changes the translation of the French word located at positionfrom einto and simulataneously inserts word eat the position that yields the alignment of highest probability.","{'title': '5 Greedy Decoding', 'number': '5'}"
Word is selected from an automatically derived list of 1024 words with high probability of having fertility 0.,"{'title': '5 Greedy Decoding', 'number': '5'}"
"When ee, this operation amounts to inserting a word of fertility 0 into the alignment. removeWordOfFertility0() deletes the word of fertility 0 at positionin the current alignment. swapSegments( ) creates a new alignment from the old one by swapping non-overlapping English word segments and .","{'title': '5 Greedy Decoding', 'number': '5'}"
"During the swap operation, all existing links between English and French words are preserved.","{'title': '5 Greedy Decoding', 'number': '5'}"
"The segments can be as small as a word or as long as words, where is the length of the English sentence. joinWords( ) eliminates from the alignment the English word at position (or ) and links the French words generated by (or ) to (or ).","{'title': '5 Greedy Decoding', 'number': '5'}"
"In a stepwise fashion, starting from the initial gloss, the greedy decoder iterates exhaustively over all alignments that are one operation away from the alignment under consideration.","{'title': '5 Greedy Decoding', 'number': '5'}"
"At every step, the decoder chooses the alignment of highest probability, until the probability of the current alignment can no longer be improved.","{'title': '5 Greedy Decoding', 'number': '5'}"
"When it starts from the gloss of the French sentence “Bien entendu, il parle de une belle victoire.”, for example, the greedy decoder alters the initial alignment incrementally as shown in Figure 2, eventually producing the translation “Quite naturally, he talks about a great victory.”.","{'title': '5 Greedy Decoding', 'number': '5'}"
"In the process, the decoder explores a total of 77421 distinct alignments/translations, of which “Quite naturally, he talks about a great victory.” has the highest probability.","{'title': '5 Greedy Decoding', 'number': '5'}"
We chose the operation types enumerated above for two reasons: (i) they are general enough to enable the decoder escape local maxima and modify in a non-trivial manner a given alignment in order to produce good translations; (ii) they are relatively inexpensive (timewise).,"{'title': '5 Greedy Decoding', 'number': '5'}"
"The most time consuming operations in the decoder are swapSegments, translateOneOrTwoWords, and translateAndInsert.","{'title': '5 Greedy Decoding', 'number': '5'}"
SwapSegments iterates over all possible non-overlapping span pairs that can be built on a sequence of length .,"{'title': '5 Greedy Decoding', 'number': '5'}"
"TranslateOneOrTwoWords iterates over alignments, where is the size of the French sentence andis the number of translations we associate with each word (in our implementation, we limit this number to the top 10 translations).","{'title': '5 Greedy Decoding', 'number': '5'}"
"TranslateAndInsert iterates over alignments, where is the size of the list of words with high probability of having fertility 0 (1024 words in our implementation).","{'title': '5 Greedy Decoding', 'number': '5'}"
"Knight (1999) likens MT decoding to finding optimal tours in the Traveling Salesman Problem (Garey and Johnson, 1979)—choosing a good word order for decoder output is similar to choosing a good TSP tour.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Because any TSP problem instance can be transformed into a decoding problem instance, Model 4 decoding is provably NP-complete in the length of f. It is interesting to consider the reverse direction—is it possible to transform a decoding problem instance into a TSP instance?","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"If so, we may take great advantage of previous research into efficient TSP algorithms.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"We may also take advantage of existing software packages, obtaining a sophisticated decoder with little programming effort.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"It is difficult to convert decoding into straight TSP, but a wide range of combinatorial optimization problems (including TSP) can be expressed in the more general framework of linear integer programming.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
A sample integer program (IP) looks like this: A solution to an IP is an assignment of integer values to variables.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
Solutions are constrained by inequalities involving linear combinations of variables.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"An optimal solution is one that respects the constraints and minimizes the value of the objective function, which is also a linear combination of variables.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"We can solve IP instances with generic problem-solving software such as lp solve or CPLEX.3 In this section we explain tence f = “CE NE EST PAS CLAIR .” There is one city for each word in f. City boundaries are marked with bold lines, and hotels are illustrated with rectangles.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
A tour of cities is a sequence of hotels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start. how to express MT decoding (Model 4 plus English bigrams) in IP format.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
We first create a salesman graph like the one in Figure 3.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"To do this, we set up a city for each word in the observed sentence f. City boundaries are shown with bold lines.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
We populate each city with ten hotels corresponding to ten likely English word translations.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
Hotels are shown as small rectangles.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
The owner of a hotel is the English word inside the rectangle.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"If two cities have hotels with the same owner x, then we build a third xowned hotel on the border of the two cities.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"More generally, ifcities all have hotels owned by x, we build new hotels (one for each non-empty, non-singleton subset of the cities) on various city borders and intersections.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Finally, we add an extra city representing the sentence boundary.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
We define a tour of cities as a sequence and hotels (starting at the sentence boundary hotel) that visits each city exactly once before returning to the start.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"If a hotel sits on the border between two cities, then staying at that hotel counts as visiting both cities.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"We can view each tour of cities as corresponding to a potential decodinge,a.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"The owners of the hotels on the tour give us e, while the hotel locations yield a.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"The next task is to establish real-valued (asymmetric) distances between pairs of hotels, such that the length of any tour is exactly the negative of log(P(e)P(a,fe)).","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Because log is monotonic, the shortest tour will correspond to the likeliest decoding.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
The distance we assign to each pair of hotels consists of some small piece of the Model 4 formula.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
The usual case is typified by the large black arrow in Figure 3.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Because the destination hotel “not” sits on the border between cities NE and PAS, it corresponds to a partial alignment in which the word “not” has fertility two: If we assume that we have already paid the price for visiting the “what” hotel, then our interhotel distance need only account for the partial alignment concerning “not”: NULL-owned hotels are treated specially.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"We require that all non-NULL hotels be visited before any NULL hotels, and we further require that at most one NULL hotel visited on a tour.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Moreover, the NULL fertility sub-formula is easy to compute if we allow only one NULL hotel to be visited:is simply the number of cities that hotel straddles, and is the number of cities minus one.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
This case is typified by the large gray arrow shown in Figure 3.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Between hotels that are located (even partially) in the same city, we assign an infinite distance in both directions, as travel from one to the other can never be part of a tour.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"For 6-word French sentences, we normally come up with a graph that has about 80 hotels and 3500 finite-cost travel segments.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
The next step is to cast tour selection as an integer program.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
Here we adapt a subtour elimination strategy used in standard TSP.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
We create a binary (0/1) integer variable for each pair of hotels and. if and only if travel from hotelto hotelis on the itinerary.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
The objective function is straightforward: This minimization is subject to three classes of constraints.,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"First, every city must be visited exactly once.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"That means exactly one tour segment must exit each city: Second, the segments must be linked to one another, i.e., every hotel has either (a) one tour segment coming in and one going out, or (b) no segments in and none out.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"To put it another way, every hotel must have an equal number of tour segments going in and out: Third, it is necessary to prevent multiple independent sub-tours.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"To do this, we require that every proper subset of cities have at least one tour segment leaving it: There are an exponential number of constraints in this third class.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"Finally, we invoke our IP solver.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
"If we assign mnemonic names to the variables, we can easily extracte,afrom the list of variables and their binary values.","{'title': '6 Integer Programming Decoding', 'number': '6'}"
The shortest tour for the graph in Figure 3 corresponds to this optimal decoding: it is not clear .,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
We can obtain the second-best decoding by adding a new constraint to the IP to stop it from choosing the same solution again.4,"{'title': '6 Integer Programming Decoding', 'number': '6'}"
"In our experiments we used a test collection of 505 sentences, uniformly distributed across the lengths 6, 8, 10, 15, and 20.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"We evaluated all decoders with respect to (1) speed, (2) search optimality, and (3) translation accuracy.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"The last two factors may not always coincide, as Model 4 is an imperfect model of the translation process—i.e., there is no guarantee that a numerically optimal decoding is actually a good translation.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"Suppose a decoder outputs, while the optimal decoding turns out to be.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"Then we consider six possible outcomes: no error (NE) : , and is a perfect , andis a perfect translation, whileis not. harmless search error (HSE): , butandare both perfectly good translations. compound error (CE): , and neither is a perfect translation.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"Here, “perfect” refers to a human-judged translation that transmits all of the meaning of the source sentence using flawless target-language syntax.","{'title': '7 Experiments and Discussion', 'number': '7'}"
We have found it very useful to have several decoders on hand.,"{'title': '7 Experiments and Discussion', 'number': '7'}"
"It is only through IP decoder output, for example, that we can know the stack decoder is returning optimal solutions for so many sentences (see Table 1).","{'title': '7 Experiments and Discussion', 'number': '7'}"
"The IP and stack decoders enabled us to quickly locate bugs in the greedy decoder, and to implement extensions to the basic greedy search that can find better solutions.","{'title': '7 Experiments and Discussion', 'number': '7'}"
(We came up with the greedy operations discussed in Section 5 by carefully analyzing error logs of the kind shown in Table 1).,"{'title': '7 Experiments and Discussion', 'number': '7'}"
The results in Table 1 also enable us to prioritize the items on our research agenda.,"{'title': '7 Experiments and Discussion', 'number': '7'}"
"Since the majority of the translation errors can be attributed to the language and translation models we use (see column PME in Table 1), it is clear that significant improvement in translation quality will come from better trigram language model.","{'title': '7 Experiments and Discussion', 'number': '7'}"
Greedyand greedyare greedy decoders optimized for speed. models.,"{'title': '7 Experiments and Discussion', 'number': '7'}"
"The results in Table 2, obtained with decoders that use a trigram language model, show that our greedy decoding algorithm is a viable alternative to the traditional stack decoding algorithm.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"Even when the greedy decoder uses an optimized-forspeed set of operations in which at most one word is translated, moved, or inserted at a time and at most 3-word-long segments are swapped—which is labeled “greedy” in Table 2—the translation accuracy is affected only slightly.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"In contrast, the translation speed increases with at least one order of magnitude.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"Depending on the application of interest, one may choose to use a slow decoder that provides optimal results or a fast, greedy decoder that provides non-optimal, but acceptable results.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"One may also run the greedy decoder using a time threshold, as any instance of anytime algorithm.","{'title': '7 Experiments and Discussion', 'number': '7'}"
"When the threshold is set to one second per sentence (the greedylabel in Table 1), the performance is affected only slightly.","{'title': '7 Experiments and Discussion', 'number': '7'}"
Acknowledgments.,"{'title': '7 Experiments and Discussion', 'number': '7'}"
This work was supported by DARPA-ITO grant N66001-00-1-9814.,"{'title': '7 Experiments and Discussion', 'number': '7'}"
