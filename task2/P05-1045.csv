col1,col2
"Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use.",{}
We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models.,{}
"By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference.",{}
"We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints.",{}
This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.,{}
Most statistical models currently used in natural language processing represent only local structure.,"{'title': '1 Introduction', 'number': '1'}"
"Although this constraint is critical in enabling tractable model inference, it is a key limitation in many tasks, since natural language contains a great deal of nonlocal structure.","{'title': '1 Introduction', 'number': '1'}"
"A general method for solving this problem is to relax the requirement of exact inference, substituting approximate inference algorithms instead, thereby permitting tractable inference in models with non-local structure.","{'title': '1 Introduction', 'number': '1'}"
"One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model, including sequence models and probabilistic context free grammars (Geman and Geman, 1984).","{'title': '1 Introduction', 'number': '1'}"
"Although Gibbs sampling is widely used elsewhere, there has been extremely little use of it in natural language processing.1 Here, we use it to add non-local dependencies to sequence models for information extraction.","{'title': '1 Introduction', 'number': '1'}"
"Statistical hidden state sequence models, such as Hidden Markov Models (HMMs) (Leek, 1997; Freitag and McCallum, 1999), Conditional Markov Models (CMMs) (Borthwick, 1999), and Conditional Random Fields (CRFs) (Lafferty et al., 2001) are a prominent recent approach to information extraction tasks.","{'title': '1 Introduction', 'number': '1'}"
These models all encode the Markov property: decisions about the state at a particular position in the sequence can depend only on a small local window.,"{'title': '1 Introduction', 'number': '1'}"
"It is this property which allows tractable computation: the Viterbi, Forward Backward, and Clique Calibration algorithms all become intractable without it.","{'title': '1 Introduction', 'number': '1'}"
"However, information extraction tasks can benefit from modeling non-local structure.","{'title': '1 Introduction', 'number': '1'}"
"As an example, several authors (see Section 8) mention the value of enforcing label consistency in named entity recognition (NER) tasks.","{'title': '1 Introduction', 'number': '1'}"
"In the example given in Figure 1, the second occurrence of the token Tanjug is mislabeled by our CRF-based statistical NER system, because by looking only at local evidence it is unclear whether it is a person or organization.","{'title': '1 Introduction', 'number': '1'}"
"The first occurrence of Tanjug provides ample evidence that it is an organization, however, and by enforcing label consistency the system should be able to get it right.","{'title': '1 Introduction', 'number': '1'}"
"We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this technique yields significant improvements on two established IE tasks.","{'title': '1 Introduction', 'number': '1'}"
"In hidden state sequence models such as HMMs, CMMs, and CRFs, it is standard to use the Viterbi algorithm, a dynamic programming algorithm, to infer the most likely hidden state sequence given the input and the model (see, e.g., Rabiner (1989)).","{'title': '1 Introduction', 'number': '1'}"
"Although this is the only tractable method for exact computation, there are other methods for computing an approximate solution.","{'title': '1 Introduction', 'number': '1'}"
Monte Carlo methods are a simple and effective class of methods for approximate inference based on sampling.,"{'title': '1 Introduction', 'number': '1'}"
Imagine we have a hidden state sequence model which defines a probability distribution over state sequences conditioned on any given input.,"{'title': '1 Introduction', 'number': '1'}"
"With such a model M we should be able to compute the conditional probability PM(s|o) of any state sequence s = {s0, ... , sN} given some observed input sequence o = {o0, ... , oN}.","{'title': '1 Introduction', 'number': '1'}"
One can then sample sequences from the conditional distribution defined by the model.,"{'title': '1 Introduction', 'number': '1'}"
"These samples are likely to be in high probability areas, increasing our chances of finding the maximum.","{'title': '1 Introduction', 'number': '1'}"
The challenge is how to sample sequences efficiently from the conditional distribution defined by the model.,"{'title': '1 Introduction', 'number': '1'}"
"Gibbs sampling provides a clever solution (Geman and Geman, 1984).","{'title': '1 Introduction', 'number': '1'}"
"Gibbs sampling defines a Markov chain in the space of possible variable assignments (in this case, hidden state sequences) such that the stationary distribution of the Markov chain is the joint distribution over the variables.","{'title': '1 Introduction', 'number': '1'}"
Thus it is called a Markov Chain Monte Carlo (MCMC) method; see Andrieu et al. (2003) for a good MCMC tutorial.,"{'title': '1 Introduction', 'number': '1'}"
"In practical terms, this means that we can walk the Markov chain, occasionally outputting samples, and that these samples are guaranteed to be drawn from the target distribution.","{'title': '1 Introduction', 'number': '1'}"
"Furthermore, the chain is defined in very simple terms: from each state sequence we can only transition to a state sequence obtained by changing the state at any one position i, and the distribution over these possible transitions is just where s−i is all states except si.","{'title': '1 Introduction', 'number': '1'}"
"In other words, the transition probability of the Markov chain is the conditional distribution of the label at the position given the rest of the sequence.","{'title': '1 Introduction', 'number': '1'}"
"This quantity is easy to compute in any Markov sequence model, including HMMs, CMMs, and CRFs.","{'title': '1 Introduction', 'number': '1'}"
"One easy way to walk the Markov chain is to loop through the positions i from 1 to N, and for each one, to resample the hidden state at that position from the distribution given in Equation 1.","{'title': '1 Introduction', 'number': '1'}"
"By outputting complete sequences at regular intervals (such as after resampling all N positions), we can sample sequences from the conditional distribution defined by the model.","{'title': '1 Introduction', 'number': '1'}"
"This is still a gravely inefficient process, however.","{'title': '1 Introduction', 'number': '1'}"
"Random sampling may be a good way to estimate the shape of a probability distribution, but it is not an efficient way to do what we want: find the maximum.","{'title': '1 Introduction', 'number': '1'}"
"However, we cannot just transition greedily to higher probability sequences at each step, because the space is extremely non-convex.","{'title': '1 Introduction', 'number': '1'}"
"We can, however, borrow a technique from the study of non-convex optimization and use simulated annealing (Kirkpatrick et al., 1983).","{'title': '1 Introduction', 'number': '1'}"
"Geman and Geman (1984) show that it is easy to modify a Gibbs Markov chain to do annealing; at time t we replace the distribution in (1) with where c = {c0, ... , cT} defines a cooling schedule.","{'title': '1 Introduction', 'number': '1'}"
"At each step, we raise each value in the conditional distribution to an exponent and renormalize before sampling from it.","{'title': '1 Introduction', 'number': '1'}"
"Note that when c = 1 the distribution is unchanged, and as c → 0 the distribution becomes sharper, and when c = 0 the distribution places all of its mass on the maximal outcome, having the effect that the Markov chain always climbs uphill.","{'title': '1 Introduction', 'number': '1'}"
"Thus if we gradually decrease c from 1 to 0, the Markov chain increasingly tends to go uphill.","{'title': '1 Introduction', 'number': '1'}"
"This annealing technique has been shown to be an effective technique for stochastic optimization (Laarhoven and Arts, 1987).","{'title': '1 Introduction', 'number': '1'}"
"To verify the effectiveness of Gibbs sampling and simulated annealing as an inference technique for hidden state sequence models, we compare Gibbs and Viterbi inference methods for a basic CRF, without the addition of any non-local model.","{'title': '1 Introduction', 'number': '1'}"
"The results, given in Table 1, show that if the Gibbs sampler is run long enough, its accuracy is the same as a Viterbi decoder.","{'title': '1 Introduction', 'number': '1'}"
Our basic CRF model follows that of Lafferty et al. (2001).,"{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"We choose a CRF because it represents the state of the art in sequence modeling, allowing both discriminative training and the bi-directional flow of probabilistic information across the sequence.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
A CRF is a conditional sequence model which represents the probability of a hidden state sequence given some observations.,"{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"In order to facilitate obtaining the conditional probabilities we need for Gibbs sampling, we generalize the CRF model in a way that is consistent with the Markov Network literature (see Cowell et al. (1999)): we create a linear chain of cliques, where each clique, c, represents the probabilistic relationship between an adjacent pair of states2 using a clique potential φc, which is just a table containing a value for each possible state assignment.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"The table is not a true probability distribution, as it only accounts for local interactions within the clique.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"The clique potentials themselves are defined in terms of exponential models conditioned on features of the observation sequence, and must be instantiated for each new observation sequence.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"The sequence of potentials in the clique chain then defines the probability of a state sequence (given the observation sequence) as where φi(si−1, si) is the element of the clique potential at position i corresponding to states si−1 and si.3 Although a full treatment of CRF training is beyond the scope of this paper (our technique assumes the model is already trained), we list the features used by our CRF for the two tasks we address in Table 2.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"During training, we regularized our exponential models with a quadratic prior and used the quasi-Newton method for parameter optimization.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"As is customary, we used the Viterbi algorithm to infer the most likely state sequence in a CRF.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"The clique potentials of the CRF, instantiated for some observation sequence, can be used to easily compute the conditional distribution over states at a position given in Equation 1.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
Recall that at position i we want to condition on the states in the rest of the sequence.,"{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"The state at this position can be influenced by any other state that it shares a clique with; in particular, when the clique size is 2, there are 2 such cliques.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"In this case the Markov blanket of the state (the minimal set of states that renders a state conditionally independent of all other states) consists of the two neighboring states and the observation sequence, all of which are observed.","{'title': '3 A Conditional Random Field Model', 'number': '2'}"
The conditional distribution at position i can then be computed simply as where the factor tables F in the clique chain are already conditioned on the observation sequence.,"{'title': '3 A Conditional Random Field Model', 'number': '2'}"
"We test the effectiveness of our technique on two established datasets: the CoNLL 2003 English named entity recognition dataset, and the CMU Seminar Announcements information extraction dataset.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"This dataset was created for the shared task of the Seventh Conference on Computational Natural Language Learning (CoNLL),4 which concerned named entity recognition.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"The English data is a collection of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC).","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"The data is separated into a training set, a development set (testa), and a test set (testb).","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"The training set contains 945 documents, and approximately 203,000 tokens.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"The development set has 216 documents and approximately 51,000 tokens, and the test set has 231 documents and approximately 46,000 tokens.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
We evaluate performance on this task in the manner dictated by the competition so that results can be properly compared.,"{'title': '4 Datasets and Evaluation', 'number': '3'}"
Precision and recall are evaluated on a per-entity basis (and combined into an F1 score).,"{'title': '4 Datasets and Evaluation', 'number': '3'}"
There is no partial credit; an incorrect entity boundary is penalized as both a false positive and as a false negative.,"{'title': '4 Datasets and Evaluation', 'number': '3'}"
This dataset was developed as part of Dayne Freitag’s dissertation research Freitag (1998).5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University.,"{'title': '4 Datasets and Evaluation', 'number': '3'}"
"It is annotated for four fields: speaker, location, start time, and end time.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"Sutton and McCallum (2004) used 5-fold cross validation when evaluating on this dataset, so we obtained and used their data splits, so that results can be properly compared.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"Because the entire dataset is used for testing, there is no development set.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"We also used their evaluation metric, which is slightly different from the method for CoNLL data.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"Instead of evaluating precision and recall on a per-entity basis, they are evaluated on a per-token basis.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"Then, to calculate the overall F1 score, the F1 scores for each class are averaged.","{'title': '4 Datasets and Evaluation', 'number': '3'}"
"Our models of non-local structure are themselves just sequence models, defining a probability distribution over all possible state sequences.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"It is possible to flexibly model various forms of constraints in a way that is sensitive to the linguistic structure of the data (e.g., one can go beyond imposing just exact identity conditions).","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"One could imagine many ways of defining such models; for simplicity we use the form where the product is over a set of violation types A, and for each violation type A we specify a penalty parameter θλ.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The exponent #(A, s, o) is the count of the number of times that the violation A occurs in the state sequence s with respect to the observation sequence o.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
This has the effect of assigning sequences with more violations a lower probability.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The particular violation types are defined specifically for each task, and are described in the following two sections.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"This model, as defined above, is not normalized, and clearly it would be expensive to do so.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"This doesn’t matter, however, because we only use the model for Gibbs sampling, and so only need to compute the conditional distribution at a single position i (as defined in Equation 1).","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"One (inefficient) way to compute this quantity is to enumerate all possible sequences differing only at position i, compute the score assigned to each by the model, and renormalize.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Although it seems expensive, this computation can be made very efficient with a straightforward memoization technique: at all times we maintain data structures representing the relationship between entity labels and token sequences, from which we can quickly compute counts of different types of violations.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Label consistency structure derives from the fact that within a particular document, different occurrences of a particular token sequence are unlikely to be labeled as different entity types.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Although any one occurrence may be ambiguous, it is unlikely that all instances are unclear when taken together.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
The CoNLL training data empirically supports the strength of the label consistency constraint.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Table 3 shows the counts of entity labels for each pair of identical token sequences within a document, where both are labeled as an entity.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Note that inconsistent labelings are very rare.6 In addition, we also want to model subsequence constraints: having seen Geoff Woods earlier in a document as a person is a good indicator that a subsequent occurrence of Woods should also be labeled as a person.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"However, if we examine all cases of the labelings of other occurrences of subsequences of a labeled entity, we find that the consistency constraint does not hold nearly so strictly in this case.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"As an example, one document contains references to both The China Daily, a newspaper, and China, the country.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
Counts of subsequence labelings within a document are listed in Table 4.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Note that there are many offdiagonal entries: the China Daily case is the most common, occurring 328 times in the dataset.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The penalties used in the long distance constraint model for CoNLL are the Empirical Bayes estimates taken directly from the data (Tables 3 and 4), except that we change counts of 0 to be 1, so that the distribution remains positive.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"So the estimate of a PER also being an ORG is 5 3151; there were 5 instance of an entity being labeled as both, PER appeared 3150 times in the data, and we add 1 to this for smoothing, because PER-MISC never occured.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"However, when we have a phrase labeled differently in two different places, continuing with the PER-ORG example, it is unclear if we should penalize it as PER that is also an ORG or an ORG that is also a PER.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"To deal with this, we multiply the square roots of each estimate together to form the penalty term.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The penalty term is then multiplied in a number of times equal to the length of the offending entity; this is meant to “encourage” the entity to shrink.7 For example, say we have a document with three entities, Rotor Volgograd twice, once labeled as PER and once as ORG, and Rotor, labeled as an ORG.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The likelihood of a PER also being an ORG is 5 3151, and of an ORG also Due to the lack of a development set, our consistency model for the CMU Seminar Announcements is much simpler than the CoNLL model, the numbers where selected due to our intuitions, and we did not spend much time hand optimizing the model.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"Specifically, we had three constraints.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
"The first is that all entities labeled as start time are normalized, and are penalized if they are inconsistent.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
The second is a corresponding constraint for end times.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
The last constraint attempts to consistently label the speakers.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
"If a phrase is labeled as a speaker, we assume that the last word is the speaker’s last name, and we penalize for each occurrance of that word which is not also labeled speaker.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
For the start and end times the penalty is multiplied in based on how many words are in the entity.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
"For the speaker, the penalty is only multiplied in once.","{'title': '5 Models of Non-local Structure', 'number': '4'}"
We used a hand selected penalty of exp −4.0.,"{'title': '5 Models of Non-local Structure', 'number': '4'}"
In the previous section we defined two models of non-local structure.,"{'title': '6 Combining Sequence Models', 'number': '5'}"
"Now we would like to incorporate them into the local model (in our case, the trained CRF), and use Gibbs sampling to find the most likely state sequence.","{'title': '6 Combining Sequence Models', 'number': '5'}"
"Because both the trained CRF and the non-local models are themselves sequence models, we simply combine the two models into a factored sequence model of the following form where M is the local CRF model, L is the new nonlocal model, and F is the factored model.8 In this form, the probability again looks difficult to compute (because of the normalizing factor, a sum over all hidden state sequences of length N).","{'title': '6 Combining Sequence Models', 'number': '5'}"
"However, since we are only using the model for Gibbs sampling, we never need to compute the distribution explicitly.","{'title': '6 Combining Sequence Models', 'number': '5'}"
"Instead, we need only the conditional probability of each position in the sequence, which can be computed as","{'title': '6 Combining Sequence Models', 'number': '5'}"
In our experiments we compare the impact of adding the non-local models with Gibbs sampling to our baseline CRF implementation.,"{'title': '7 Results and Discussion', 'number': '6'}"
"In the CoNLL named entity recognition task, the non-local models increase the F1 accuracy by about 1.3%.","{'title': '7 Results and Discussion', 'number': '6'}"
"Although such gains may appear modest, note that they are achieved relative to a near state-of-the-art NER system: the winner of the CoNLL English task reported an F1 score of 88.76.","{'title': '7 Results and Discussion', 'number': '6'}"
"In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task.","{'title': '7 Results and Discussion', 'number': '6'}"
Our performance is similar on the CMU Seminar Announcements dataset.,"{'title': '7 Results and Discussion', 'number': '6'}"
"We show the per-field F1 results that were reported by Sutton and McCallum (2004) for comparison, and note that we are again achieving gains against a more competitive baseline system.","{'title': '7 Results and Discussion', 'number': '6'}"
"For all experiments involving Gibbs sampling, we used a linear cooling schedule.","{'title': '7 Results and Discussion', 'number': '6'}"
"For the CoNLL dataset we collected 200 samples per trial, and for the CMU Seminar Announcements we collected 100 samples.","{'title': '7 Results and Discussion', 'number': '6'}"
"We report the average of all trials, and in all cases we outperform the baseline with greater than 95% confidence, using the standard t-test.","{'title': '7 Results and Discussion', 'number': '6'}"
"The trials had low standard deviations - 0.083% and 0.007% and high minimun F-scores - 86.72%, and 92.28% - for the CoNLL and CMU Seminar Announcements respectively, demonstrating the stability of our method.","{'title': '7 Results and Discussion', 'number': '6'}"
The biggest drawback to our model is the computational cost.,"{'title': '7 Results and Discussion', 'number': '6'}"
Taking 100 samples dramatically increases test time.,"{'title': '7 Results and Discussion', 'number': '6'}"
"Averaged over 3 runs on both Viterbi and Gibbs, CoNLL testing time increased from 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to 6436 seconds.","{'title': '7 Results and Discussion', 'number': '6'}"
Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems.,"{'title': '8 Related Work', 'number': '7'}"
Mikheev et al. (1999) and Finkel et al.,"{'title': '8 Related Work', 'number': '7'}"
(2004) incorporate label consistency information by using adhoc multi-stage labeling procedures that are effective but special-purpose.,"{'title': '8 Related Work', 'number': '7'}"
Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document.,"{'title': '8 Related Work', 'number': '7'}"
"Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference.","{'title': '8 Related Work', 'number': '7'}"
"Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences.","{'title': '8 Related Work', 'number': '7'}"
"This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest.","{'title': '8 Related Work', 'number': '7'}"
"Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.","{'title': '8 Related Work', 'number': '7'}"
This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones.,"{'title': '8 Related Work', 'number': '7'}"
"However, this approach cannot easily be extended to incorporate other types of non-local structure.","{'title': '8 Related Work', 'number': '7'}"
"The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes.","{'title': '8 Related Work', 'number': '7'}"
"Unfortunately, in the RMN model, the dependencies must be defined in the model structure before doing any inference, and so the authors use crude heuristic part-of-speech patterns, and then add dependencies between these text spans using clique templates.","{'title': '8 Related Work', 'number': '7'}"
"This generates a extremely large number of overlapping candidate entities, which then necessitates additional templates to enforce the constraint that text subsequences cannot both be different entities, something that is more naturally modeled by a CRF.","{'title': '8 Related Work', 'number': '7'}"
Another disadvantage of this approach is that it uses loopy beliefpropagation and a voted perceptron for approximate learning and inference – ill-founded and inherently unstable algorithms which are noted by the authors to have caused convergence problems.,"{'title': '8 Related Work', 'number': '7'}"
"In the skip-chain CRFs model, the decision of which nodes to connect is also made heuristically, and because the authors focus on named entity recognition, they chose to connect all pairs of identical capitalized words.","{'title': '8 Related Work', 'number': '7'}"
They also utilize loopy belief propagation for approximate learning and inference.,"{'title': '8 Related Work', 'number': '7'}"
"While the technique we propose is similar mathematically and in spirit to the above approaches, it differs in some important ways.","{'title': '8 Related Work', 'number': '7'}"
"Our model is implemented by adding additional constraints into the model at inference time, and does not require the preprocessing step necessary in the two previously mentioned works.","{'title': '8 Related Work', 'number': '7'}"
"This allows for a broader class of long-distance dependencies, because we do not need to make any initial assumptions about which nodes should be connected, and is helpful when you wish to model relationships between nodes which are the same class, but may not be similar in any other way.","{'title': '8 Related Work', 'number': '7'}"
"For instance, in the CMU Seminar Announcements dataset, we can normalize all entities labeled as a start time and penalize the model if multiple, nonconsistent times are labeled.","{'title': '8 Related Work', 'number': '7'}"
"This type of constraint cannot be modeled in an RMN or a skip-CRF, because it requires the knowledge that both entities are given the same class label.","{'title': '8 Related Work', 'number': '7'}"
"We also allow dependencies between multi-word phrases, and not just single words.","{'title': '8 Related Work', 'number': '7'}"
"Additionally, our model can be applied on top of a pre-existing trained sequence model.","{'title': '8 Related Work', 'number': '7'}"
"As such, our method does not require complex training procedures, and can instead leverage all of the established methods for training high accuracy sequence models.","{'title': '8 Related Work', 'number': '7'}"
"It can indeed be used in conjunction with any statistical hidden state sequence model: HMMs, CMMs, CRFs, or even heuristic models.","{'title': '8 Related Work', 'number': '7'}"
"Third, our technique employs Gibbs sampling for approximate inference, a simple and probabilistically well-founded algorithm.","{'title': '8 Related Work', 'number': '7'}"
"As a consequence of these differences, our approach is easier to understand, implement, and adapt to new applications.","{'title': '8 Related Work', 'number': '7'}"
We have shown that a constraint model can be effectively combined with an existing sequence model in a factored architecture to successfully impose various sorts of long distance constraints.,"{'title': '9 Conclusions', 'number': '8'}"
Our model generalizes naturally to other statistical models and other tasks.,"{'title': '9 Conclusions', 'number': '8'}"
"In particular, it could in the future be applied to statistical parsing.","{'title': '9 Conclusions', 'number': '8'}"
"Statistical context free grammars provide another example of statistical models which are restricted to limiting local structure, and which could benefit from modeling nonlocal structure.","{'title': '9 Conclusions', 'number': '8'}"
This work was supported in part by the Advanced Researchand Development Activity (ARDA)’s Advanced Question Answeringfor Intelligence (AQUAINT) Program.,"{'title': 'Acknowledgements', 'number': '9'}"
"Additionally, we would like to that our reviewers for their helpful comments.","{'title': 'Acknowledgements', 'number': '9'}"
