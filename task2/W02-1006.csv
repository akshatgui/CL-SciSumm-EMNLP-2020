col1,col2
"In this paper, we evaluate a variety of knowledge sources and supervised learning algorithms for word sense disambiguation on SENSEVAL-2 and SENSEVAL-1 data.",{}
"Our knowledge sources include the part-of-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations.",{}
"The learning algorithms evaluated include Support Vector Machines (SVM), Naive Bayes, AdaBoost, and decision tree algorithms.",{}
We present empirical results showing the relative contribution of the component knowledge sources and the different learning algorithms.,{}
"In particular, using all of these knowledge sources and SVM (i.e., a single learning algorithm) achieves accuracy higher than the best official scores on both SENSEVAL-2 and SENSEVAL-1 test data.",{}
Natural language is inherently ambiguous.,"{'title': '1 Introduction', 'number': '1'}"
A word can have multiple meanings (or senses).,"{'title': '1 Introduction', 'number': '1'}"
"Given an occurrence of a word in a natural language text, the task of word sense disambiguation (WSD) is to determine the correct sense of in that context.","{'title': '1 Introduction', 'number': '1'}"
WSD is a fundamental problem of natural language processing.,"{'title': '1 Introduction', 'number': '1'}"
"For example, effective WSD is crucial for high quality machine translation.","{'title': '1 Introduction', 'number': '1'}"
One could envisage building a WSD system using handcrafted rules or knowledge obtained from linguists.,"{'title': '1 Introduction', 'number': '1'}"
"Such an approach would be highly laborintensive, with questionable scalability.","{'title': '1 Introduction', 'number': '1'}"
Another approach involves the use of dictionary or thesaurus to perform WSD.,"{'title': '1 Introduction', 'number': '1'}"
"In this paper, we focus on a corpus-based, supervised learning approach.","{'title': '1 Introduction', 'number': '1'}"
"In this approach, to disambiguate a word , we first collect training texts in which instances of occur.","{'title': '1 Introduction', 'number': '1'}"
Each occurrence of is manually tagged with the correct sense.,"{'title': '1 Introduction', 'number': '1'}"
"We then train a WSD classifier based on these sample texts, such that the trained classifier is able to assign the sense of in a new context.","{'title': '1 Introduction', 'number': '1'}"
"Two WSD evaluation exercises, SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Edmonds and Cotton, 2001), were conducted in 1998 and 2001, respectively.","{'title': '1 Introduction', 'number': '1'}"
"The lexical sample task in these two SENSEVALs focuses on evaluating WSD systems in disambiguating a subset of nouns, verbs, and adjectives, for which manually sense-tagged training data have been collected.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we conduct a systematic evaluation of the various knowledge sources and supervised learning algorithms on the English lexical sample data sets of both SENSEVALs.","{'title': '1 Introduction', 'number': '1'}"
There is a large body of prior research on WSD.,"{'title': '2 Related Work', 'number': '2'}"
"Due to space constraints, we will only highlight prior research efforts that have investigated (1) contribution of various knowledge sources, or (2) relative performance of different learning algorithms.","{'title': '2 Related Work', 'number': '2'}"
"Early research efforts on comparing different learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words.","{'title': '2 Related Work', 'number': '2'}"
"Ng (1997) compared two learning algorithms, k-nearest neighbor and Naive Bayes, on the DSO corpus (191 words).","{'title': '2 Related Work', 'number': '2'}"
"Escudero et al. (2000) evaluated k-nearest neighbor, Naive Bayes, Winnow-based, and LazyBoosting algorithms on the DSO corpus.","{'title': '2 Related Work', 'number': '2'}"
The recent work of Pedersen (2001a) and Zavrel et al. (2000) evaluated a variety of learning algorithms on the SENSEVAL1 data set.,"{'title': '2 Related Work', 'number': '2'}"
"However, all of these research efforts concentrate only on evaluating different learning algorithms, without systematically considering their interaction with knowledge sources.","{'title': '2 Related Work', 'number': '2'}"
"Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word “interest”.","{'title': '2 Related Work', 'number': '2'}"
"Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD.","{'title': '2 Related Work', 'number': '2'}"
"However, they do not evaluate their method on a common benchmark data set, and there is no exploration on the interaction of knowledge sources with different learning algorithms.","{'title': '2 Related Work', 'number': '2'}"
"Participating systems at SENSEVAL-1 and SENSEVAL-2 tend to report accuracy using a particular set of knowledge sources and some particular learning algorithm, without investigating the effect of varying knowledge sources and learning algorithms.","{'title': '2 Related Work', 'number': '2'}"
"In SENSEVAL-2, the various Duluth systems (Pedersen, 2001b) attempted to investigate whether features or learning algorithms are more important.","{'title': '2 Related Work', 'number': '2'}"
"However, relative contribution of knowledge sources was not reported and only two main types of algorithms (Naive Bayes and decision tree) were tested.","{'title': '2 Related Work', 'number': '2'}"
"In contrast, in this paper, we systematically vary both knowledge sources and learning algorithms, and investigate the interaction between them.","{'title': '2 Related Work', 'number': '2'}"
"We also base our evaluation on both SENSEVAL-2 and SENSEVAL-1 official test data sets, and compare with the official scores of participating systems.","{'title': '2 Related Work', 'number': '2'}"
"To disambiguate a word occurrence , we consider four knowledge sources listed below.","{'title': '3 Knowledge Sources', 'number': '3'}"
Each training (or test) context of generates one training (or test) feature vector.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"We use 7 features to encode this knowledge source: , where ( ) is the POS of theth token to the left (right) of , and is the POS of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"A token can be a word or a punctuation symbol, and each of these neighboring tokens must be in the same sentence as .","{'title': '3 Knowledge Sources', 'number': '3'}"
"We use a sentence segmentation program (Reynar and Ratnaparkhi, 1997) and a POS tagger (Ratnaparkhi, 1996) to segment the tokens surrounding into sentences and assign POS tags to these tokens.","{'title': '3 Knowledge Sources', 'number': '3'}"
"For example, to disambiguate the word bars in the POS-tagged sentence “Reid/NNP saw/VBD me/PRP looking/VBG at/IN the/DT iron/NN bars/NNS ./.”, the POS feature vector is where denotes For this knowledge source, we consider all single words (unigrams) in the surrounding context of , and these words can be in a different sentence from .","{'title': '3 Knowledge Sources', 'number': '3'}"
"For each training or test example, the SENSEVAL data sets provide up to a few sentences as the surrounding context.","{'title': '3 Knowledge Sources', 'number': '3'}"
"In the results reported in this paper, we consider all words in the provided context.","{'title': '3 Knowledge Sources', 'number': '3'}"
"Specifically, all tokens in the surrounding context of are converted to lower case and replaced by their morphological root forms.","{'title': '3 Knowledge Sources', 'number': '3'}"
Tokens present in a list of stop words or tokens that do not contain at least an alphabet character (such as numbers and punctuation symbols) are removed.,"{'title': '3 Knowledge Sources', 'number': '3'}"
All remaining tokens from all training contexts provided for are gathered.,"{'title': '3 Knowledge Sources', 'number': '3'}"
Each remaining token contributes one feature.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"In a training (or test) example, the feature corresponding to is set to 1 iff the context of in that training (or test) example contains.","{'title': '3 Knowledge Sources', 'number': '3'}"
We attempted a simple feature selection method to investigate if a learning algorithm performs better with or without feature selection.,"{'title': '3 Knowledge Sources', 'number': '3'}"
The feature selection method employed has one parameter: .,"{'title': '3 Knowledge Sources', 'number': '3'}"
A feature is selected if occurs in some sense of or more times in the training data.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"This parameter is also used by (Ng and Lee, 1996).","{'title': '3 Knowledge Sources', 'number': '3'}"
"We have tried and (i.e., no feature selection) in the results reported in this paper. the POS tag of a null token.","{'title': '3 Knowledge Sources', 'number': '3'}"
"For example, if is the word bars and the set of selected unigrams is chocolate, iron, beer , the feature vector for the sentence “Reid saw me looking at the iron bars .” is 0, 1, 0 .","{'title': '3 Knowledge Sources', 'number': '3'}"
"A local collocation refers to the ordered sequence of tokens in the local, narrow context of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"Offsets and denote the starting and ending position (relative to ) of the sequence, where a negative (positive) offset refers to a token to its left (right).","{'title': '3 Knowledge Sources', 'number': '3'}"
"For example, let be the word bars in the sentence “Reid saw me looking at the iron bars where denotes a null token.","{'title': '3 Knowledge Sources', 'number': '3'}"
"Like POS, a collocation does not cross sentence boundary.","{'title': '3 Knowledge Sources', 'number': '3'}"
"To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: , , , , , , , , ,,and .","{'title': '3 Knowledge Sources', 'number': '3'}"
This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997).,"{'title': '3 Knowledge Sources', 'number': '3'}"
"To extract the feature values of the collocation feature , we first collect all possible collocation strings (converted into lower case) corresponding to in all training contexts of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"Unlike the case for surrounding words, we do not remove stop words, numbers, or punctuation symbols.","{'title': '3 Knowledge Sources', 'number': '3'}"
Each collocation string is a possible feature value.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"Feature value selection using , analogous to that used to select surrounding words, can be optionally applied.","{'title': '3 Knowledge Sources', 'number': '3'}"
"If a training (or test) context of has collocation , and is a selected feature value, then the feature of has value .","{'title': '3 Knowledge Sources', 'number': '3'}"
"Otherwise, it has the value , denoting the null string.","{'title': '3 Knowledge Sources', 'number': '3'}"
"Note that each collocation is represented by one feature that can have many possible feature values (the local collocation strings), whereas each distinct surrounding word is represented by one feature that takes binary values (indicating presence or absence of that word).","{'title': '3 Knowledge Sources', 'number': '3'}"
"For example, if is the word bars and suppose the set of selected collocations for is a chocolate, the wine, the iron , then the feature value for collocation in the sentence “Reid saw me looking at the iron bars .” is the iron.","{'title': '3 Knowledge Sources', 'number': '3'}"
"We first parse the sentence containing with a statistical parser (Charniak, 2000).","{'title': '3 Knowledge Sources', 'number': '3'}"
The constituent tree structure generated by Charniak’s parser is then converted into a dependency tree in which every word points to a parent headword.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"For example, in the sentence “Reid saw me looking at the iron bars .”, the word Reid points to the parent headword saw.","{'title': '3 Knowledge Sources', 'number': '3'}"
"Similarly, the word me also points to the parent headword saw.","{'title': '3 Knowledge Sources', 'number': '3'}"
"We use different types of syntactic relations, depending on the POS of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"If is a noun, we use four features: its parent headword , the POS of , the voice of (active, passive, or if is nota verb), and the relative position of from (whether is to the left or right of ).","{'title': '3 Knowledge Sources', 'number': '3'}"
"If is a verb, we use six features: the nearest word to the left of such that is the parent headword of, the nearest word to the right of such that is the parent headword of , the POS of , the POS of , the POS of , and the voice of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"If is an adjective, we use two features: its parent headword and the POS of .","{'title': '3 Knowledge Sources', 'number': '3'}"
"We also investigated the effect of feature selection on syntactic-relation features that are words (i.e., POS, voice, and relative position are excluded).","{'title': '3 Knowledge Sources', 'number': '3'}"
Some examples are shown in Table 1.,"{'title': '3 Knowledge Sources', 'number': '3'}"
"Each POS noun, verb, or adjective is illustrated by one example.","{'title': '3 Knowledge Sources', 'number': '3'}"
"For each example, (a) shows and its POS; (b) shows the sentence where occurs; and (c) shows the feature vector corresponding to syntactic relations.","{'title': '3 Knowledge Sources', 'number': '3'}"
"We evaluated four supervised learning algorithms: Support Vector Machines (SVM), AdaBoost with decision stumps (AdB), Naive Bayes (NB), and decision trees (DT).","{'title': '4 Learning Algorithms', 'number': '4'}"
"All the experimental results reported in this paper are obtained using the implementation of these algorithms in WEKA (Witten and Frank, 2000).","{'title': '4 Learning Algorithms', 'number': '4'}"
All learning parameters use the default values in WEKA unless otherwise stated.,"{'title': '4 Learning Algorithms', 'number': '4'}"
"The SVM (Vapnik, 1995) performs optimization to find a hyperplane with the largest margin that separates training examples into two classes.","{'title': '4 Learning Algorithms', 'number': '4'}"
A test example is classified depending on the side of the hyperplane it lies in.,"{'title': '4 Learning Algorithms', 'number': '4'}"
Input features can be mapped into high dimensional space before performing the optimization and classification.,"{'title': '4 Learning Algorithms', 'number': '4'}"
A kernel function (linear by default) can be used to reduce the computational cost of training and testing in high dimensional space.,"{'title': '4 Learning Algorithms', 'number': '4'}"
"If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training error.","{'title': '4 Learning Algorithms', 'number': '4'}"
"In WEKA’s implementation of SVM, each nominal feature with possible values is converted into binary (0 or 1) features.","{'title': '4 Learning Algorithms', 'number': '4'}"
"If a nominal feature takes the th feature value, then the th binary feature is set to 1 and all the other binary features are set to 0.","{'title': '4 Learning Algorithms', 'number': '4'}"
"We tried higher order polynomial kernels, but they gave poorer results.","{'title': '4 Learning Algorithms', 'number': '4'}"
Our reported results in this paper used the linear kernel.,"{'title': '4 Learning Algorithms', 'number': '4'}"
"AdaBoost (Freund and Schapire, 1996) is a method of training an ensemble of weak learners such that the performance of the whole ensemble is higher than its constituents.","{'title': '4 Learning Algorithms', 'number': '4'}"
"The basic idea of boosting is to give more weights to misclassified training examples, forcing the new classifier to concentrate on these hard-to-classify examples.","{'title': '4 Learning Algorithms', 'number': '4'}"
A test example is classified by a weighted vote of all trained classifiers.,"{'title': '4 Learning Algorithms', 'number': '4'}"
We use the decision stump (decision tree with only the root node) as the weak learner in AdaBoost.,"{'title': '4 Learning Algorithms', 'number': '4'}"
WEKA implements AdaBoost.M1.,"{'title': '4 Learning Algorithms', 'number': '4'}"
We used 100 iterations in AdaBoost as it gives higher accuracy than the default number of iterations in WEKA (10).,"{'title': '4 Learning Algorithms', 'number': '4'}"
"The Naive Bayes classifier (Duda and Hart, 1973) assumes the features are independent given the class.","{'title': '4 Learning Algorithms', 'number': '4'}"
"During classification, it chooses the class with the highest posterior probability.","{'title': '4 Learning Algorithms', 'number': '4'}"
The default setting uses Laplace (“add one”) smoothing.,"{'title': '4 Learning Algorithms', 'number': '4'}"
"The decision tree algorithm (Quinlan, 1993) partitions the training examples using the feature with the highest information gain.","{'title': '4 Learning Algorithms', 'number': '4'}"
It repeats this process recursively for each partition until all examples in each partition belong to one class.,"{'title': '4 Learning Algorithms', 'number': '4'}"
A test example is classified by traversing the learned decision tree.,"{'title': '4 Learning Algorithms', 'number': '4'}"
"WEKA implements Quinlan’s C4.5 decision tree algorithm, with pruning by default.","{'title': '4 Learning Algorithms', 'number': '4'}"
"In the SENSEVAL-2 English lexical sample task, participating systems are required to disambiguate 73 words that have their POS predetermined.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"There are 8,611 training instances and 4,328 test instances tagged with WORDNET senses.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
Our evaluation is based on all the official training and test data of SENSEVAL-2.,"{'title': '5 Evaluation Data Sets', 'number': '5'}"
"For SENSEVAL-1, we used the 36 trainable words for our evaluation.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"There are 13,845 training instances1 for these trainable words, and 7,446 test instances.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"For SENSEVAL-1, 4 trainable words belong to the indeterminate category, i.e., the POS is not provided.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"For these words, we first used a POS tagger (Ratnaparkhi, 1996) to determine the correct POS.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"For a word that may occur in phrasal word form (eg, the verb “turn” and the phrasal form “turn down”), we train a separate classifier for each phrasal word form.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"During testing, if appears in a phrasal word form, the classifier for that phrasal word form is used.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
"Otherwise, the classifier for is used.","{'title': '5 Evaluation Data Sets', 'number': '5'}"
We ran the different learning algorithms using various knowledge sources.,"{'title': '6 Empirical Results', 'number': '6'}"
Table 2 (Table 3) shows each algorithm evaluated and official scores of the top 3 participating systems of SENSEVAL-2 and SENSEVAL-1 the accuracy figures for the different combinations of knowledge sources and learning algorithms for the SENSEVAL-2 (SENSEVAL-1) data set.,"{'title': '6 Empirical Results', 'number': '6'}"
The nine columns correspond to: (i) using only POS of neighboring words (ii) using only single words in the surrounding context with feature selection ( ) (iii) same as (ii) but without feature selection ( ) (iv) using only local collocations with feature selection ( ) (v) same as (iv) but without feature selection ( ) (vi) using only syntactic relations with feature selection on words ( ) (vii) same as (vi) but without feature selection ( ) (viii) combining all four knowledge sources with feature selection (ix) combining all four knowledge sources without feature selection.,"{'title': '6 Empirical Results', 'number': '6'}"
SVM is only capable of handling binary class problems.,"{'title': '6 Empirical Results', 'number': '6'}"
The usual practice to deal with multiclass problems is to build one binary classifier per output class (denoted “1-per-class”).,"{'title': '6 Empirical Results', 'number': '6'}"
"The original AdaBoost, Naive Bayes, and decision tree algoalgorithm is significantly better.","{'title': '6 Empirical Results', 'number': '6'}"
"“ ”) correspond to the p-value , , and respectively.","{'title': '6 Empirical Results', 'number': '6'}"
"“ ” or “ ” means our rithms can already handle multi-class problems, and we denote runs using the original AdB, NB, and DT algorithms as “normal” in Table 2 and Table 3.","{'title': '6 Empirical Results', 'number': '6'}"
"Accuracy for each word task can be measured by recall (r) or precision (p), defined by: no. of test instances correctly labeled no. of test instances in word task no. of test instances correctly labeled no. of test instances output in word task Recall is very close (but not always identical) to precision for the top SENSEVAL participating systems.","{'title': '6 Empirical Results', 'number': '6'}"
"In this paper, our reported results are based on the official fine-grained scoring method.","{'title': '6 Empirical Results', 'number': '6'}"
"To compute an average recall figure over a set of words, we can either adopt micro-averaging (mi) or macro-averaging (ma), defined by: total no. of test instances correctly labeled mi total no. of test instances in all word tasks That is, micro-averaging treats each test instance equally, so that a word task with many test instances will dominate the micro-averaged recall.","{'title': '6 Empirical Results', 'number': '6'}"
"On the other hand, macro-averaging treats each word task equally.","{'title': '6 Empirical Results', 'number': '6'}"
"As shown in Table 2 and Table 3, the best microaveraged recall for SENSEVAL-2 (SENSEVAL-1) is 65.4% (79.2%), obtained by combining all knowledge sources (without feature selection) and using SVM as the learning algorithm.","{'title': '6 Empirical Results', 'number': '6'}"
"In Table 4, we tabulate the best micro-averaged recall for each learning algorithm, broken down according to nouns, verbs, adjectives, indeterminates (for SENSEVAL-1), and all words.","{'title': '6 Empirical Results', 'number': '6'}"
We also tabulate analogous figures for the top three participating systems for both SENSEVALs.,"{'title': '6 Empirical Results', 'number': '6'}"
"The top three systems for SENSEVAL-2 are: JHU (S1) (Yarowsky et al., 2001), SMUls (S2) (Mihalcea and Moldovan, 2001), and KUNLP (S3) (Seo et al., 2001).","{'title': '6 Empirical Results', 'number': '6'}"
"The top three systems for SENSEVAL-1 are: hopkins (s1) (Yarowsky, 2000), ets-pu (s2) (Chodorow et al., 2000), and tilburg (s3) (Veenstra et al., 2000).","{'title': '6 Empirical Results', 'number': '6'}"
"As shown in Table 4, SVM with all four knowledge sources achieves accuracy higher than the best official scores of both SENSEVALs.","{'title': '6 Empirical Results', 'number': '6'}"
We also conducted paired t test to see if one system is significantly better than another.,"{'title': '6 Empirical Results', 'number': '6'}"
"The t statistic of the difference between each pair of recall figures (between each test instance pair for micro-averaging and between each word task pair for macro-averaging) is computed, giving rise to a p value.","{'title': '6 Empirical Results', 'number': '6'}"
A large p value indicates that the two systems are not significantly different from each other.,"{'title': '6 Empirical Results', 'number': '6'}"
The comparison between our learning algorithms and the top three participating systems is given in Table 5.,"{'title': '6 Empirical Results', 'number': '6'}"
"Note that we can only compare macroaveraged recall for SENSEVAL-1 systems, since the sense of each individual test instance output by the SENSEVAL-1 participating systems is not available.","{'title': '6 Empirical Results', 'number': '6'}"
The comparison indicates that our SVM system is better than the best official SENSEVAL-2 and SENSEVAL-1 systems at the level of significance 0.05.,"{'title': '6 Empirical Results', 'number': '6'}"
"Note that we are able to obtain state-of-the-art results using a single learning algorithm (SVM), without resorting to combining multiple learning algorithms.","{'title': '6 Empirical Results', 'number': '6'}"
Several top SENSEVAL-2 participating systems have attempted the combination of classifiers using different learning algorithms.,"{'title': '6 Empirical Results', 'number': '6'}"
"In SENSEVAL-2, JHU used a combination of various learning algorithms (decision lists, cosinebased vector models, and Bayesian models) with various knowledge sources such as surrounding words, local collocations, syntactic relations, and morphological information.","{'title': '6 Empirical Results', 'number': '6'}"
"SMUls used a k-nearest neighbor algorithm with features such as keywords, collocations, POS, and name entities.","{'title': '6 Empirical Results', 'number': '6'}"
"KUNLP used Classification Information Model, an entropy-based learning algorithm, with local, topical, and bigram contexts and their POS.","{'title': '6 Empirical Results', 'number': '6'}"
"In SENSEVAL-1, hopkins used hierarchical decision lists with features similar to those used by JHU in SENSEVAL-2. ets-pu used a Naive Bayes classifier with topical and local words and their POS. tilburg used a k-nearest neighbor algorithm with features similar to those used by (Ng and Lee, 1996). tilburg also used dictionary examples as additional training data.","{'title': '6 Empirical Results', 'number': '6'}"
"Based on our experimental results, there appears to be no single, universally best knowledge source.","{'title': '7 Discussions', 'number': '7'}"
"Instead, knowledge sources and learning algorithms interact and influence each other.","{'title': '7 Discussions', 'number': '7'}"
"For example, local collocations contribute the most for SVM, while parts-of-speech (POS) contribute the most for NB.","{'title': '7 Discussions', 'number': '7'}"
NB even outperforms SVM if only POS is used.,"{'title': '7 Discussions', 'number': '7'}"
"In addition, different learning algorithms benefit differently from feature selection.","{'title': '7 Discussions', 'number': '7'}"
"SVM performs best without feature selection, whereas NB performs best with some feature selection ( ).","{'title': '7 Discussions', 'number': '7'}"
We will investigate the effect of more elaborate feature selection schemes on the performance of different learning algorithms for WSD in future work.,"{'title': '7 Discussions', 'number': '7'}"
"Also, using the combination of four knowledge sources gives better performance than using any single individual knowledge source for most algorithms.","{'title': '7 Discussions', 'number': '7'}"
"On the SENSEVAL-2 test set, SVM achieves 65.4% (all 4 knowledge sources), 64.8% (remove syntactic relations), 61.8% (further remove POS), and 60.5% (only collocations) as knowledge sources are removed one at a time.","{'title': '7 Discussions', 'number': '7'}"
"Before concluding, we note that the SENSEVAL2 participating system UMD-SST (Cabezas et al., 2001) also used SVM, with surrounding words and local collocations as features.","{'title': '7 Discussions', 'number': '7'}"
"However, they reported recall of only 56.8%.","{'title': '7 Discussions', 'number': '7'}"
"In contrast, our implementation of SVM using the two knowledge sources of surrounding words and local collocations achieves recall of 61.8%.","{'title': '7 Discussions', 'number': '7'}"
"Following the description in (Cabezas et al., 2001), our own re-implementation of UMD-SST gives a recall of 58.6%, close to their reported figure of 56.8%.","{'title': '7 Discussions', 'number': '7'}"
The performance drop from 61.8% may be due to the different collocations used in the two systems.,"{'title': '7 Discussions', 'number': '7'}"
