col1,col2
We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers.,Introduction
Our system learns these classifiers for every production in theformal language grammar.,Introduction
Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.,Introduction
Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.,Introduction
Computational systems that learn to transform natural language sentences into formal meaning rep resentations have important practical applicationsin enabling user-friendly natural language com munication with computers.,Experiment/Discussion
"However, most of the research in natural language processing (NLP) has been focused on lower-level tasks like syntactic parsing, word-sense disambiguation, information extraction etc. In this paper, we have considered the important task of doing deep semantic parsing to map sentences into their computer-executable meaning representations.",Experiment/Discussion
"Previous work on learning semantic parsers either employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta tistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).",Experiment/Discussion
"In this paper, we present anovel kernel-based statistical method for learn ing semantic parsers.",Experiment/Discussion
"Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mappingphrases of natural language (NL) sentences to semantic concepts in a meaning representation lan guage (MRL).",Experiment/Discussion
"Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept.",Experiment/Discussion
It is difficultfor rule-based methods or even statistical featurebased methods to capture the full range of NL con texts which map to a semantic concept because they tend to enumerate these contexts.,Experiment/Discussion
"In contrast, kernel methods allow a convenient mechanism to implicitly work with a potentially infinite number of features which can robustly capture these range of contexts even when the data is noisy.Our system, KRISP (Kernel-based Robust Interpretation for Semantic Parsing), takes NL sentences paired with their formal meaning representations as training data.",Experiment/Discussion
The productions of the formal MRL grammar are treated like semantic concepts.,Experiment/Discussion
"For each of these productions, a SupportVector Machine (SVM) (Cristianini and ShaweTaylor, 2000) classifier is trained using string sim ilarity as the kernel (Lodhi et al, 2002).",Experiment/Discussion
Eachclassifier then estimates the probability of the production covering different substrings of the sen tence.,Experiment/Discussion
"This information is used to compositionally build a complete meaning representation (MR) of the sentence.Some of the previous work on semantic parsing has focused on fairly simple domains, primar ily, ATIS (Air Travel Information Service) (Price,1990) whose semantic analysis is equivalent to fill ing a single semantic frame (Miller et al, 1996; Popescu et al, 2004).",Experiment/Discussion
"In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex withricher predicates and nested structures.",Experiment/Discussion
Our experiments demonstrate that KRISP compares favor 913 NL: ?If the ball is in our goal area then our player 1 should intercept it.?,Experiment/Discussion
CLANG: ((bpos (goal-area our)) (do our {1} intercept)) Figure 1: An example of an NL advice and its CLANG MR. ably to other existing systems and is particularly robust to noise.,Experiment/Discussion
We call the process of mapping natural language (NL) utterances into their computer-executablemeaning representations (MRs) as semantic pars ing.,Experiment/Discussion
These MRs are expressed in formal languages which we call meaning representation languages(MRLs).,Experiment/Discussion
"We assume that all MRLs have deter ministic context free grammars, which is true for almost all computer languages.",Experiment/Discussion
This ensures thatevery MR will have a unique parse tree.,Experiment/Discussion
A learn ing system for semantic parsing is given a trainingcorpus of NL sentences paired with their respec tive MRs from which it has to induce a semantic parser which can map novel NL sentences to their correct MRs. Figure 1 shows an example of an NL sentence and its MR from the CLANG domain.,Experiment/Discussion
"CLANG (Chen et al, 2003) is the standard formal coachlanguage in which coaching advice is given to soc cer agents which compete on a simulated soccer field in the RoboCup 1 Coach Competition.",Experiment/Discussion
"In theMR of the example, bpos stands for ?ball posi tion?.",Experiment/Discussion
"The second domain we have considered is the GEOQUERY domain (Zelle and Mooney, 1996) which is a query language for a small database of about 800 U.S. geographical facts.",Experiment/Discussion
Figure 2 shows an NL query and its MR form in a functional querylanguage.,Experiment/Discussion
The parse of the functional query language is also shown along with the involved productions.,Experiment/Discussion
This example is also used later to illus trate how our system does semantic parsing.,Experiment/Discussion
The MR in the functional query language can be readas if processing a list which gets modified by various functions.,Experiment/Discussion
"From the innermost expression go ing outwards it means: the state of Texas, the list containing all the states next to the state of Texas and the list of all the rivers which flow throughthese states.",Experiment/Discussion
This list is finally returned as the an swer.,Experiment/Discussion
1http://www.robocup.org/ NL: ?Which rivers run through the states bordering Texas??,Experiment/Discussion
Functional query language: answer(traverse(next to(stateid(?texas?)))) Parse tree of the MR in functional query language: ANSWER answer RIVER TRAVERSE traverse STATE NEXT TO next to STATE STATEID stateid ?texas?,Experiment/Discussion
Productions: ANSWER ? answer(RIVER) RIVER ? TRAVERSE(STATE) STATE ? NEXT TO(STATE) STATE ? STATEID TRAVERSE ? traverse NEXT TO ? next to STATEID ? stateid(?texas?),Experiment/Discussion
Figure 2: An example of an NL query and its MR in a functional query language with its parse tree.,Experiment/Discussion
KRISP does semantic parsing using the notion of a semantic derivation of an NL sentence.,Experiment/Discussion
"Inthe following subsections, we define the semantic derivation of an NL sentence and its probabil ity.",Experiment/Discussion
The task of semantic parsing then is to find the most probable semantic derivation of an NL sentence.,Experiment/Discussion
"In section 3, we describe how KRISP learns the string classifiers that are used to obtainthe probabilities needed in finding the most prob able semantic derivation.",Experiment/Discussion
2.1 Semantic Derivation.,Experiment/Discussion
"We define a semantic derivation, D, of an NL sen tence, s, as a parse tree of an MR (not necessarily the correct MR) such that each node of the parse tree also contains a substring of the sentence in addition to a production.",Experiment/Discussion
"We denote nodes of the derivation tree by tuples (pi, [i..j]), where pi is its production and [i..j] stands for the substring s[i..j] of s (i.e. the substring from the ith word to the jth word), and we say that the node or its productioncovers the substring s[i..j].",Experiment/Discussion
"The substrings cov ered by the children of a node are not allowed to overlap, and the substring covered by the parentmust be the concatenation of the substrings cov ered by its children.",Experiment/Discussion
Figure 3 shows a semantic derivation of the NL sentence and the MR parsewhich were shown in figure 2.,Experiment/Discussion
The words are num bered according to their position in the sentence.,Experiment/Discussion
"Instead of non-terminals, productions are shown in the nodes to emphasize the role of productions in semantic derivations.",Experiment/Discussion
"Sometimes, the children of an MR parse tree 914 (ANSWER?",Experiment/Discussion
"answer(RIVER), [1..9]) (RIVER?",Experiment/Discussion
"TRAVERSE(STATE), [1..9]) (TRAVERSE?traverse, [1..4]) which1 rivers2 run3 through4 (STATE?",Experiment/Discussion
"NEXT TO(STATE), [5..9]) (NEXT TO?",Experiment/Discussion
"next to, [5..7]) the5 states6 bordering7 (STATE?",Experiment/Discussion
"STATEID, [8..9]) (STATEID?",Experiment/Discussion
"stateid ?texas?, [8..9]) Texas8 ?9 Figure 3: Semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas??",Experiment/Discussion
which gives MR as answer(traverse(next to(stateid(texas)))).node may not be in the same order as are the substrings of the sentence they should cover in a se mantic derivation.,Experiment/Discussion
"For example, if the sentence was ?Through the states that border Texas whichrivers run??, which has the same MR as the sen tence in figure 3, then the order of the children of the node ?RIVER ? TRAVERSE(STATE)?",Experiment/Discussion
wouldneed to be reversed.,Experiment/Discussion
"To accommodate this, a se mantic derivation tree is allowed to contain MR parse tree nodes in which the children have been permuted.",Experiment/Discussion
"Note that given a semantic derivation of an NL sentence, it is trivial to obtain the corresponding MR simply as the string generated by the parse.",Experiment/Discussion
"Since children nodes may be permuted, this step also needs to permute them back to the way they should be according to the MRL productions.",Experiment/Discussion
"If a semantic derivation gives the correct MR of the NL sentence, then we call it a correct semantic derivation otherwise it is an incorrect semantic derivation.",Experiment/Discussion
2.2 Most Probable Semantic Derivation.,Experiment/Discussion
"Let Ppi(u) denote the probability that a production pi of the MRL grammar covers the NL substring u. In other words, the NL substring u expressesthe semantic concept of a production pi with probability Ppi(u).",Experiment/Discussion
In the next subsection we will de scribe how KRISP obtains these probabilities using string-kernel based SVM classifiers.,Experiment/Discussion
"Assuming these probabilities are independent of each other,the probability of a semantic derivationD of a sen tence s is then: P (D) = ?",Experiment/Discussion
"(pi,[i..j])?D Ppi(s[i..j]) The task of the semantic parser is to find the most probable derivation of a sentence s. Thistask can be recursively performed using the no tion of a partial derivation En,s[i..j], which stands for a subtree of a semantic derivation tree with n as the left-hand-side (LHS) non-terminal of the root production and which covers s from index i to j. For example, the subtree rooted at the node ?(STATE ? NEXT TO(STATE),[5..9])?",Experiment/Discussion
"inthe derivation shown in figure 3 is a partial deriva tion which would be denoted as ESTATE,s[5..9].",Experiment/Discussion
"Note that the derivation D of sentence s is thensimply Estart,s[1..|s|], where start is the start sym bol of the MRL?s context free grammar, G.Our procedure to find the most probable partial derivation E?n,s[i..j] considers all possible subtrees whose root production has n as its LHS non terminal and which cover s from index i to j.Mathematically, the most probable partial deriva tion E?n,s[i..j] is recursively defined as: E?n,s[i..j] = makeTree( argmax pi = n ? n1..nt ? G, (p1, .., pt) ? partition(s[i..j], t) (Ppi(s[i..j]) ? k=1..t P (E?nk,pk )))where partition(s[i..j], t) is a function which returns the set of all partitions of s[i..j] with t elements including their permutations.",Experiment/Discussion
A parti tion of a substring s[i..j] with t elements is a t?tuple containing t non-overlapping substrings of s[i..j] which give s[i..j] when concatenated.,Experiment/Discussion
"For example, (?the states bordering?, ?Texas ??)is a partition of the substring ?the states bordering Texas ??",Experiment/Discussion
with 2 elements.,Experiment/Discussion
"The proce duremakeTree(pi, (p1, .., pt)) constructs a partial derivation tree by making pi as its root production and making the most probable partial derivationtrees found through the recursion as children sub trees which cover the substrings according to the partition (p1, .., pt).",Experiment/Discussion
"The most probable partial derivation E?n,s[i..j] is found using the above equation by trying all productions pi = n ? n1..nt in G which have 915 n as the LHS, and all partitions with t elementsof the substring s[i..j] (n1 to nt are right-hand side (RHS) non-terminals of pi, terminals do not play any role in this process and are not shownfor simplicity).",Experiment/Discussion
"The most probable partial derivation E?STATE,s[5..9] for the sentence shown in fig ure 3 will be found by trying all the productionsin the grammar with STATE as the LHS, for ex ample, one of them being ?STATE ? NEXT TOSTATE?.",Experiment/Discussion
"Then for this sample production, all partitions, (p1, p2), of the substring s[5..9] with two el ements will be considered, and the most probable derivations E?NEXT TO,p1 and E ? STATE,p2 will be found recursively.",Experiment/Discussion
"The recursion reaches base cases when the productions which have n on the LHS do not have any non-terminal on the RHS or when the substring s[i..j] becomes smaller than the length t. According to the equation, a production pi ? G and a partition (p1, .., pt) ? partition(s[i..j], t) will be selected in constructing the most probable partial derivation.",Experiment/Discussion
"These will be the ones whichmaximize the product of the probability of pi covering the substring s[i..j] with the product of probabilities of all the recursively found most proba ble partial derivations consistent with the partition (p1, .., pt).",Experiment/Discussion
"A naive implementation of the above recursionis computationally expensive, but by suitably extending the well known Earley?s context-free parsing algorithm (Earley, 1970), it can be implemented efficiently.",Experiment/Discussion
"The above task has some re semblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways.First, the probability of a production is not independent of the sentence but depends on which sub string of the sentence it covers, and second, the leaves of the tree are not individual terminals of the grammar but are substrings of words of the NLsentence.",Experiment/Discussion
"The extensions needed for Earley?s al gorithm are straightforward and are described in detail in (Kate, 2005) but due to space limitationwe do not describe them here.",Experiment/Discussion
Our extended Ear ley?s algorithm does a beam search and attempts to find the ?,Experiment/Discussion
(a parameter) most probable semanticderivations of an NL sentence s using the probabil ities Ppi(s[i..j]).,Experiment/Discussion
"To make this search faster, it uses a threshold, ?, to prune low probability derivation trees.",Experiment/Discussion
"In this section, we describe how KRISP learns the classifiers which give the probabilities Ppi(u) needed for semantic parsing as described in the previous section.",Experiment/Discussion
"Given the training corpus of NL sentences paired with their MRs {(si,mi)|i = 1..N}, KRISP first parses the MRs using the MRL grammar, G. We represent the parse of MR, mi, by parse(mi).Figure 4 shows pseudo-code for KRISP?s training algorithm.",Experiment/Discussion
"KRISP learns a semantic parser it eratively, each iteration improving upon the parserlearned in the previous iteration.",Experiment/Discussion
"In each itera tion, for every production pi of G, KRISP collects positive and negative example sets.",Experiment/Discussion
"In the first iteration, the set P(pi) of positive examples for production pi contains all sentences, si, such thatparse(mi) uses the production pi.",Experiment/Discussion
"The set of nega tive examples,N (pi), for production pi includes all of the remaining training sentences.",Experiment/Discussion
"Using thesepositive and negative examples, an SVM classi fier 2, Cpi, is trained for each production pi usinga normalized string subsequence kernel.",Experiment/Discussion
"Following the framework of Lodhi et al (2002), we de fine a kernel between two strings as the number of common subsequences they share.",Experiment/Discussion
"One difference, however, is that their strings are over characters while our strings are over words.",Experiment/Discussion
"The more the two strings share, the greater the similarity score will be.",Experiment/Discussion
"Normally, SVM classifiers only predict the classof the test example but one can obtain class probability estimates by mapping the distance of the ex ample from the SVM?s separating hyperplane to the range [0,1] using a learned sigmoid function (Platt, 1999).",Experiment/Discussion
The classifier Cpi then gives us the probabilities Ppi(u).,Experiment/Discussion
We represent the set of these classifiers by C = {Cpi|pi ? G}.,Experiment/Discussion
"Next, using these classifiers, the extendedEarley?s algorithm, which we call EX TENDED EARLEY in the pseudo-code, is invoked to obtain the ? best semantic derivations for each of the training sentences.",Experiment/Discussion
The procedure getMR returns the MR for a semantic derivation.,Experiment/Discussion
"At this point, for many training sentences, the resulting most-probable semantic derivation may not give the correct MR. Hence, next, the system collects more refined positive and negative examples to improve the result in the next iteration.",Experiment/Discussion
"It 2We use the LIBSVM package available at: http:// www.csie.ntu.edu.tw/?cjlin/libsvm/ 916 function TRAIN KRISP(training corpus {(si,mi)|i = 1..N}, MRL grammar G) for each pi ?G // collect positive and negative examples for the first iteration for i = 1 to N do if pi is used in parse(mi) then include si in P(pi) else include si in N (pi) for iteration = 1 to MAX ITR do for each pi ?G do Cpi = trainSVM(P(pi),N (pi)) // SVM training for each pi ?G P(pi) = ? // empty the positive examples, accumulate negatives though for i = 1 to N do D =EXTENDED EARLEY(si, G, P ) // obtain best derivations if 6 ? d ? D such that parse(mi) = getMR(d) then D = D ? EXTENDED EARLEY CORRECT(si, G, P,mi) // if no correct derivation then force to find one d?",Experiment/Discussion
"= argmaxd?D&getMR(d)=parse(mi) P (d) COLLECT POSITIVES(d?) // collect positives from maximum probability correct derivation for each d ? D do if P (d) > P (d?) and getMR(d) 6= parse(mi) then // collect negatives from incorrect derivation with larger probability than the correct one COLLECT NEGATIVES(d, d?)",Experiment/Discussion
"return classifiers C = {Cpi|pi ? G} Figure 4: KRISP?s training algorithm is also possible that for some sentences, none of the obtained ? derivations give the correct MR. But as will be described shortly, the most probable derivation which gives the correct MR is needed to collect positive and negative examples for the next iteration.",Experiment/Discussion
"Hence in these cases, aversion of the extended Earley?s algorithm, EX TENDED EARLEY CORRECT, is invoked which also takes the correct MR as an argument and returns the best ? derivations it finds, all of which give the correct MR. This is easily done by making sure all subtrees derived in the process are present in the parse of the correct MR. From these derivations, positive and negativeexamples are collected for the next iteration.",Experiment/Discussion
"Positive examples are collected from the most probable derivation which gives the correct MR, fig ure 3 showed an example of a derivation which gives the correct MR. At each node in such aderivation, the substring covered is taken as a positive example for its production.",Experiment/Discussion
Negative exam ples are collected from those derivations whoseprobability is higher than the most probable correct derivation but which do not give the correct MR. Figure 5 shows an example of an in correct derivation.,Experiment/Discussion
Here the function ?next to?is missing from the MR it produces.,Experiment/Discussion
The following procedure is used to collect negative ex amples from incorrect derivations.,Experiment/Discussion
The incorrectderivation and the most probable correct deriva tion are traversed simultaneously starting from the root using breadth-first traversal.,Experiment/Discussion
"The first nodes where their productions differ is detected, and all of the words covered by the these nodes (in bothderivations) are marked.",Experiment/Discussion
"In the correct and incorrect derivations shown in figures 3 and 5 respec tively, the first nodes where the productions differ are ?(STATE ? NEXT TO(STATE), [5..9])?",Experiment/Discussion
"and ?(STATE ? STATEID, [8..9])?.",Experiment/Discussion
"Hence, the union of words covered by them: 5 to 9 (?the states bordering Texas??), will be marked.",Experiment/Discussion
"For each of these marked words, the procedure considers all of the productions which cover it in the two derivations.",Experiment/Discussion
The nodes of the productions which cover a marked word in the incorrect derivationbut not in the correct derivation are used to col lect negative examples.,Experiment/Discussion
"In the example, the node ?(TRAVERSE?traverse,[1..7])?",Experiment/Discussion
will be used to collect a negative example (i.e. the words 1to 7 ??which rivers run through the states bordering?,Experiment/Discussion
"will be a negative example for the production TRAVERSE?traverse) because the pro duction covers the marked words ?the?, ?states?",Experiment/Discussion
and ?bordering?,Experiment/Discussion
in the incorrect derivation butnot in the correct derivation.,Experiment/Discussion
"With this as a neg ative example, hopefully in the next iteration, theprobability of this derivation will decrease significantly and drop below the probability of the cor rect derivation.",Experiment/Discussion
"In each iteration, the positive examples from the previous iteration are first removed so thatnew positive examples which lead to better cor rect derivations can take their place.",Experiment/Discussion
"However,negative examples are accumulated across iterations for better accuracy because negative examples from each iteration only lead to incor rect derivations and it is always good to includethem.",Experiment/Discussion
"To further increase the number of negative examples, every positive example for a pro duction is also included as a negative example for all the other productions having the same LHS.",Experiment/Discussion
"After a specified number of MAX ITR iterations, 917 (ANSWER?",Experiment/Discussion
"answer(RIVER), [1..9]) (RIVER?",Experiment/Discussion
"TRAVERSE(STATE), [1..9]) (TRAVERSE?traverse, [1..7]) Which1 rivers2 run3 through4 the5 states6 bordering7 (STATE?",Experiment/Discussion
"STATEID, [8..9]) (STATEID?",Experiment/Discussion
"stateid texas, [8..9]) Texas8 ?9 Figure 5: An incorrect semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas??",Experiment/Discussion
which gives the incorrect MR answer(traverse(stateid(texas))).,Experiment/Discussion
the trained classifiers from the last iteration are returned.,Experiment/Discussion
"Testing involves using these classifiers to generate the most probable derivation of a test sentence as described in the subsection 2.2, and returning its MR. The MRL grammar may contain productions corresponding to constants of the domain, for e.g., state names like ?STATEID ? ?texas??, or river names like ?RIVERID ? ?colorado??",Experiment/Discussion
"etc. Oursystem allows the user to specify such productions as constant productions giving the NL sub strings, called constant substrings, which directly relate to them.",Experiment/Discussion
"For example, the user may give?Texas?",Experiment/Discussion
as the constant substring for the produc tion ?STATEID ? ?texas?.,Experiment/Discussion
Then KRISP does not learn classifiers for these constant productions and instead decides if they cover a substring of the sentence or not by matching it with the provided constant substrings.,Experiment/Discussion
4.1 Methodology.,Results/Conclusion
KRISP was evaluated on CLANG and GEOQUERY domains as described in section 2.,Results/Conclusion
The CLANG corpus was built by randomly selecting 300 pieces of coaching advice from the log files of the 2003RoboCup Coach Competition.,Results/Conclusion
"These formal ad vice instructions were manually translated intoEnglish (Kate et al, 2005).",Results/Conclusion
"The GEOQUERY cor pus contains 880 English queries collected from undergraduates and from real users of a web-based interface (Tang and Mooney, 2001).",Results/Conclusion
"These were manually translated into their MRs. The average length of an NL sentence in the CLANG corpus is 22.52 words while in the GEOQUERY corpus it is 7.48 words, which indicates that CLANG is the harder corpus.",Results/Conclusion
The average length of the MRs is 13.42 tokens in the CLANG corpus while it is 6.46 tokens in the GEOQUERY corpus.,Results/Conclusion
KRISP was evaluated using standard 10-fold cross validation.,Results/Conclusion
"For every test sentence, only thebest MR corresponding to the most probable se mantic derivation is considered for evaluation, and its probability is taken as the system?s confidence in that MR. Since KRISP uses a threshold, ?, toprune low probability derivation trees, it sometimes may fail to return any MR for a test sen tence.",Results/Conclusion
"We computed the number of test sentences for which KRISP produced MRs, and the number of these MRs that were correct.",Results/Conclusion
"For CLANG, an output MR is considered correct if and only if it exactly matches the correct MR. For GEOQUERY, an output MR is considered correct if and only if the resulting query retrieves the same answer as the correct MR when submitted to the database.",Results/Conclusion
Performance was measured in terms of precision(the percentage of generated MRs that were cor rect) and recall (the percentage of all sentences for which correct MRs were obtained).,Results/Conclusion
"In our experiments, the threshold ? was fixedto 0.05 and the beam size ? was 20.",Results/Conclusion
"These pa rameters were found through pilot experiments.The maximum number of iterations (MAX ITR) re quired was only 3, beyond this we found that the system only overfits the training corpus.",Results/Conclusion
"We compared our system?s performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser.",Results/Conclusion
"We also compared with the CCG-based semantic parser by Zettlemoyer et al (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours.",Results/Conclusion
"Like KRISP, WASP and SCISSOR also give confidences to the MRs they generate which are used to plot precision-recallcurves by measuring precisions and recalls at vari 918 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Pr ec is io n Recall KRISP WASP SCISSOR SILT-tree SILT-string Figure 6: Results on the CLANG corpus.",Results/Conclusion
50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Pr ec is io n Recall KRISP WASP SCISSOR SILT-tree SILT-string CHILL Zettlemoyer et al (2005) Figure 7: Results on the GEOQUERY corpus.ous confidence levels.,Results/Conclusion
The results of the other sys tems are shown as points on the precision-recall graph.,Results/Conclusion
4.2 Results.,Results/Conclusion
Figure 6 shows the results on the CLANG cor pus.,Results/Conclusion
KRISP performs better than either versionof SILT and performs comparable to WASP.,Results/Conclusion
"Although SCISSOR gives less precision at lower re call values, it gives much higher maximum recall.However, we note that SCISSOR requires more supervision for the training corpus in the form of se mantically annotated syntactic parse trees for thetraining sentences.",Results/Conclusion
CHILL could not be run beyond 160 training examples because its Prolog im plementation runs out of memory.,Results/Conclusion
For 160 trainingexamples it gave 49.2% precision with 12.67% re call.,Results/Conclusion
Figure 7 shows the results on the GEOQUERY corpus.,Results/Conclusion
KRISP achieves higher precisions than WASP on this corpus.,Results/Conclusion
"Overall, the results show that KRISP performs better than deterministic rule-based semantic parsers like CHILL and SILTand performs comparable to other statistical se mantic parsers like WASP and SCISSOR.",Results/Conclusion
4.3 Experiments with Other Natural.,Results/Conclusion
Languages We have translations of a subset of the GEOQUERY corpus with 250 examples (GEO250 corpus) in 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100 Pr ec is io n Recall English Japanese Spanish Turkish Figure 8: Results of KRISP on the GEO250 corpus for different natural languages.,Results/Conclusion
"three other natural languages: Spanish, Turkish and Japanese.",Results/Conclusion
"Since KRISP?s learning algorithmdoes not use any natural language specific knowledge, it is directly applicable to other natural languages.",Results/Conclusion
Figure 8 shows that KRISP performs com petently on other languages as well.,Results/Conclusion
4.4 Experiments with Noisy NL Sentences.,Results/Conclusion
Any real world application in which semantic parsers would be used to interpret natural language of a user is likely to face noise in the input.,Results/Conclusion
"If the user is interacting through spontaneous speech and the input to the semantic parser is coming form the output of a speech recognition system then there are many ways in which noise could creep in the NL sentences: interjections (like um?s and ah?s), environment noise (like door slams, phone rings etc.), out-of-domain words, grammatically ill-formed utterances etc.",Results/Conclusion
"(Zue and Glass, 2000).As opposed to the other systems, KRISP?s stringkernel-based semantic parsing does not use hard matching rules and should be thus more flexible and robust to noise.",Results/Conclusion
We tested this hypothesis by running experiments on data which was artificiallycorrupted with simulated speech recognition er rors.,Results/Conclusion
"The interjections, environment noise etc. are likely to be recognized as real words by a speech recognizer.",Results/Conclusion
"To simulate this, after every word ina sentence, with some probability Padd, an extra word is added which is chosen with proba bility proportional to its word frequency found inthe British National Corpus (BNC), a good representative sample of English.",Results/Conclusion
"A speech recog nizer may sometimes completely fail to detect words, so with a probability of Pdrop a word is sometimes dropped.",Results/Conclusion
A speech recognizer could also introduce noise by confusing a word with ahigh frequency phonetically close word.,Results/Conclusion
We sim 919 0 20 40 60 80 100 0 1 2 3 4 5F m ea su re Noise level KRISP WASP SCISSORFigure 9: Results on the CLANG corpus with in creasing amounts of noise in the test sentences.,Results/Conclusion
"ulate this type of noise by substituting a word in the corpus by another word, w, with probability ped(w)?P (w), where p is a parameter, ed(w) isw?s edit distance (Levenshtein, 1966) from the original word and P (w) is w?s probability proportional toits word frequency.",Results/Conclusion
"The edit distance which calcu lates closeness between words is character-based rather than based on phonetics, but this should not make a significant difference in the experimental results.Figure 9 shows the results on the CLANG cor pus with increasing amounts of noise, from level 0 to level 4.",Results/Conclusion
The noise level 0 corresponds to no noise.,Results/Conclusion
"The noise parameters, Padd and Pdrop, were varied uniformly from being 0 at level 0 and 0.1 at level 4, and the parameter p was varied uniformly from being 0 at level 0 and 0.01 at level 4.",Results/Conclusion
We are showing the best F-measure (harmonic meanof precision and recall) for each system at different noise levels.,Results/Conclusion
"As can be seen, KRISP?s perfor mance degrades gracefully in the presence of noise while other systems?",Results/Conclusion
"performance degrade muchfaster, thus verifying our hypothesis.",Results/Conclusion
"In this exper iment, only the test sentences were corrupted, we get qualitatively similar results when both training and test sentences are corrupted.",Results/Conclusion
The results are also similar on the GEOQUERY corpus.,Results/Conclusion
We presented a new kernel-based approach to learn semantic parsers.,Results/Conclusion
SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.,Results/Conclusion
These classifiers are then used to compositionally build complete meaning representa tions of natural language sentences.,Results/Conclusion
We evaluatedour system on two real-world corpora.,Results/Conclusion
The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.,Results/Conclusion
AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.,Results/Conclusion
