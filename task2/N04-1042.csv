col1,col2
"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance.",{}
This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers.,{}
"The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration.",{}
"This paper makes an empirical exploration of several factors, including variations on Gaussian, expoand priors for improved regularization, and several classes of features and Markov order.",{}
"On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results.",{}
Accuracy compares even more favorably against HMMs.,{}
"Research paper search engines, such as CiteSeer (Lawrence et al., 1999) and Cora (McCallum et al., 2000), give researchers tremendous power and convenience in their research.","{'title': '1 Introduction', 'number': '1'}"
They are also becoming increasingly used for recruiting and hiring decisions.,"{'title': '1 Introduction', 'number': '1'}"
Thus the information quality of such systems is of significant importance.,"{'title': '1 Introduction', 'number': '1'}"
"This quality critically depends on an information extraction component that extracts meta-data, such as title, author, institution, etc, from paper headers and references, because these meta-data are further used in many component applications such as field-based search, author analysis, and citation analysis.","{'title': '1 Introduction', 'number': '1'}"
Previous work in information extraction from research papers has been based on two major machine learning techniques.,"{'title': '1 Introduction', 'number': '1'}"
"The first is hidden Markov models (HMM) (Seymore et al., 1999; Takasu, 2003).","{'title': '1 Introduction', 'number': '1'}"
An HMM learns a generative model over input sequence and labeled sequence pairs.,"{'title': '1 Introduction', 'number': '1'}"
"While enjoying wide historical success, standard HMM models have difficulty modeling multiple non-independent features of the observation sequence.","{'title': '1 Introduction', 'number': '1'}"
"The second technique is based on discriminatively-trained SVM classifiers (Han et al., 2003).","{'title': '1 Introduction', 'number': '1'}"
These SVM classifiers can handle many nonindependent features.,"{'title': '1 Introduction', 'number': '1'}"
"However, for this sequence labeling problem, Han et al. (2003) work in a two stages process: first classifying each line independently to assign it label, then adjusting these labels based on an additional classifier that examines larger windows of labels.","{'title': '1 Introduction', 'number': '1'}"
Solving the information extraction problem in two steps looses the tight interaction between state transitions and observations.,"{'title': '1 Introduction', 'number': '1'}"
"In this paper, we present results on this research paper meta-data extraction task using a Conditional Random Field (Lafferty et al., 2001), and explore several practical issues in applying CRFs to information extraction in general.","{'title': '1 Introduction', 'number': '1'}"
"The CRF approach draws together the advantages of both finite state HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences.","{'title': '1 Introduction', 'number': '1'}"
"CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003).","{'title': '1 Introduction', 'number': '1'}"
"The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase.","{'title': '1 Introduction', 'number': '1'}"
"Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000), exponential (Goodman, 2003), and hyperbolic-Ll (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.","{'title': '1 Introduction', 'number': '1'}"
We describe a large collection of experimental results on two traditional benchmark data sets.,"{'title': '1 Introduction', 'number': '1'}"
Dramatic improvements are obtained in comparison with previous SVM and HMM based results.,"{'title': '1 Introduction', 'number': '1'}"
"Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al., 2001).","{'title': '2 Conditional Random Fields', 'number': '2'}"
"A common special-case graph structure is a linear chain, which corresponds to a finite state machine, and is suitable for sequence labeling.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"A linear-chain CRF with parameters A = {A,...} defines a conditional probability for a state (or label1) sequence y = y1...yT given an input sequence x = x1...xT to be where ZX is the normalization constant that makes the probability of all state sequences sum to one, fk(yt−1, yt, x, t) is a feature function which is often binary-valued, but can be real-valued, and Ak is a learned weight associated with feature fk.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"The feature functions can measure any aspect of a state transition, yt−1 → yt, and the observation sequence, x, centered at the current time step, t. For example, one feature function might have value 1 when yt−1 is the state TITLE, yt is the state AUTHOR, and xt is a word appearing in a lexicon of people’s first names.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Large positive values for Ak indicate a preference for such an event, while large negative values make the event unlikely.","{'title': '2 Conditional Random Fields', 'number': '2'}"
Given such a model as defined in Equ.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"(1), the most probable labeling sequence for an input x, can be efficiently calculated by dynamic programming using the Viterbi algorithm.","{'title': '2 Conditional Random Fields', 'number': '2'}"
Calculating the marginal probability of states or transitions at each position in the sequence by a dynamic-programming-based inference procedure very similar to forward-backward for hidden Markov models.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"The parameters may be estimated by maximum likelihood—maximizing the conditional probability of a set of label sequences, each given their corresponding input sequences.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"The log-likelihood of training set Maximizing (2) corresponds to satisfying the following equality, wherein the the empirical count of each feature matches its expected count according to the model PΛ(y|x).","{'title': '2 Conditional Random Fields', 'number': '2'}"
"CRFs share many of the advantageous properties of standard maximum entropy models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995), can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003).","{'title': '2 Conditional Random Fields', 'number': '2'}"
We use BFGS for optimization.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"In our experiments, we shall focus instead on two other aspects of CRF deployment, namely regularization and selection of different model structure and feature types.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"To avoid over-fitting, log-likelihood is often penalized by some prior distribution over the parameters.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Figure 1 shows an empirical distribution of parameters, A, learned from an unpenalized likelihood, including only features with non-zero count in the training set.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Three prior distributions that have shape similar to this empirical distribution are the Gaussian prior, exponential prior, and hyperbolic-L1 prior, each shown in Figure 2.","{'title': '2 Conditional Random Fields', 'number': '2'}"
In this paper we provide an empirical study of these three priors.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"With a Gaussian prior, log-likelihood (2) is penalized as follows: This adjusted constraint (as well as the adjustments imposed by the other two priors) is intuitively understandable: rather than matching exact empirical feature frequencies, the model is tuned to match discounted feature frequencies.","{'title': '2 Conditional Random Fields', 'number': '2'}"
Chen and Rosenfeld (2000) discuss this in the context of other discounting procedures common in language modeling.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
We call the term subtracted from the empirical counts (in this case λk/σ2) a discounted value.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
The variance can be feature dependent.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"However for simplicity, constant variance is often used for all features.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"In this paper, however, we experiment with several alternate versions of Gaussian prior in which the variance is feature dependent.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Although Gaussian (and other) priors are gradually overcome by increasing amounts of training data, perhaps not at the right rate.","{'title': '2 Conditional Random Fields', 'number': '2'}"
The three methods below all provide ways to alter this rate by changing the variance of the Gaussian prior dependent on feature counts. ckxσ2 where σ is a constant over all features.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"In this way, we increase the smoothing on the low frequency features more so than the high frequency features. λk fck/ xσ2 where ck is the count of features, N is the bin size, and ra] is the ceiling function.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Alternatively, the variance in each bin may be set independently by cross-validation.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Whereas the Gaussian prior penalizes according to the square of the weights (an L2 penalizer), the intention here is to create a smoothly differentiable analogue to penalizing the absolute-value of the weights (an L1 penalizer).","{'title': '2 Conditional Random Fields', 'number': '2'}"
"L1 penalizers often result in more “sparse solutions,” in which many features have weight nearly at zero, and thus provide a kind of soft feature selection that improves generalization.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Goodman (2003) proposes an exponential prior, specifically a Laplacian prior, as an alternative to Gaussian prior.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Under this prior, This corresponds to the absolute smoothing method in language modeling.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"We set the αk = α; i.e. all features share the same constant whose value can be determined using absolute discounting α = n1 n1+2n2 , where n1 and n2 are the number of features occurring once and twice (Ney et al., 1995).","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Another L1 penalizer is the hyperbolic-L1 prior, described in (Pinto et al., 2003).","{'title': '2 Conditional Random Fields', 'number': '2'}"
The hyperbolic distribution has log-linear tails.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"Consequently the class of hyperbolic distribution is an important alternative to the class of normal distributions and has been used for analyzing data from various scientific areas such as finance, though less frequently used in natural language processing.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Under a hyperbolic prior, The hyperbolic prior was also tested with CRFs in McCallum and Li (2003).","{'title': '2 Conditional Random Fields', 'number': '2'}"
Wise choice of features is always vital the performance of any machine learning solution.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"Feature induction (McCallum, 2003) has been shown to provide significant improvements in CRFs performance.","{'title': '2 Conditional Random Fields', 'number': '2'}"
In some experiments described below we use feature induction.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
The focus in this section is on three other aspects of the feature space.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"In CRFs, state transitions are also represented as features.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"The feature function fk(yt−1, yt, x, t) in Equ.","{'title': '2 Conditional Random Fields', 'number': '2'}"
(1) is a general function over states and observations.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
Different state transition features can be defined to form different Markov-order structures.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
We define four different state transitions features corresponding to different Markov order for different classes of features.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"Higher order features model dependencies better, but also create more data sparse problem and require more memory in training.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Before the use of prior distributions over parameters was common in maximum entropy classifiers, standard practice was to eliminate all features with zero count in the training data (the so-called unsupported features).","{'title': '2 Conditional Random Fields', 'number': '2'}"
"However, unsupported, zero-count features can be extremely useful for pushing Viterbi inference away from certain paths by assigning such features negative weight.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"The use of a prior allows the incorporation of unsupported features, however, doing so often greatly increases the number parameters and thus the memory requirements.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"Below we experiment with models containing and not containing unsupported features—both with and without regularization by priors, and we argue that non-supported features are useful.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"We present here incremental support, a method of introducing some useful unsupported features without exploding the number of parameters with all unsupported features.","{'title': '2 Conditional Random Fields', 'number': '2'}"
The model is trained for several iterations with supported features only.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
Then inference determines the label sequences assigned high probability by the model.,"{'title': '2 Conditional Random Fields', 'number': '2'}"
"Incorrect transitions assigned high probability by the model are used to selectively add to the model those unsupported features that occur on those transitions, which may help improve performance by being assigned negative weight in future training.","{'title': '2 Conditional Random Fields', 'number': '2'}"
"If desired, several iterations of this procedure may be performed.","{'title': '2 Conditional Random Fields', 'number': '2'}"
One of the advantages of CRFs and maximum entropy models in general is that they easily afford the use of arbitrary features of the input.,"{'title': '2.2.3 Local features, layout features and lexicon features', 'number': '3'}"
"One can encode local spelling features, layout features such as positions of line breaks, as well as external lexicon features, all in one framework.","{'title': '2.2.3 Local features, layout features and lexicon features', 'number': '3'}"
"We study all these features in our research paper extraction problem, evaluate their individual contributions, and give some guidelines for selecting good features.","{'title': '2.2.3 Local features, layout features and lexicon features', 'number': '3'}"
Here we also briefly describe a HMM model we used in our experiments.,"{'title': '3 Empirical Study', 'number': '4'}"
"We relax the independence assumption made in standard HMM and allow Markov dependencies among observations, e.g., P(otlst, ot−1).","{'title': '3 Empirical Study', 'number': '4'}"
We can vary Markov orders in state transition and observation transitions.,"{'title': '3 Empirical Study', 'number': '4'}"
"In our experiments, a model with second order state transitions and first order observation transitions performs the best.","{'title': '3 Empirical Study', 'number': '4'}"
"The state transition probabilities and emission probabilities are estimated using maximum likelihood estimation with absolute smoothing, which was found to be effective in previous experiments, including Seymore et al. (1999).","{'title': '3 Empirical Study', 'number': '4'}"
We experiment with two datasets of research paper content.,"{'title': '3 Empirical Study', 'number': '4'}"
One consists of the headers of research papers.,"{'title': '3 Empirical Study', 'number': '4'}"
The other consists of pre-segmented citations from the reference sections of research papers.,"{'title': '3 Empirical Study', 'number': '4'}"
"These data sets have been used as standard benchmarks in several previous studies (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003).","{'title': '3 Empirical Study', 'number': '4'}"
"The header of a research paper is defined to be all of the words from the beginning of the paper up to either the first section of the paper, usually the introduction, or to the end of the first page, whichever occurs first.","{'title': '3 Empirical Study', 'number': '4'}"
"It contains 15 fields to be extracted: title, author, affiliation, address, note, email, date, abstract, introduction, phone, keywords, web, degree, publication number, and page (Seymore et al., 1999).","{'title': '3 Empirical Study', 'number': '4'}"
The header dataset contains 935 headers.,"{'title': '3 Empirical Study', 'number': '4'}"
"Following previous research (Seymore et al., 1999; McCallum et al., 2000; Han et al., 2003), for each trial we randomly select 500 for training and the remaining 435 for testing.","{'title': '3 Empirical Study', 'number': '4'}"
"We refer this dataset as H. The reference dataset was created by the Cora project (McCallum et al., 2000).","{'title': '3 Empirical Study', 'number': '4'}"
"It contains 500 references, we use 350 for training and the rest 150 for testing.","{'title': '3 Empirical Study', 'number': '4'}"
"References contain 13 fields: author, title, editor, booktitle, date, journal, volume, tech, institution, pages, location, publisher, note.","{'title': '3 Empirical Study', 'number': '4'}"
"We refer this dataset as R. To give a comprehensive evaluation, we measure performance using several different metrics.","{'title': '3 Empirical Study', 'number': '4'}"
"In addition to the previously-used word accuracy measure (which overemphasizes accuracy of the abstract field), we use perfield F1 measure (both for individual fields and averaged over all fields—called a “macro average” in the information retrieval literature), and whole instance accuracy for measuring overall performance in a way that is sensitive to even a single error in any part of header or citation.","{'title': '3 Empirical Study', 'number': '4'}"
"Thus, we consider both word accuracy and average F-measure in evaluation.","{'title': '3 Empirical Study', 'number': '4'}"
3.,"{'title': '3 Empirical Study', 'number': '4'}"
Whole instance accuracy: An instance here is defined to be a single header or reference.,"{'title': '3 Empirical Study', 'number': '4'}"
Whole instance accuracy is the percentage of instances in which every word is correctly labeled.,"{'title': '3 Empirical Study', 'number': '4'}"
"We first report the overall results by comparing CRFs with HMMs, and with the previously best benchmark results obtained by SVMs (Han et al., 2003).","{'title': '3 Empirical Study', 'number': '4'}"
We then break down the results to analyze various factors individually.,"{'title': '3 Empirical Study', 'number': '4'}"
"Table 1 shows the results on dataset H with the best results in bold; (intro and page fields are not shown, following past practice (Seymore et al., 1999; Han et al., 2003)).","{'title': '3 Empirical Study', 'number': '4'}"
"The results we obtained with CRFs use secondorder state transition features, layout features, as well as supported and unsupported features.","{'title': '3 Empirical Study', 'number': '4'}"
Feature induction is used in experiments on dataset R; (it didn’t improve accuracy on H).,"{'title': '3 Empirical Study', 'number': '4'}"
"The results we obtained with the HMM model use a second order model for transitions, and a first order for observations.","{'title': '3 Empirical Study', 'number': '4'}"
"The results on SVM is obtained from (Han et al., 2003) by computing F1 measures from the precision and recall numbers they report.","{'title': '3 Empirical Study', 'number': '4'}"
Table 2 shows the results on dataset R. SVM results are not available for these datasets.,"{'title': '3 Empirical Study', 'number': '4'}"
"From Table (1, 2), one can see that CRF performs significantly better than HMMs, which again supports the previous findings (Lafferty et al., 2001; Pinto et al., 2003).","{'title': '3 Empirical Study', 'number': '4'}"
"CRFs also perform significantly better than SVMbased approach, yielding new state of the art performance on this task.","{'title': '3 Empirical Study', 'number': '4'}"
CRFs increase the performance on nearly all the fields.,"{'title': '3 Empirical Study', 'number': '4'}"
"The overall word accuracy is improved from 92.9% to 98.3%, which corresponds to a 78% error rate reduction.","{'title': '3 Empirical Study', 'number': '4'}"
"However, as we can see word accuracy can be misleading since HMM model even has a higher word accuracy than SVM, although it performs much worse than SVM in most individual fields except abstract.","{'title': '3 Empirical Study', 'number': '4'}"
"Interestingly, HMM performs much better on abstract field (98% versus 93.8% F-measure) which pushes the overall accuracy up.","{'title': '3 Empirical Study', 'number': '4'}"
A better comparison can be made by comparing the field-based F-measures.,"{'title': '3 Empirical Study', 'number': '4'}"
"Here, in comparison to the SVM, CRFs improve the F1 measure from 89.7% to 93.9%, an error reduction of 36%.","{'title': '3 Empirical Study', 'number': '4'}"
The results of different regularization methods are summarized in Table (3).,"{'title': '3 Empirical Study', 'number': '4'}"
"Setting Gaussian variance of features depending on feature count performs better, from 90.5% to 91.2%, an error reduction of 7%, when only using supported features, and an error reduction of 9% when using supported and unsupported features.","{'title': '3 Empirical Study', 'number': '4'}"
"Results are averaged over 5 random runs, with an average variance of 0.2%.","{'title': '3 Empirical Study', 'number': '4'}"
In our experiments we found the Gaussian prior to consistently perform better than the others.,"{'title': '3 Empirical Study', 'number': '4'}"
"Surprisingly, exponential prior hurts the performance significantly.","{'title': '3 Empirical Study', 'number': '4'}"
It over penalizes the likelihood (significantly increasing cost—defined as negative penalized log-likelihood).,"{'title': '3 Empirical Study', 'number': '4'}"
We hypothesized that the problem could be that the choice of constant α is inappropriate.,"{'title': '3 Empirical Study', 'number': '4'}"
"So we tried varying α instead of computing it using absolute discounting, but found the alternatives to perform worse.","{'title': '3 Empirical Study', 'number': '4'}"
"These results suggest that Gaussian prior is a safer prior non-regularized, Gaussian variance = X sets variance to be X. Gaussian cut 7 refers to the Threshold Cut method, Gaussian divide count refers to the Divide Count method, Gaussian bin N refers to the Bin-Based method with bin size equals N, as described in 2.1.1 to use in practice.","{'title': '3 Empirical Study', 'number': '4'}"
State transition features and unsupported features.,"{'title': '3 Empirical Study', 'number': '4'}"
We summarize the comparison of different state transition models using or not using unsupported features in Table 4.,"{'title': '3 Empirical Study', 'number': '4'}"
"The first column describes the four different state transition models, the second column contains the overall word accuracy of these models using only support features, and the third column contains the result of using all features, including unsupported features.","{'title': '3 Empirical Study', 'number': '4'}"
"Comparing the rows, one can see that the second-order model performs the best, but not dramatically better than the firstorder+transitions and the third order model.","{'title': '3 Empirical Study', 'number': '4'}"
"However, the first-order model performs significantly worse.","{'title': '3 Empirical Study', 'number': '4'}"
"The difference does not come from sharing the weights, but from ignoring the f(yt−1i yt).","{'title': '3 Empirical Study', 'number': '4'}"
The first order transition feature is vital here.,"{'title': '3 Empirical Study', 'number': '4'}"
We would expect the third order model to perform better if enough training data were available.,"{'title': '3 Empirical Study', 'number': '4'}"
"Comparing the second and the third columns, we can see that using all features including unsupported features, consistently performs better than ignoring them.","{'title': '3 Empirical Study', 'number': '4'}"
"Our preliminary experiments with incremental support have shown performance in between that of supported-only and all features, and are still ongoing.","{'title': '3 Empirical Study', 'number': '4'}"
"Effects of layout features To analyze the contribution of different kinds of features, we divide the features into three categories: local features, layout features, and external lexicon resources.","{'title': '3 Empirical Study', 'number': '4'}"
The features we used are summarized in Table 5.,"{'title': '3 Empirical Study', 'number': '4'}"
The results of using different features are shown in Table 6.,"{'title': '3 Empirical Study', 'number': '4'}"
"The layout feature dramatically increases the performance, raising the F1 measure from 88.8% to 93.9%, whole sentence accuracy from 40.1% to 72.4%.","{'title': '3 Empirical Study', 'number': '4'}"
Adding lexicon features alone improves the performance.,"{'title': '3 Empirical Study', 'number': '4'}"
"However, when combing lexicon features and layout features, the performance is worse than using layout features alone.","{'title': '3 Empirical Study', 'number': '4'}"
"The lexicons were gathered from a large collection of BibTeX files, and upon examination had difficult to remove noise, for example words in the author lexicon that were also affiliations.","{'title': '3 Empirical Study', 'number': '4'}"
"In previous work, we have gained significant benefits by dividing each lexicon into sections based on point-wise information gain with respect to the lexicon’s class.","{'title': '3 Empirical Study', 'number': '4'}"
errors happen at the boundaries between two fields.,"{'title': '3.5.4 Error analysis', 'number': '5'}"
"Especially the transition from author to affiliation, from abstract to keyword.","{'title': '3.5.4 Error analysis', 'number': '5'}"
"The note field is the one most confused with others, and upon inspection is actually labeled inconsistently in the training data.","{'title': '3.5.4 Error analysis', 'number': '5'}"
"Other errors could be fixed with additional feature engineering—for example, including additional specialized regular expressions should make email accuracy nearly perfect.","{'title': '3.5.4 Error analysis', 'number': '5'}"
"Increasing the amount of training data would also be expected to help significantly, as indicated by consistent nearly perfect accuracy on the training set.","{'title': '3.5.4 Error analysis', 'number': '5'}"
"This paper investigates the issues of regularization, feature spaces, and efficient use of unsupported features in CRFs, with an application to information extraction from research papers.","{'title': '4 Conclusions and Future Work', 'number': '6'}"
For regularization we find that the Gaussian prior with variance depending on feature frequencies performs better than several other alternatives in the literature.,"{'title': '4 Conclusions and Future Work', 'number': '6'}"
Feature engineering is a key component of any machine learning solution—especially in conditionally-trained models with such freedom to choose arbitrary features—and plays an even more important role than regularization.,"{'title': '4 Conclusions and Future Work', 'number': '6'}"
"We obtain new state-of-the-art performance in extracting standard fields from research papers, with a significant error reduction by several metrics.","{'title': '4 Conclusions and Future Work', 'number': '6'}"
"We also suggest better evaluation metrics to facilitate future research in this task—especially field-F1, rather than word accuracy.","{'title': '4 Conclusions and Future Work', 'number': '6'}"
We have provided an empirical exploration of a few previously-published priors for conditionally-trained loglinear models.,"{'title': '4 Conclusions and Future Work', 'number': '6'}"
Fundamental advances in regularization for CRFs remains a significant open research area.,"{'title': '4 Conclusions and Future Work', 'number': '6'}"
"This work was supported in part by the Center for Intelligent Information Retrieval, in part by SPAWARSYSCEN-SD grant number N66001-02-18903, in part by the National Science Foundation Cooperative Agreement number ATM-9732665 through a subcontract from the University Corporation for Atmospheric Research (UCAR) and in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS0326249.","{'title': '5 Acknowledgments', 'number': '7'}"
"Any opinions, findings and conclusions or rectitle auth. pubnum date abs. aff. addr. email deg. note ph. intro k.w. web title 3446 0 6 0 22 0 0 0 9 25 0 0 12 0 author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0 pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0 date 0 0 3 336 0 1 3 0 0 18 0 0 0 0 abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0 affil.","{'title': '5 Acknowledgments', 'number': '7'}"
19 13 0 0 10 3852 27 0 28 34 0 0 0 1 address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0 email 0 0 1 0 12 2 3 461 0 2 2 0 15 0 degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0 note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3 phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0 intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0 keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0 web 0 0 0 0 2 0 0 0 0 31 0 0 0 294 ommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.,"{'title': '5 Acknowledgments', 'number': '7'}"
