col1,col2
"We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning.",{}
We utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates.,{}
"Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.",{}
Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.,{}
"We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation.",{}
"Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.",{}
Lexical resources are crucial in most NLP tasks and are extensively used by people.,"{'title': '1 Introduction', 'number': '1'}"
"Manual compilation of lexical resources is labor intensive, error prone, and susceptible to arbitrary human decisions.","{'title': '1 Introduction', 'number': '1'}"
Hence there is a need for automatic authoring that would be as unsupervised and languageindependent as possible.,"{'title': '1 Introduction', 'number': '1'}"
An important type of lexical resource is that given by grouping words into categories.,"{'title': '1 Introduction', 'number': '1'}"
"In general, the notion of a category is a fundamental one in cognitive psychology (Matlin, 2005).","{'title': '1 Introduction', 'number': '1'}"
"A lexical category is a set of words that share a significant aspect of their meaning, e.g., sets of words denoting vehicles, types of food, tool names, etc.","{'title': '1 Introduction', 'number': '1'}"
A word can obviously belong to more than a single category.,"{'title': '1 Introduction', 'number': '1'}"
We will use ‘category’ instead of ‘lexical category’ for brevity'.,"{'title': '1 Introduction', 'number': '1'}"
"Grouping of words into categories is useful in itself (e.g., for the construction of thesauri), and can serve as the starting point in many applications, such as ontology construction and enhancement, discovery of verb subcategorization frames, etc.","{'title': '1 Introduction', 'number': '1'}"
Our goal in this paper is a fully unsupervised discovery of categories from large unannotated text corpora.,"{'title': '1 Introduction', 'number': '1'}"
We aim for categories containing single words (multi-word lexical items will be dealt with in future papers.),"{'title': '1 Introduction', 'number': '1'}"
"Our approach is based on patterns, and utilizes the following stages: We performed a thorough evaluation on two English corpora (the BNC and a 68GB web corpus) and on a 33GB Russian corpus, and a sanity-check test on smaller Danish, Irish and Portuguese corpora.","{'title': '1 Introduction', 'number': '1'}"
Evaluations were done using both human judgments and WordNet in a setting quite similar to that done (for the BNC) in previous work.,"{'title': '1 Introduction', 'number': '1'}"
"Our unsupervised results are superior to previous work that used a POS tagged corpus, are less language dependent, and are very efficient computationally2.","{'title': '1 Introduction', 'number': '1'}"
Patterns are a common approach in lexical acquisition.,"{'title': '1 Introduction', 'number': '1'}"
"Our approach is novel in several aspects: (1) we discover patterns in a fully unsupervised manner, as opposed to using a manually prepared pattern set, pattern seed or words seeds; (2) our pattern discovery requires no annotation of the input corpus, as opposed to requiring POS tagging or partial or full parsing; (3) we discover general symmetric patterns, as opposed to using a few hard-coded ones such as ‘x and y’; (4) the cliqueset graph algorithm in stage 3 is novel.","{'title': '1 Introduction', 'number': '1'}"
"In addition, we demonstrated the relatively language independent nature of our approach by evaluating on very large corpora in two languages3.","{'title': '1 Introduction', 'number': '1'}"
Section 2 surveys previous work.,"{'title': '1 Introduction', 'number': '1'}"
"Section 3 describes pattern discovery, and Section 4 describes the formation of categories.","{'title': '1 Introduction', 'number': '1'}"
"Evaluation is presented in Section 5, and a discussion in Section 6.","{'title': '1 Introduction', 'number': '1'}"
Much work has been done on lexical acquisition of all sorts.,"{'title': '2 Previous Work', 'number': '2'}"
The three main distinguishing axes are (1) the type of corpus annotation and other human input used; (2) the type of lexical relationship targeted; and (3) the basic algorithmic approach.,"{'title': '2 Previous Work', 'number': '2'}"
The two main approaches are pattern-based discovery and clustering of context feature vectors.,"{'title': '2 Previous Work', 'number': '2'}"
Many of the papers cited below aim at the construction of hyponym (is-a) hierarchies.,"{'title': '2 Previous Work', 'number': '2'}"
"Note that they can also be viewed as algorithms for category discovery, because a subtree in such a hierarchy defines a lexical category.","{'title': '2 Previous Work', 'number': '2'}"
"A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002).","{'title': '2 Previous Work', 'number': '2'}"
Pereira (1993) and Lin (1998) use syntactic features in the vector definition.,"{'title': '2 Previous Work', 'number': '2'}"
"(Pantel and Lin, 2002) improves on the latter by clustering by committee.","{'title': '2 Previous Work', 'number': '2'}"
Caraballo (1999) uses conjunction and appositive annotations in the vector representation.,"{'title': '2 Previous Work', 'number': '2'}"
"The only previous works addressing our problem and not requiring any syntactic annotation are those that decompose a lexically-defined matrix (by SVD, PCA etc), e.g.","{'title': '2 Previous Work', 'number': '2'}"
"(Sch¨utze, 1998; Deerwester et al, 1990).","{'title': '2 Previous Work', 'number': '2'}"
Such matrix decomposition is computationally heavy and has not been proven to scale well when the number of words assigned to categories grows.,"{'title': '2 Previous Work', 'number': '2'}"
"Agglomerative clustering (e.g., (Brown et al, 1992; Li, 1996)) can produce hierarchical word categories from an unannotated corpus.","{'title': '2 Previous Work', 'number': '2'}"
"However, we are not aware of work in this direction that has been evaluated with good results on lexical category acquisition.","{'title': '2 Previous Work', 'number': '2'}"
The technique is also quite demanding computationally.,"{'title': '2 Previous Work', 'number': '2'}"
The second main algorithmic approach is to use lexico-syntactic patterns.,"{'title': '2 Previous Work', 'number': '2'}"
"Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004).","{'title': '2 Previous Work', 'number': '2'}"
"Hearst (1992) uses a manually prepared set of initial lexical patterns in order to discover hierarchical categories, and utilizes those categories in order to automatically discover additional patterns.","{'title': '2 Previous Work', 'number': '2'}"
"(Berland and Charniak, 1999) use hand crafted patterns to discover part-of (meronymy) relationships, and (Chklovski and Pantel, 2004) discover various interesting relations between verbs.","{'title': '2 Previous Work', 'number': '2'}"
Both use information obtained by parsing.,"{'title': '2 Previous Work', 'number': '2'}"
"(Pantel et al, 2004) reduce the depth of the linguistic data used but still requires POS tagging.","{'title': '2 Previous Work', 'number': '2'}"
"Many papers directly target specific applications, and build lexical resources as a side effect.","{'title': '2 Previous Work', 'number': '2'}"
"Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering.","{'title': '2 Previous Work', 'number': '2'}"
"Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999).","{'title': '2 Previous Work', 'number': '2'}"
"(Widdows and Dorow, 2002; Dorow et al, 2005) discover categories using two hard-coded symmetric patterns, and are thus the closest to us.","{'title': '2 Previous Work', 'number': '2'}"
They also introduce an elegant graph representation that we adopted.,"{'title': '2 Previous Work', 'number': '2'}"
They report good results.,"{'title': '2 Previous Work', 'number': '2'}"
"However, they require POS tagging of the corpus, use only two hard-coded patterns (‘x and y’, ‘x or y’), deal only with nouns, and require non-trivial computations on the graph.","{'title': '2 Previous Work', 'number': '2'}"
"A third, less common, approach uses settheoretic inference, for example (Cimiano et al, 2005).","{'title': '2 Previous Work', 'number': '2'}"
"Again, that paper uses syntactic information.","{'title': '2 Previous Work', 'number': '2'}"
"In summary, no previous work has combined the accuracy, scalability and performance advantages of patterns with the fully unsupervised, unannotated nature possible with clustering approaches.","{'title': '2 Previous Work', 'number': '2'}"
This severely limits the applicability of previous work on the huge corpora available at present.,"{'title': '2 Previous Work', 'number': '2'}"
Our first step is the discovery of patterns that are useful for lexical category acquisition.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"We use two main stages: discovery of pattern candidates, and identification of the symmetric patterns among the candidates.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"An examination of the patterns found useful in previous work shows that they contain one or more very frequent word, such as ‘and’, ‘is’, etc.","{'title': '3 Discovery of Patterns', 'number': '3'}"
Our approach towards unsupervised pattern induction is to find such words and utilize them.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"We define a high frequency word (HFW) as a word appearing more than TH times per million words, and a content word (CW) as a word appearing less than TC times per a million words4.","{'title': '3 Discovery of Patterns', 'number': '3'}"
Now define a meta-pattern as any sequence of HFWs and CWs.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
In this paper we require that meta-patterns obey the following constraints: (1) at most 4 words; (2) exactly two content words; (3) no two consecutive CWs.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
The rationale is to see what can be achieved using relatively short patterns and where the discovered categories contain single words only.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
We will relax these constraints in future papers.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"Our meta-patterns here are thus of four types: CHC, CHCH, CHHC, and HCHC.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"In order to focus on patterns that are more likely to provide high quality categories, we removed patterns that appear in the corpus less than TP times per million words.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"Since we can ensure that the number of HFWs is bounded, the total number of pattern candidates is bounded as well.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"Hence, this stage can be computed in time linear in the size of the corpus (assuming the corpus has been already pre-processed to allow direct access to a word by its index.)","{'title': '3 Discovery of Patterns', 'number': '3'}"
Many of the pattern candidates discovered in the previous stage are not usable.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"In order to find a usable subset, we focus on the symmetric patterns.","{'title': '3 Discovery of Patterns', 'number': '3'}"
Our rationale is that two content-bearing words that appear in a symmetric pattern are likely to be semantically similar in some sense.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"This simple observation turns out to be very powerful, as shown by our results.","{'title': '3 Discovery of Patterns', 'number': '3'}"
We will eventually combine data from several patterns and from different corpus windows (Section 4.),"{'title': '3 Discovery of Patterns', 'number': '3'}"
"For identifying symmetric patterns, we use a version of the graph representation of (Widdows and Dorow, 2002).","{'title': '3 Discovery of Patterns', 'number': '3'}"
We first define the singlepattern graph G(P) as follows.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"Nodes correspond to content words, and there is a directed arc A(x, y) from node x to node y iff (1) the words x and y both appear in an instance of the pattern P as its two CWs; and (2) x precedes y in P. Denote by Nodes(G), Arcs(G) the nodes and arcs in a graph G, respectively.","{'title': '3 Discovery of Patterns', 'number': '3'}"
We now compute three measures on G(P) and combine them for all pattern candidates to filter asymmetric ones.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"The first measure (M1) counts the proportion of words that can appear in both slots of the pattern, out of the total number of words.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"The reasoning here is that if a pattern allows a large percentage of words to participate in both slots, its chances of being a symmetric pattern are greater: M1 filters well patterns that connect words having different parts of speech.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"However, it may fail to filter patterns that contain multiple levels of asymmetric relationships.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"For example, in the pattern ‘x belongs to y’, we may find a word B on both sides (‘A belongs to B’, ‘B belongs to C’) while the pattern is still asymmetric.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"In order to detect symmetric relationships in a finer manner, for the second and third measures we define 5ymG(P), the symmetric subgraph of G(P), containing only the bidirectional arcs and nodes of G(P): The second and third measures count the proportion of the number of symmetric nodes and edges in G(P), respectively: All three measures yield values in [0, 1], and in all three a higher value indicates more symmetry.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"M2 and M3 are obviously correlated, but they capture different aspects of a pattern’s nature: M3 is informative for highly interconnected but small word categories (e.g., month names), while M2 is useful for larger categories that are more loosely connected in the corpus.","{'title': '3 Discovery of Patterns', 'number': '3'}"
We use the three measures as follows.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"For each measure, we prepare a sorted list of all candidate patterns.","{'title': '3 Discovery of Patterns', 'number': '3'}"
"We remove patterns that are not in the top ZT (we use 100, see Section 5) in any of the three lists, and patterns that are in the bottom ZB in at least one of the lists.","{'title': '3 Discovery of Patterns', 'number': '3'}"
The remaining patterns constitute our final list of symmetric patterns.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"We do not rank the final list, since the category discovery algorithm of the next section does not need such a ranking.","{'title': '3 Discovery of Patterns', 'number': '3'}"
Defining and utilizing such a ranking is a subject for future work.,"{'title': '3 Discovery of Patterns', 'number': '3'}"
"A sparse matrix representation of each graph can be computed in time linear in the size of the input corpus, since (1) the number of patterns |P |is bounded, (2) vocabulary size |V  |(the total number of graph nodes) is much smaller than corpus size, and (3) the average node degree is much smaller than |V  |(in practice, with the thresholds used, it is a small constant.)","{'title': '3 Discovery of Patterns', 'number': '3'}"
After the end of the previous stage we have a set of symmetric patterns.,"{'title': '4 Discovery of Categories', 'number': '4'}"
We now use them in order to discover categories.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"In this section we describe the graph clique-set method for generating initial categories, and category pruning techniques for increased quality.","{'title': '4 Discovery of Categories', 'number': '4'}"
"Our approach to category discovery is based on connectivity structures in the all-pattern word relationship graph G, resulting from merging all of the single-pattern graphs into a single unified graph.","{'title': '4 Discovery of Categories', 'number': '4'}"
The graph G can be built in time O(|V  |x |P |x AverageDegree(G(P))) = O(|V |) (we use V rather than Nodes(G) for brevity.),"{'title': '4 Discovery of Categories', 'number': '4'}"
"When building G, no special treatment is done when one pattern is contained within another.","{'title': '4 Discovery of Categories', 'number': '4'}"
"For example, any pattern of the form CHC is contained in a pattern of the form HCHC (‘x and y’, ‘both x and y’.)","{'title': '4 Discovery of Categories', 'number': '4'}"
The shared part yields exactly the same subgraph.,"{'title': '4 Discovery of Categories', 'number': '4'}"
This policy could be changed for a discovery of finer relationships.,"{'title': '4 Discovery of Categories', 'number': '4'}"
The main observation on G is that words that are highly interconnected are good candidates to form a category.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"This is the same general observation exploited by (Widdows and Dorow, 2002), who try to find graph regions that are more connected internally than externally.","{'title': '4 Discovery of Categories', 'number': '4'}"
We use a different algorithm.,"{'title': '4 Discovery of Categories', 'number': '4'}"
We find all strong n-cliques (subgraphs containing n nodes that are all bidirectionally interconnected.),"{'title': '4 Discovery of Categories', 'number': '4'}"
"A clique Q defines a category that contains the nodes in Q plus all of the nodes that are (1) at least unidirectionally connected to all nodes in Q, and (2) bidirectionally connected to at least one node in Q.","{'title': '4 Discovery of Categories', 'number': '4'}"
In practice we use 2-cliques.,"{'title': '4 Discovery of Categories', 'number': '4'}"
The strongly connected cliques are the bidirectional arcs in G and their nodes.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"For each such arc A, a category is generated that contains the nodes of all triangles that contain A and at least one additional bidirectional arc.","{'title': '4 Discovery of Categories', 'number': '4'}"
"For example, suppose the corpus contains the text fragments ‘book and newspaper’, ‘newspaper and book’, ‘book and note’, ‘note and book’ and ‘note and newspaper’.","{'title': '4 Discovery of Categories', 'number': '4'}"
In this case the three words are assigned to a category.,"{'title': '4 Discovery of Categories', 'number': '4'}"
Note that a pair of nodes connected by a symmetric arc can appear in more than a single category.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"For example, suppose a graph G containing five nodes and seven arcs that define exactly three strongly connected triangles, ABC, ABD, ACE.","{'title': '4 Discovery of Categories', 'number': '4'}"
"The arc (A, B) yields a category {A, B, C, D}, and the arc (A, C) yields a category {A, C, B, E}.","{'title': '4 Discovery of Categories', 'number': '4'}"
Nodes A and C appear in both categories.,"{'title': '4 Discovery of Categories', 'number': '4'}"
Category merging is described below.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"This stage requires an O(1) computation for each bidirectional arc of each node, so its complexity is O(|V  |x AverageDegree(G)) O(|V |).","{'title': '4 Discovery of Categories', 'number': '4'}"
"In order to cover as many words as possible, we use the smallest clique, a single symmetric arc.","{'title': '4 Discovery of Categories', 'number': '4'}"
This creates redundant categories.,"{'title': '4 Discovery of Categories', 'number': '4'}"
We enhance the quality of the categories by merging them and by windowing on the corpus.,"{'title': '4 Discovery of Categories', 'number': '4'}"
We use two simple merge heuristics.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"First, if two categories are identical we treat them as one.","{'title': '4 Discovery of Categories', 'number': '4'}"
"Second, given two categories Q, R, we merge them iff there’s more than a 50% overlap between them: (|Q n R |> |Q|/2) n (|Q n R |> |R|/2).","{'title': '4 Discovery of Categories', 'number': '4'}"
"= This could be added to the clique-set stage, but the phrasing above is simpler to explain and implement.","{'title': '4 Discovery of Categories', 'number': '4'}"
"In order to increase category quality and remove categories that are too context-specific, we use a simple corpus windowing technique.","{'title': '4 Discovery of Categories', 'number': '4'}"
"Instead of running the algorithm of this section on the whole corpus, we divide the corpus into windows of equal size (see Section 5 for size determination) and perform the category discovery algorithm of this section on each window independently.","{'title': '4 Discovery of Categories', 'number': '4'}"
Merging is also performed in each window separately.,"{'title': '4 Discovery of Categories', 'number': '4'}"
We now have a set of categories for each window.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"For the final set, we select only those categories that appear in at least two of the windows.","{'title': '4 Discovery of Categories', 'number': '4'}"
This technique reduces noise at the potential cost of lowering coverage.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"However, the numbers of categories discovered and words they contain is still very large (see Section 5), so windowing achieves higher precision without hurting coverage in practice.","{'title': '4 Discovery of Categories', 'number': '4'}"
The complexity of the merge stage is O(|V |) times the average number of categories per word times the average number of words per category.,"{'title': '4 Discovery of Categories', 'number': '4'}"
"The latter two are small in practice, so complexity amounts to O(|V |).","{'title': '4 Discovery of Categories', 'number': '4'}"
Lexical acquisition algorithms are notoriously hard to evaluate.,"{'title': '5 Evaluation', 'number': '5'}"
"We have attempted to be as thorough as possible, using several languages and both automatic and human evaluation.","{'title': '5 Evaluation', 'number': '5'}"
"In the automatic part, we followed as closely as possible the methodology and data used in previous work, so that meaningful comparisons could be made.","{'title': '5 Evaluation', 'number': '5'}"
"We performed in-depth evaluation on two languages, English and Russian, using three corpora, two for English and one for Russian.","{'title': '5 Evaluation', 'number': '5'}"
"The first English corpus is the BNC, containing about 100M words.","{'title': '5 Evaluation', 'number': '5'}"
"The second English corpus, Dmoz (Gabrilovich and Markovitch, 2005), is a web corpus obtained by crawling and cleaning the URLs in the Open Directory Project (dmoz.org), resulting in 68GB containing about 8.2G words from 50M web pages.","{'title': '5 Evaluation', 'number': '5'}"
"The Russian corpus was assembled from many web sites and carefully filtered for duplicates, to yield 33GB and 4G words.","{'title': '5 Evaluation', 'number': '5'}"
"It is a varied corpus comprising literature, technical texts, news, newsgroups, etc.","{'title': '5 Evaluation', 'number': '5'}"
"As a preliminary sanity-check test we also applied our method to smaller corpora in Danish, Irish and Portuguese, and noted some substantial similarities in the discovered patterns.","{'title': '5 Evaluation', 'number': '5'}"
"For example, in all 5 languages the pattern corresponding to ‘x and y’ was among the 50 selected.","{'title': '5 Evaluation', 'number': '5'}"
"The thresholds TH, TC, TP, ZT, ZB, were determined by memory size considerations: we computed thresholds that would give us the maximal number of words, while enabling the pattern access table to reside in main memory.","{'title': '5 Evaluation', 'number': '5'}"
"The resulting numbers are 100, 50,20, 100, 100.","{'title': '5 Evaluation', 'number': '5'}"
"Corpus window size was determined by starting from a very small window size, defining at random a single window of that size, running the algorithm, and iterating this process with increased window sizes until reaching a desired vocabulary category participation percentage (i.e., x% of the different words in the corpus assigned into categories.","{'title': '5 Evaluation', 'number': '5'}"
We used 5%.),"{'title': '5 Evaluation', 'number': '5'}"
"This process has only a negligible effect on running times, because each iteration is run only on a single window, not on the whole corpus.","{'title': '5 Evaluation', 'number': '5'}"
The table below gives some statistics.,"{'title': '5 Evaluation', 'number': '5'}"
V is the total number of different words in the corpus.,"{'title': '5 Evaluation', 'number': '5'}"
W is the number of words belonging to at least one of our categories.,"{'title': '5 Evaluation', 'number': '5'}"
C is the number of categories (after merging and windowing.),"{'title': '5 Evaluation', 'number': '5'}"
A5 is the average category size.,"{'title': '5 Evaluation', 'number': '5'}"
Running times are in minutes on a 2.53Ghz Pentium 4 XP machine with 1GB memory.,"{'title': '5 Evaluation', 'number': '5'}"
"Note how small they are, when compared to (Pantel et al, 2004), which took 4 days for a smaller corpus using the same CPU.","{'title': '5 Evaluation', 'number': '5'}"
"Among the patterns discovered are the ubiquitous ‘x and y’, ‘x or y’ and many patterns containing them.","{'title': '5 Evaluation', 'number': '5'}"
"Additional patterns include ‘from x to y’, ‘x and/or y’ (punctuation is treated here as white space), ‘x and a y’, and ‘neither x nor y’.","{'title': '5 Evaluation', 'number': '5'}"
We discover categories of different parts of speech.,"{'title': '5 Evaluation', 'number': '5'}"
"Among the noun ones, there are many whose precision is 100%: 37 countries, 18 languages, 51 chemical elements, 62 animals, 28 types of meat, 19 fruits, 32 university names, etc.","{'title': '5 Evaluation', 'number': '5'}"
"A nice verb category example is {dive, snorkel, swim, float, surf, sail, canoe, kayak, paddle, tube, drift}.","{'title': '5 Evaluation', 'number': '5'}"
"A nice adjective example is {amazing, The purpose of the human evaluation was dual: to assess the quality of the discovered categories in terms of precision, and to compare with those obtained by a baseline clustering algorithm.","{'title': '5 Evaluation', 'number': '5'}"
"For the baseline, we implemented k-means as follows.","{'title': '5 Evaluation', 'number': '5'}"
"We have removed stopwords from the corpus, and then used as features the words which appear before or after the target word.","{'title': '5 Evaluation', 'number': '5'}"
"In the calculation of feature values and inter-vector distances, and in the removal of less informative features, we have strictly followed (Pantel and Lin, 2002).","{'title': '5 Evaluation', 'number': '5'}"
"We ran the algorithm 10 times using k = 500 with randomly selected centroids, producing 5000 clusters.","{'title': '5 Evaluation', 'number': '5'}"
We then merged the resulting clusters using the same 50% overlap criterion as in our algorithm.,"{'title': '5 Evaluation', 'number': '5'}"
"The result included 3090, 2116, and 3206 clusters for Dmoz, BNC and Russian respectively.","{'title': '5 Evaluation', 'number': '5'}"
We used 8 subjects for evaluation of the English categories and 15 subjects for evaluation of the Russian ones.,"{'title': '5 Evaluation', 'number': '5'}"
"In order to assess the subjects’ reliability, we also included random categories (see below.)","{'title': '5 Evaluation', 'number': '5'}"
The experiment contained two parts.,"{'title': '5 Evaluation', 'number': '5'}"
"In Part I, subjects were given 40 triplets of words and were asked to rank them using the following scale: (1) the words definitely share a significant part of their meaning; (2) the words have a shared meaning but only in some context; (3) the words have a shared meaning only under a very unusual context/situation; (4) the words do not share any meaning; (5) I am not familiar enough with some/all of the words.","{'title': '5 Evaluation', 'number': '5'}"
The 40 triplets were obtained as follows.,"{'title': '5 Evaluation', 'number': '5'}"
"20 of our categories were selected at random from the non-overlapping categories we have discovered, and three words were selected from each of these at random.","{'title': '5 Evaluation', 'number': '5'}"
"10 triplets were selected in the same manner from the categories produced by k-means, and 10 triplets were generated by random selection of content words from the same window in the corpus.","{'title': '5 Evaluation', 'number': '5'}"
"In Part II, subjects were given the full categories of the triplets that were graded as 1 or 2 in Part I (that is, the full ‘good’ categories in terms of sharing of meaning.)","{'title': '5 Evaluation', 'number': '5'}"
They were asked to grade the categories from 1 (worst) to 10 (best) according to how much the full category had met the expectations they had when seeing only the triplet.,"{'title': '5 Evaluation', 'number': '5'}"
Results are given in Table 1.,"{'title': '5 Evaluation', 'number': '5'}"
"The first line gives the average percentage of triplets that were given scores of 1 or 2 (that is, ‘significant shared meaning’.)","{'title': '5 Evaluation', 'number': '5'}"
The 2nd line gives the average score of a triplet (1 is best.),"{'title': '5 Evaluation', 'number': '5'}"
In these lines scores of 5 were not counted.,"{'title': '5 Evaluation', 'number': '5'}"
The 3rd line gives the average score given to a full category (10 is best.),"{'title': '5 Evaluation', 'number': '5'}"
"Interevaluator Kappa between scores 1,2 and 3,4 was 0.56, 0.67 and 0.72 for Dmoz, BNC and Russian respectively.","{'title': '5 Evaluation', 'number': '5'}"
"Our algorithm clearly outperforms k-means, which outperforms random.","{'title': '5 Evaluation', 'number': '5'}"
We believe that the Russian results are better because the percentage of native speakers among our subjects for Russian was larger than that for English.,"{'title': '5 Evaluation', 'number': '5'}"
"The major guideline in this part of the evaluation was to compare our results with previous work having a similar goal (Widdows and Dorow, 2002).","{'title': '5 Evaluation', 'number': '5'}"
"We have followed their methodology as best as we could, using the same WordNet (WN) categories and the same corpus (BNC) in addition to the Dmoz and Russian corporal.","{'title': '5 Evaluation', 'number': '5'}"
The evaluation method is as follows.,"{'title': '5 Evaluation', 'number': '5'}"
"We took the exact 10 WN subsets referred to as ‘subjects’ in (Widdows and Dorow, 2002), and removed all multi-word items.","{'title': '5 Evaluation', 'number': '5'}"
We now selected at random 10 pairs of words from each subject.,"{'title': '5 Evaluation', 'number': '5'}"
"For each pair, we found the largest of our discovered categories containing it (if there isn’t one, we pick another pair.","{'title': '5 Evaluation', 'number': '5'}"
"This is valid because our Recall is obviously not even close to 100%, so if we did not pick another pair we would seriously harm the validity of the evaluation.)","{'title': '5 Evaluation', 'number': '5'}"
The various morphological forms of the same word were treated as one during the evaluation.,"{'title': '5 Evaluation', 'number': '5'}"
"The only difference from the (Widdows and Dorow, 2002) experiment is the usage of pairs rather than single words.","{'title': '5 Evaluation', 'number': '5'}"
We did this in order to disambiguate our categories.,"{'title': '5 Evaluation', 'number': '5'}"
"This was not needed in (Widdows and Dorow, 2002) because they had directly accessed the word graph, which may be an advantage in some applications.","{'title': '5 Evaluation', 'number': '5'}"
The Russian evaluation posed a bit of a problem because the Russian WordNet is not readily available and its coverage is rather small.,"{'title': '5 Evaluation', 'number': '5'}"
"Fortunately, the subject list is such that WordNet words could be translated unambiguously to Russian and words in our discovered categories could be translated unambiguously into English.","{'title': '5 Evaluation', 'number': '5'}"
This was the methodology taken.,"{'title': '5 Evaluation', 'number': '5'}"
"For each found category C containing N words, we computed the following (see Table 2): (1) Precision: the number of words present in both C and WN divided by N; (2) Precision*: the number of correct words divided by N. Correct words are either words that appear in the WN subtree, or words whose entry in the American Heritage Dictionary or the Britannica directly defines them as belonging to the given class (e.g., ‘keyboard’ is defined as ‘a piano’; ‘mitt’ is defined by ‘a type of glove’.)","{'title': '5 Evaluation', 'number': '5'}"
This was done in order to overcome the relative poorness of WordNet; (3) Recall: the number of words present in both C and WN divided by the number of (single) words in WN; (4) The number of correctly discovered words (New) that are not in WN.,"{'title': '5 Evaluation', 'number': '5'}"
"The Table also shows the number of WN words (:WN), in order to get a feeling by how much WN could be improved here.","{'title': '5 Evaluation', 'number': '5'}"
"For each subject, we show the average over the 10 randomly selected pairs.","{'title': '5 Evaluation', 'number': '5'}"
"Table 2 also shows the average of each measure over the subjects, and the two precision measures when computed on the total set of WN words.","{'title': '5 Evaluation', 'number': '5'}"
"The (uncorrected) precision is the only metric given in (Widdows and Dorow, 2002), who reported 82% (for the BNC.)","{'title': '5 Evaluation', 'number': '5'}"
Our method gives 90.47% for this metric on the same corpus.,"{'title': '5 Evaluation', 'number': '5'}"
Our human-evaluated and WordNet-based results are better than the baseline and previous work respectively.,"{'title': '5 Evaluation', 'number': '5'}"
Both are also of good standalone quality.,"{'title': '5 Evaluation', 'number': '5'}"
"Clearly, evaluation methodology for lexical acquisition tasks should be improved, which is an interesting research direction in itself.","{'title': '5 Evaluation', 'number': '5'}"
"Examining our categories at random, we found a nice example that shows how difficult it is to evaluate the task and how useful automatic category discovery can be, as opposed to manual definition.","{'title': '5 Evaluation', 'number': '5'}"
"Consider the following category, discovered in the Dmoz corpus: {nightcrawlers, chicken, shrimp, liver, leeches}.","{'title': '5 Evaluation', 'number': '5'}"
"We did not know why these words were grouped together; if asked in an evaluation, we would give the category a very low score.","{'title': '5 Evaluation', 'number': '5'}"
"However, after some web search, we found that this is a ‘fish bait’ category, especially suitable for catfish.","{'title': '5 Evaluation', 'number': '5'}"
We have presented a novel method for patternbased discovery of lexical semantic categories.,"{'title': '6 Discussion', 'number': '6'}"
"It is the first pattern-based lexical acquisition method that is fully unsupervised, requiring no corpus annotation or manually provided patterns or words.","{'title': '6 Discussion', 'number': '6'}"
"Pattern candidates are discovered using meta-patterns of high frequency and content words, and symmetric patterns are discovered using simple graph-theoretic measures.","{'title': '6 Discussion', 'number': '6'}"
Categories are generated using a novel graph clique-set algorithm.,"{'title': '6 Discussion', 'number': '6'}"
"The only other fully unsupervised lexical category acquisition approach is based on decomposition of a matrix defined by context feature vectors, and it has not been shown to scale well yet.","{'title': '6 Discussion', 'number': '6'}"
"Our algorithm was evaluated using both human judgment and automatic comparisons with WordNet, and results were superior to previous work (although it used a POS tagged corpus) and more efficient computationally.","{'title': '6 Discussion', 'number': '6'}"
Our algorithm is also easy to implement.,"{'title': '6 Discussion', 'number': '6'}"
"Computational efficiency and specifically lack of annotation are important criteria, because they allow usage of huge corpora, which are presently becoming available and growing in size.","{'title': '6 Discussion', 'number': '6'}"
"There are many directions to pursue in the future: (1) support multi-word lexical items; (2) increase category quality by improved merge algorithms; (3) discover various relationships (e.g., hyponymy) between the discovered categories; (4) discover finer inter-word relationships, such as verb selection preferences; (5) study various properties of discovered patterns in a detailed manner; and (6) adapt the algorithm to morphologically rich languages. words’ precision of 90.47%.","{'title': '6 Discussion', 'number': '6'}"
"This metric was reported to be 82% in (Widdows and Dorow, 2002).","{'title': '6 Discussion', 'number': '6'}"
"It should be noted that our algorithm can be viewed as one for automatic discovery of word senses, because it allows a word to participate in more than a single category.","{'title': '6 Discussion', 'number': '6'}"
"When merged properly, the different categories containing a word can be viewed as the set of its senses.","{'title': '6 Discussion', 'number': '6'}"
We are planning an evaluation according to this measure after improving the merge stage.,"{'title': '6 Discussion', 'number': '6'}"
