col1,col2
Our work adopts major components of the algorithm from (Luo & Roukos 1996): language model (LM) parameter estimation from a segmented corpus and input segmentation on the basis of LM probabilities.,"{'title': '2 Related Work', 'number': '1'}"
"However, our work diverges from their work in two crucial respects: (i) new technique of computing all possible segmentations of a word into prefix*-stem-suffix* for decoding, and (ii) unsupervised algorithm for new stem acquisition based on a stem candidate's similarity to stems occurring in the training corpus.","{'title': '2 Related Work', 'number': '1'}"
(Darwish 2002) presents a supervised technique which identifies the root of an Arabic word by stripping away the prefix and the suffix of the word on the basis of manually acquired dictionary of word-root pairs and the likelihood that a prefix and a suffix would occur with the template from which the root is derived.,"{'title': '2 Related Work', 'number': '1'}"
"He reports 92.7% segmentation marking a prefix with '#&quot; and a suffix with '+' will be adopted throughout the paper. accuracy on a 9,606 word evaluation corpus.","{'title': '2 Related Work', 'number': '1'}"
His technique pre-supposes at most one prefix and one suffix per stem regardless of the actual number and meanings of prefixes/suffixes associated with the stem.,"{'title': '2 Related Work', 'number': '1'}"
"(Beesley 1996) presents a finite-state morphological analyzer for Arabic, which displays the root, pattern, and prefixes/suffixes.","{'title': '2 Related Work', 'number': '1'}"
The analyses are based on manually acquired lexicons and rules.,"{'title': '2 Related Work', 'number': '1'}"
"Although his analyzer is comprehensive in the types of knowledge it presents, it has been criticized for their extensive development time and lack of robustness, cf.","{'title': '2 Related Work', 'number': '1'}"
(Darwish 2002).,"{'title': '2 Related Work', 'number': '1'}"
"(Yarowsky and Wicentowsky 2000) presents a minimally supervised morphological analysis with a performance of over 99.2% accuracy for the 3,888 past-tense test cases in English.","{'title': '2 Related Work', 'number': '1'}"
The core algorithm lies in the estimation of a probabilistic alignment between inflected forms and root forms.,"{'title': '2 Related Work', 'number': '1'}"
"The probability estimation is based on the lemma alignment by frequency ratio similarity among different inflectional forms derived from the same lemma, given a table of inflectional parts-of-speech, a list of the canonical suffixes for each part of speech, and a list of the candidate noun, verb and adjective roots of the language.","{'title': '2 Related Work', 'number': '1'}"
Their algorithm does not handle multiple affixes per word.,"{'title': '2 Related Work', 'number': '1'}"
"(Goldsmith 2000) presents an unsupervised technique based on the expectationmaximization algorithm and minimum description length to segment exactly one suffix per word, resulting in an F-score of 81.8 for suffix identification in English according to (Schone and Jurafsky 2001).","{'title': '2 Related Work', 'number': '1'}"
(Schone and Jurafsky 2001) proposes an unsupervised algorithm capable of automatically inducing the morphology of inflectional languages using only text corpora.,"{'title': '2 Related Work', 'number': '1'}"
"Their algorithm combines cues from orthography, semantics, and contextual information to induce morphological relationships in German, Dutch, and English, among others.","{'title': '2 Related Work', 'number': '1'}"
They report Fscores between 85 and 93 for suffix analyses and between 78 and 85 for circumfix analyses in these languages.,"{'title': '2 Related Work', 'number': '1'}"
"Although their algorithm captures prefix-suffix combinations or circumfixes, it does not handle the multiple affixes per word we observe in Arabic.","{'title': '2 Related Work', 'number': '1'}"
"Given an Arabic sentence, we use a trigram language model on morphemes to segment it into a sequence of morphemes {m1, m2, ...,mn}.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"The input to the morpheme segmenter is a sequence of Arabic tokens – we use a tokenizer that looks only at white space and other punctuation, e.g. quotation marks, parentheses, period, comma, etc.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
A sample of a manually segmented corpus is given below2.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
Here multiple occurrences of prefixes and suffixes per word are marked with an underline.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
Many instances of prefixes and suffixes in Arabic are meaning bearing and correspond to a word in English such as pronouns and prepositions.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Therefore, we choose a segmentation into multiple prefixes and suffixes.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Segmentation into one prefix and one suffix per word, cf.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"(Darwish 2002), is not very useful for applications like statistical machine translation, (Brown et al. 1993), for which an accurate word-to-word alignment between the source and the target languages is critical for high quality translations.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"The trigram language model probabilities of morpheme sequences, p(mi|mi-1, mi-2), are estimated from the morpheme-segmented corpus.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"At token boundaries, the morphemes from previous tokens constitute the histories of the current morpheme in the trigram language model.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"The trigram model is smoothed using deleted interpolation with the bigram and unigram models, (Jelinek 1997), as in (1): w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# Awl fy jA}z +p Al# nmsA Al# EAm Al# mADy Ely syAr +p fyrAry $Er b# AlAm fy bTn +h ADTr +t +h Aly Al# AnsHAb mn Al# tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# fHwS +At Al# Drwry +p Hsb mA A$Ar fryq 2 A manually segmented Arabic corpus containing about 140K word tokens has been provided by LDC (http://www.ldc.upenn.edu).","{'title': '3 Morpheme Segmentation', 'number': '2'}"
We divided this corpus into training and the development test sets as described in Section 5.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
A small morpheme-segmented corpus results in a relatively high out of vocabulary rate for the stems.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
We describe below an unsupervised acquisition of new stems from a large unsegmented Arabic corpus.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
"However, we first describe the segmentation algorithm.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
We take the unit of decoding to be a sentence that has been tokenized using white space and punctuation.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
"The task of a decoder is to find the morpheme sequence which maximizes the trigram probability of the input sentence, as in (2): (2) SEGMENTATIONbest = Argmax IIi=1, N p(mi|mi-1mi-2), N = number of morphemes in the input.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
Search algorithm for (2) is informally described for each word token as follows: Step 1: Compute all possible segmentations of the token (to be elaborated in 3.2.1).,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
Step 2: Compute the trigram language model score of each segmentation.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
"For some segmentations of a token, the stem may be an out of vocabulary item.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"In that case, we use an “UNKNOWN” class in the trigram language model with the model probability given by p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where UNK_Fraction is 1e-9 determined on empirical grounds.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"This allows us to segment new words with a high accuracy even with a relatively high number of unknown stems in the language model vocabulary, cf. experimental results in Tables 5 & 6.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
Step 3: Keep the top N highest scored segmentations.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
Possible segmentations of a word token are restricted to those derivable from a table of prefixes and suffixes of the language for decoder speed-up and improved accuracy.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
Table 2 shows examples of atomic (e.g.,"{'title': '3 Morpheme Segmentation', 'number': '2'}"
"لا, تا) and multi-component (e.g.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"لﺎѧﺑو, ﺎѧﻬﺗا) prefixes and suffixes, along with their component morphemes in native Arabic.3 Each token is assumed to have the structure prefix*-stem-suffix*, and is compared against the prefix/suffix table for segmentation.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Given a word token, (i) identify all of the matching prefixes and suffixes from the table, (ii) further segment each matching prefix/suffix at each character position, and (iii) enumerate all prefix*-stem-suffix* sequences derivable from (i) and (ii).","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Table 3 shows all of its possible segmentations of the token ﺎهرﺮآاو (wAkrrhA; 'and I repeat it'),4 where 0 indicates the null prefix/suffix and the Seg Score is the language model probabilities of each segmentation S1 ... S12.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"For this token, there are two matching prefixes #و(w#) and #او(wA#) from the prefix table, and two matching suffixes ا+(+A) and ﺎه+(+hA) from the suffix table.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"S1, S2, & S3 are the segmentations given the null prefix 0 and suffixes 0, +A, +hA.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"S4, S5, & S6 are the segmentations given the prefix w# and suffixes 0, +A, +hA.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"S7, S8, & S9 are the segmentations given the prefix wA# and suffixes 0, +A, +hA.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"S10, S11, & S12 are the segmentations given the prefix sequence w# A# derived from the prefix wA# and suffixes 0, +A, +hA.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"As illustrated by S12, derivation of sub-segmentations of the matching prefixes/suffixes enables the system to identify possible segmentations which would have been missed otherwise.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"In this case, segmentation including the derived prefix sequence ﺎه+ رﺮآ #ا #و(n# A# krr +hA) happens to be the correct one.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"While the number of possible segmentations is maximized by sub-segmenting matching prefixes and suffixes, some of illegitimate subsegmentations are filtered out on the basis of the knowledge specific to the manually segmented corpus.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"For instance, subsegmentation of the suffix hA into +h +A is ruled out because there is no suffix sequence +h +A in the training corpus.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Likewise, subsegmentation of the prefix Al into A# l# is filtered out.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Filtering out improbable prefix/suffix sequences improves the segmentation accuracy, as shown in Table 5.","{'title': '3 Morpheme Segmentation', 'number': '2'}"
"Once the seed segmenter is developed on the basis of a manually segmented corpus, the performance may be improved by iteratively expanding the stem vocabulary and retraining the language model on a large automatically segmented Arabic corpus.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Given a small manually segmented corpus and a large unsegmented corpus, segmenter development proceeds as follows.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Initialization: Develop the seed segmenter Segmenter0 trained on the manually segmented corpus Corpus0, using the language model vocabulary, Vocab0, acquired from Corpus0.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Iteration: For i = 1 to N, N = the number of partitions of the unsegmented corpus Vocabi-1, creating an expanded vocabulary Vocabi. iii.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
Develop Segmenteri trained on Corpus0 through Corpusi with Vocabi.,"{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Optimal Performance Identification: Identify the Corpusi and Vocabi, which result in the best performance, i.e. system training with Corpusi+1 and Vocabi+1 does not improve the performance any more.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Unsupervised acquisition of new stems from an automatically segmented new corpus is a three-step process: (i) select new stem candidates on the basis of a frequency threshold, (ii) filter out new stem candidates containing a sub-string with a high likelihood of being a prefix, suffix, or prefix-suffix.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"The likelihood of a sub-string being a prefix, suffix, and prefix-suffix of a token is computed as in (5) to (7), (iii) further filter out new stem candidates on the basis of contextual information, as in (8).","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Stem candidates containing a sub-string with a high prefix, suffix, or prefix-suffix likelihood are filtered out.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"Example sub-strings with the prefix, suffix, prefix-suffix likelihood 0.85 or higher in a 110K word manually segmented corpus are given in Table 4.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"If a token starts with the sub-string ـﻨﺱ (sn), and end with ﺎﻬـ (hA), the sub-string's likelihood of being the prefix-suffix of the token is 1.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"If a token starts with the sub-string ﻞѧﻟ (ll), the sub-string's likelihood of being the prefix of the token is 0.945, etc.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"According to (8), if a stem is followed by a potential suffix +m, not present in the training corpus, then it is filtered out as an illegitimate stem.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"In addition, if a stem is preceded by a prefix and/or followed by a suffix with a significantly higher proportion than that observed in the training corpus, it is filtered out.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"For instance, the probability for the suffix +A to follow a stem is less than 50% in the training corpus regardless of the stem properties, and therefore, if a candidate stem is followed by +A with the probability of over 70%, e.g. mAnyl +A, then it is filtered out as an illegitimate stem.","{'title': '4 Unsupervised Acquisition of New Stems', 'number': '3'}"
"We present experimental results illustrating the impact of three factors on segmentation error rate: (i) the base algorithm, i.e. language model training and decoding, (ii) language model vocabulary and training corpus size, and (iii) manually segmented training corpus size.","{'title': '5 Performance Evaluations', 'number': '4'}"
Segmentation error rate is defined in (9).,"{'title': '5 Performance Evaluations', 'number': '4'}"
"Evaluations have been performed on a development test corpus containing 28,449 word tokens.","{'title': '5 Performance Evaluations', 'number': '4'}"
The test set is extracted from 20001115_AFP_ARB.0060.xml.txt through 20001115_AFP_ARB.0236.xml.txt of the LDC Arabic Treebank: Part 1 v 2.0 Corpus.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"Impact of the core algorithm and the unsupervised stem acquisition has been measured on segmenters developed from 4 different sizes of manually segmented seed corpora: 10K, 20K, 40K, and 110K words.","{'title': '5 Performance Evaluations', 'number': '4'}"
The experimental results are shown in Table 5.,"{'title': '5 Performance Evaluations', 'number': '4'}"
The baseline performances are obtained by assigning each token the most frequently occurring segmentation in the manually segmented training corpus.,"{'title': '5 Performance Evaluations', 'number': '4'}"
The column headed by '3-gram LM' indicates the impact of the segmenter using only trigram language model probabilities for decoding.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"Regardless of the manually segmented training corpus size, use of trigram language model probabilities reduces the word error rate of the corresponding baseline by approximately 50%.","{'title': '5 Performance Evaluations', 'number': '4'}"
The column headed by '3-gram LM + PS Filter' indicates the impact of the core algorithm plus Prefix-Suffix Filter discussed in Section 3.2.2.,"{'title': '5 Performance Evaluations', 'number': '4'}"
Prefix-Suffix Filter reduces the word error rate ranging from 7.4% for the smallest (10K word) manually segmented corpus to 21.8% for the largest (110K word) manually segmented corpus - around 1% absolute reduction for all segmenters.,"{'title': '5 Performance Evaluations', 'number': '4'}"
The column headed by '3-gram LM + PS Filter + New Stems' shows the impact of unsupervised stem acquisition from a 155 million word Arabic corpus.,"{'title': '5 Performance Evaluations', 'number': '4'}"
Word error rate reduction due to the unsupervised stem acquisition is 38% for the segmenter developed from the 10K word manually segmented corpus and 32% for the segmenter developed from 110K word manually segmented corpus.,"{'title': '5 Performance Evaluations', 'number': '4'}"
Language model vocabulary size (LM VOC Size) and the unknown stem ratio (OOV ratio) of various segmenters is given in Table 6.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"For unsupervised stem acquisition, we have set the frequency threshold at 10 for every 10-15 million word corpus, i.e. any new morphemes occurring more than 10 times in a 10-15 million word corpus are considered to be new stem candidates.","{'title': '5 Performance Evaluations', 'number': '4'}"
"Prefix, suffix, prefix-suffix likelihood score to further filter out illegitimate stem candidates was set at 0.5 for the segmenters developed from 10K, 20K, and 40K manually segmented corpora, whereas it was set at 0.85 for the segmenters developed from a 110K manually segmented corpus.","{'title': '5 Performance Evaluations', 'number': '4'}"
"Both the frequency threshold and the optimal prefix, suffix, prefix-suffix likelihood scores were determined on empirical grounds.","{'title': '5 Performance Evaluations', 'number': '4'}"
Contextual Filter stated in (8) has been applied only to the segmenter developed from 110K manually segmented training corpus.5 Comparison of Tables 5 and 6 indicates a high correlation between the segmentation error rate and the unknown stem ratio.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"Table 7 gives the error analyses of four segmenters according to three factors: (i) errors due to unknown stems, (ii) errors involving مﻮﻴѧﻟا (Alywm), and (iii) errors due to other factors.","{'title': '5 Performance Evaluations', 'number': '4'}"
"Interestingly, the segmenter developed from a 110K manually segmented corpus has the lowest percentage of “unknown stem” errors at 39.6% indicating that our unsupervised acquisition of new stems is working well, as well as suggesting to use a larger unsegmented corpus for unsupervised stem acquisition.","{'title': '5 Performance Evaluations', 'number': '4'}"
مﻮﻴѧﻟا (Alywm) should be segmented differently depending on its part-of-speech to capture the semantic ambiguities.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"If it is an adverb or a proper noun, it is segmented as مﻮﻴѧﻟا 'today/Al-Youm', whereas if it is a noun, it is segmented as مﻮѧﻳ #لا 'the day.'","{'title': '5 Performance Evaluations', 'number': '4'}"
"Proper segmentation of مﻮﻴѧﻟا primarily requires its part-of-speech information, and cannot be easily handled by morpheme trigram models alone.","{'title': '5 Performance Evaluations', 'number': '4'}"
Other errors include over-segmentation of foreign words such as ﻦѧﻴѧﺗﻮѧﺑ (bwtyn) as ب# ﻦѧﻴѧﺗو and ﺮѧﺘѧﻴѧﻟ (lytr) 'litre' as ﺮѧﺗ #ي #ل.,"{'title': '5 Performance Evaluations', 'number': '4'}"
These errors are attributed to the segmentation ambiguities of these tokens: ﻦѧﻴѧﺗﻮѧﺑ is ambiguous between 'ﻦѧﻴѧﺗﻮѧﺑ (Putin)' and 'ب# ﻦѧﻴѧﺗو (by aorta)'.,"{'title': '5 Performance Evaluations', 'number': '4'}"
ﺮѧﺘѧﻴѧﻟ is ambiguous between 'ﺮѧﺘѧﻴѧﻟ (litre)' and ' ﺮѧﺗ #ي #ل (for him to harm)'.,"{'title': '5 Performance Evaluations', 'number': '4'}"
These errors may also be corrected by incorporating part-of-speech information for disambiguation.,"{'title': '5 Performance Evaluations', 'number': '4'}"
"To address the segmentation ambiguity problem, as illustrated by ' ﻦѧﻴѧﺗﻮѧﺑ (Putin)' vs. ' ﻦѧﻴѧﺗو #ب (by aorta)', we have developed a joint model for segmentation and part-ofspeech tagging for which the best segmentation of an input sentence is obtained according to the formula (10), where ti is the part-of-speech of morpheme mi, and N is the number of morphemes in the input sentence.","{'title': '5 Performance Evaluations', 'number': '4'}"
"(10) SEGMENTATIONbest = Argmax Πi=1,N p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) By using the joint model, the segmentation word error rate of the best performing segmenter has been reduced by about 10% from 2.9% (cf. the last column of Table 5) to 2.6%.","{'title': '5 Performance Evaluations', 'number': '4'}"
"We have presented a robust word segmentation algorithm which segments a word into a prefix*-stem-suffix* sequence, along with experimental results.","{'title': '5 Summary and Future Work', 'number': '5'}"
"Our Arabic word segmentation system implementing the algorithm achieves around 97% segmentation accuracy on a development test corpus containing 28,449 word tokens.","{'title': '5 Summary and Future Work', 'number': '5'}"
"Since the algorithm can identify any number of prefixes and suffixes of a given token, it is generally applicable to various language families including agglutinative languages (Korean, Turkish, Finnish), highly inflected languages (Russian, Czech) as well as semitic languages (Arabic, Hebrew).","{'title': '5 Summary and Future Work', 'number': '5'}"
"Our future work includes (i) application of the current technique to other highly inflected languages, (ii) application of the unsupervised stem acquisition technique on about 1 billion word unsegmented Arabic corpus, and (iii) adoption of a novel morphological analysis technique to handle irregular morphology, as realized in Arabic broken plurals YL+S (ktAb) 'book' vs. ��„�< (ktb) 'books'.","{'title': '5 Summary and Future Work', 'number': '5'}"
This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.,"{'title': 'Acknowledgment', 'number': '6'}"
N66001-99-2-8916.,"{'title': 'Acknowledgment', 'number': '6'}"
The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.,"{'title': 'Acknowledgment', 'number': '6'}"
"We would like to thank Martin Franz for discussions on language model building, and his help with the use of ViaVoice language model toolkit.","{'title': 'Acknowledgment', 'number': '6'}"
