col1,col2
We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions.,{}
Our method learns vector space representations for multi-word phrases.,{}
"In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules.",{}
We also evaluate the model’s ability to predict sentiment distributions on a new dataset based on confessions from the experience project.,{}
"The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions.",{}
Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines.,{}
"The ability to identify sentiments about personal experiences, products, movies etc. is crucial to understand user generated content in social networks, blogs or product reviews.","{'title': '1 Introduction', 'number': '1'}"
"Detecting sentiment in these data is a challenging task which has recently spawned a lot of interest (Pang and Lee, 2008).","{'title': '1 Introduction', 'number': '1'}"
"Current baseline methods often use bag-of-words representations which cannot properly capture more complex linguistic phenomena in sentiment analysis (Pang et al., 2002).","{'title': '1 Introduction', 'number': '1'}"
"For instance, while the two phrases “white blood cells destroying an infection” and “an infection destroying white blood cells” have the same bag-of-words representation, the former is a positive reaction while the later is very negative.","{'title': '1 Introduction', 'number': '1'}"
"More advanced methods such as (Nakagawa et al., tecture which learns semantic vector representations of phrases.","{'title': '1 Introduction', 'number': '1'}"
Word indices (orange) are first mapped into a semantic vector space (blue).,"{'title': '1 Introduction', 'number': '1'}"
Then they are recursively merged by the same autoencoder network into a fixed length sentence representation.,"{'title': '1 Introduction', 'number': '1'}"
The vectors at each node are used as features to predict a distribution over sentiment labels.,"{'title': '1 Introduction', 'number': '1'}"
"2010) that can capture such phenomena use many manually constructed resources (sentiment lexica, parsers, polarity-shifting rules).","{'title': '1 Introduction', 'number': '1'}"
This limits the applicability of these methods to a broader range of tasks and languages.,"{'title': '1 Introduction', 'number': '1'}"
"Lastly, almost all previous work is based on single, positive/negative categories or scales such as star ratings.","{'title': '1 Introduction', 'number': '1'}"
"Examples are movie reviews (Pang and Lee, 2005), opinions (Wiebe et al., 2005), customer reviews (Ding et al., 2008) or multiple aspects of restaurants (Snyder and Barzilay, 2007).","{'title': '1 Introduction', 'number': '1'}"
Such a one-dimensional scale does not accurately reflect the complexity of human emotions and sentiments.,"{'title': '1 Introduction', 'number': '1'}"
"In this work, we seek to address three issues.","{'title': '1 Introduction', 'number': '1'}"
"(i) Instead of using a bag-of-words representation, our model exploits hierarchical structure and uses compositional semantics to understand sentiment.","{'title': '1 Introduction', 'number': '1'}"
"(ii) Our system can be trained both on unlabeled domain data and on supervised sentiment data and does not require any language-specific sentiment lexica, Sorry, Hugs You Rock Teehee I Understand Wow, Just Wow i walked into a parked car parsers, etc.","{'title': '1 Introduction', 'number': '1'}"
"(iii) Rather than limiting sentiment to a positive/negative scale, we predict a multidimensional distribution over several complex, interconnected sentiments.","{'title': '1 Introduction', 'number': '1'}"
"We introduce an approach based on semisupervised, recursive autoencoders (RAE) which use as input continuous word vectors.","{'title': '1 Introduction', 'number': '1'}"
Fig.,"{'title': '1 Introduction', 'number': '1'}"
1 shows an illustration of the model which learns vector representations of phrases and full sentences as well as their hierarchical structure from unsupervised text.,"{'title': '1 Introduction', 'number': '1'}"
We extend our model to also learn a distribution over sentiment labels at each node of the hierarchy.,"{'title': '1 Introduction', 'number': '1'}"
We evaluate our approach on several standard datasets where we achieve state-of-the art performance.,"{'title': '1 Introduction', 'number': '1'}"
"Furthermore, we show results on the recently introduced experience project (EP) dataset (Potts, 2010) that captures a broader spectrum of human sentiments and emotions.","{'title': '1 Introduction', 'number': '1'}"
The dataset consists of very personal confessions anonymously made by people on the experience project website www.experienceproject.com.,"{'title': '1 Introduction', 'number': '1'}"
Confessions are labeled with a set of five reactions by other users.,"{'title': '1 Introduction', 'number': '1'}"
"Reaction labels are you rock (expressing approvement), tehee (amusement), I understand, Sorry, hugs and Wow, just wow (displaying shock).","{'title': '1 Introduction', 'number': '1'}"
For evaluation on this dataset we predict both the label with the most votes as well as the full distribution over the sentiment categories.,"{'title': '1 Introduction', 'number': '1'}"
On both tasks our model outperforms competitive baselines.,"{'title': '1 Introduction', 'number': '1'}"
"A set of over 31,000 confessions as well as the code of our model are available at www.socher.org.","{'title': '1 Introduction', 'number': '1'}"
"After describing the model in detail, we evaluate it qualitatively by analyzing the learned n-gram vector representations and compare quantitatively against other methods on standard datasets and the EP dataset.","{'title': '1 Introduction', 'number': '1'}"
Our model aims to find vector representations for variable-sized phrases in either unsupervised or semi-supervised training regimes.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
These representations can then be used for subsequent tasks.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We first describe neural word representations and then proceed to review a related recursive model based on autoencoders, introduce our recursive autoencoder (RAE) and describe how it can be modified to jointly learn phrase representations, phrase structure and sentiment distributions.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We represent words as continuous vectors of parameters.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We explore two settings.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In the first setting we simply initialize each word vector x E Rn by sampling it from a zero mean Gaussian distribution: x — N(0, U2).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"These word vectors are then stacked into a word embedding matrix L E Rn×|V |, where |V  |is the size of the vocabulary.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This initialization works well in supervised settings where a network can subsequently modify these vectors to capture certain label distributions.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In the second setting, we pre-train the word vectors with an unsupervised neural language model (Bengio et al., 2003; Collobert and Weston, 2008).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
These models jointly learn an embedding of words into a vector space and use these vectors to predict how likely a word occurs given its context.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
After learning via gradient ascent the word vectors capture syntactic and semantic information from their co-occurrence statistics.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
In both cases we can use the resulting matrix of word vectors L for subsequent tasks as follows.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Assume we are given a sentence as an ordered list of m words.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Each word has an associated vocabulary index k into the embedding matrix which we use to retrieve the word’s vector representation.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Mathematically, this look-up operation can be seen as a simple projection layer where we use a binary vector b which is zero in all positions except at the kth index, In the remainder of this paper, we represent a sentence (or any n-gram) as an ordered list of these vectors (x1, ... , xm).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"This word representation is better suited to autoencoders than the binary number representations used in previous related autoencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The goal of autoencoders is to learn a representation of their inputs.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
In this section we describe how to obtain a reduced dimensional vector representation for sentences.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
In the past autoencoders have only been used in setting where the tree structure was given a-priori.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We review this setting before continuing with our model which does not require a given tree structure.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Fig.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
2 shows an instance of a recursive autoencoder (RAE) applied to a given tree.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Assume we are given a list of word vectors x = (x1,... , xm) as described in the previous section as well as a binary tree structure for this input in the form of branching triplets of parents with children: (p -+ c1c2).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Each child can be either an input word vector xi or a nonterminal node in the tree.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
For the example in Fig.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"2, we have the following triplets: ((y1 -+ x3x4), (y2 -+ x2y1), (y1 -+ x1y2)).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In order to be able to apply the same neural network to each pair of children, the hidden representations yi have to have the same dimensionality as the xi’s.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Given this tree structure, we can now compute the parent representations.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"The first parent vector y1 is computed from the children (c1, c2) = (x3, x4): where we multiplied a matrix of parameters W (1) E Rnx2n by the concatenation of the two children.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
After adding a bias term we applied an elementwise activation function such as tanh to the resulting vector.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"One way of assessing how well this ndimensional vector represents its children is to try to reconstruct the children in a reconstruction layer: During training, the goal is to minimize the reconstruction errors of this input pair.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For each pair, we compute the Euclidean distance between the original input and its reconstruction: This model of a standard autoencoder is boxed in Fig.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
2.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Now that we have defined how an autoencoder can be used to compute an n-dimensional vector representation (p) of two n-dimensional children (c1, c2), we can describe how such a network can be used for the rest of the tree.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Essentially, the same steps repeat.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Now that y1 is given, we can use Eq.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"2 to compute y2 by setting the children to be (c1, c2) = (x2, y1).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Again, after computing the intermediate parent vector y2, we can assess how well this vector capture the content of the children by computing the reconstruction error as in Eq.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
4.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The process repeat until the full tree is constructed and we have a reconstruction error at each nonterminal node.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"This model is similar to the RAAM model (Pollack, 1990) which also requires a fixed tree structure.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Now, assume there is no tree structure given for the input vectors in x.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The goal of our structureprediction RAE is to minimize the reconstruction error of all vector pairs of children in a tree.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We define A(x) as the set of all possible trees that can be built from an input sentence x.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Further, let T (y) be a function that returns the triplets of a tree indexed by s of all the non-terminal nodes in a tree.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Using the reconstruction error of Eq.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"4, we compute We now describe a greedy approximation that constructs such a tree.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Greedy Unsupervised RAE.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For a sentence with m words, we apply the autoencoder recursively.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"It takes the first pair of neighboring vectors, defines them as potential children of a phrase (c1; c2) = (x1; x2), concatenates them and gives them as input to the autoencoder.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For each word pair, we save the potential parent node p and the resulting reconstruction error.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"After computing the score for the first pair, the network is shifted by one position and takes as input vectors (c1, c2) = (x2, x3) and again computes a potential parent node and a score.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"This process repeats until it hits the last pair of words in the sentence: (c1, c2) = (xm−1, xm).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Next, it selects the pair which had the lowest reconstruction error (ETec) and its parent representation p will represent this phrase and replace both children in the sentence word list.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For instance, consider the sequence (x1, x2, x3, x4) and assume the lowest ETec was obtained by the pair (x3, x4).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"After the first pass, the new sequence then consists of (x1, x2, p(3,4)).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"The process repeats and treats the new vector p(3,4) like any other input vector.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For instance, subsequent states could be either: (x1,p(2,(3,4))) or (p(1,2),p(3,4)).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Both states would then finish with a deterministic choice of collapsing the remaining two states into one parent to obtain (p(1,(2,(3,4)))) or (p((1,2),(3,4))) respectively.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The tree is then recovered by unfolding the collapsing decisions.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The resulting tree structure captures as much of the single-word information as possible (in order to allow reconstructing the word vectors) but does not necessarily follow standard syntactic constraints.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We also experimented with a method that finds better solutions to Eq.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"5 based on CKY-like beam search algorithms (Socher et al., 2010; Socher et al., 2011) but the performance is similar and the greedy version is much faster.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Weighted Reconstruction.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
One problem with simply using the reconstruction error of both children equally as describe in Eq.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
4 is that each child could represent a different number of previously collapsed words and is hence of bigger importance for the overall meaning reconstruction of the sentence.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For instance in the case of (x1,p(2,(3,4))) one would like to give more importance to reconstructing p than x1.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We capture this desideratum by adjusting the reconstruction error.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Let n1, n2 be the number of words underneath a current potential child, we re-define the reconstruction error to be Length Normalization.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
One of the goals of RAEs is to induce semantic vector representations that allow us to compare n-grams of different lengths.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The RAE tries to lower reconstruction error of not only the bigrams but also of nodes higher in the tree.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Unfortunately, since the RAE computes the hidden representations it then tries to reconstruct, it can just lower reconstruction error by making the hidden layer very small in magnitude.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"To prevent such undesirable behavior, we modify the hidden layer such that the resulting parent representation always has length one, after computing p as in Eq.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"2, we simply set: p = p ||p||.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"So far, the RAE was completely unsupervised and induced general representations that capture the semantics of multi-word phrases.In this section, we extend RAEs to a semi-supervised setting in order to predict a sentence- or phrase-level target distribution t.1 One of the main advantages of the RAE is that each node of the tree built by the RAE has associated with it a distributed vector representation (the parent vector p) which could also be seen as features describing that phrase.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We can leverage this representation by adding on top of each parent node a simple softmax layer to predict class distributions: Assuming there are K labels, d E RK is a K-dimensional multinomial distribution and P k=1 dk = 1.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Fig.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
3 shows such a semi-supervised RAE unit.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Let tk be the kth element of the multinomial target label distribution t for one entry.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"The softmax layer’s outputs are interpreted as conditional probabilities dk = p(kJ[c1; c2]), hence the cross-entropy error is 1For the binary label classification case, the distribution is of the form [1, 0] for class 1 and [0, 1] for class 2.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Using this cross-entropy error for the label and the reconstruction error from Eq.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"6, the final semisupervised RAE objective over (sentences,label) pairs (x, t) in a corpus becomes where we have an error for each entry in the training set that is the sum over the error at the nodes of the tree that is constructed by the greedy RAE: Let θ = (W (1), b(1), W(2), b(1), Wlabel, L) be the set of our model parameters, then the gradient becomes: To compute this gradient, we first greedily construct all trees and then derivatives for these trees are computed efficiently via backpropagation through structure (Goller and K¨uchler, 1996).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Because the algorithm is greedy and the derivatives of the supervised cross-entropy error also modify the matrix W(1), this objective is not necessarily continuous and a step in the gradient descent direction may not necessarily decrease the objective.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"However, we found that L-BFGS run over the complete training data (batch mode) to minimize the objective works well in practice, and that convergence is smooth, with the algorithm typically finding a good solution quickly.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"The error at each nonterminal node is the weighted sum of reconstruction and cross-entropy errors, The hyperparameter α weighs reconstruction and cross-entropy error.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"When minimizing the crossentropy error of this softmax layer, the error will backpropagate and influence both the RAE parameters and the word representations.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Initially, words such as good and bad have very similar representations.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This is also the case for Brown clusters and other methods that use only cooccurrence statistics in a small window around each word.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"When learning with positive/negative sentiment, the word embeddings get modified and capture less syntactic and more sentiment information.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In order to predict the sentiment distribution of a sentence with this model, we use the learned vector representation of the top tree node and train a simple logistic regression classifier.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We first describe the new experience project (EP) dataset, results of standard classification tasks on this dataset and how to predict its sentiment label distributions.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We then show results on other commonly used datasets and conclude with an analysis of the important parameters of the model.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In all experiments involving our model, we represent words using 100-dimensional word vectors.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We explore the two settings mentioned in Sec.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
2.1.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We compare performance on standard datasets when using randomly initialized word vectors (random word init.),"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
or word vectors trained by the model of Collobert and Weston (2008) and provided by Turian et al. (2010).2 These vectors were trained on an unlabeled corpus of the English Wikipedia.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Note that alternatives such as Brown clusters are not suitable since they do not capture sentiment information (good and bad are usually in the same cluster) and cannot be modified via backpropagation.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The confessions section of the experience project website3 lets people anonymously write short personal stories or “confessions”.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Once a story is on the site, each user can give a single vote to one of five label categories (with our interpretation): The EP dataset has 31,676 confession entries, a total number of 74,859 votes for the 5 labels above, the average number of votes per entry is 2.4 (with a variance of 33).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For the five categories, the numbers of votes are [14, 816;13, 325;10, 073; 30, 844; 5, 801].","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Since an entry with less than 4 votes is not very well identified, we train and test only on entries with at least 4 total votes.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"There are 6,129 total such entries.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The distribution over total votes in the 5 classes is similar: [0.22; 0.2; 0.11; 0.37; 0.1].,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The average length of entries is 129 words.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Some entries contain multiple sentences.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In these cases, we average the predicted label distributions from the sentences.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Table 1 shows statistics of this and other commonly used sentiment datasets (which we compare on in later experiments).,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Table 2 shows example entries as well as gold and predicted label distributions as described in the next sections.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Compared to other datasets, the EP dataset contains a wider range of human emotions that goes far beyond positive/negative product or movie reviews.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Each item is labeled with a multinomial distribution over interconnected response categories.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This is in contrast to most other datasets (including multiaspect rating) where several distinct aspects are rated independently but on the same scale.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"The topics range from generic happy statements, daily clumsiness reports, love, loneliness, to relationship abuse and suicidal notes.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"As is evident from the total number of label votes, the most common user reaction is one of empathy and an ability to relate to the authors experience.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"However, some stories describe horrible scenarios that are not common and hence receive more offers of condolence.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
In the following sections we show some examples of stories with predicted and true distributions but refrain from listing the most horrible experiences.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For all experiments on the EP dataset, we split the data into train (49%), development (21%) and test data (30%).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
The first task for our evaluation on the EP dataset is to simply predict the single class that receives the most votes.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In order to compare our novel joint phrase representation and classifier learning framework to traditional methods, we use the following baselines: Random Since there are five classes, this gives 20% accuracy.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Most Frequent Selecting the class which most frequently has the most votes (the class I understand).,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Baseline 1: Binary BoW This baseline uses logistic regression on binary bag-of-word representations that are 1 if a word is present and 0 otherwise.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Baseline 2: Features This model is similar to traditional approaches to sentiment classification in that it uses many hand-engineered resources.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We first used a spell-checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer (Stone, 1966) and LIWC (Pennebaker et al., 2007).","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Lastly, we used tf-idf weighting on the bag-of-word representations and trained an SVM.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
KL Predicted&Gold V. Entry (Shortened if it ends with ...) .03 .16 .16 .16 .33 .16 6 I reguarly shoplift.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"I got caught once and went to jail, but I’ve found that this was not a deterrent.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"I don’t buy groceries, I don’t buy school supplies for my kids, I don’t buy gifts for my kids, we don’t pay for movies, and I dont buy most incidentals for the house (cleaning supplies, toothpaste, etc.)...","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
".03 .38 .04 .06 .35 .14 165 i am a very succesfull buissnes man.i make good money but i have been addicted to crack for 13 years.i moved 1 hour away from my dealers 10 years ago to stop using now i dont use daily but once a week usally friday nights. i used to use 1 or 2 hundred a day now i use 4 or 5 hundred on a friday.my problem is i am a funcational addict... .05 .14 .28 .14 .28 .14 7 Hi there, Im a guy that loves a girl, the same old bloody story...","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"I met her a while ago, while studying, she Is so perfect, so mature and yet so lonely, I get to know her and she get ahold of me, by opening her life to me and so did I with her, she has been the first person, male or female that has ever made that bond with me,... .07 .27 .18 .00 .45 .09 11 be kissing you right now. i should be wrapped in your arms in the dark, but instead i’ve ruined everything. i’ve piled bricks to make a wall where there never should have been one. i feel an ache that i shouldn’t feel because i’ve never had you close enough. we’ve never touched, but i still feel as though a part of me is missing.... .05 23 Dear Love, I just want to say that I am looking for you.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Tonight I felt the urge to write, and I am becoming more and more frustrated that I have not found you yet.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
I’m also tired of spending so much heart on an old dream.... .05 5 I wish I knew somone to talk to here.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
.06 24 I loved her but I screwed it up.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Now she’s moved on.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
I’ll never have her again.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
I don’t know if I’ll ever stop thinking about her.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
.06 5 i am 13 years old and i hate my father he is alwas geting drunk and do’s not care about how it affects me or my sisters i want to care but the truthis i dont care if he dies .13 6 well i think hairy women are attractive .35 5 As soon as I put clothings on I will go down to DQ and get a thin mint blizzard.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
I need it.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"It’ll make my soul feel a bit better :) .36 6 I am a 45 year old divoced woman, and I havent been on a date or had any significant relationship in 12 years...yes, 12 yrs. the sad thing is, Im not some dried up old granny who is no longer interested in men, I just can’t meet men.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"(before you judge, no Im not terribly picky!)","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
What is wrong with me?,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
.63 6 When i was in kindergarden i used to lock myself in the closet and eat all the candy.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Then the teacher found out it was one of us and made us go two days without freetime.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"It might be a little late now, but sorry guys it was me haha .92 4 My paper is due in less than 24 hours and I’m still dancing round my room!","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Baseline 3: Word Vectors We can ignore the RAE tree structure and only train softmax layers directly on the pre-trained words in order to influence the word vectors.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This is followed by an SVM trained on the average of the word vectors.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We also experimented with latent Dirichlet allocation (Blei et al., 2003) but performance was very low.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Table 3 shows the results for predicting the class with the most votes.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Even the approach that is based on sentiment lexica and other resources is outperformed by our model by almost 3%, showing that for tasks involving complex broad-range human sentiment, the often used sentiment lexica lack in coverage and traditional bag-of-words representations are not powerful enough.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We now turn to evaluating our distributionprediction approach.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In both this and the previous maximum label task, we backprop using the gold multinomial distribution as a target.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Since we maximize likelihood and because we want to predict a distribution that is closest to the distribution of labels that people would assign to a story, we evaluate using KL divergence: KL(g||p) = Ei gi log(gi/pi), where g is the gold distribution and p is the predicted one.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We report the average KL divergence, where a smaller value indicates better predictive power.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"To get an idea of the values of KL divergence, predicting random distributions gives a an average of 1.2 in KL divergence, predicting simply the average distribution in the training data give 0.83.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Fig.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
4 shows that our RAE-based model outperforms the other baselines.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"We compare to the state-of-the-art system of (Nakagawa et al., 2010), a dependency tree based classification method that uses CRFs with hidden variables.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We use the same training and testing regimen (10-fold cross validation) as well as their baselines: majority phrase voting using sentiment and reversal lexica; rule-based reversal using a dependency tree; Bag-of-Features and their full Tree-CRF model.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"As shown in Table 4, our algorithm outperforms their approach on both datasets.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"For the movie review (MR) data set, we do not use any handdesigned lexica.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
An error analysis on the MPQA dataset showed several cases of single words which never occurred in the training set.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Correctly classifying these instances can only be the result of having them in the original sentiment lexicon.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"Hence, for the experiment on MPQA we added the same sentiment lexicon that (Nakagawa et al., 2010) used in their system to our training set.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This improved accuracy from 86.0 to 86.4.Using the pre-trained word vectors boosts performance by less than 1% compared to randomly initialized word vectors (setting: random word init).,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This shows that our method can work well even in settings with little training data.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
We visualize the semantic vectors that the recursive autoencoder learns by listing n-grams that give the highest probability for each polarity.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Table 5 shows such n-grams for different lengths when the RAE is trained on the movie review polarity dataset.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"On a 4-core machine, training time for the smaller corpora such as the movie reviews takes around 3 hours and for the larger EP corpus around 12 hours until convergence.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Testing of hundreds of movie reviews takes only a few seconds.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
"In this experiment, we show how the hyperparameter α influences accuracy on the development set of one of the cross-validation splits of the MR dataset.","{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
This parameter essentially trade-off the supervised and unsupervised parts of the objective.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Fig.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
5 shows that a larger focus on the supervised objective is important but that a weight of α = 0.2 for the reconstruction error prevents overfitting and achieves the highest performance.,"{'title': '2 Semi-Supervised Recursive Autoencoders', 'number': '2'}"
Autoencoders are neural networks that learn a reduced dimensional representation of fixed-size inputs such as image patches or bag-of-word representations of text documents.,"{'title': '5 Related Work', 'number': '3'}"
They can be used to efficiently learn feature encodings which are useful for classification.,"{'title': '5 Related Work', 'number': '3'}"
"Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives.","{'title': '5 Related Work', 'number': '3'}"
The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990).,"{'title': '5 Related Work', 'number': '3'}"
"Pollack’s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model.","{'title': '5 Related Work', 'number': '3'}"
"However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure.","{'title': '5 Related Work', 'number': '3'}"
"More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that is able to better generalize to novel combinations of previously seen constituents.","{'title': '5 Related Work', 'number': '3'}"
One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec.,"{'title': '5 Related Work', 'number': '3'}"
2.1.,"{'title': '5 Related Work', 'number': '3'}"
"Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.","{'title': '5 Related Work', 'number': '3'}"
Their models are applicable to natural language and computer vision tasks such as parsing or object detection.,"{'title': '5 Related Work', 'number': '3'}"
The current work is related in that it uses a recursive deep learning model.,"{'title': '5 Related Work', 'number': '3'}"
"However, RNNs require labeled tree structures and use a supervised score at each node.","{'title': '5 Related Work', 'number': '3'}"
"Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible.","{'title': '5 Related Work', 'number': '3'}"
The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors.,"{'title': '5 Related Work', 'number': '3'}"
"Other recent deep learning methods for sentiment analysis include (Maas et al., 2011).","{'title': '5 Related Work', 'number': '3'}"
Pang et al. (2002) were one of the first to experiment with sentiment classification.,"{'title': '5 Related Work', 'number': '3'}"
"They show that simple bag-of-words approaches based on Naive Bayes, MaxEnt models or SVMs are often insufficient for predicting sentiment of documents even though they work well for general topic-based document classification.","{'title': '5 Related Work', 'number': '3'}"
"Even adding specific negation words, bigrams or part-of-speech information to these models did not add significant improvements.","{'title': '5 Related Work', 'number': '3'}"
"Other document-level sentiment work includes (Turney, 2002; Dave et al., 2003; Beineke et al., 2004; Pang and Lee, 2004).","{'title': '5 Related Work', 'number': '3'}"
"For further references, see (Pang and Lee, 2008).","{'title': '5 Related Work', 'number': '3'}"
"Instead of document level sentiment classification, (Wilson et al., 2005) analyze the contextual polarity of phrases and incorporate many well designed features including dependency trees.","{'title': '5 Related Work', 'number': '3'}"
They also show improvements by first distinguishing between neutral and polar sentences.,"{'title': '5 Related Work', 'number': '3'}"
Our model naturally incorporates the recursive interaction between context and polarity words in sentences in a unified framework while simultaneously learning the necessary features to make accurate predictions.,"{'title': '5 Related Work', 'number': '3'}"
"Other approaches for sentence-level sentiment detection include (Yu and Hatzivassiloglou, 2003; Grefenstette et al., 2004; Ikeda et al., 2008).","{'title': '5 Related Work', 'number': '3'}"
"Most previous work is centered around a given sentiment lexicon or building one via heuristics (Kim and Hovy, 2007; Esuli and Sebastiani, 2007), manual annotation (Das and Chen, 2001) or machine learning techniques (Turney, 2002).","{'title': '5 Related Work', 'number': '3'}"
"In contrast, we do not require an initial or constructed sentiment lexicon of positive and negative words.","{'title': '5 Related Work', 'number': '3'}"
"In fact, when training our approach on documents or sentences, it jointly learns such lexica for both single words and n-grams (see Table 5).","{'title': '5 Related Work', 'number': '3'}"
"(Mao and Lebanon, 2007) propose isotonic conditional random fields and differentiate between local, sentence-level and global, document-level sentiment.","{'title': '5 Related Work', 'number': '3'}"
"The work of (Polanyi and Zaenen, 2006; Choi and Cardie, 2008) focuses on manually constructing several lexica and rules for both polar words and related content-word negators, such as “prevent cancer”, where prevent reverses the negative polarity of cancer.","{'title': '5 Related Work', 'number': '3'}"
Like our approach they capture compositional semantics.,"{'title': '5 Related Work', 'number': '3'}"
"However, our model does so without manually constructing any rules or lexica.","{'title': '5 Related Work', 'number': '3'}"
"Recently, (Velikovich et al., 2010) showed how to use a seed lexicon and a graph propagation framework to learn a larger sentiment lexicon that also includes polar multi-word phrases such as “once in a life time”.","{'title': '5 Related Work', 'number': '3'}"
While our method can also learn multiword phrases it does not require a seed set or a large web graph.,"{'title': '5 Related Work', 'number': '3'}"
"(Nakagawa et al., 2010) introduced an approach based on CRFs with hidden variables with very good performance.","{'title': '5 Related Work', 'number': '3'}"
We compare to their stateof-the-art system.,"{'title': '5 Related Work', 'number': '3'}"
"We outperform them on the standard corpora that we tested on without requiring external systems such as POS taggers, dependency parsers and sentiment lexica.","{'title': '5 Related Work', 'number': '3'}"
Our approach jointly learns the necessary features and tree structure.,"{'title': '5 Related Work', 'number': '3'}"
"In multi-aspect rating (Snyder and Barzilay, 2007) one finds several distinct aspects such as food or service in a restaurant and then rates them on a fixed linear scale such as 1-5 stars, where all aspects could obtain just 1 star or all aspects could obtain 5 stars independently.","{'title': '5 Related Work', 'number': '3'}"
"In contrast, in our method a single aspect (a complex reaction to a human experience) is predicted not in terms of a fixed scale but in terms of a multinomial distribution over several interconnected, sometimes mutually exclusive emotions.","{'title': '5 Related Work', 'number': '3'}"
A single story cannot simultaneously obtain a strong reaction in different emotional responses (by virtue of having to sum to one).,"{'title': '5 Related Work', 'number': '3'}"
We presented a novel algorithm that can accurately predict sentence-level sentiment distributions.,"{'title': '6 Conclusion', 'number': '4'}"
"Without using any hand-engineered resources such as sentiment lexica, parsers or sentiment shifting rules, our model achieves state-of-the-art performance on commonly used sentiment datasets.","{'title': '6 Conclusion', 'number': '4'}"
"Furthermore, we introduce a new dataset that contains distributions over a broad range of human emotions.","{'title': '6 Conclusion', 'number': '4'}"
Our evaluation shows that our model can more accurately predict these distributions than other models.,"{'title': '6 Conclusion', 'number': '4'}"
We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.,"{'title': 'Acknowledgments', 'number': '5'}"
FA8750-09-C-0181.,"{'title': 'Acknowledgments', 'number': '5'}"
"Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of DARPA, AFRL, or the US government.","{'title': 'Acknowledgments', 'number': '5'}"
This work was also supported in part by the DARPA Deep Learning program under contract number FA8650-10-C-7020.,"{'title': 'Acknowledgments', 'number': '5'}"
"We thank Chris Potts for help with the EP data set, Raymond Hsu, Bozhi See, and Alan Wu for letting us use their system as a baseline and Jiquan Ngiam, Quoc Le, Gabor Angeli and Andrew Maas for their feedback.","{'title': 'Acknowledgments', 'number': '5'}"
