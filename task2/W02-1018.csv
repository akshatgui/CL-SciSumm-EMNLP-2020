col1,col2
"We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora.",Introduction
Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.,Introduction
"1 Motivation Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.",Introduction
"In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).",Introduction
The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.,Introduction
"A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.",Introduction
"Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL.",Introduction
We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.,Introduction
Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.,Introduction
Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.,Introduction
"Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.",Introduction
"Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.",Introduction
"Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.",Introduction
"Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the probabilities shown in Figure Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).",Introduction
"IBM−4 T−Table IBM−4 Intuitive Joint Joint T−Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities.",Introduction
(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).),Introduction
"In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.",Introduction
"In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.",Introduction
"In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.",Introduction
The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.,Introduction
"In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).",Introduction
We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).,Introduction
We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.,Introduction
"2 A Phrase-Based Joint Probability Model 2.1 Model 1 In developing our joint probability model, we started out with a very simple generative story.",Introduction
We assume that each sentence pair in our corpus is generated by the following stochastic process: 1.,Introduction
Generate a bag of concepts.,Introduction
2.,Introduction
"For each concept , generate a pair of phrases , according to the distribution contain at least one word.",Introduction
3.,Introduction
Order the phrases generated in each language so as to create two linear sequences of phrases; these sequences correspond to the sentence pairs in a bilingual corpus.,Introduction
"For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.",Introduction
"We do not assume that is a hidden variable that generates pair , but rather that .",Introduction
"Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts given by the product of all phrase-tophrase translation probabilities, yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.",Introduction
"However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.",Introduction
"Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.",Introduction
"We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize concepts .",Introduction
We denote this property using the predicate .,Introduction
"Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can linearized to (E, F).",Introduction
"(1) 2.2 Model 2 Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.",Introduction
"However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.",Introduction
"In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.",Introduction
The generative story of Model 2 is this: 1.,Introduction
Generate a bag of concepts.,Introduction
2.,Introduction
Initialize E and F to empty sequences.,Introduction
3.,Introduction
"Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word.",Introduction
Remove then from.,Introduction
4.,Introduction
Append phraseat the end of F. Letbe the start position ofin F. 5.,Introduction
"Insert phraseat positionin E provided that no other phrase occupies any of the positions betweenand , wheregives length of the phrase.",Introduction
We hence create the alignment between the two phrasesand with probability is a position-based distortion distribution.,Introduction
6.,Introduction
Repeat steps 3 to 5 untilis empty.,Introduction
"In Model 2, the probability to generate a sentence pair (E, F) is given by formula (2), where the position of wordof phrasein sen- F and denotes the position in tence E of the center of mass of phrase.",Introduction
"(2) Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.",Introduction
We have tried many types of distortion models.,Introduction
We eventually settled for the model discussed here because it produces better translations during decoding.,Introduction
"Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.",Introduction
3 Training Training the models described in Section 2 is computationally challenging.,Introduction
"Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1.",Introduction
Determine high-frequency ngrams in the bilingual corpus.,Introduction
2.,Introduction
Initialize the t-distribution table.,Introduction
3.,Introduction
"Apply EM training on the Viterbi alignments, while using smoothing.",Introduction
4.,Introduction
Generate conditional model probabilities.,Introduction
Figure 2: Training algorithm for the phrase-based joint probability model.,Introduction
EM training algorithm exhaustively.,Introduction
"To estimate the parameters of our model, we apply the algorithm in Figure 2, whose steps are motivated and described below.",Introduction
"3.1 Determine high-frequency n-grams in E and F If one assumes from the outset that any phrases can be generated from a cept , one would need a supercomputer in order to store in the memory a table that models the distribution.",Introduction
"Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.",Introduction
"Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.",Introduction
"In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.",Introduction
"3.2 Initialize the t-distribution table Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.",Introduction
"In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.",Introduction
"Under these conditions, the evidence that a sentence pair (E, F) contributes to fact that are generated by the same cept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase sentence F divided by the total number of alignments that can be built between the two sentences.",Introduction
Both these numbers can be easily approximated.,Introduction
"Given a sentence E ofwords, there are ways in which thewords can be partitioned into sets/concepts, where is the ling number of second kind.",Introduction
There are also ways in which the words a sentence F can be partitioned into nonempty sets.,Introduction
"Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.",Introduction
"When a concept generates two phrases of lengthand, respectively, there are only and words left to link.",Introduction
"Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).",Introduction
Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.,Introduction
"However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.",Introduction
"In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.",Introduction
We sum over all these t-counts and we normalize to obtain an initial joint distribution.,Introduction
This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.,Introduction
"3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.",Introduction
"Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.",Introduction
"We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.",Introduction
We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.,Introduction
We apply this Viterbi-based EM training procedure for a few iterations.,Introduction
The first iterations estimate the alignment probabilities using Model 1.,Introduction
The rest of the iterations estimate the alignment probabilities using Model 2.,Introduction
"During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.",Introduction
"3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand .",Introduction
This yields conditional probability distributions and which we use for decoding.,Introduction
"3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.",Introduction
"At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.",Introduction
"However, note that the choice made by our model is quite reasonable.",Introduction
"After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.",Introduction
"The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.",Introduction
"Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.",Introduction
"The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.",Introduction
The conditional distribution mirrors perfectly our intuitions.,Introduction
"4 Decoding For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).",Introduction
"Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat the probability .",Introduction
We then tively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .,Introduction
We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.,Introduction
"These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.",Introduction
"The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).",Introduction
The language model is estimated at the word (not phrase) level.,Introduction
Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.,Introduction
"5 Evaluation To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.",Introduction
The sentences in the corpus were at most 20 words long.,Introduction
"The English side had a total of 1,073,480 words (21,484 unique tokens).",Introduction
"The French side had a total of 1,177,143 words (28,132 unique tokens).",Introduction
"We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.",Introduction
"For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et (2001) and the decoder that uses the joint prob- Model Percent perfect translations IBM Bleu score Sentence length Sentence length 6 8 10 15 20 Avg.",Introduction
6 8 10 15 20 Avg.,Introduction
"IBM 36 26 35 11 2 22 0.2076 0.2040 0.2414 0.2248 0.2011 0.2158 Phrase-based 43 37 33 19 6 28 0.2574 0.2181 0.2435 0.2407 0.2028 0.2325 Table 1: Comparison of IBM and Phrase-Based, Joint Probability Models on a translation task. je vais me arreter la . je vais me arreter la .",Introduction
9.46e−08 i am going to stop there .,Introduction
Figure 3: Example of phrase-based greedy decoding. ability model.,Introduction
"We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).",Introduction
The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.,Introduction
6 Discussion 6.1 Limitations The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.,Introduction
"To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.",Introduction
"Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.",Introduction
We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.,Introduction
"Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.",Introduction
"The English word “not”, for example, is often translated into two French words, “ne” and “pas”.",Introduction
But “ne” and “pas” almost never occur in adjacent positions in French texts.,Introduction
"At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.",Introduction
But we were not able to scale and train it on large amounts of data.,Introduction
The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.,Introduction
"However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).",Introduction
6.2 Comparison with other work A number of researchers have already gone beyond word-level translations in various MT settings.,Introduction
"For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.",Introduction
"Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.",Introduction
And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.,Introduction
"However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.",Introduction
"As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the je vais me arreter la .",Introduction
"7.50e−11 FuseAndChangeTrans(&quot;la .&quot;, &quot;there .&quot;) i want me to that . je vais me arreter la .",Introduction
"2.97e−10 ChangeWordTrans(&quot;arreter&quot;,&quot;stop&quot;) 7.75e−10 1.09e−09 i want me to there . je vais me arreter la . i want me stop there . je vais me arreter la . let me to stop there .",Introduction
"FuseAndChange(&quot;je vais&quot;,&quot;let me&quot;) FuseAndChange(&quot;je vais me&quot;, &quot;i am going to&quot;) 1.28e−14 changeWordTrans(&quot;vais&quot;, &quot;want&quot;) i . me to that . data.",Introduction
"In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.",Introduction
Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.,Introduction
"For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).",Introduction
"The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.",Introduction
"The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.",Introduction
"However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.",Introduction
The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.,Introduction
"Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.",Experiment/Discussion
"In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to “generate” a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).",Experiment/Discussion
The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.,Experiment/Discussion
"A variety of methods are used to account for the re-ordering stage: word-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.",Experiment/Discussion
"Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are individually translated into target words.1 We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.",Experiment/Discussion
Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.,Experiment/Discussion
Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.,Experiment/Discussion
"Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.",Experiment/Discussion
"Consider, for example, the parallel corpus of three sentence pairs shown in Figure 1.",Experiment/Discussion
"Intuitively, if we allow any Source words to be aligned to any Target words, the best alignment that we can come up with is the one in Figure 1.c.",Experiment/Discussion
"Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure yields unintuitive translation probabilities.",Experiment/Discussion
(Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).),Experiment/Discussion
"In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.",Experiment/Discussion
"In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.",Experiment/Discussion
"In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.",Experiment/Discussion
The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.,Experiment/Discussion
"In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).",Experiment/Discussion
We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).,Experiment/Discussion
We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.,Experiment/Discussion
"In developing our joint probability model, we started out with a very simple generative story.",Experiment/Discussion
"We assume that each sentence pair in our corpus is generated by the following stochastic process: phrases , according to the distribution , whereandeach contain at least one word.",Experiment/Discussion
"For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.",Experiment/Discussion
"We do not assume that is a hidden variable that generates the pair , but rather that .",Experiment/Discussion
"Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts is given by the product of all phrase-tophrase translation probabilities, that yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.",Experiment/Discussion
"However, the same sentence pair cannot be generated using the concepts (“a b” : “y”) and (“c” : “y”) because the sequence “x y” cannot be recreated from the two phrases “y” and “y”.",Experiment/Discussion
"Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.",Experiment/Discussion
"We say that a set of concepts can be linearized into a sentence pair (E, F) if E and F can be obtained by permuting the phrasesandthat characterize all concepts .",Experiment/Discussion
We denote this property using the predicate .,Experiment/Discussion
"Under this model, the probability of a given sentence pair (E, F) can then be obtained by summing up over all possible ways of generating bags of concepts that can be linearized to (E, F).",Experiment/Discussion
"Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.",Experiment/Discussion
"However, this model is clearly unsuited for translating unseen sentences as it imposes no constraints on the ordering of the phrases associated with a given concept.",Experiment/Discussion
"In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.",Experiment/Discussion
"The generative story of Model 2 is this: a pair of phrases , according to the distribution , whereandeach contain at least one word.",Experiment/Discussion
"Remove then from. betweenand , wheregives the length of the phrase.",Experiment/Discussion
We hence create the alignment between the two phrasesand with probability where is a position-based distortion distribution.,Experiment/Discussion
"Model 2 implements an absolute position-based distortion model, in the style of IBM Model 3.",Experiment/Discussion
We have tried many types of distortion models.,Experiment/Discussion
We eventually settled for the model discussed here because it produces better translations during decoding.,Experiment/Discussion
"Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.",Experiment/Discussion
Training the models described in Section 2 is computationally challenging.,Experiment/Discussion
"Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the If one assumes from the outset that any phrases and can be generated from a concept , one would need a supercomputer in order to store in the memory a table that models the distribution.",Experiment/Discussion
"Since we don’t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.",Experiment/Discussion
"Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.",Experiment/Discussion
"In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.",Experiment/Discussion
"Before the EM training procedure starts, one has no idea what word/phrase pairs are likely to share the same meaning.",Experiment/Discussion
"In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.",Experiment/Discussion
"Under these conditions, the evidence that a sentence pair (E, F) contributes to the fact that are generated by the same concept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase in sentence F divided by the total number of alignments that can be built between the two sentences.",Experiment/Discussion
Both these numbers can be easily approximated.,Experiment/Discussion
"Given a sentence E ofwords, there are ways in which thewords can be partitioned into non-empty sets/concepts, where is the Stirling number of second kind.",Experiment/Discussion
There are also ways in which the words of a sentence F can be partitioned into nonempty sets.,Experiment/Discussion
"Given that any words in E can be mapped to any words in F, it follows that there are alignments that can be built between two sentences (E, F) of lengthsand , respectively.",Experiment/Discussion
"When a concept generates two phrases of lengthand, respectively, there are only and words left to link.",Experiment/Discussion
"Hence, in the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).",Experiment/Discussion
Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.,Experiment/Discussion
"However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.",Experiment/Discussion
"In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.",Experiment/Discussion
We sum over all these t-counts and we normalize to obtain an initial joint distribution.,Experiment/Discussion
This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.,Experiment/Discussion
"Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.",Experiment/Discussion
"Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.",Experiment/Discussion
"We then hillclimb towards the Viterbi alignment of highest probability by breaking and merging concepts, swapping words between concepts, and moving words across concepts.",Experiment/Discussion
We compute the probabilities associated with all the alignments we generate during the hillclimbing process and collect t counts over all concepts in these alignments.,Experiment/Discussion
We apply this Viterbi-based EM training procedure for a few iterations.,Experiment/Discussion
The first iterations estimate the alignment probabilities using Model 1.,Experiment/Discussion
The rest of the iterations estimate the alignment probabilities using Model 2.,Experiment/Discussion
"During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.",Experiment/Discussion
"At the end of the training procedure, we take marginals on the joint probability distributionsand .",Experiment/Discussion
This yields conditional probability distributions and which we use for decoding.,Experiment/Discussion
"When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.",Experiment/Discussion
"At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.",Experiment/Discussion
"However, note that the choice made by our model is quite reasonable.",Experiment/Discussion
"After all, in the absence of additional information, the model can either assume that “a” and “y” mean the same thing or that phrases “a b c” and “x y” mean the same thing.",Experiment/Discussion
"The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.",Experiment/Discussion
"Also note that although the joint distribution puts the second hypothesis at an advantage, the conditional distribution does not.",Experiment/Discussion
"The conditional distribution in Figure 1.e is consistent with our intuitions that tell us that it is reasonable both to translate “a b c” into “x y”, as well as “a” into “y”.",Experiment/Discussion
The conditional distribution mirrors perfectly our intuitions.,Experiment/Discussion
"For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).",Experiment/Discussion
"Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat maximize the probability .",Experiment/Discussion
We then iteratively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .,Experiment/Discussion
We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.,Experiment/Discussion
"These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.",Experiment/Discussion
"The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).",Experiment/Discussion
The language model is estimated at the word (not phrase) level.,Experiment/Discussion
Figure 3 shows the steps taken by our decoder in order to find the translation of sentence “je vais me arrˆeter l`a .” Each intermediate translation in Figure 3 is preceded by its probability and succeded by the operation that changes it to yield a translation of higher probability.,Experiment/Discussion
"To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.",Results/Conclusion
The sentences in the corpus were at most 20 words long.,Results/Conclusion
"The English side had a total of 1,073,480 words (21,484 unique tokens).",Results/Conclusion
"The French side had a total of 1,177,143 words (28,132 unique tokens).",Results/Conclusion
"We translated 500 unseen sentences, which were uniformly distributed across lengths 6, 8, 10, 15, and 20.",Results/Conclusion
"For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et al. (2001) and the decoder that uses the joint probje vais me arreter la . je vais me arreter la .",Results/Conclusion
9.46e−08 i am going to stop there . ability model.,Results/Conclusion
"We also evaluated the translations automatically, using the IBM-Bleu metric (Papineni et al., 2002).",Results/Conclusion
The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.,Results/Conclusion
The main shortcoming of the phrase-based model in this paper concerns the size of the t-table and the cost of the training procedure we currently apply.,Results/Conclusion
"To keep the memory requirements manageable, we arbitrarily restricted the system to learning phrase translations of at most six words on each side.",Results/Conclusion
"Also, the swap, break, and merge operations used during the Viterbi training are computationally expensive.",Results/Conclusion
We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.,Results/Conclusion
"Clearly, there are language pairs for which it would be helpful to allow concepts to be realized as non-contiguous phrases.",Results/Conclusion
"The English word “not”, for example, is often translated into two French words, “ne” and “pas”.",Results/Conclusion
But “ne” and “pas” almost never occur in adjacent positions in French texts.,Results/Conclusion
"At the outset of this work, we attempted to develop a translation model that enables concepts to be mapped into non-contiguous phrases.",Results/Conclusion
But we were not able to scale and train it on large amounts of data.,Results/Conclusion
The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.,Results/Conclusion
"However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).",Results/Conclusion
A number of researchers have already gone beyond word-level translations in various MT settings.,Results/Conclusion
"For example, Melamed (2001) uses wordlevel alignments in order to learn translations of noncompositional compounds.",Results/Conclusion
"Och and Ney (1999) learn phrase-to-phrase mappings involving word classes, which they call “templates”, and exploit them in a statistical machine translation system.",Results/Conclusion
And Marcu (2001) extracts phrase translations from automatically aligned corpora and uses them in conjunction with a word-for-word statistical translation system.,Results/Conclusion
"However, none of these approaches learn simultaneously the translation of phrases/templates and the translation of words.",Results/Conclusion
"As a consequence, there is a chance that the learning procedure will not discover phrase-level patterns that occur often in the data.",Results/Conclusion
"In our approach, phrases are not treated differently from individual words, and as a consequence the likelihood of the EM algorithm converging to a better local maximum is increased.",Results/Conclusion
Working with phrase translations that are learned independent of a translation model can also affect the decoder performance.,Results/Conclusion
"For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).",Results/Conclusion
"The phrases in the translation memory were automatically extracted from the Viterbi alignments produced by Giza (Al-Onaizan et al., 1999) and reused in decoding.",Results/Conclusion
"The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.",Results/Conclusion
"However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.",Results/Conclusion
The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.,Results/Conclusion
Acknowledgments.,Results/Conclusion
This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.,Results/Conclusion
