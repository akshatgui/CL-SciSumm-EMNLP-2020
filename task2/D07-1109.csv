col1,col2
"We develop latent Dirichlet alocation with WORDNET (LDAWN), an unsupervised probabilistic topic model that includes word sense as a hidden variable.",{}
We develop a probabilistic posterior inference algorithm for simultaneously disambiguating a corpusand learning the domains in which to consider each word.,{}
"Using the WORDNET hierarchy, we embed the construction of Ab ney and Light (1999) in the topic model and show that automatically learned domainsimprove WSD accuracy compared to alter native contexts.",{}
Word sense disambiguation (WSD) is the task of determining the meaning of an ambiguous word in its context.,"{'title': 'Introduction', 'number': '1'}"
"It is an important problem in natural language processing (NLP) because effective WSD can improve systems for tasks such as information retrieval, machine translation, and summarization.In this paper, we develop latent Dirichlet alocation with WORDNET (LDAWN), a generative prob abilistic topic model for WSD where the sense of the word is a hidden random variable that is inferred from data.","{'title': 'Introduction', 'number': '1'}"
"There are two central advantages to this approach.First, with LDAWN we automatically learn the con text in which a word is disambiguated.","{'title': 'Introduction', 'number': '1'}"
"Rather than disambiguating at the sentence-level or the document-level, our model uses the other words that share the same hidden topic across many documents.","{'title': 'Introduction', 'number': '1'}"
"Second, LDAWN is a fully-fledged generative model.","{'title': 'Introduction', 'number': '1'}"
Generative models are modular and can beeasily combined and composed to form more complicated models.,"{'title': 'Introduction', 'number': '1'}"
"(As a canonical example, the ubiq uitous hidden Markov model is a series of mixturemodels chained together.)","{'title': 'Introduction', 'number': '1'}"
"Thus, developing a gen erative model for WSD gives other generative NLP algorithms a natural way to take advantage of the hidden senses of words.","{'title': 'Introduction', 'number': '1'}"
"In general, topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al, 2003).","{'title': 'Introduction', 'number': '1'}"
"Given a corpus, posterior inference in topic models amounts to automatically discovering the underlying themesthat permeate the collection.","{'title': 'Introduction', 'number': '1'}"
"Topic models have re cently been applied to information retrieval (Wei and Croft, 2006), text classification (Blei et al, 2003), and dialogue segmentation (Purver et al, 2006).","{'title': 'Introduction', 'number': '1'}"
"While topic models capture the polysemous use of words, they do not carry the explicit notion of sense that is necessary for WSD.","{'title': 'Introduction', 'number': '1'}"
LDAWN extends the topic modeling framework to include a hidden meaning in the word generation process.,"{'title': 'Introduction', 'number': '1'}"
"In this case, posterior inference discovers both the topics of the corpus and the meanings assigned to each of its words.","{'title': 'Introduction', 'number': '1'}"
"After introducing a disambiguation scheme basedon probabilistic walks over the WORDNET hierar chy (Section 2), we embed the WORDNET-WALK in a topic model, where each topic is associated withwalks that prefer different neighborhoods of WORDNET (Section 2.1).","{'title': 'Introduction', 'number': '1'}"
"Then, we describe a Gibbs sam pling algorithm for approximate posterior inference that learns the senses and topics that best explain a corpus (Section 3).","{'title': 'Introduction', 'number': '1'}"
"Finally, we evaluate our system on real-world WSD data, discuss the properties of the topics and disambiguation accuracy results, and draw connections to other WSD algorithms from the research literature.","{'title': 'Introduction', 'number': '1'}"
1024 1740 entity 1930 3122object 20846 15024 animal 1304946 1305277 artifact male 2354808 2354559 foalcolt 3042424 colt 4040311 revolver Synset ID Word six-gun six-shooter 0.00 0.25 0.58 0.00 0.04 0.02 0.010.16 0.05 0.04 0.690.00 0.00 0.381.000.42 0.00 0.000.57 1.00 0.38 0.07 Figure 1: The possible paths to reach the word ?colt?,"{'title': 'Introduction', 'number': '1'}"
in WORDNET.,"{'title': 'Introduction', 'number': '1'}"
Dashed lines represent omitted links.,"{'title': 'Introduction', 'number': '1'}"
All words in the synset containing ?revolver?,"{'title': 'Introduction', 'number': '1'}"
"are shown, but only one word from other synsets is shown.","{'title': 'Introduction', 'number': '1'}"
"Edge labels are probabilities of transitioningfrom synset i to synset j. Note how this favors frequent terms, such as ?revolver,?","{'title': 'Introduction', 'number': '1'}"
over ones like ?six shooter.?,"{'title': 'Introduction', 'number': '1'}"
"The WORDNET-WALK is a probabilistic process ofword generation that is based on the hyponomy relationship in WORDNET (Miller, 1990).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"WORD NET, a lexical resource designed by psychologistsand lexicographers to mimic the semantic organiza tion in the human mind, links ?synsets?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
(short forsynonym sets) with myriad connections.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"The spe cific relation we?re interested in, hyponomy, points from general concepts to more specific ones and is sometimes called the ?is-a?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
relationship.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"As first described by Abney and Light (1999), we imagine an agent who starts at synset [entity], which points to every noun in WORDNET 2.1 by some sequence of hyponomy relations, and then chooses the next node in its random walk from the hyponyms of its current position.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"The agent repeatsthis process until it reaches a leaf node, which corre sponds to a single word (each of the synset?s words are unique leaves of a synset in our construction).For an example of all the paths that might generate the word ?colt?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
see Figure 1.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"The WORDNET WALK is parameterized by a set of distributions over children for each synset s in WORDNET, ?s. Symbol Meaning K number of topics ?k,s multinomial probability vector over the successors of synset s in topic k S scalar that, when multiplied by ?s gives the prior for ?k,s ?s normalized vector whose ith entry, when multiplied by S, gives the prior probability for going from s to i ?d multinomial probability vector over the topics that generate document d ? prior for ? z assignment of a word to a topic ? a path assignment through WORDNET ending at a word.?i,j one link in a path ? going from syn set i to synset j.Table 1: A summary of the notation used in the paper.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Bold vectors correspond to collections of vari ables (i.e. zu refers to a topic of a single word, butz1:D are the topics assignments of words in docu ment 1 through D).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
2.1 A topic model for WSDThe WORDNET-WALK has two important proper ties.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"First, it describes a random process for word generation.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Thus, it is a distribution over words and thus can be integrated into any generative model of text, such as topic models.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Second, the synsetthat produces each word is a hidden random vari able.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Given a word assumed to be generated by a WORDNET-WALK, we can use posterior inference to predict which synset produced the word.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"These properties allow us to develop LDAWN, which is a fusion of these WORDNET-WALKs and latent Dirichlet alocation (LDA) (Blei et al, 2003),a probabilistic model of documents that is an im provement to pLSI (Hofmann, 1999).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"LDA assumes that there are K ?topics,?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"multinomial distributionsover words, which describe a collection.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Each docu ment exhibits multiple topics, and each word in each document is associated with one of them.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
Although the term ?topic?,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"evokes a collection of ideas that share a common theme and although the topics derived by LDA seem to possess semantic coherence, there is no reason to believe this would 1025 be true of the most likely multinomial distributions that could have created the corpus given the assumed generative model.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"That semantically similar words are likely to occur together is a byproduct of how language is actually used.In LDAWN, we replace the multinomial topic dis tributions with a WORDNET-WALK, as described above.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"LDAWN assumes a corpus is generated bythe following process (for an overview of the nota tion used in this paper, see Table 1).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
1.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"For each topic, k ? {1, . . .","{'title': 'Topic models and WordNet. ', 'number': '2'}"
",K}.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"(a) For each synset s, randomly choose transition prob abilities ?k,s ? Dir(S?s).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
2.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"For each document d ? {1, . . .","{'title': 'Topic models and WordNet. ', 'number': '2'}"
", D}.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
(a) Select a topic distribution ?d ? Dir(?),"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"(b) For each word n ? {1, . . .","{'title': 'Topic models and WordNet. ', 'number': '2'}"
", Nd} i. Select a topic z ? Mult(1, ?d) ii.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Create a path ?d,n starting with ?0 as the root node.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
iii.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"From children of ?i: A. Choose the next node in the walk ?i+1 ? Mult(1, ?z,?i)B. If ?i+1 is a leaf node, generate the associ ated word.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Otherwise, repeat.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Every element of this process, including thesynsets, is hidden except for the words of the doc uments.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Thus, given a collection of documents, our goal is to perform posterior inference, which is the task of determining the conditional distribution of the hidden variables given the observations.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"In thecase of LDAWN, the hidden variables are the parameters of the K WORDNET-WALKs, the topic assign ments of each word in the collection, and the synset path of each word.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"In a sense, posterior inference reverses the process described above.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Specifically, given a document collection w1:D, the full posterior is p(?1:K ,z1:D,?1:D,?1:D |w1:D, ?, S?)","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"?(?K k=1 p(?k |S?) ?D d=1 p(?d | ?) ?Nd n=1 p(?d,n |?1:K)p(wd,n |?d,n) ) , (1) where the constant of proportionality is the marginal likelihood of the observed data.Note that by encoding the synset paths as a hid den variable, we have posed the WSD problem asa question of posterior probabilistic inference.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
Fur ther note that we have developed an unsupervised model.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
No labeled data is needed to disambiguate a corpus.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
Learning the posterior distribution amounts to simultaneously decomposing a corpus into topics and its words into their synsets.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
The intuition behind LDAWN is that the words in a topic will have similar meanings and thus share paths within WORDNET.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"For example, WORDNET has two senses for the word ?colt;?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"one referring to a young male horse and the other to a type of handgun (see Figure 1).Although we have no a priori way of know ing which of the two paths to favor for a document, we assume that similar concepts will also appear in the document.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
Documents with unambiguous nouns such as ?six-shooter?,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
and ?smoothbore?,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"would make paths that pass through the synset [firearm, piece,small-arm] more likely than those go ing through [animal, animate being, beast, brute, creature, fauna].","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"In practice, we hope to see a WORDNET-WALK that looks like Figure 2, which points to the right sense of cancer for a medical context.LDAWN is a Bayesian framework, as each vari able has a prior distribution.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"In particular, the Dirichlet prior for ?s, specified by a scaling factor S and a normalized vector ?s fulfills two functions.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"First, as the overall strength of S increases, we place a greater emphasis on the prior.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
This is equivalent to the need for balancing as noted by Abney and Light (1999).,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
The other function that the Dirichlet prior serves is to enable us to encode any information we have about how we suspect the transitions to childrennodes will be distributed.,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"For instance, we might ex pect that the words associated with a synset will beproduced in a way roughly similar to the token prob ability in a corpus.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"For example, even though ?meal?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
might refer to both ground cereals or food eaten ata single sitting and ?repast?,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"exclusively to the lat ter, the synset [meal, repast, food eatenat a single sitting] still prefers to transi tion to ?meal?","{'title': 'Topic models and WordNet. ', 'number': '2'}"
over ?repast?,"{'title': 'Topic models and WordNet. ', 'number': '2'}"
"given the overall corpus counts (see Figure 1, which shows prior transition probabilities for ?revolver?).By setting ?s,i, the prior probability of transitioning from synset s to node i, proportional to the to tal number of observed tokens in the children of i, 1026we introduce a probabilistic variation on information content (Resnik, 1995).","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"As in Resnik?s defini tion, this value for non-word nodes is equal to thesum of all the frequencies of hyponym words.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"Un like Resnik, we do not divide frequency among all senses of a word; each sense of a word contributes its full frequency to ?.","{'title': 'Topic models and WordNet. ', 'number': '2'}"
"As described above, the problem of WSD corresponds to posterior inference: determining the probability distribution of the hidden variables given ob served words and then selecting the synsets of themost likely paths as the correct sense.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"Directly com puting this posterior distribution, however, is not tractable because of the difficulty of calculating the normalizing constant in Equation 1.To approximate the posterior, we use Gibbs sampling, which has proven to be a successful approx imate inference technique for LDA (Griffiths and Steyvers, 2004).","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"In Gibbs sampling, like all Markov chain Monte Carlo methods, we repeatedly sample from aMarkov chain whose stationary distribution is the posterior of interest (Robert and Casella, 2004).","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"Even though we don?t know the full posterior, the samples can be used to form an empirical estimate of the target distribution.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"In LDAWN, the samples contain a configuration of the latent semantic states of the system, revealing the hidden topics and paths that likely led to the observed data.Gibbs sampling reproduces the posterior distri bution by repeatedly sampling each hidden variable conditioned on the current state of the other hidden variables and observations.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"More precisely, the state is given by a set of assignments where each wordis assigned to a path through one of K WORDNET WALK topics: uth word wu has a topic assignment zu and a path assignment ?u. We use z?u and ??u to represent the topic and path assignments of all words except for u, respectively.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
Sampling a new topic for the word wu requires us to consider all of the paths that wu can take in each topic and the topics of the other words in the document u is in.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
The probability of wu taking on topic i is proportional to p(zu = i |z?u) ? ?,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
p(?,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"|??u)1[wu ? ?], (2) which is the probability of selecting z from ?d times the probability of a path generating wu from a path in the ith WORDNET-WALK.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"The first term, the topic probability of the uth word, is based on the assignments to the K topics for words other than u in this document, p(zu = i|z?u) = n(d)?u,i + ?i ? j n (d) ?u,j + ?K j=1 ?j , (3) where n(d)?u,j is the number of words other than u in topic j for the document d that u appears in.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
The second term in Equation 2 is a sum over the probabilities of every path that could have generatedthe word wu.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"In practice, this sum can be com puted using a dynamic program for all nodes that have unique parent (i.e. those that can?t be reached by more than one path).","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"Although the probability ofa path is specific to the topic, as the transition prob abilities for a synset are different across topics, we will omit the topic index in the equation, p(?u = ?|??u, ) = ?l?1 i=1 ? ?u ?i,?i+1 .","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
(4) 3.1 Transition Probabilities.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
Computing the probability of a path requires us to take a product over our estimate of the probability from transitioning from i to j for all nodes i and j in the path ?.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"The other path assignments within this topic, however, play an important role in shaping the transition probabilities.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"From the perspective of a single node i, only paths that pass through that node affect the probability of u also passing through that node.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"It?s convenient tohave an explicit count of all of the paths that tran sition from i to j in this topic?s WORDNET-WALK, so we use T?ui,j to represent all of the paths that go from i to j in a topic other than the path currently assigned to u. Given the assignment of all other words to paths, calculating the probability of transitioning from i to j with word u requires us to consider the prior ? and the observations Ti,j in our estimate of the expected value of the probability of transitioning from i to j, ??ui,j = T?ui,j + Si?i,j Si + ? k T ?u i,k .","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"(5) 1027 As mentioned in Section 2.1, we paramaterize the prior for synset i as a vector ?i, which sums to one, and a scale parameter S. The next step, once we?ve selected a topic, is to select a path within that topic.","{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
This requires the computation of the path probabilities as specified in Equation 4 for all of the paths wu can take in thesampled topic and then sampling from the path prob abilities.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
The Gibbs sampler is essentially a randomized hill climbing algorithm on the posterior likelihood as a function of the configuration of hidden variables.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
The numerator of Equation 1 is proportional to that posterior and thus allows us to track the sampler?s progress.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
We assess convergence to a local mode of the posterior by monitoring this quantity.,"{'title': 'Posterior Inference with Gibbs Sampling. ', 'number': '3'}"
"In this section, we describe the properties of the topics induced by running the previously described Gibbs sampling method on corpora and how these topics improve WSD accuracy.","{'title': 'Experiments. ', 'number': '4'}"
"Of the two data sets used during the course of our evaluation, the primary dataset was SEMCOR (Miller et al, 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct WORDNET sense.","{'title': 'Experiments. ', 'number': '4'}"
"The words in this dataset are lemmatized, and multi-word expressions that are present in WORDNET are identified.","{'title': 'Experiments. ', 'number': '4'}"
Only the wordsin SEMCOR were used in the Gibbs sampling pro cedure; the synset assignments were only used for assessing the accuracy of the final predictions.,"{'title': 'Experiments. ', 'number': '4'}"
"We also used the British National Corpus, whichis not lemmatized and which does not have multi word expressions.","{'title': 'Experiments. ', 'number': '4'}"
"The text was first run through a lemmatizer, and then sequences of words which matched a multi-word expression in WORDNET were joined together into a single word.","{'title': 'Experiments. ', 'number': '4'}"
"We took nouns that appeared in SEMCOR twice or in theBNC at least 25 times and used the BNC to compute the information-content analog ? for individ ual nouns (For example, the probabilities in Figure 1 correspond to ?).","{'title': 'Experiments. ', 'number': '4'}"
4.1 Topics.,"{'title': 'Experiments. ', 'number': '4'}"
"Like the topics created by structures such as LDA, the topics in Table 2 coalesce around reasonable themes.","{'title': 'Experiments. ', 'number': '4'}"
The word list was compiled by summingover all of the possible leaves that could have gen erated each of the words and sorting the words by decreasing probability.,"{'title': 'Experiments. ', 'number': '4'}"
"In the vast majority of cases, a single synset?s high probability is responsible for the words?","{'title': 'Experiments. ', 'number': '4'}"
positions on the list.,"{'title': 'Experiments. ', 'number': '4'}"
"Reassuringly, many of the top senses for the present words correspond to the most frequent sense in SEMCOR.","{'title': 'Experiments. ', 'number': '4'}"
"For example, in Topic 4, the senses for ?space?","{'title': 'Experiments. ', 'number': '4'}"
and ?function?,"{'title': 'Experiments. ', 'number': '4'}"
"correspond to the top sensesin SEMCOR, and while the top sense for ?set?","{'title': 'Experiments. ', 'number': '4'}"
corresponds to ?an abstract collection of numbers or symbols?,"{'title': 'Experiments. ', 'number': '4'}"
"rather than ?a group of the same kind that be long together and are so used,?","{'title': 'Experiments. ', 'number': '4'}"
it makes sense given the math-based words in the topic.,"{'title': 'Experiments. ', 'number': '4'}"
"?Point,?","{'title': 'Experiments. ', 'number': '4'}"
"however, corresponds to the sense used in the phrase ?I got to the point of boiling the water,?","{'title': 'Experiments. ', 'number': '4'}"
which is neither the top SEMCOR sense nor a sense which makes sense given the other words in the topic.,"{'title': 'Experiments. ', 'number': '4'}"
"While the topics presented in Table 2 resemble the topics one would obtain through models likeLDA (Blei et al, 2003), they are not identical.","{'title': 'Experiments. ', 'number': '4'}"
"Be cause of the lengthy process of Gibbs sampling, we initially thought that using LDA assignments as aninitial state would converge faster than a random initial assignment.","{'title': 'Experiments. ', 'number': '4'}"
"While this was the case, it con verged to a state that less probable than the randomlyinitialized state and no better at sense disambigua tion (and sometimes worse).","{'title': 'Experiments. ', 'number': '4'}"
The topics presented in 2 represent words both that co-occur together in a corpus and co-occur on paths through WORDNET.,"{'title': 'Experiments. ', 'number': '4'}"
"Because topics created through LDA only have the first property, they usually do worse in terms of both total probability and disambiguation accuracy (see Figure 3).","{'title': 'Experiments. ', 'number': '4'}"
"Another interesting property of topics in LDAWN is that, with higher levels of smoothing, words that don?t appear in a corpus (or appear rarely) but are in similar parts of WORDNET might have relatively high probability in a topic.","{'title': 'Experiments. ', 'number': '4'}"
"For example, ?maturity?","{'title': 'Experiments. ', 'number': '4'}"
in topic two in Table 2 is sandwiched between ?foot?,"{'title': 'Experiments. ', 'number': '4'}"
"and ?center,?","{'title': 'Experiments. ', 'number': '4'}"
both of which occur about five timesmore than ?maturity.?,"{'title': 'Experiments. ', 'number': '4'}"
"This might improve LDA based information retrieval schemes (Wei and Croft, 2006) . 1028 1740 1930 0.23 0.76 3122 0.42 0.01 2236 0.10 0.00 0.00 0.00 0.00 7626 someone 0.00 9609711 0.00 9120316 1743824 0.00 cancer 7998922 genus0.04 0.04 8564599 star_sign 0.06 8565580 0.06 cancer 0.5 9100327 cancer 1 constellation 0.01 0.01 cancer 0.5 crab 0.5 13875408 0.58 0.19 14049094 14046733 tumor 0.97 14050958 0.00 malignancy 0.06 0.94 14051451 0.90 cancer 0.96 Synset ID Transition Prob Word 1957888 1.0 Figure 2: The possible paths to reach the word ?cancer?","{'title': 'Experiments. ', 'number': '4'}"
"in WORDNET along with transition probabilities from the medically-themed Topic 2 in Table 2, with the most probable path highlighted.","{'title': 'Experiments. ', 'number': '4'}"
"The dashed lines represent multiple links that have been consolidated, and synsets are represented by their offsets within WORDNET 2.1.","{'title': 'Experiments. ', 'number': '4'}"
Some words for immediate hypernyms have also been included to give context.,"{'title': 'Experiments. ', 'number': '4'}"
"In all other topics, the person, animal, or constellation senses were preferred.","{'title': 'Experiments. ', 'number': '4'}"
Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 president growth material point water plant music party age object number house change film city treatment color value road month work election feed form function area worker life administration day subject set city report time official period part square land mercer world office head self space home requirement group bill portion picture polynomial farm bank audience yesterday length artist operator spring farmer play court level art component bridge production thing meet foot patient corner pool medium style police maturity communication direction site petitioner year service center movement curve interest relationship show Table 2: The most probable words from six randomly chosen WORDNET-walks from a thirty-two topic model trained on the words in SEMCOR.,"{'title': 'Experiments. ', 'number': '4'}"
These are summed over all of the possible synsets that generate the words.,"{'title': 'Experiments. ', 'number': '4'}"
"However, the vast majority of the contributions come from a single synset.","{'title': 'Experiments. ', 'number': '4'}"
"1029 0.275 0.28 0.285 0.29 0.295 0.3 0.305 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Accuracy Iteratio n Unsee ded Seede d with LDA 96000 94000 92000 90000 88000 86000 84000 82000 80000 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Model Probability Iteratio n Unsee ded Seede d with LDA Figure 3: Topics seeded with LDA initially have a higher disambiguation accuracy, but are quickly matched by unseeded topics.","{'title': 'Experiments. ', 'number': '4'}"
The probability for the seeded topics starts lower and remains lower.,"{'title': 'Experiments. ', 'number': '4'}"
4.2 Topics and the Weight of the Prior.,"{'title': 'Experiments. ', 'number': '4'}"
"Because the Dirichlet smoothing factor in partdetermines the topics, it also affects the disambiguation.","{'title': 'Experiments. ', 'number': '4'}"
"Figure 4 shows the modal disambigua tion achieved for each of the settings of S = {0.1, 1, 5, 10, 15, 20}.","{'title': 'Experiments. ', 'number': '4'}"
"Each line is one setting of K and each point on the line is a setting of S. Each data point is a run for the Gibbs sampler for 10,000 iterations.","{'title': 'Experiments. ', 'number': '4'}"
"The disambiguation, taken at the mode,improved with moderate settings of S, which sug gests that the data are still sparse for many of thewalks, although the improvement vanishes if S dom inates with much larger values.","{'title': 'Experiments. ', 'number': '4'}"
"This makes sense, as each walk has over 100,000 parameters, there are fewer than 100,000 words in SEMCOR, and each 0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 S=20 S=15 S=10 S=5 S=1 S=0.1 Accuracy Smoot hing F actor 64 top ics 32 top ics 16 top ics 8 topic s 4 topic s 2 topic s 1 topic Rando m Figure 4: Each line represents experiments with a setnumber of topics and variable amounts of smooth ing on the SEMCOR corpus.","{'title': 'Experiments. ', 'number': '4'}"
"The random baselineis at the bottom of the graph, and adding topics im proves accuracy.","{'title': 'Experiments. ', 'number': '4'}"
"As smoothing increases, the prior(based on token frequency) becomes stronger.","{'title': 'Experiments. ', 'number': '4'}"
Ac curacy is the percentage of correctly disambiguated polysemous words in SEMCOR at the mode.word only serves as evidence to at most 19 parame ters (the length of the longest path in WORDNET).,"{'title': 'Experiments. ', 'number': '4'}"
"Generally, a greater number of topics increased the accuracy of the mode, but after around sixteen topics, gains became much smaller.","{'title': 'Experiments. ', 'number': '4'}"
"The effect of ? is also related to the number of topics, as a value of S for a very large number of topics might overwhelm the observed data, while the same value of S might be the perfect balance for a smaller number of topics.For comparison, the method of using a WORDNET WALK applied to smaller contexts such as sentences or documents achieves an accuracy of between 26% and 30%, depending on the level of smoothing.","{'title': 'Experiments. ', 'number': '4'}"
This method works well in cases where the delineation can be readily determined from the over all topic of the document.,"{'title': 'Error Analysis. ', 'number': '5'}"
"Words such as ?kid,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?may,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?shear,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?coach,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?incident,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?fence,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?bee,?","{'title': 'Error Analysis. ', 'number': '5'}"
and (previously used as an example) ?colt?,"{'title': 'Error Analysis. ', 'number': '5'}"
were all perfectly disambiguated by this method.,"{'title': 'Error Analysis. ', 'number': '5'}"
Figure 2 shows the WORDNET-WALK corresponding to a medical topic that correctly disambiguates ?cancer.?,"{'title': 'Error Analysis. ', 'number': '5'}"
"Problems arose, however, with highly frequent 1030 words, such as ?man? and ?time?","{'title': 'Error Analysis. ', 'number': '5'}"
that have many senses and can occur in many types of documents.,"{'title': 'Error Analysis. ', 'number': '5'}"
"For example, ?man? can be associated with manypossible meanings: island, game equipment, ser vant, husband, a specific mammal, etc. Although we know that the ?adult male?","{'title': 'Error Analysis. ', 'number': '5'}"
"sense should be preferred, the alternative meanings will also be likely if they can be assigned to a topicthat shares common paths in WORDNET; the doc uments contain, however, many other places, jobs, and animals which are reasonable explanations (toLDAWN) of how ?man? was generated.","{'title': 'Error Analysis. ', 'number': '5'}"
"Unfortunately, ?man? is such a ubiquitous term that top ics, which are derived from the frequency of wordswithin an entire document, are ultimately uninfor mative about its usage.","{'title': 'Error Analysis. ', 'number': '5'}"
"While mistakes on these highly frequent terms significantly hurt our accuracy, errors associated with less frequent terms reveal that WORDNET?sstructure is not easily transformed into a probabilis tic graph.","{'title': 'Error Analysis. ', 'number': '5'}"
"For instance, there are two senses ofthe word ?quarterback,?","{'title': 'Error Analysis. ', 'number': '5'}"
a player in American football.,"{'title': 'Error Analysis. ', 'number': '5'}"
One is position itself and the other is a per son playing that position.,"{'title': 'Error Analysis. ', 'number': '5'}"
"While one would expect co-occurrence in sentences such as ?quarterback is a easy position, so our quarterback is happy,?","{'title': 'Error Analysis. ', 'number': '5'}"
"the paths to both terms share only the root node, thus making it highly unlikely a topic would cover both senses.","{'title': 'Error Analysis. ', 'number': '5'}"
"Because of WORDNET?s breadth, rare senses also impact disambiguation.","{'title': 'Error Analysis. ', 'number': '5'}"
"For example, the metonymical use of ?door?","{'title': 'Error Analysis. ', 'number': '5'}"
to represent a wholebuilding as in the phrase ?girl next door?,"{'title': 'Error Analysis. ', 'number': '5'}"
"is under the same parent as sixty other synsets contain ing ?bridge,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?balcony,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?body,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?arch,?","{'title': 'Error Analysis. ', 'number': '5'}"
"?floor,?","{'title': 'Error Analysis. ', 'number': '5'}"
and ?corner.?,"{'title': 'Error Analysis. ', 'number': '5'}"
"Surrounded by such common terms thatare also likely to co-occur with the more conventional meanings of door, this very rare sense be comes the preferred disambiguation of ?door.?","{'title': 'Error Analysis. ', 'number': '5'}"
"Abney and Light?s initial probabilistic WSD ap proach (1999) was further developed into a Bayesian network model by Ciaramita and Johnson (2000), who likewise used the appearance of monosemous terms close to ambiguous ones to ?explain away?","{'title': 'Related Work. ', 'number': '6'}"
the usage of ambiguous terms in selectional restrictions.,"{'title': 'Related Work. ', 'number': '6'}"
We have adapted these approaches and put them into the context of a topic model.,"{'title': 'Related Work. ', 'number': '6'}"
"Recently, other approaches have created ad hoc connections between synsets in WORDNET and then considered walks through the newly created graph.","{'title': 'Related Work. ', 'number': '6'}"
"Given the difficulties of using existing connections in WORDNET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the Lesk overlap between the pairs, and then using the PageRank algorithm to determine the stationary distribution over synsets.","{'title': 'Related Work. ', 'number': '6'}"
6.1 Topics and Domains.,"{'title': 'Related Work. ', 'number': '6'}"
Yarowsky was one of the first to contend that ?there is one sense for discourse?,"{'title': 'Related Work. ', 'number': '6'}"
(1992).,"{'title': 'Related Work. ', 'number': '6'}"
"This has lead to the approaches like that of Magnini (Magnini et al., 2001) that attempt to find the category of a text, select the most appropriate synset, and then assign the selected sense using domain annotation attached to WORDNET.","{'title': 'Related Work. ', 'number': '6'}"
LDAWN is different in that the categories are notan a priori concept that must be painstakingly annotated within WORDNET and require no augmenta tion of WORDNET.,"{'title': 'Related Work. ', 'number': '6'}"
This technique could indeed be used with any hierarchy.,"{'title': 'Related Work. ', 'number': '6'}"
Our concepts are the ones that best partition the space of documents and do the best job of describing the distinctions of diction that separate documents from different domains.,"{'title': 'Related Work. ', 'number': '6'}"
6.2 Similarity Measures.,"{'title': 'Related Work. ', 'number': '6'}"
"Our approach gives a probabilistic method of using information content (Resnik, 1995) as a start ing point that can be adjusted to cluster words ina given topic together; this is similar to the Jiang Conrath similarity measure (1997), which has beenused in many applications in addition to disambigua tion.","{'title': 'Related Work. ', 'number': '6'}"
Patwardhan (2003) offers a broad evaluation of similarity measures for WSD.,"{'title': 'Related Work. ', 'number': '6'}"
"Our technique for combining the cues of topicsand distance in WORDNET is adjusted in a way sim ilar in spirit to Buitelaar and Sacaleanu (2001), but we consider the appearance of a single term to be evidence for not just that sense and its immediate neighbors in the hyponomy tree but for all of the sense?s children and ancestors.","{'title': 'Related Work. ', 'number': '6'}"
"Like McCarthy (2004), our unsupervised system acquires a single predominant sense for a domain based on a synthesis of information derived from a 1031textual corpus, topics, and WORDNET-derived sim ilarity, a probabilistic information content measure.","{'title': 'Related Work. ', 'number': '6'}"
"By adding syntactic information from a thesaurusderived from syntactic features (taken from Lin?s au tomatically generated thesaurus (1998)), McCarthy achieved 48% accuracy in a similar evaluation onSEMCOR; LDAWN is thus substantially less effec tive in disambiguation compared to state-of-the-artmethods.","{'title': 'Related Work. ', 'number': '6'}"
"This suggests, however, that other meth ods might be improved by adding topics and that ourmethod might be improved by using more informa tion than word counts.","{'title': 'Related Work. ', 'number': '6'}"
The LDAWN model presented here makes two contributions to research in automatic word sense disambiguation.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"First, we demonstrate a method for au tomatically partitioning a document into topics that includes explicit semantic information.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"Second, we show that, at least for one simple model of WSD,embedding a document in probabilistic latent struc ture, i.e., a ?topic,?","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
can improve WSD.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
There are two avenues of research with LDAWN that we will explore.,"{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"First, the statistical nature ofthis approach allows LDAWN to be used as a com ponent in larger models for other language tasks.Other probabilistic models of language could insert the ability to query synsets or paths of WORDNET.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"Similarly, any topic based information retrieval scheme could employ topics that include se mantically relevant (but perhaps unobserved) terms.Incorporating this model in a larger syntactically aware model, which could benefit from the local context as well as the document level context, is an important component of future research.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"Second, the results presented here show a marked improvement in accuracy as more topics are added to the baseline model, although the final result is not comparable to state-of-the-art techniques.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
"As most errors were attributable to the hyponomy structure of WORDNET, incorporating the novel use of topicmodeling presented here with a more mature unsu pervised WSD algorithm to replace the underlyingWORDNET-WALK could lead to advances in state of-the-art unsupervised WSD accuracy.","{'title': 'Conclusion and Future Work. ', 'number': '7'}"
