col1,col2
"This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.",{}
"When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.",{}
"This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).","{'title': '1 Introduction', 'number': '1'}"
"This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.","{'title': '1 Introduction', 'number': '1'}"
"The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.","{'title': '1 Introduction', 'number': '1'}"
"All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.","{'title': '2 The Classification Method and Features Used', 'number': '2'}"
"RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).","{'title': '2 The Classification Method and Features Used', 'number': '2'}"
Feature design and integration is of utmost importance in the overall classifier design – a rich feature space is the key to good performance.,"{'title': '2 The Classification Method and Features Used', 'number': '2'}"
"Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).","{'title': '2 The Classification Method and Features Used', 'number': '2'}"
"In accordance with this observation, the classifiers used in this research can access a diverse set of features when examining a word in context, including: In addition, a ngram-based capitalization restoration algorithm has been applied on the sentences that appear in all caps2, for the English task.","{'title': '2 The Classification Method and Features Used', 'number': '2'}"
This section describes only briefly the classifiers used in combination in Section 4; a full description of the algorithms and their properties is beyond the scope of this paper – the reader is instead referred to the original articles.,"{'title': '3 The Algorithms', 'number': '3'}"
"This classifier is described in detail in (Zhang and Johnson, 2003, this volume), along with a comprehensive evaluation of its performance, and therefore is not presented here.","{'title': '3 The Algorithms', 'number': '3'}"
The MaxEnt classifier computes the posterior class probability of an example by evaluating the normalized product of the weights active for the particular example.,"{'title': '3 The Algorithms', 'number': '3'}"
"The model weights are trained using the improved iterative scaling algorithm (Berger et al., 1996).","{'title': '3 The Algorithms', 'number': '3'}"
"To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.","{'title': '3 The Algorithms', 'number': '3'}"
"At decoding time, the best sequence of classifications is identified with the Viterbi algorithm.","{'title': '3 The Algorithms', 'number': '3'}"
"Transformation-based learning is an error-driven algorithm which has two major steps: it starts by assigning some classification to each example, and then automatically proposing, evaluating and selecting the classification changes that maximally decrease the number of errors.","{'title': '3 The Algorithms', 'number': '3'}"
"TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling, it is error–driven, and has an inherently dynamic behavior.","{'title': '3 The Algorithms', 'number': '3'}"
"The particular setup in which fnTBL is used in this work is described in Florian (2002a): in a first phase, TBL is used to identify the entity boundaries, followed by a sequence classification stage, where the entities identified at the first step are classified using internal and external clues3.","{'title': '3 The Algorithms', 'number': '3'}"
"The HMM classifier used in the experiments in Section 4 follows the system description in (Bikel et al., 1999), and it performs sequence classification by assigning each word either one of the named entity types or the label NOT-A-NAME to represent &quot;not a named entity&quot;.","{'title': '3 The Algorithms', 'number': '3'}"
"The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.","{'title': '3 The Algorithms', 'number': '3'}"
"Within each of the regions, a statistical bigram language model is used to compute the likelihood of words occurring within that region (named entity type).","{'title': '3 The Algorithms', 'number': '3'}"
"The transition probabilities are computed by deleted interpolation (Jelinek, 1997), and the decoding is done through the Viterbi algorithm.","{'title': '3 The Algorithms', 'number': '3'}"
"The particular implementation we used underperformed consistently all the other classifiers on German, and is not included.","{'title': '3 The Algorithms', 'number': '3'}"
"The results obtained by each individual classifier, broken down by entity type, are presented in Table 1.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"Out of the four classifiers, the MaxEnt and RRM classifiers are the best performers, followed by the modified fnTBL classifier and the HMM classifier.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"The error-based classifiers (RRM and fnTBL) tend to obtain balanced precision/recall numbers, while the other two tend to be more precise at the expense of recall.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"To facilitate comparison with other classifiers for this task, most reported results 3 The method of retaining only the boundaries and reclassifying the entities was shown to improve the performance of 11 of the 12 systems participating in the CoNLL-2002 shared tasks, in both languages (Florian, 2002b). are obtained by using features exclusively extracted from the training data.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In general, given n classifiers, one can interpret the classifier combination framework as combining probability distributions: where Ci is the classifier i’s classification output, f is a combination function.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"A widely used combination scheme is through linear interpolation of the classifiers’ class probability distribution The weights Ai (w) encode the importance given to classifier i in combination, for the context of word w, and Pi (C|w, Ci) is an estimation of the probability that the correct classification is C, given that the output of the classifier i on word w is Ci.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"To estimate the parameters in Equation (2), the provided training data was split into 5 equal parts, and each classifier was trained, in a round-robin fashion, on 4 fifths of the data and applied on the remaining fifth.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"This way, the entire training data can be used to estimate the weight parameters Ai (w) and Pi (C|w, Ci) but, at decoding time, the individual classifier outputs Ci are computed by using the entire training data.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"Table 2 presents the combination results, for different ways of estimating the interpolation parameters.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"A simple combination method is the equal voting method (van Halteren et al., 2001; Tjong Kim Sang et al., 2000), where the parameters are computed as Ai (w) = 1n and Pi (C|w, Ci) = S (C, Ci), where S is the Kronecker operator (S (x, y) := (x = y?1 : 0)) – each of the classifiers votes with equal weight for the class that is most likely under its model, and the class receiving the largest number of votes wins.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"However, this procedure may lead to ties, where some classes receive the same number of votes – one usually resorts to randomly selecting one of the tied candidates in this case – Table 2 presents the average results obtained by this method, together with the variance obtained over 30 trials.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"To make the decision deterministically, the weights associated with the classifiers can be chosen as Ai (w) = Pi (error).","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In this method, presented in Table 2 as weighted voting, better performing classifiers will have a higher impact in the final classification.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In the voting methods, each classifier gave its entire vote to one class – its own output.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"However, Equation (2) allows for classifiers to give partial credit to alternative classifications, through the probability Pi (C|w, Ci).","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In our experiments, this value is computed through 5fold cross-validation on the training data.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"The space of possible choices for C, w and Ci is large enough to make the estimation unreliable, so we use two approximations, named Model 1 and Model 2 in Table 2: Pi (C|w, Ci) = Pi (C|w)and Pi (C|w, Ci) = Pi (C|Ci), respectively.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"On the development data, the former estimation type obtains a lower performance than the latter.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In a last experiment using only features extracted from the training data, we use the RRM method to compute the function f in Equation (1), allowing the system to select a good performing combination of features.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"At training time, the system was fed the output of each classifier on the cross-classified data, the part-of-speech and chunk boundary tags.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"At test time, the system was fed the classifications of each system trained on the entire training data, and the corresponding POS and chunk boundary tags.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"The result obtained rivals the one obtained by model 2, both displaying a 17% reduction in F-measure error4, indicating that maybe all sources of information have been explored and incorporated.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
The RRM method is showing its combining power when additional information sources are used.,"{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"Specifically, the system was fed additional feature streams from a list of gazetteers and the output of two other named entity systems trained on 1.7M words annotated with 32 name categories.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"The RRM system alone obtains an Fmeasure of 92.1, and can effectively integrate these information streams with the output of the four classifiers, gazetteers and the two additional classifiers into obtaining 93.9 F-measure, as detailed in Table 4, a 21% reduction in F-measure error.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In contrast, combination model 2 obtains only a performance of 92.4, showing its limitations in combining diverse sources of information.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
German poses a completely different problem for named entity recognition: the data is considerably sparser.,"{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
Table 3 shows the relative distribution of unknown words in the development and test corpora.,"{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
We note that the numbers are roughly twice as large for the development data in German as they are for English.,"{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"Since the unknown words are classed by most classifiers, this results in few data points to estimate classifier combinations.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"Also, specifically for the German data, traditional approaches which utilize capitalization do not work as well as in English, because all nouns are capitalized in German.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"For German, in addition to the entity lists provided, we also used a small gazetteer of names (4500 first and last names, 4800 locations in Germany and 190 countries), which was collected by browsing web pages in about two person-hours.","{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
The average classifier performance gain by using these features is about 1.5F for the testa data and about .6F for the testb data.,"{'title': '4 Combination Methodology and Experimental Results', 'number': '4'}"
"In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.","{'title': '5 Conclusion', 'number': '5'}"
"For the German task, the improvement yielded by classifier combination is smaller.","{'title': '5 Conclusion', 'number': '5'}"
"As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.","{'title': '5 Conclusion', 'number': '5'}"
