col1,col2
"This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited.",Introduction
We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed.,Introduction
We further show that different features are needed for different subtasks.,Introduction
"Finally, we show that by using a Maximum Entropy classifier and fewer features, we achieved results comparable with the best previously reported results obtained with SVM models.",Introduction
We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the stateof-the-art in semantic analysis.,Introduction
"There has been growing interest in domainindependent semantic analysis, fed off recent efforts in semantic annotation.",Experiment/Discussion
"The availability of semantically annotated corpora such as the Proposition Banks (Kingsbury and Palmer, 2002; Xue and Palmer, 2003) and FrameNet (Baker et al., 1998) have enabled the development of a rapidly growing list of statistical semantic analyzers (Giidea and Jurafsky, 2002; Giidea and Palmer, 2002; Chen and Rambow, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Sun and Jurafsky, 2004; Palmer et al., submitted).",Experiment/Discussion
"The shared task of the CoNLL-2004 is devoted to semantic role labeling (Carreras and Marquez, 2004).",Experiment/Discussion
Most of these systems generally take as input a syntactic parse tree and use the syntactic information as features to tag the syntactic constituents with semantic role labels.,Experiment/Discussion
"Although these systems have shown great promise, we demonstrate that the features used in previous work have not fully exploited the information that a parse tree provides.",Experiment/Discussion
In this paper we prepose an additional set of features and show that these features lead to fairly significant improvements in the tasks we performed.,Experiment/Discussion
This paper is organized as follows.,Experiment/Discussion
"In the next section, we briefly describe the annotation of the Proposition Bank, the data for our automatic semantic role labeling experiments.",Experiment/Discussion
Section 3 describes the architecture of our system.,Experiment/Discussion
We take a critical look at the previously used features against each subtask and propose a new set of features in Section 4.,Experiment/Discussion
Section 5 presents experimental results that show the effectiveness of these new features and a comparison with previous results.,Experiment/Discussion
We conclude in Section 6.,Experiment/Discussion
"2 The PropBank and Semantic Role Labeling The PropBank adds a layer of semantic annotation to the Treebank II (Marcus et al., 1993; Marcus et al., 1994) to capture generalizations that are not adequately represented in the treebank parse trees.",Experiment/Discussion
"For example, in both John broke the window into a million pieces yesterday and The window broke into a million pieces yesterday, the window plays the same role with regard to the verb break in both sentences even though they occur in different syntactic positions.",Experiment/Discussion
The PropBank annotation captures this regularity by assigning a semantic role label to each argument of the verb independently of its syntactic position.,Experiment/Discussion
This means a fixed set of roles are specified for each verb and a different label is assigned to each role.,Experiment/Discussion
"In PropBank annotations, these roles are labeled with a sequence of integers, starting with 01 and prefixed with ARG.",Experiment/Discussion
"For example, the verb break, has four such numbered arguments: ARGO: the breaker, AR Cl: thing broken, ARC?",Experiment/Discussion
: instrument and ARCS: pieces.,Experiment/Discussion
"It is worth pointing out that even though the same numbers (0-5) are used to label the semantic roles of all verbs, these roles can only be interpreted in a verb-specific 'There are some exceptions. manner.",Experiment/Discussion
"That is, an argument marked with the same number, e.g.",Experiment/Discussion
"ARG, may not share any semantic similarities for different verbs.",Experiment/Discussion
"In addition to the numbered arguments, which are considered to be core to a verb, there are also elements that are less closely related to the verb.",Experiment/Discussion
This roughly parallels the argument/adjunct dichotomy but the distinction may not be drawn along the same lines as in the theoretic linguistics literature.,Experiment/Discussion
"These adjunct-like elements are labeled AR GM, followed by a secondary tag indicating the type of adjunct.",Experiment/Discussion
"For example, yesterday in those abovementioned sentences is not specific to the verb break and instead it applies to a wide variety of verbs.",Experiment/Discussion
"Therefore it will be marked as AR GM, followed by a secondary tag - TMP, indicating the temporal nature of this constituent.",Experiment/Discussion
The secondary tags are effectively a global classification of adjunct-like elements.,Experiment/Discussion
"There are 12 secondary tags for ARCMs in the Proposition Bank: DIR, LOC, MNR, TMP, EXT, REC, PRD, PRP, DIS, ADV, MOD, NE.",Experiment/Discussion
"Some verbs require different sets of arguments for different senses, and accurately characterizing the semantic roles of their arguments necessitates first distinguishing these senses.",Experiment/Discussion
"For example, the verb &quot;pass&quot; takes three arguments, legislative body, bill and law when it means &quot;vote and pass&quot;, while it takes only two arguments entity moving ahead and entity falling behind when it means &quot;overtake&quot;.",Experiment/Discussion
Each sense of this verb is likely to be realized in a set of distinct subcategorization frames and is therefore called a frameset.,Experiment/Discussion
"Semantic role tagging There are different ways to formulate the semantic role tagging task based on the annotation of the PropBank, depending on what type information one wants to learn automatically.",Experiment/Discussion
"For comparison purposes we ignore the frameset information for now, following the practice of Gildea and Palmer (Gildea and Jurafsky, 2002; Pradhan et al., 2003) and others.",Experiment/Discussion
"For each verb, we will predict the core arguments ARG/O-5], as well as the secondary tags for ARCMs.",Experiment/Discussion
The total tagset will 2Modals (MOD) and negation markers (NEG) are clearly not adjuncts.,Experiment/Discussion
"They are included because they are critical to the interpretation of the events be ARG/O-5], ARGa3 ARGM x secondary tags.",Experiment/Discussion
There are also constituents that are not semantic arguments (by semantic arguments we mean both numbered arguments and ARCMs) to a given verb and we will label such constituents NULL.,Experiment/Discussion
Semantic role tagging is thus an one of N classification task.,Experiment/Discussion
"Although it is conceivable that one can simply treat this as a multi-category classification problem, there are at least two reasons why such a simple approach will not work effectively.",Experiment/Discussion
"One is that for a given verb, the majority of the constituents in a syntactic tree are not its semantic arguments.",Experiment/Discussion
"When negative samples (constituents marked NULL) overwhelm positive samples, the current machine-learning algorithms will not be effective.",Experiment/Discussion
"The second reason, which is more subtle, is that information that is effective in separating arguments from NULL elements may not be as effective in distinguishing different types of arguments and vice versa, as we will show in our experiments.",Experiment/Discussion
"Based on these considerations, we will adopt a three-stage architecture: Stage 1: To save training time, we use a simple algorithm to filter out constituents that are clearly not semantic arguments to the predicate in question.",Experiment/Discussion
Stage 2: We then classify the candidates derived from the first stage as either semantic arguments or non-arguments.,Experiment/Discussion
Stage 3: Finally we run a multi-category classifier to classify the constituents that are labeled as arguments into one of the classes plus NULL.,Experiment/Discussion
Step 1: Designate the predicate as the current node and collect its sisters (constituents attached at the same level as the predicate) unless its sisters are coordinated with the predicate.,Experiment/Discussion
"If a sister is a PP, also collect its immediate children. each argument of the verb.",Experiment/Discussion
There are again two experiment conditions.,Experiment/Discussion
"In the first experiment, the constituents that are arguments to a verb is already known, and the task is only to assign the correct semantic role label to the constituents.",Experiment/Discussion
"In the second experiment, this same task is performed on the output of the argument identification task presented in Table 1.",Experiment/Discussion
The same experiments are repeated using automatic parses produced by the Collins parser.,Experiment/Discussion
The results are presented in Table 2.,Experiment/Discussion
"Row 1 presents results of all arguments when functional tags of the ArgMs are predicted, while Row 2 presents results of all arguments when functional tags are ignored.",Experiment/Discussion
Finally Row 3 presents results when only the core arguments (numbered arguments) are calculated.,Experiment/Discussion
"Standard accuracy, CS (f) = Cold Standard fscore, CP = Collins Parser Feature performance Table 3 shows the performance of the new features.",Experiment/Discussion
"The baseline system uses the original features proposed in (Giidea and Palmer, 2002) and each row shows the improvement over the baseline when that feature is added to the baseline features.",Experiment/Discussion
The results are on known (when constituents that are semantic arguments are given) and unknown (when constituents that are arguments have to be identified first before being classified) constituents respectively using Cold Standard Treebank parses.,Experiment/Discussion
"It is clear that the syntactic frame feature results in the most improvement (more than 1.7%) over the baseline, with the head of the PP parent feature being a close second.",Experiment/Discussion
"It is also worth noting that although the feature combining position and voice results in an improvement when the constituents are known, it actually results in a small loss when the constituents are unknown.",Experiment/Discussion
"This indicates that the slight change in the classification task (for classification of unknown constituents, an additional category NULL is added) could change the feature performance.",Experiment/Discussion
"The last three features are from (Pradhan et al., 2004), and they also result in an improvement in performance.",Experiment/Discussion
Rapid progress has been made in semantic role labeling since the Propbank annotation became first available in 2002.,Experiment/Discussion
"The progress can be attributed to better modeling techniques, more relevant features and in a small measure, cleaner annotation.",Experiment/Discussion
"The first system trained on the Propbank is by Giidea and Palmer (2002), who reported 82.8% in accuracy on Cold Standard parses when the constituents that are semantic arguments are given, 67.6% and 53.6% (fmeasure) using Cold Standard and automatic parses respectively when the constituents for the arguments have to be first identified.",Experiment/Discussion
"Since then, various degrees of improvement have been reported (Giidea and Hockenmaier, 2003; Pradhan et al., 2003; Chen and Rambow, 2003).",Experiment/Discussion
"As far as we know the best results so far are reported by (Pradhan et al., 2004), where a wide range of features, including features extracted from named entities, verb clusters and verb senses, temporal cue words, dynamic context, are tested with an SVM classifier.",Experiment/Discussion
Their system achieved an accuracy of 93.0% on known constituents and 89.4% (f-measure) on unknown constituents using Cold Standard parses.,Experiment/Discussion
"They did not report results that use automatic parses with this version of the data, but using a previous version of the data, they reported an fscore of 79.4% using automatic parses (Charniak, 2001).",Experiment/Discussion
"By carefully designing features that can all be directly extracted from the treebank parse trees, our system achieved very comparable results using a Maxent classifier and a much smaller feature set: 92.95% on known constituents, 88.51% on unknown constituents and 76.21% when the Collins parser is used.",Experiment/Discussion
"The results on known constituents are almost identical and the larger difference when automatic parses are used could be attributed to the different parsers, as we used output from an earlier version of the Collins parser.",Experiment/Discussion
"This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, generally a syntactic parse tree, has yet to be fully exploited.",Results/Conclusion
We propose an additional set of features and our experiments show that these features lead to fairly significant improvements in the tasks we performed.,Results/Conclusion
We further show that different features are needed for different subtasks.,Results/Conclusion
"Finally, we show that using a maximum entropy classifier and fewer features, we achieved results that are comparable to the best previously reported results obtained with SVM models.",Results/Conclusion
We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the state-of-the-art in semantic analysis.,Results/Conclusion
We also believe that the features we proposed here are to a large extent complementary to those proposed in a recent work by Pradhan et al (2004) and we intend to incorporate them in our system.,Results/Conclusion
We would like to thank Scott Cotton for providing the PropBank API 6 which greatly simplifies the implementation of our system.,Results/Conclusion
"This work is funded in part by the DOD via grant MDA904-02-C-0412, and in part by the NSF ITR via grant 130-1303-4-541984-XXXX-20001070.",Results/Conclusion
.,Results/Conclusion
