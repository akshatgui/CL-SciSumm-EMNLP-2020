col1,col2
We consider the task of unsupervised lecture segmentation.,{}
We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.,{}
Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.,{}
Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.,{}
The development of computational models of text structure is a central concern in natural language processing.,"{'title': '1 Introduction', 'number': '1'}"
Text segmentation is an important instance of such work.,"{'title': '1 Introduction', 'number': '1'}"
The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text.,"{'title': '1 Introduction', 'number': '1'}"
"The applications of the derived representation are broad, encompassing information retrieval, question-answering and summarization.","{'title': '1 Introduction', 'number': '1'}"
"Not surprisingly, text segmentation has been extensively investigated over the last decade.","{'title': '1 Introduction', 'number': '1'}"
"Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes.","{'title': '1 Introduction', 'number': '1'}"
"When documents exhibit sharp variations in lexical distribution, these algorithms are likely to detect segment boundaries accurately.","{'title': '1 Introduction', 'number': '1'}"
"For example, most algorithms achieve high performance on synthetic collections, generated by concatenation of random text blocks (Choi, 2000).","{'title': '1 Introduction', 'number': '1'}"
"The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle.","{'title': '1 Introduction', 'number': '1'}"
"This is evident in the performance of existing unsupervised algorithms on less structured datasets, such as spoken meeting transcripts (Galley et al., 2003).","{'title': '1 Introduction', 'number': '1'}"
"Therefore, a more refined analysis of lexical distribution is needed.","{'title': '1 Introduction', 'number': '1'}"
Our work addresses this challenge by casting text segmentation in a graph-theoretic framework.,"{'title': '1 Introduction', 'number': '1'}"
"We abstract a text into a weighted undirected graph, where the nodes of the graph correspond to sentences and edge weights represent the pairwise sentence similarity.","{'title': '1 Introduction', 'number': '1'}"
"In this framework, text segmentation corresponds to a graph partitioning that optimizes the normalized-cut criterion (Shi and Malik, 2000).","{'title': '1 Introduction', 'number': '1'}"
This criterion measures both the similarity within each partition and the dissimilarity across different partitions.,"{'title': '1 Introduction', 'number': '1'}"
"Thus, our approach moves beyond localized comparisons and takes into account long-range changes in lexical distribution.","{'title': '1 Introduction', 'number': '1'}"
Our key hypothesis is that global analysis yields more accurate segmentation results than local models.,"{'title': '1 Introduction', 'number': '1'}"
We tested our algorithm on a corpus of spoken lectures.,"{'title': '1 Introduction', 'number': '1'}"
Segmentation in this domain is challenging in several respects.,"{'title': '1 Introduction', 'number': '1'}"
"Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication.","{'title': '1 Introduction', 'number': '1'}"
"In addition, the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in-domain spoken data for training.","{'title': '1 Introduction', 'number': '1'}"
"Finally, pedagogical considerations call for fluent transitions between different topics in a lecture, further complicating the segmentation task.","{'title': '1 Introduction', 'number': '1'}"
Our experimental results confirm our hypothesis: considering long-distance lexical dependencies yields substantial gains in segmentation performance.,"{'title': '1 Introduction', 'number': '1'}"
Our graph-theoretic approach compares favorably to state-of-the-art segmentation algorithms and attains results close to the range of human agreement scores.,"{'title': '1 Introduction', 'number': '1'}"
Another attractive property of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output.,"{'title': '1 Introduction', 'number': '1'}"
Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments.,"{'title': '2 Previous Work', 'number': '2'}"
"Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003).","{'title': '2 Previous Work', 'number': '2'}"
"The focus of our work, however, is on an orthogonal yet fundamental aspect of this analysis — the impact of long-range cohesion dependencies on segmentation performance.","{'title': '2 Previous Work', 'number': '2'}"
"In contrast to previous approaches, the homogeneity of a segment is determined not only by the similarity of its words, but also by their relation to words in other segments of the text.","{'title': '2 Previous Work', 'number': '2'}"
We show that optimizing our global objective enables us to detect subtle topical changes. mentation Our work is inspired by minimum-cutbased segmentation algorithms developed for image analysis.,"{'title': '2 Previous Work', 'number': '2'}"
Shi and Malik (2000) introduced the normalized-cut criterion and demonstrated its practical benefits for segmenting static images.,"{'title': '2 Previous Work', 'number': '2'}"
"Our method, however, is not a simple application of the existing approach to a new task.","{'title': '2 Previous Work', 'number': '2'}"
"First, in order to make it work in the new linguistic framework, we had to redefine the underlying representation and introduce a variety of smoothing and lexical weighting techniques.","{'title': '2 Previous Work', 'number': '2'}"
"Second, the computational techniques for finding the optimal partitioning are also quite different.","{'title': '2 Previous Work', 'number': '2'}"
"Since the minimization of the normalized cut is NP-complete in the general case, researchers in vision have to approximate this computation.","{'title': '2 Previous Work', 'number': '2'}"
"Fortunately, we can find an exact solution due to the linearity constraint on text segmentation.","{'title': '2 Previous Work', 'number': '2'}"
"Linguistic research has shown that word repetition in a particular section of a text is a device for creating thematic cohesion (Halliday and Hasan, 1976), and that changes in the lexical distributions usually signal topic transitions.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
Figure 1 illustrates these properties in a lecture transcript from an undergraduate Physics class.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"The intensity of a point (i, j) on the plot indicates the degree to which the i-th sentence in the text is similar to the j-th sentence.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
The true segment boundaries are denoted by vertical lines.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
This similarity plot reveals a block structure where true boundaries delimit blocks of text with high inter-sentential similarity.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"Sentences found in different blocks, on the other hand, tend to exhibit low similarity.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"Formalizing the Objective Whereas previous unsupervised approaches to segmentation rested on intuitive notions of similarity density, we formalize the objective of text segmentation through cuts on graphs.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
We aim to jointly maximize the intra-segmental similarity and minimize the similarity between different segments.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"In other words, we want to find the segmentation with a maximally homogeneous set of segments that are also maximally different from each other.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"Let G = {V, E} be an undirected, weighted graph, where V is the set of nodes corresponding to sentences in the text and E is the set of weighted edges (See Figure 2).","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"The edge weights, w(u, v), define a measure of similarity between pairs of nodes u and v, where higher scores indicate higher similarity.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
Section 4 provides more details on graph construction.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
We consider the problem of partitioning the graph into two disjoint sets of nodes A and B.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"We aim to minimize the cut, which is defined to be the sum of the crossing edges between the two sets of nodes.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"In other words, we want to split the sentences into two maximally dissimilar classes by choosing A and B to minimize: Decoding Papadimitriou proved that the problem of minimizing normalized cuts on graphs is NP-complete (Shi and Malik, 2000).","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"However, in our case, the multi-way cut is constrained to preserve the linearity of the segmentation.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"By segmentation linearity, we mean that all of the nodes between the leftmost and the rightmost nodes of a particular partition have to belong to that partition.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"With this constraint, we formulate a dynamic programming algorithm for exactly finding the minimum normalized multiway cut in polynomial time: However, we need to ensure that the two partitions are not only maximally different from each other, but also that they are themselves homogeneous by accounting for intra-partition node similarity.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"We formulate this requirement in the framework of normalized cuts (Shi and Malik, 2000), where the cut value is normalized by the volume of the corresponding partitions.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
The volume of the partition is the sum of its edges to the whole graph: The normalized cut criterion (Ncut) is then defined as follows: By minimizing this objective we simultaneously minimize the similarity across partitions and maximize the similarity within partitions.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"This formulation also allows us to decompose the objective into a sum of individual terms, and formulate a dynamic programming solution to the multiway cut problem.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"This criterion is naturally extended to a k-way normalized cut: where A1 ... Ak form a partition of the graph, and V −Ak is the set difference between the entire graph and partition k. C [i, k] is the normalized cut value of the optimal segmentation of the first k sentences into i segments.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"The i-th segment, Aj,k, begins at node uj and ends at node uk.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"B [i, k] is the back-pointer table from which we recover the optimal sequence of segment boundaries.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
Equations 3 and 4 capture respectively the condition that the normalized cut value of the trivial segmentation of an empty text into one segment is zero and the constraint that the first segment starts with the first node.,"{'title': '3 Minimum Cut Framework', 'number': '3'}"
"The time complexity of the dynamic programming algorithm is O(KN2), where K is the number of partitions and N is the number of nodes in the graph or sentences in the transcript.","{'title': '3 Minimum Cut Framework', 'number': '3'}"
"Clearly, the performance of our model depends on the underlying representation, the definition of the pairwise similarity function, and various other model parameters.","{'title': '4 Building the Graph', 'number': '4'}"
In this section we provide further details on the graph construction process.,"{'title': '4 Building the Graph', 'number': '4'}"
"Preprocessing Before building the graph, we apply standard text preprocessing techniques to the text.","{'title': '4 Building the Graph', 'number': '4'}"
"We stem words with the Porter stemmer (Porter, 1980) to alleviate the sparsity of word counts through stem equivalence classes.","{'title': '4 Building the Graph', 'number': '4'}"
We also remove words matching a prespecified list of stop words.,"{'title': '4 Building the Graph', 'number': '4'}"
"Graph Topology As we noted in the previous section, the normalized cut criterion considers long-term similarity relationships between nodes.","{'title': '4 Building the Graph', 'number': '4'}"
This effect is achieved by constructing a fullyconnected graph.,"{'title': '4 Building the Graph', 'number': '4'}"
"However, considering all pairwise relations in a long text may be detrimental to segmentation accuracy.","{'title': '4 Building the Graph', 'number': '4'}"
"Therefore, we discard edges between sentences exceeding a certain threshold distance.","{'title': '4 Building the Graph', 'number': '4'}"
This reduction in the graph size also provides us with computational savings.,"{'title': '4 Building the Graph', 'number': '4'}"
"Similarity Computation In computing pairwise sentence similarities, sentences are represented as vectors of word counts.","{'title': '4 Building the Graph', 'number': '4'}"
"Cosine similarity is commonly used in text segmentation (Hearst, 1994).","{'title': '4 Building the Graph', 'number': '4'}"
"To avoid numerical precision issues when summing a series of very small scores, we compute exponentiated cosine similarity scores between pairs of sentence vectors: We further refine our analysis by smoothing the similarity metric.","{'title': '4 Building the Graph', 'number': '4'}"
"When comparing two sentences, we also take into account similarity between their immediate neighborhoods.","{'title': '4 Building the Graph', 'number': '4'}"
The smoothing is achieved by adding counts of words that occur in adjoining sentences to the current sentence feature vector.,"{'title': '4 Building the Graph', 'number': '4'}"
"These counts are weighted in accordance to their distance from the current sentence: e−α(j−i)sj, where si are vectors of word counts, and α is a parameter that controls the degree of smoothing.","{'title': '4 Building the Graph', 'number': '4'}"
In the formulation above we use sentences as our nodes.,"{'title': '4 Building the Graph', 'number': '4'}"
"However, we can also represent graph nodes with non-overlapping blocks of words of fixed length.","{'title': '4 Building the Graph', 'number': '4'}"
"This is desirable, since the lecture transcripts lack sentence boundary markers, and short utterances can skew the cosine similarity scores.","{'title': '4 Building the Graph', 'number': '4'}"
The optimal length of the block is tuned on a heldout development set.,"{'title': '4 Building the Graph', 'number': '4'}"
"Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001).","{'title': '4 Building the Graph', 'number': '4'}"
Of particular concern are words that may not be common in general English discourse but that occur throughout the text for a particular lecture or subject.,"{'title': '4 Building the Graph', 'number': '4'}"
"For example, in a lecture about support vector machines, the occurrence of the term “SVM” is not going to convey a lot of information about the distribution of sub-topics, even though it is a fairly rare term in general English and bears much semantic content.","{'title': '4 Building the Graph', 'number': '4'}"
"The same words can convey varying degrees of information across different lectures, and term weighting specific to individual lectures becomes important in the similarity computation.","{'title': '4 Building the Graph', 'number': '4'}"
"In order to address this issue, we introduce a variation on the tf-idf scoring scheme used in the information-retrieval literature (Salton and Buckley, 1988).","{'title': '4 Building the Graph', 'number': '4'}"
A transcript is split uniformly into N chunks; each chunk serves as the equivalent of documents in the tf-idf computation.,"{'title': '4 Building the Graph', 'number': '4'}"
"The weights are computed separately for each transcript, since topic and word distributions vary across lectures.","{'title': '4 Building the Graph', 'number': '4'}"
In this section we present the different corpora used to evaluate our model and provide a brief overview of the evaluation metrics.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Next, we describe our human segmentation study on the corpus of spoken lecture data.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"A heldout development set of three lectures isused for estimating the optimal word block length for representing nodes, the threshold distances for discarding node edges, the number of uniform chunks for estimating tf-idf lexical weights, the alpha parameter for smoothing, and the length of the smoothing window.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
We use a simple greedy search procedure for optimizing the parameters.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We evaluate our segmentation algorithm on three sets of data.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Two of the datasets we use are new segmentation collections that we have compiled for this study,1 and the remaining set includes a standard collection previously used for evaluation of segmentation algorithms.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Various corpus statistics for the new datasets are presented in Table 1.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
Below we briefly describe each corpus.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
Physics Lectures Our first corpus consists of spoken lecture transcripts from an undergraduate Physics class.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"In contrast to other segmentation datasets, our corpus contains much longer texts.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"A typical lecture of 90 minutes has 500 to 700 sentences with 8500 words, which corresponds to about 15 pages of raw text.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
We have access both to manual transcriptions of these lectures and also output from an automatic speech recognition system.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The word error rate for the latter is 19.4%,2 which is representative of state-of-the-art performance on lecture material (Leeuwis et al., 2003).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
The Physics lecture transcript segmentations were produced by the teaching staff of the introductory Physics course at the Massachusetts Institute of Technology.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
Their objective was to facilitate access to lecture recordings available on the class website.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
This segmentation conveys the high-level topical structure of the lectures.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"On average, a lecture was annotated with six segments, and a typical segment corresponds to two pages of a transcript.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Artificial Intelligence Lectures Our second lecture corpus differs in subject matter, lecturing style, and segmentation granularity.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The graduate Artificial Intelligence class has, on average, twelve segments per lecture, and a typical segment is about half of a page.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
One segment roughly corresponds to the content of a slide.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
This time the segmentation was obtained from the lecturer herself.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
The lecturer went through the transcripts of lecture recordings and segmented the lectures with the objective of making the segments correspond to presentation slides for the lectures.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Due to the low recording quality, we were unable to obtain the ASR transcripts for this class.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Therefore, we only use manual transcriptions of these lectures.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created by Choi (2000) which is commonly used in the evaluation of segmentation algorithms.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
This corpus consists of a set of concatenated segments randomly sampled from the Brown corpus.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
The length of the segments in this corpus ranges from three to eleven sentences.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"It is important to note that the lexical transitions in these concatenated texts are very sharp, since the segments come from texts written in widely varying language styles on completely different topics.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
The Pk measure estimates the probability that a randomly chosen pair of words within a window of length k words is inconsistently classified.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The WindowDiff metric is a variant of the Pk measure, which penalizes false positives on an equal basis with near misses.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Both of these metrics are defined with respect to the average segment length of texts and exhibit high variability on real data.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We follow Choi (2000) and compute the mean segment length used in determining the parameter k on each reference text separately.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We also plot the Receiver Operating Characteristic (ROC) curve to gauge performance at a finer level of discrimination (Swets, 1988).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
The ROC plot is the plot of the true positive rate against the false positive rate for various settings of a decision criterion.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"In our case, the true positive rate is the fraction of boundaries correctly classified, and the false positive rate is the fraction of non-boundary positions incorrectly classified as boundaries.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"In computing the true and false positive rates, we vary the threshold distance to the true boundary within which a hypothesized boundary is considered correct.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Larger areas under the ROC curve of a classifier indicate better discriminative performance.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Spoken lectures are very different in style from other corpora used in human segmentation studies (Hearst, 1994; Galley et al., 2003).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
We are interested in analyzing human performance on a corpus of lecture transcripts with much longer texts and a less clear-cut concept of a sub-topic.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We define a segment to be a sub-topic that signals a prominent shift in subject matter.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
Disregarding this sub-topic change would impair the high-level understanding of the structure and the content of the lecture.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"As part of our human segmentation analysis, we asked three annotators to segment the Physics lecture corpus.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
These annotators had taken the class in the past and were familiar with the subject matter under consideration.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We wrote a detailed instruction manual for the task, with annotation guidelines for the most part following the model used by Gruenstein et al. (2005).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
The annotators were instructed to segment at a level of granularity that would identify most of the prominent topical transitions necessary for a summary of the lecture.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The annotators used the NOMOS annotation software toolkit, developed for meeting segmentation (Gruenstein et al., 2005).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
They were provided with recorded audio of the lectures and the corresponding text transcriptions.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We intentionally did not provide the subjects with the target number of boundaries, since we wanted to see if the annotators would converge on a common segmentation granularity.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Table 2 presents the annotator segmentation statistics.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We see two classes of segmentation granularities.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The original reference (O) and annotator A segmented at a coarse level with an average of 6.6 and 8.9 segments per lecture, respectively.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Annotators B and C operated at much finer levels of discrimination with 18.4 and 13.8 segments per lecture on average.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We conclude that multiple levels of granularity are acceptable in spoken lecture segmentation.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
This is expected given the length of the lectures and varying human judgments in selecting relevant topical content.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Following previous studies, we quantify the level of annotator agreement with the Pk measure (Gruenstein et al., 2005).3 Table 3 shows the annotator agreement scores between different pairs of annotators.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Pk measures ranged from 0.24 and 0.42.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We observe greater consistency at similar levels of granularity, and less so across the two classes.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Note that annotator A operated at a level of granularity consistent with the original reference segmentation.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Hence, the 0.24 Pk measure score serves as the benchmark with which we can compare the results attained by segmentation algorithms on the Physics lecture data.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"As an additional point of reference we note that the uniform and random baseline segmentations attain 0.469 and 0.493 Pk measure, respectively, on the Physics lecture set.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Figure 3: ROC plot for the Minimum Cut Segmenter on thirty Physics Lectures, with edge cutoffs set at five and hundred sentences.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Benefits of global analysis We first determine the impact of long-range pairwise similarity dependencies on segmentation performance.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Our rithms using the Pk and WindowDiff measures, with three lectures heldout for development. key hypothesis is that considering long-distance lexical relations contributes to the effectiveness of the algorithm.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"To test this hypothesis, we discard edges between nodes that are more than a certain number of sentences apart.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
We also include segmentation results on Physics ASR transcripts.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
The results in Table 4 confirm our hypothesis — taking into account non-local lexical dependencies helps across different domains.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"On manually transcribed Physics lecture data, for example, the algorithm yields 0.394 Pk measure when taking into account edges separated by up to ten sentences.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"When dependencies up to a hundred sentences are considered, the algorithm yields a 25% reduction in Pk measure.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Figure 3 shows the ROC plot for the segmentation of the Physics lecture data with different cutoff parameters, again demonstrating clear gains attained by employing longrange dependencies.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"As Table 4 shows, the improvement is consistent across all lecture datasets.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We note, however, that after some point increasing the threshold degrades performance, because it introduces too many spurious dependencies (see the last column of Table 4).","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The speaker will occasionally return to a topic described at the beginning of the lecture, and this will bias the algorithm to put the segment boundary closer to the end of the lecture.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Long-range dependencies do not improve the performance on the synthetic dataset.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"This is expected since the segments in the synthetic dataset are randomly selected from widely-varying documents in the Brown corpus, even spanning different genres of written language.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"So, effectively, there are no genuine long-range dependencies that can be exploited by the algorithm.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Comparison with local dependency models We compare our system with the state-of-the-art similarity-based segmentation system developed by Choi (2000).,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a heldout development set of three lectures.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"To control for segmentation granularity, we specify the number of segments in the reference (“O”) segmentation for both our system and the baseline.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Table 5 shows that the Minimum Cut algorithm consistently outperforms the similarity-based baseline on all the lecture datasets.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We attribute this gain to the presence of more attenuated topic transitions in spoken language.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Since spoken language is more spontaneous and less structured than written language, the speaker needs to keep the listener abreast of the changes in topic content by introducing subtle cues and references to prior topics in the course of topical transitions.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Non-local dependencies help to elucidate shifts in focus, because the strength of a particular transition is measured with respect to other local and long-distance contextual discourse relationships.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
Our system does not outperform Choi’s algorithm on the synthetic data.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
This again can be attributed to the discrepancy in distributional properties of the synthetic corpus which lacks coherence in its thematic shifts and the lecture corpus of spontaneous speech with smooth distributional variations.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We also note that we did not try to adjust our model to optimize its performance on the synthetic data.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
The smoothing method developed for lecture segmentation may not be appropriate for short segments ranging from three to eleven sentences that constitute the synthetic set.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
We also compared our method with another state-of-the-art algorithm which does not explicitly rely on pairwise similarity analysis.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
We used the publicly available implementation of the system that does not require parameter tuning on a heldout development set.,"{'title': '5 Evaluation Set-Up', 'number': '5'}"
"Again, our method achieves favorable performance on a range of lecture data sets (See Table 5), and both algorithms attain results close to the range of human agreement scores.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The attractive feature of our algorithm, however, is robustness to recognition errors — testing it on the ASR transcripts caused only 7.8% relative increase in Pk measure (from 0.298 to 0.322), compared to a 13.5% relative increase for the UI system.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"We attribute this feature to the fact that the model is less dependent on individual recognition errors, which have a detrimental effect on the local segment language modeling probability estimates for the UI system.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
"The block-level similarity function is not as sensitive to individual word errors, because the partition volume normalization factor dampens their overall effect on the derived models.","{'title': '5 Evaluation Set-Up', 'number': '5'}"
In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation.,"{'title': '7 Conclusions', 'number': '6'}"
We modeled text segmentation as a graphpartitioning task aiming to simultaneously optimize the total similarity within each segment and dissimilarity across various segments.,"{'title': '7 Conclusions', 'number': '6'}"
We showed that global analysis of lexical distribution improves the segmentation accuracy and is robust in the presence of recognition errors.,"{'title': '7 Conclusions', 'number': '6'}"
"Combining global analysis with advanced methods for smoothing (Ji and Zha, 2003) and weighting could further boost the segmentation performance.","{'title': '7 Conclusions', 'number': '6'}"
Our current implementation does not automatically determine the granularity of a resulting segmentation.,"{'title': '7 Conclusions', 'number': '6'}"
"This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.","{'title': '7 Conclusions', 'number': '6'}"
"We believe that the algorithm has to produce segmentations for various levels of granularity, depending on the needs of the application that employs it.","{'title': '7 Conclusions', 'number': '6'}"
Our ultimate goal is to automatically generate tables of content for lectures.,"{'title': '7 Conclusions', 'number': '6'}"
We plan to investigate strategies for generating titles that will succinctly describe the content of each segment.,"{'title': '7 Conclusions', 'number': '6'}"
We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.,"{'title': '7 Conclusions', 'number': '6'}"
"The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168, grant IIS0415865, and the NSF Graduate Fellowship).","{'title': '8 Acknowledgements', 'number': '7'}"
"Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.","{'title': '8 Acknowledgements', 'number': '7'}"
We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.,"{'title': '8 Acknowledgements', 'number': '7'}"
We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.,"{'title': '8 Acknowledgements', 'number': '7'}"
"We also would like to acknowledge the MIT NLP and Speech Groups, the three annotators, and the three anonymous reviewers for valuable comments, suggestions, and help.","{'title': '8 Acknowledgements', 'number': '7'}"
