col1,col2
In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.,{}
"Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.",{}
"We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.",{}
"In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.",{}
Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context.,"{'title': '1 Introduction', 'number': '1'}"
"Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build.","{'title': '1 Introduction', 'number': '1'}"
"Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin.","{'title': '1 Introduction', 'number': '1'}"
"As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.","{'title': '1 Introduction', 'number': '1'}"
Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context.,"{'title': '1 Introduction', 'number': '1'}"
"Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004).","{'title': '1 Introduction', 'number': '1'}"
One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words.,"{'title': '1 Introduction', 'number': '1'}"
"Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time.","{'title': '1 Introduction', 'number': '1'}"
"Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008).","{'title': '1 Introduction', 'number': '1'}"
These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.,"{'title': '1 Introduction', 'number': '1'}"
"Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.","{'title': '1 Introduction', 'number': '1'}"
"Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context.","{'title': '1 Introduction', 'number': '1'}"
"In this sense, they provide a principled solution to the exponential explosion problem, with excellent performance.","{'title': '1 Introduction', 'number': '1'}"
Graph-based WSD is performed over a graph composed by senses (nodes) and relations between pairs of senses (edges).,"{'title': '1 Introduction', 'number': '1'}"
"The relations may be of several types (lexico-semantic, coocurrence relations, etc.) and may have some weight attached to them.","{'title': '1 Introduction', 'number': '1'}"
"The disambiguation is typically performed by applying a ranking algorithm over the graph, and then assigning the concepts with highest rank to the corresponding words.","{'title': '1 Introduction', 'number': '1'}"
"Given the computational cost of using large graphs like WordNet, many researchers use smaller subgraphs built online for each target context.","{'title': '1 Introduction', 'number': '1'}"
"In this paper we present a novel graph-based WSD algorithm which uses the full graph of WordNet efficiently, performing significantly better that previously published approaches in English all-words datasets.","{'title': '1 Introduction', 'number': '1'}"
"We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.","{'title': '1 Introduction', 'number': '1'}"
The algorithm is publicly available2 and can be applied easily to sense inventories and knowledge bases different from WordNet.,"{'title': '1 Introduction', 'number': '1'}"
"Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.","{'title': '1 Introduction', 'number': '1'}"
The paper is structured as follows.,"{'title': '1 Introduction', 'number': '1'}"
We first describe the PageRank and Personalized PageRank algorithms.,"{'title': '1 Introduction', 'number': '1'}"
Section 3 introduces the graph based methods used for WSD.,"{'title': '1 Introduction', 'number': '1'}"
"Section 4 shows the experimental setting and the main results, and Section 5 compares our methods with related experiments on graph-based WSD systems.","{'title': '1 Introduction', 'number': '1'}"
Section 6 shows the results of the method when applied to a Spanish dataset.,"{'title': '1 Introduction', 'number': '1'}"
Section 7 analyzes the performance of the algorithm.,"{'title': '1 Introduction', 'number': '1'}"
"Finally, we draw some conclusions in Section 8.","{'title': '1 Introduction', 'number': '1'}"
"The celebrated PageRank algorithm (Brin and Page, 1998) is a method for ranking the vertices in a graph according to their relative structural importance.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the probability of a random walk over the graph ending on node i, at a sufficiently large time.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Let G be a graph with N vertices vi, ... , vN and di be the outdegree of node i; let M be a In the equation, v is a N × 1 vector whose elements are 1N and c is the so called damping factor, a scalar value between 0 and 1.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
The first term of the sum on the equation models the voting scheme described in the beginning of the section.,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"The second term represents, loosely speaking, the probability of a surfer randomly jumping to any node, e.g. without following any paths on the graph.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"The damping factor, usually set in the [0.85..0.95] range, models the way in which these two terms are combined at each step.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
The second term on Eq.,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"(1) can also be seen as a smoothing factor that makes any graph fulfill the property of being aperiodic and irreducible, and thus guarantees that PageRank calculation converges to a unique stationary distribution.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"In the traditional PageRank formulation the vector v is a stochastic normalized vector whose element values are all 1N, thus assigning equal probabilities to all nodes in the graph in case of random jumps.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"However, as pointed out by (Haveliwala, 2002), the vector v can be non-uniform and assign stronger probabilities to certain kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"For example, if we concentrate all the probability mass on a unique node i, all random jumps on the walk will return to i and thus its rank will be high; moreover, the high rank of i will make all the nodes in its vicinity also receive a high rank.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Thus, the importance of node i given by the initial distribution of v spreads along the graph on successive iterations of the algorithm.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"In this paper, we will use traditional PageRank to refer to the case when a uniform v vector is used in Eq.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"(1); and whenever a modified v is used, we will call it Personalized PageRank.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
The next section shows how we define a modified v. PageRank is actually calculated by applying an iterative algorithm which computes Eq.,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"(1) successively until convergence below a given threshold is achieved, or, more typically, until a fixed number of iterations are executed.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
We did not try other damping factors.,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
Section 7 for further details).,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"Note that, in order to discard the effect of dangling nodes (i.e. nodes without outlinks) we slightly modified Eq.","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
(1).,"{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
"For the sake of brevity we omit the details, which the interested reader can check in (Langville and Meyer, 2003).","{'title': '2 PageRank and Personalized PageRank', 'number': '2'}"
In this section we present the application of PageRank to WSD.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"If we were to apply the traditional PageRank over the whole WordNet we would get a context-independent ranking of word senses, which is not what we want.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Given an input piece of text (typically one sentence, or a small set of contiguous sentences), we want to disambiguate all open-class words in the input taken the rest as context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In this framework, we need to rank the senses of the target words according to the other words in the context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
Theare two main alternatives to achieve this: The first method has been explored in the literature (cf.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In both cases, the algorithms return a list of ranked senses for each target word in the context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"We will see each of them in turn, but first we will present some notation and a preliminary step.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"A LKB is formed by a set of concepts and relations among them, and a dictionary, i.e., a list of words (typically, word lemmas) each of them linked to at least one concept of the LKB.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Given any such LKB, we build an undirected graph G = (V, E) where nodes represent LKB concepts (vi), and each relation between concepts vi and vj is represented by an undirected edge ei,j.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In our experiments we have tried our algorithms using three different LKBs: Given an input text, we extract the list Wi i = 1... m of content words (i.e. nouns, verbs, adjectives and adverbs) which have an entry in the dictionary, and thus can be related to LKB concepts.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Let Conceptsi = {v1, ... , vi.} be the im associated concepts of word Wi in the LKB graph.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Note that monosemous words will be related to just one concept, whereas polysemous words may be attached to several.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"As a result of the disambiguation process, every concept in Conceptsi, i = 1, ... , m receives a score.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Then, for each target word to be disambiguated, we just choose its associated concept in G with maximal score.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In our experiments we build a context of at least 20 content words for each sentence to be disambiguated, taking the sentences immediately before and after it in the case that the original sentence was too short.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
The main idea of the subgraph method is to extract the subgraph of GKB whose vertices and relations are particularly relevant for a given input context.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Such a subgraph is called a “disambiguation subgraph” GD, and it is built in the following way.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"For each word Wi in the input context and each concept vi E Conceptsi, a standard breathfirst search (BFS) over GKB is performed, starting at node vi.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB .,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In particular, we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context, vj E Uj=,4i Conceptsj.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
Let mdpvi be the set of these shortest paths.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"This BFS computation is repeated for every concept of every word in the input context, storing mdpvi accordingly.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"At the end, we obtain a set of minimum length paths each of them having a different concept as a source.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"The disambiguation graph GD is then just the union of the vertices and edges of the shortest paths, GD = Umi=1{mdpv,/vj E Conceptsi}.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
The disambiguation graph GD is thus a subgraph of the original GKB graph obtained by computing the shortest paths between the concepts of the words co-occurring in the context.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Thus, we hypothesize that it captures the most relevant concepts and relations in the knowledge base for the particular input context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Once the GD graph is built, we compute the traditional PageRank algorithm over it.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"The intuition behind this step is that the vertices representing the correct concepts will be more relevant in GD than the rest of the possible concepts of the context words, which should have less relations on average and be more isolated.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"As usual, the disambiguation step is performed by assigning to each word Wi the associated concept in Conceptsi which has maximum rank.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
In case of ties we assign all the concepts with maximum rank.,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Note that the standard evaluation script provided in the Senseval competitions treats multiple senses as if one was chosen at random, i.e. for evaluation purposes our method is equivalent to breaking ties at random.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"As mentioned before, personalized PageRank allows us to use the full LKB.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"We first insert the context words into the graph G as nodes, and link them with directed edges to their respective concepts.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Then, we compute the personalized PageRank of the graph G by concentrating the initial probability mass uniformly over the newly introduced word nodes.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"As the words are linked to the concepts by directed edges, they act as source nodes injecting mass into the concepts they are associated with, which thus become relevant nodes, and spread their mass over the LKB graph.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Therefore, the resulting personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"One problem with Personalized PageRank is that if one of the target words has two senses which are related by semantic relations, those senses reinforce each other, and could thus dampen the effect of the other senses in the context.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"With this observation in mind we devised a variant (dubbed Ppr w2w), where we build the graph for each target word in the context: for each target word Wi, we concentrate the initial probability mass in the senses of the words surrounding Wi, but not in the senses of the target word itself, so that context words increase its relative importance in the graph.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
"Contrary to the other two approaches, Ppr w2w does not disambiguate all target words of the context in a single run, which makes it less efficient (cf.","{'title': '3 Using PageRank for WSD', 'number': '3'}"
Section 7).,"{'title': '3 Using PageRank for WSD', 'number': '3'}"
"In this paper we will use two datasets for comparing graph-based WSD methods, namely, the Senseval-2 (S2AW) and Senseval-3 (S3AW) all words datasets (Snyder and Palmer, 2004; Palmer et al., 2001), which are both labeled with WordNet 1.7 tags.","{'title': '4 Evaluation framework and results', 'number': '4'}"
"We did not use the Semeval dataset, for the sake of comparing our results to related work, none of which used Semeval data.","{'title': '4 Evaluation framework and results', 'number': '4'}"
Table 1 shows the results as recall of the graph-based WSD system over these datasets on the different LKBs.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
"We detail overall results, as well as results per PoS, and the confidence interval for the overall results.","{'title': '4 Evaluation framework and results', 'number': '4'}"
The interval was computed using bootstrap resampling with 95% confidence.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
The table shows that Ppr w2w is consistently the best method in both datasets and for all LKBs.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
"Ppr and Spr obtain comparable results, which is remarkable, given the simplicity of the Ppr algobaseline and the best results of supervised systems at competition time (SMUaw,GAMBL). rithm, compared to the more elaborate algorithm to construct the graph.","{'title': '4 Evaluation framework and results', 'number': '4'}"
"The differences between methods are not statistically significant, which is a common problem on this relatively small datasets (Snyder and Palmer, 2004; Palmer et al., 2001).","{'title': '4 Evaluation framework and results', 'number': '4'}"
"Regarding LKBs, the best results are obtained using WordNet 1.7 and eXtended WordNet.","{'title': '4 Evaluation framework and results', 'number': '4'}"
Here the differences are in many cases significant.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
"These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the eXtended WordNet (linked to version 1.7).","{'title': '4 Evaluation framework and results', 'number': '4'}"
The lower performance of WNet30+gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
"When using a different LKB for WSD, a mapping to WordNet 1.7 is required.","{'title': '4 Evaluation framework and results', 'number': '4'}"
"Although the mapping is cited as having a correctness on the high 90s (Daude et al., 2000), it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses.","{'title': '4 Evaluation framework and results', 'number': '4'}"
"Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively).","{'title': '4 Evaluation framework and results', 'number': '4'}"
"The MFS is a baseline for supervised systems, but it is considered a difficult competitor for unsupervised systems, which rarely come close to it.","{'title': '4 Evaluation framework and results', 'number': '4'}"
In this case the MFS baseline was computed using previously availabel training data like SemCor.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
Our best results are close to the MFS in both Senseval-2 and Senseval-3 datasets.,"{'title': '4 Evaluation framework and results', 'number': '4'}"
"The results for the supervised system are given for reference, and we can see that the gap is relatively small, specially for Senseval3.","{'title': '4 Evaluation framework and results', 'number': '4'}"
In this section we will briefly describe some graph-based methods for knowledge-based WSD.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"The methods here presented cope with the problem of sequence-labeling, i.e., they disambiguate all the words coocurring in a sequence (typically, all content words of a sentence).","{'title': '5 Comparison to Related work', 'number': '5'}"
"All the methods rely on the information represented on some LKB, which typically is some version of WordNet, sometimes enriched with proprietary relations.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The results on our datasets, when available, are shown in Table 2.","{'title': '5 Comparison to Related work', 'number': '5'}"
The table also shows the performance of supervised systems.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"The TexRank algorithm (Mihalcea, 2005) for WSD creates a complete weighted graph (e.g. a graph where every pair of distinct vertices is connected by a weighted edge) formed by the synsets of the words in the input context.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The weight of the links joining two synsets is calculated by executing Lesk’s algorithm (Lesk, 1986) between them, i.e., by calculating the overlap between the words in the glosses of the correspongind senses.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Once the complete graph is built, the PageRank algorithm is executed over it and words are assigned to the most relevant synset.","{'title': '5 Comparison to Related work', 'number': '5'}"
"In this sense, PageRank is used an alternative to simulated annealing to find the optimal pairwise combinations.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The method was evaluated on the Senseval-3 dataset, as shown in row Mih05 on Table 2.","{'title': '5 Comparison to Related work', 'number': '5'}"
"(Sinha and Mihalcea, 2007) extends their previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets.","{'title': '5 Comparison to Related work', 'number': '5'}"
They also compare different graph-based centrality algorithms to rank the vertices of the complete graph.,"{'title': '5 Comparison to Related work', 'number': '5'}"
They use different similarity metrics for different POS types and a voting scheme among the centrality algorithm ranks.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"Here, the Senseval-3 corpus was used as a development data set, and we can thus see those results as the upper-bound of their method.","{'title': '5 Comparison to Related work', 'number': '5'}"
We can see in Table 2 that the methods presented in this paper clearly outperform both Mih05 and Sin07.,"{'title': '5 Comparison to Related work', 'number': '5'}"
This result suggests that analyzing the LKB structure as a whole is preferable than computing pairwise similarity measures over synsets.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"The results of various in-house made experiments replicating (Mihalcea, 2005) also confirm this observation.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Note also that our methods are simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did.","{'title': '5 Comparison to Related work', 'number': '5'}"
"In (Navigli and Velardi, 2005) the authors develop a knowledge-based WSD method based on lexical chains called structural semantic interconnections (SSI).","{'title': '5 Comparison to Related work', 'number': '5'}"
"Although the system was first designed to find the meaning of the words in WordNet glosses, the authors also apply the method for labeling text sequences.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Given a text sequence, SSI first identifies monosemous words and assigns the corresponding synset to them.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Then, it iteratively disambiguates the rest of terms by selecting the senses that get the strongest interconnection with the synsets selected so far.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The interconnection is calculated by searching for paths on the LKB, constrained by some hand-made rules of possible semantic patterns.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The method was evaluated on the Senseval-3 dataset, as shown in row Nav05 on Table 2.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Note that the method labels an instance with the most frequent sense of the word if the algorithm produces no output for that instance, which makes comparison to our system unfair, specially given the fact that the MFS performs better than SSI.","{'title': '5 Comparison to Related work', 'number': '5'}"
In fact it is not possible to separate the effect of SSI from that of the MFS.,"{'title': '5 Comparison to Related work', 'number': '5'}"
For this reason we place this method close to the MFS baseline in Table 2.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"In (Navigli and Lapata, 2007), the authors perform a two-stage process for WSD.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Given an input context, the method first explores the whole LKB in order to find a subgraph which is particularly relevant for the words of the context.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Then, they study different graph-based centrality algorithms for deciding the relevance of the nodes on the subgraph.","{'title': '5 Comparison to Related work', 'number': '5'}"
"As a result, every word of the context is attached to the highest ranking concept among its possible senses.","{'title': '5 Comparison to Related work', 'number': '5'}"
"The Spr method is very similar to (Navigli and Lapata, 2007), the main difference lying on the initial method for extracting the context subgraph.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Whereas (Navigli and Lapata, 2007) apply a depth-first search algorithm over the LKB graph —and restrict the depth of the subtree to a value of 3—, Spr relies on shortest paths between word synsets.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Navigli and Lapata don’t report overall results and therefore, we can’t directly compare our results with theirs.","{'title': '5 Comparison to Related work', 'number': '5'}"
"However, we can see that on a PoS-basis evaluation our results are consistently better for nouns and verbs (especially the Ppr w2w method) and rather similar for adjectives.","{'title': '5 Comparison to Related work', 'number': '5'}"
"(Tsatsaronis et al., 2007) is another example of a two-stage process, the first one consisting on finding a relevant subgraph by performing a BFS dataset, including MFS and the best supervised system in the competition. search over the LKB.","{'title': '5 Comparison to Related work', 'number': '5'}"
The authors apply a spreading activation algorithm over the subgraph for node ranking.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"Edges of the subgraph are weighted according to its type, following a tf.idf like approach.","{'title': '5 Comparison to Related work', 'number': '5'}"
The results show that our methods clearly outperform Tsatsa07.,"{'title': '5 Comparison to Related work', 'number': '5'}"
The fact that the Spr method works better suggests that the traditional PageRank algorithm is a superior method for ranking the subgraph nodes.,"{'title': '5 Comparison to Related work', 'number': '5'}"
"As stated before, all methods presented here use some LKB for performing WSD.","{'title': '5 Comparison to Related work', 'number': '5'}"
"(Mihalcea, 2005) and (Sinha and Mihalcea, 2007) use WordNet relations as a knowledge source, but neither of them specify which particular version did they use.","{'title': '5 Comparison to Related work', 'number': '5'}"
"(Tsatsaronis et al., 2007) uses WordNet 1.7 enriched with eXtended WordNet relations, just as we do.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Both (Navigli and Velardi, 2005; Navigli and Lapata, 2007) use WordNet 2.0 as the underlying LKB, albeit enriched with several new relations, which are manually created.","{'title': '5 Comparison to Related work', 'number': '5'}"
"Unfortunately, those manual relations are not publicly available, so we can’t directly compare their results with the rest of the methods.","{'title': '5 Comparison to Related work', 'number': '5'}"
"In (Agirre and Soroa, 2008) we experiment with different LKBs formed by combining relations of different MCR versions along with relations extracted from SemCor, which we call supervised and unsupervised relations, respectively.","{'title': '5 Comparison to Related work', 'number': '5'}"
The unsupervised relations that yielded bests results are also used in this paper (c.f Section 3.1).,"{'title': '5 Comparison to Related work', 'number': '5'}"
"Our WSD algorithm can be applied over nonenglish texts, provided that a LKB for this particular language exists.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"We have tested the graphalgorithms proposed in this paper on a Spanish dataset, using the Spanish WordNet as knowledge source (Atserias et al., 2004a).","{'title': '6 Experiments on Spanish', 'number': '6'}"
"We used the Semeval-2007 Task 09 dataset as evaluation gold standard (M`arquez et al., 2007).","{'title': '6 Experiments on Spanish', 'number': '6'}"
"The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish WordNet synsets.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"It is split into a train and test part, and has an “all words” shape i.e. input consists on sentences, each one having at least one occurrence of a target noun.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"It contains 105, 501 nodes and 623,316 relations.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"The results in Table 3 are consistent with those for English, with our algorithm approaching MFS performance.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"Note that for this dataset the supervised algorithm could barely improve over the MFS, suggesting that for this particular dataset MFS is particularly strong.","{'title': '6 Experiments on Spanish', 'number': '6'}"
"Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset, using the WNet17 + Xwn as LKB.","{'title': '7 Performance analysis', 'number': '7'}"
The dataset consists on 2473 word instances appearing on 476 different sentences.,"{'title': '7 Performance analysis', 'number': '7'}"
The experiments were done on a computer with four 2.66 Ghz processors and 16 Gb memory.,"{'title': '7 Performance analysis', 'number': '7'}"
The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).,"{'title': '7 Performance analysis', 'number': '7'}"
"The Spr method lies in between, requiring 2 hours for completing the task, but its overall performance is well below the PageRank based Ppr w2w method.","{'title': '7 Performance analysis', 'number': '7'}"
"Note that the algorithm is coded in C++ for greater efficiency, and uses the Boost Graph Library.","{'title': '7 Performance analysis', 'number': '7'}"
"Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.","{'title': '7 Performance analysis', 'number': '7'}"
Figure 1 depicts the performance of the Ppr w2w method for different iterations of the algorithm.,"{'title': '7 Performance analysis', 'number': '7'}"
"As before, the algorithm is applied over the MCR17 + Xwn LKB, and evaluated on the Senseval-2 all words dataset.","{'title': '7 Performance analysis', 'number': '7'}"
"The algorithm converges very quickly: one sole iteration suffices for achieving a relatively high performance, and 20 iterations are enough for achieving convergence.","{'title': '7 Performance analysis', 'number': '7'}"
"The figure shows that, depending on the LKB complexity, the user can tune the algorithm and lower the number of iterations, thus considerably reducing the time required for disambiguation.","{'title': '7 Performance analysis', 'number': '7'}"
In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambuation.,"{'title': '8 Conclusions', 'number': '8'}"
"Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.","{'title': '8 Conclusions', 'number': '8'}"
"We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.","{'title': '8 Conclusions', 'number': '8'}"
Both for Spanish and English the algorithm attains performances close to the MFS.,"{'title': '8 Conclusions', 'number': '8'}"
The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.,"{'title': '8 Conclusions', 'number': '8'}"
"Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.","{'title': '8 Conclusions', 'number': '8'}"
This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01).,"{'title': 'Acknowledgments', 'number': '9'}"
