col1,col2
Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text.,Introduction
This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods.,Introduction
"Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis.",Introduction
Each source of information is represented by kernel functions.,Introduction
Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels.,Introduction
"We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task.",Introduction
"When evaluated on the official test data, our approach produced very competitive ACE value scores.",Introduction
We also compare the SVM with KNN on different kernels.,Introduction
"Information extraction subsumes a broad range of tasks, including the extraction of entities, relations and events from various text sources, such as newswire documents and broadcast transcripts.",Experiment/Discussion
"One such task, relation detection, finds instances of predefined relations between pairs of entities, such as a Located-In relation between the entities Centre College and Danville, KY in the phrase Centre College in Danville, KY.",Experiment/Discussion
"The ‘entities’ are the individuals of selected semantic types (such as people, organizations, countries, ...) which are referred to in the text.",Experiment/Discussion
"Prior approaches to this task (Miller et al., 2000; Zelenko et al., 2003) have relied on partial or full syntactic analysis.",Experiment/Discussion
Syntactic analysis can find relations not readily identified based on sequences of tokens alone.,Experiment/Discussion
"Even ‘deeper’ representations, such as logical syntactic relations or predicate-argument structure, can in principle capture additional generalizations and thus lead to the identification of additional instances of relations.",Experiment/Discussion
"However, a general problem in Natural Language Processing is that as the processing gets deeper, it becomes less accurate.",Experiment/Discussion
"For instance, the current accuracy of tokenization, chunking and sentence parsing for English is about 99%, 92%, and 90% respectively.",Experiment/Discussion
Algorithms based solely on deeper representations inevitably suffer from the errors in computing these representations.,Experiment/Discussion
"On the other hand, low level processing such as tokenization will be more accurate, and may also contain useful information missed by deep processing of text.",Experiment/Discussion
"Systems based on a single level of representation are forced to choose between shallower representations, which will have fewer errors, and deeper representations, which may be more general.",Experiment/Discussion
"Based on these observations, Zhao et al. (2004) proposed a discriminative model to combine information from different syntactic sources using a kernel SVM (Support Vector Machine).",Experiment/Discussion
We showed that adding sentence level word trigrams as global information to local dependency context boosted the performance of finding slot fillers for management succession events.,Experiment/Discussion
"This paper describes an extension of this approach to the identification of entity relations, in which syntactic information from sentence tokenization, parsing and deep dependency analysis is combined using kernel methods.",Experiment/Discussion
"At each level, kernel functions (or kernels) are developed to represent the syntactic information.",Experiment/Discussion
"Five kernels have been developed for this task, including two at the surface level, one at the parsing level and two at the deep dependency level.",Experiment/Discussion
"Our experiments show that each level of processing may contribute useful clues for this task, including surface information like word bigrams.",Experiment/Discussion
Adding kernels one by one continuously improves performance.,Experiment/Discussion
The experiments were carried out on the ACE RDR (Relation Detection and Recognition) task with annotated entities.,Experiment/Discussion
Using SVM as a classifier along with the full composite kernel produced the best performance on this task.,Experiment/Discussion
This paper will also show a comparison of SVM and KNN (k-Nearest-Neighbors) under different kernel setups.,Experiment/Discussion
"Many machine learning algorithms involve only the dot product of vectors in a feature space, in which each vector represents an object in the object domain.",Experiment/Discussion
"Kernel methods (Muller et al., 2001) can be seen as a generalization of feature-based algorithms, in which the dot product is replaced by a kernel function (or kernel) Ψ(X,Y) between two vectors, or even between two objects.",Experiment/Discussion
"Mathematically, as long as Ψ(X,Y) is symmetric and the kernel matrix formed by Ψ is positive semi-definite, it forms a valid dot product in an implicit Hilbert space.",Experiment/Discussion
"In this implicit space, a kernel can be broken down into features, although the dimension of the feature space could be infinite.",Experiment/Discussion
"Normal feature-based learning can be implemented in kernel functions, but we can do more than that with kernels.",Experiment/Discussion
"First, there are many wellknown kernels, such as polynomial and radial basis kernels, which extend normal features into a high order space with very little computational cost.",Experiment/Discussion
This could make a linearly non-separable problem separable in the high order feature space.,Experiment/Discussion
"Second, kernel functions have many nice combination properties: for example, the sum or product of existing kernels is a valid kernel.",Experiment/Discussion
This forms the basis for the approach described in this paper.,Experiment/Discussion
"With these combination properties, we can combine individual kernels representing information from different sources in a principled way.",Experiment/Discussion
Many classifiers can be used with kernels.,Experiment/Discussion
"The most popular ones are SVM, KNN, and voted perceptrons.",Experiment/Discussion
"Support Vector Machines (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are linear classifiers that produce a separating hyperplane with largest margin.",Experiment/Discussion
"This property gives it good generalization ability in high-dimensional spaces, making it a good classifier for our approach where using all the levels of linguistic clues could result in a huge number of features.",Experiment/Discussion
"Given all the levels of features incorporated in kernels and training data with target examples labeled, an SVM can pick up the features that best separate the targets from other examples, no matter which level these features are from.",Experiment/Discussion
"In cases where an error occurs in one processing result (especially deep processing) and the features related to it become noisy, SVM may pick up clues from other sources which are not so noisy.",Experiment/Discussion
This forms the basic idea of our approach.,Experiment/Discussion
"Therefore under this scheme we can overcome errors introduced by one processing level; more particularly, we expect accurate low level information to help with less accurate deep level information.",Experiment/Discussion
Collins et al. (1997) and Miller et al.,Experiment/Discussion
"(2000) used statistical parsing models to extract relational facts from text, which avoided pipeline processing of data.",Experiment/Discussion
"However, their results are essentially based on the output of sentence parsing, which is a deep processing of text.",Experiment/Discussion
So their approaches are vulnerable to errors in parsing.,Experiment/Discussion
Collins et al. (1997) addressed a simplified task within a confined context in a target sentence.,Experiment/Discussion
"Zelenko et al. (2003) described a recursive kernel based on shallow parse trees to detect personaffiliation and organization-location relations, in which a relation example is the least common subtree containing two entity nodes.",Experiment/Discussion
The kernel matches nodes starting from the roots of two subtrees and going recursively to the leaves.,Experiment/Discussion
"For each pair of nodes, a subsequence kernel on their child nodes is invoked, which matches either contiguous or non-contiguous subsequences of node.",Experiment/Discussion
"Compared with full parsing, shallow parsing is more reliable.",Experiment/Discussion
But this model is based solely on the output of shallow parsing so it is still vulnerable to irrecoverable parsing errors.,Experiment/Discussion
"In their experiments, incorrectly parsed sentences were eliminated.",Experiment/Discussion
Culotta and Sorensen (2004) described a slightly generalized version of this kernel based on dependency trees.,Experiment/Discussion
"Since their kernel is a recursive match from the root of a dependency tree down to the leaves where the entity nodes reside, a successful match of two relation examples requires their entity nodes to be at the same depth of the tree.",Experiment/Discussion
This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall.,Experiment/Discussion
In their solution a bag-of-words kernel was used to compensate for this problem.,Experiment/Discussion
"In our approach, more flexible kernels are used to capture regularization in syntax, and more levels of syntactic information are considered.",Experiment/Discussion
"Kambhatla (2004) described a Maximum Entropy model using features from various syntactic sources, but the number of features they used is limited and the selection of features has to be a manual process.1 In our model, we use kernels to incorporate more syntactic information and let a Support Vector Machine decide which clue is crucial.",Experiment/Discussion
Some of the kernels are extended to generate high order features.,Experiment/Discussion
We think a discriminative classifier trained with all the available syntactic features should do better on the sparse data.,Experiment/Discussion
ACE (Automatic Content Extraction)2 is a research and development program in information extraction sponsored by the U.S. Government.,Experiment/Discussion
The 2004 evaluation defined seven major types of relations between seven types of entities.,Experiment/Discussion
"The entity types are PER (Person), ORG (Organization), FAC (Facility), GPE (Geo-Political Entity: countries, cities, etc.",Experiment/Discussion
"), LOC (Location), WEA (Weapon) and VEH (Vehicle).",Experiment/Discussion
"Each mention of an entity has a mention type: NAM (proper name), NOM (nominal) or 1 Kambhatla also evaluated his system on the ACE relation detection task, but the results are reported for the 2003 task, which used different relations and different training and test data, and did not use hand-annotated entities, so they cannot be readily compared to our results.",Experiment/Discussion
"PRO (pronoun); for example George W. Bush, the president and he respectively.",Experiment/Discussion
"The seven relation types are EMP-ORG (Employment/Membership/Subsidiary), PHYS (Physical), PER-SOC (Personal/Social), GPE-AFF (GPEAffiliation), Other-AFF (Person/ORG Affiliation), ART (Agent-Artifact) and DISC (Discourse).",Experiment/Discussion
"There are also 27 relation subtypes defined by ACE, but this paper only focuses on detection of relation types.",Experiment/Discussion
Table 1 lists examples of each relation type. heads of the two entity arguments in a relation are marked.,Experiment/Discussion
Types are listed in decreasing order of frequency of occurrence in the ACE corpus.,Experiment/Discussion
"Figure 1 shows a sample newswire sentence, in which three relations are marked.",Experiment/Discussion
"In this sentence, we expect to find a PHYS relation between Hezbollah forces and areas, a PHYS relation between Syrian troops and areas and an EMP-ORG relation between Syrian troops and Syrian.",Experiment/Discussion
"In our approach, input text is preprocessed by the Charniak sentence parser (including tokenization and POS tagging) and the GLARF (Meyers et al., 2001) dependency analyzer produced by NYU.",Experiment/Discussion
"Based on treebank parsing, GLARF produces labeled deep dependencies between words (syntactic relations such as logical subject and logical object).",Experiment/Discussion
"It handles linguistic phenomena like passives, relatives, reduced relatives, conjunctions, etc.",Experiment/Discussion
That's because Israel was expected to retaliate against Hezbollah forces in areas controlled by Syrian troops.,Experiment/Discussion
"In our model, kernels incorporate information from tokenization, parsing and deep dependency analysis.",Experiment/Discussion
"A relation candidate R is defined as where arg1 and arg2 are the two entity arguments which may be related; seq=(t1, t2, ..., tn) is a token vector that covers the arguments and intervening words; link=(t1, t2, ..., tm) is also a token vector, generated from seq and the parse tree; path is a dependency path connecting arg1 and arg2 in the dependency graph produced by GLARF. path can be empty if no such dependency path exists.",Experiment/Discussion
The difference between link and seq is that link only retains the “important” words in seq in terms of syntax.,Experiment/Discussion
"For example, all noun phrases occurring in seq are replaced by their heads.",Experiment/Discussion
"Words and constituent types in a stop list, such as time expressions, are also removed.",Experiment/Discussion
"A token T is defined as a string triple, where word, pos and base are strings representing the word, part-of-speech and morphological base form of T. Entity is a token augmented with other attributes, where tk is the token associated with E; type, subtype and mtype are strings representing the entity type, subtype and mention type of E. The subtype contains more specific information about an entity.",Experiment/Discussion
"For example, for a GPE entity, the subtype tells whether it is a country name, city name and so on.",Experiment/Discussion
"Mention type includes NAM, NOM and PRO.",Experiment/Discussion
"It is worth pointing out that we always treat an entity as a single token: for a nominal, it refers to its head, such as boys in the two boys; for a proper name, all the words are connected into one token, such as Bashar_Assad.",Experiment/Discussion
"So in a relation example R whose seq is (t1, t2, ..., tn), it is always true that arg1=t1 and arg2=tn.",Experiment/Discussion
"For names, the base form of an entity is its ACE type (person, organization, etc.).",Experiment/Discussion
"To introduce dependencies, we define a dependency token to be a token augmented with a vector of dependency arcs, DT=(word, pos, base, dseq), where dseq = (arc1, ... , arcn ).",Experiment/Discussion
"A dependency arc is ARC = (w, dw, label, e), where w is the current token; dw is a token connected by a dependency to w; and label and e are the role label and direction of this dependency arc respectively.",Experiment/Discussion
From now on we upgrade the type of tk in arg1 and arg2 to be dependency tokens.,Experiment/Discussion
"Finally, path is a vector of dependency arcs, where l is the length of the path and arci (1<i<l) satisfies arc1.w=arg1.tk, arci+1.w=arci.dw and arcl.dw=arg2.tk.",Experiment/Discussion
So path is a chain of dependencies connecting the two arguments in R. The arcs in it do not have to be in the same direction.,Experiment/Discussion
Figure 2 shows a relation example generated from the text “... in areas controlled by Syrian troops”.,Experiment/Discussion
"In this relation example R, arg-, is ((“areas”, “NNS”, “area”, dseq), “LOC”, “Region”, “NOM”), and arg-,.dseq is ((OBJ, areas, in, 1), (OBJ, areas, controlled, 1)). arg2 is ((“troops”, “NNS”, “troop”, dseq), “ORG”, “Government”, “NOM”) and arg2.dseq = ((A-POS, troops, Syrian, 0), (SBJ, troops, controlled, 1)). path is ((OBJ, areas, controlled, 1), (SBJ, controlled, troops, 0)).",Experiment/Discussion
"The value 0 in a dependency arc indicates forward direction from w to dw, and 1 indicates backward direction.",Experiment/Discussion
The seq and link sequences of R are shown in Figure 2.,Experiment/Discussion
"Some relations occur only between very restricted types of entities, but this is not true for every type of relation.",Experiment/Discussion
"For example, PER-SOC is a relation mainly between two person entities, while PHYS can happen between any type of entity and a GPE or LOC entity.",Experiment/Discussion
In this section we will describe the kernels designed for different syntactic sources and explain the intuition behind them.,Experiment/Discussion
We define two kernels to match relation examples at surface level.,Experiment/Discussion
"Using the notation just defined, we can write the two surface kernels as follows: KT is a kernel that matches two tokens.",Experiment/Discussion
"I(x, y) is a binary string match operator that gives 1 if x=y and 0 otherwise.",Experiment/Discussion
"Kernel Ψ1 matches attributes of two entity arguments respectively, such as type, subtype and lexical head of an entity.",Experiment/Discussion
This is based on the observation that there are type constraints on the two arguments.,Experiment/Discussion
For instance PER-SOC is a relation mostly between two person entities.,Experiment/Discussion
So the attributes of the entities are crucial clues.,Experiment/Discussion
Lexical information is also important to distinguish relation types.,Experiment/Discussion
"For instance, in the phrase U.S. president there is an EMP-ORG relation between president and U.S., while in a U.S. businessman there is a GPE-AFF relation between businessman and U.S. where Operator <t1, t2> concatenates all the string elements in tokens t1 and t2 to produce a new token.",Experiment/Discussion
So Ψ2 is a kernel that simply matches unigrams and bigrams between the seq sequences of two relation examples.,Experiment/Discussion
The information this kernel provides is faithful to the text. where min_len is the length of the shorter link sequence in R1 and R2.,Experiment/Discussion
Ψ3 is a kernel that matches token by token between the link sequences of two relation examples.,Experiment/Discussion
"Since relations often occur in a short context, we expect many of them have similar link sequences.",Experiment/Discussion
"ψ 4 (R1 , R2)= Kpath (R1. path, R2 . path ), where Intuitively the dependency path connecting two arguments could provide a high level of syntactic regularization.",Experiment/Discussion
"However, a complete match of two dependency paths is rare.",Experiment/Discussion
So this kernel matches the component arcs in two dependency paths in a pairwise fashion.,Experiment/Discussion
Two arcs can match only when they are in the same direction.,Experiment/Discussion
"In cases where two paths do not match exactly, this kernel can still tell us how similar they are.",Experiment/Discussion
In our experiments we placed an upper bound on the length of dependency paths for which we computed a non-zero kernel. where This kernel matches the local dependency context around the relation arguments.,Experiment/Discussion
This can be helpful especially when the dependency path between arguments does not exist.,Experiment/Discussion
We also hope the dependencies on each argument may provide some useful clues about the entity or connection of the entity to the context outside of the relation example.,Experiment/Discussion
"Having defined all the kernels representing shallow and deep processing results, we can define composite kernels to combine and extend the individual kern d Ψ3 covers the most important clues for this task: information about the two arguments and the word link between them.",Experiment/Discussion
The polynomial extension is equivalent to adding pairs of features as new features.,Experiment/Discussion
"Intuitively this introduces new features like: the subtype of the first argument is a country name and the word of the second argument is president, which could be a good clue for an EMP-ORG relation.",Experiment/Discussion
The polynomial kernel is down weighted by a normalization factor because we do not want the high order features to overwhelm the original ones.,Experiment/Discussion
"In our experiment, using polynomial kernels with degree higher than 2 does not produce better results.",Experiment/Discussion
"2) Full kernel This is the final kernel we used for this task, which is a combination of all the previous kernels.",Experiment/Discussion
"In our experiments, we set all the scalar factors to 1.",Experiment/Discussion
"Different values were tried, but keeping the original weight for each kernel yielded the best results for this task.",Experiment/Discussion
All the individual kernels we designed are explicit.,Experiment/Discussion
Each kernel can be seen as a matching of features and these features are enumerable on the given data.,Experiment/Discussion
So it is clear that they are all valid kernels.,Experiment/Discussion
"Since the kernel function set is closed under linear combination and polynomial extension, the composite kernels are also valid.",Experiment/Discussion
The reason we propose to use a feature-based kernel is that we can have a clear idea of what syntactic clues it represents and what kind of information it misses.,Experiment/Discussion
"This is important when developing or refining kernels, so that we can make them generate complementary information from different syntactic processing results.",Experiment/Discussion
"Experiments were carried out on the ACE RDR (Relation Detection and Recognition) task using hand-annotated entities, provided as part of the ACE evaluation.",Experiment/Discussion
The ACE corpora contain documents from two sources: newswire (nwire) documents and broadcast news transcripts (bnews).,Experiment/Discussion
"In this section we will compare performance of different kernel setups trained with SVM, as well as different classifiers, KNN and SVM, with the same kernel setup.",Experiment/Discussion
The SVM package we used is SVMlight.,Experiment/Discussion
The training parameters were chosen using cross-validation.,Experiment/Discussion
One-against-all classification was applied to each pair of entities in a sentence.,Experiment/Discussion
"When SVM predictions conflict on a relation example, the one with larger margin will be selected as the final answer.",Experiment/Discussion
"The ACE RDR training data contains 348 documents, 125K words and 4400 relations.",Experiment/Discussion
It consists of both nwire and bnews documents.,Experiment/Discussion
Evaluation of kernels was done on the training data using 5-fold cross-validation.,Experiment/Discussion
"We also evaluated the full kernel setup with SVM on the official test data, which is about half the size of the training data.",Experiment/Discussion
All the data is preprocessed by the Charniak parser and GLARF dependency analyzer.,Experiment/Discussion
Then relation examples are generated based these results.,Experiment/Discussion
Table 2 shows the performance of the SVM on different kernel setups.,Experiment/Discussion
The kernel setups in this experiment are incremental.,Experiment/Discussion
"From this table we can see that adding kernels continuously improves the performance, which indicates they provide additional clues to the previous setup.",Experiment/Discussion
The argument kernel treats the two arguments as independent entities.,Experiment/Discussion
"The link sequence kernel introduces the syntactic connection between arguments, so adding it to the argument kernel boosted the performance.",Experiment/Discussion
Setup F shows the performance of adding only dependency kernels to the argument kernel.,Experiment/Discussion
"The performance is not as good as setup B, indicating that dependency information alone is not as crucial as the link sequence. setups.",Experiment/Discussion
Each setup adds one level of kernels to the previous one except setup F. Evaluated on the ACE training data with 5-fold cross-validation.,Experiment/Discussion
Fscores marked by * are significantly better than the previous setup (at 95% confidence level).,Experiment/Discussion
"Another observation is that adding the bigram kernel in the presence of all other level of kernels improved both precision and recall, indicating that it helped in both correcting errors in other processing results and providing supplementary information missed by other levels of analysis.",Experiment/Discussion
"In another experiment evaluated on the nwire data only (about half of the training data), adding the bigram kernel improved F-score 0.5% and this improvement is statistically significant. different kernel setups.",Experiment/Discussion
Types are ordered in decreasing order of frequency of occurrence in the ACE corpus.,Experiment/Discussion
"In SVM training, the same parameters were used for all 7 types.",Experiment/Discussion
Table 3 shows the performance of SVM and KNN (k Nearest Neighbors) on different kernel setups.,Experiment/Discussion
"For KNN, k was set to 3.",Experiment/Discussion
"In the first setup of KNN, the two kernels which seem to contain most of the important information are used.",Experiment/Discussion
It performs quite well when compared with the SVM result.,Experiment/Discussion
The other two tests are based on the full kernel setup.,Experiment/Discussion
"For the two KNN experiments, adding more kernels (features) does not help.",Experiment/Discussion
The reason might be that all kernels (features) were weighted equally in the composite kernel Φ2 and this may not be optimal for KNN.,Experiment/Discussion
Another reason is that the polynomial extension of kernels does not have any benefit in KNN because it is a monotonic transformation of similarity values.,Experiment/Discussion
So the results of KNN on kernel (Ψ1+Ψ3) and Φ1 would be exactly the same.,Experiment/Discussion
We also tried different k for KNN and k=3 seems to be the best choice in either case.,Experiment/Discussion
"For the four major types of relations SVM does better than KNN, probably due to SVM’s generalization ability in the presence of large numbers of features.",Experiment/Discussion
"For the last three types with many fewer examples, performance of SVM is not as good as KNN.",Experiment/Discussion
The reason we think is that training of SVM on these types is not sufficient.,Experiment/Discussion
"We tried different training parameters for the types with fewer examples, but no dramatic improvement obtained.",Experiment/Discussion
"We also evaluated our approach on the official ACE RDR test data and obtained very competitive scores.3 The primary scoring metric4 for the ACE evaluation is a 'value' score, which is computed by deducting from 100 a penalty for each missing and spurious relation; the penalty depends on the types of the arguments to the relation.",Experiment/Discussion
The value scores produced by the ACE scorer for nwire and bnews test data are 71.7 and 68.0 repectively.,Experiment/Discussion
The value score on all data is 70.1.5 The scorer also reports an F-score based on full or partial match of relations to the keys.,Experiment/Discussion
The unweighted F-score for this test produced by the ACE scorer on all data is 76.0%.,Experiment/Discussion
For this evaluation we used nearest neighbor to determine argument ordering and relation subtypes.,Experiment/Discussion
The classification scheme in our experiments is one-against-all.,Experiment/Discussion
It turned out there is not so much confusion between relation types.,Experiment/Discussion
The confusion matrix of predictions is fairly clean.,Experiment/Discussion
"We also tried pairwise classification, and it did not help much.",Experiment/Discussion
"In this paper, we have shown that using kernels to combine information from different syntactic sources performed well on the entity relation detection task.",Experiment/Discussion
Our experiments show that each level of syntactic processing contains useful information for the task.,Experiment/Discussion
Combining them may provide complementary information to overcome errors arising from linguistic analysis.,Experiment/Discussion
"Especially, low level information obtained with high reliability helped with the other deep processing results.",Experiment/Discussion
"This design feature of our approach should be best employed when the preprocessing errors at each level are independent, namely when there is no dependency between the preprocessing modules.",Experiment/Discussion
"The model was tested on text with annotated entities, but its design is generic.",Experiment/Discussion
It can work with noisy entity detection input from an automatic tagger.,Experiment/Discussion
"With all the existing information from other processing levels, this model can be also expected to recover from errors in entity tagging.",Experiment/Discussion
Kernel functions have many nice properties.,Results/Conclusion
"There are also many well known kernels, such as radial basis kernels, which have proven successful in other areas.",Results/Conclusion
"In the work described here, only linear combinations and polynomial extensions of kernels have been evaluated.",Results/Conclusion
We can explore other kernel properties to integrate the existing syntactic kernels.,Results/Conclusion
"In another direction, training data is often sparse for IE tasks.",Results/Conclusion
String matching is not sufficient to capture semantic similarity of words.,Results/Conclusion
One solution is to use general purpose corpora to create clusters of similar words; another option is to use available resources like WordNet.,Results/Conclusion
These word similarities can be readily incorporated into the kernel framework.,Results/Conclusion
"To deal with sparse data, we can also use deeper text analysis to capture more regularities from the data.",Results/Conclusion
"Such analysis may be based on newly-annotated corpora like PropBank (Kingsbury and Palmer, 2002) at the University of Pennsylvania and NomBank (Meyers et al., 2004) at New York University.",Results/Conclusion
Analyzers based on these resources can generate regularized semantic representations for lexically or syntactically related sentence structures.,Results/Conclusion
"Although deeper analysis may even be less accurate, our framework is designed to handle this and still obtain some improvement in performance.",Results/Conclusion
"This research was supported in part by the Defense Advanced Research Projects Agency under Grant N66001-04-1-8920 from SPAWAR San Diego, and by the National Science Foundation under Grant ITS-0325657.",Results/Conclusion
This paper does not necessarily reflect the position of the U.S. Government.,Results/Conclusion
We wish to thank Adam Meyers of the NYU NLP group for his help in producing deep dependency analyses.,Results/Conclusion
