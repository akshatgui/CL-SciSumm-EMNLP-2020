col1,col2
Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.,{}
"However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.",{}
We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.,{}
"The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.",{}
"Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages.","{'title': '1 Introduction', 'number': '1'}"
"Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.","{'title': '1 Introduction', 'number': '1'}"
"The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings.","{'title': '1 Introduction', 'number': '1'}"
"These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology.","{'title': '1 Introduction', 'number': '1'}"
There have been many attempts to incorporate syntax into alignment; we will not present a complete list here.,"{'title': '1 Introduction', 'number': '1'}"
"Some methods parse two flat strings at once using a bitext grammar (Wu, 1997).","{'title': '1 Introduction', 'number': '1'}"
"Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001).","{'title': '1 Introduction', 'number': '1'}"
The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts.,"{'title': '1 Introduction', 'number': '1'}"
"They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996).","{'title': '1 Introduction', 'number': '1'}"
"Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005).","{'title': '1 Introduction', 'number': '1'}"
Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search.,"{'title': '1 Introduction', 'number': '1'}"
"Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled.","{'title': '1 Introduction', 'number': '1'}"
"In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model.","{'title': '1 Introduction', 'number': '1'}"
"This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint.","{'title': '1 Introduction', 'number': '1'}"
"We add our syntactic features to an already strong flat-string discriminative solution, and we show that they provide new information resulting in improved alignments.","{'title': '1 Introduction', 'number': '1'}"
"Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment.","{'title': '2 Constrained Alignment', 'number': '2'}"
All word alignment methods benefit from some set of constraints.,"{'title': '2 Constrained Alignment', 'number': '2'}"
These limit the alignment search space and encourage competition between potential links.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has exactly one generator in the source.","{'title': '2 Constrained Alignment', 'number': '2'}"
"Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link.","{'title': '2 Constrained Alignment', 'number': '2'}"
Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints.,"{'title': '2 Constrained Alignment', 'number': '2'}"
Suppose we are given a parse tree for one of the two sentences in our sentence pair.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"We will refer to the parsed language as English, and the unparsed language as Foreign.","{'title': '2 Constrained Alignment', 'number': '2'}"
"Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign.","{'title': '2 Constrained Alignment', 'number': '2'}"
"When this occurs, the alignment is said to maintain phrasal cohesion.","{'title': '2 Constrained Alignment', 'number': '2'}"
Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings.,"{'title': '2 Constrained Alignment', 'number': '2'}"
Crossings occur when the projections of two disjoint phrases overlap.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot ... le, is interrupted by the projection of its head, cause.","{'title': '2 Constrained Alignment', 'number': '2'}"
Alignments with no crossings maintain phrasal cohesion.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures.","{'title': '2 Constrained Alignment', 'number': '2'}"
Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner.,"{'title': '2 Constrained Alignment', 'number': '2'}"
This constraint produces a significant reduction in alignment error rate.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained.","{'title': '2 Constrained Alignment', 'number': '2'}"
"These include incorrect parses, systematic violations such as not —* ne ... pas, paraphrases, and linguistic exceptions.","{'title': '2 Constrained Alignment', 'number': '2'}"
"We aim to create an alignment system that obeys cohesion constraints most of the time, but can violate them when necessary.","{'title': '2 Constrained Alignment', 'number': '2'}"
"Unfortunately, Cherry and Lin’s beam search solution does not lend itself to a soft cohesion constraint.","{'title': '2 Constrained Alignment', 'number': '2'}"
The imperfect beam search may not be able to find the optimal alignment under a soft constraint.,"{'title': '2 Constrained Alignment', 'number': '2'}"
"Furthermore, it is not clear what penalty to assign to crossings, or how to learn such a penalty from an iterative training process.","{'title': '2 Constrained Alignment', 'number': '2'}"
"The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations.","{'title': '2 Constrained Alignment', 'number': '2'}"
"We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003).","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
An ITG aligns bitext through synchronous parsing.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Both sentences are decomposed into constituent phrases simultaneously, producing a word alignment as a byproduct.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Viewed generatively, an ITG writes to two streams at once.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Terminal productions produce a token in each stream, or a token in one stream with the null symbol 0 in the other.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"We will use standard ITG notation: A —* e/f indicates that the token e is produced on the English stream, while f is produced on the Foreign stream.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"To allow for some degree of movement during translation, non-terminal productions are allowed to be either straight or inverted.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Straight productions, with their non-terminals inside square brackets [...], produce their symbols in the same order on both streams.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Inverted productions, indicated by angled brackets (...), have their nonterminals produced in the given order on the English stream, but this order is reversed in the Foreign stream. the tax causes unrest l' impTMt cause le malaise An ITG chart parser provides a polynomialtime algorithm to conduct a complete enumeration of all alignments that are possible according to its grammar.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"We will use a binary bracketing ITG, the simplest interesting grammar in this formalism: A — [AA]  |(AA)  |e/f This grammar enforces its own weak cohesion constraint: for every possible alignment, a corresponding binary constituency tree must exist for which the alignment maintains phrasal cohesion.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
Figure 2 shows a word alignment and the corresponding tree found by an ITG parser.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
Wu (1997) provides anecdotal evidence that only incorrect alignments are eliminated by ITG constraints.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"In our French-English data set, an ITG rules out only 0.3% of necessary links beyond those already eliminated by the one-to-one constraint (Cherry and Lin, 2006).","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
An ITG will search all alignments that conform to a possible binary constituency tree.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
We wish to confine that search to a specific n-array dependency tree.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Fortunately, Wu (1997) provides a method to have an ITG respect a known partial structure.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
One can seed the ITG parse chart so that spans that do not agree with the provided structure are assigned a value of —oc before parsing begins.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
The result is that no constituent is ever constructed with any of these invalid spans.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"In the case of phrasal cohesion, the invalid spans correspond to spans of the English sentence that interrupt the phrases established by the provided dependency tree.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"To put this notion formally, we first define some terms: given a subtree T[Z,k], where i is the left index of the leftmost leaf in T[Z,k] and k is the right index of its rightmost leaf, we say any index j E (i, k) is internal to T[Z,k].","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Similarly, any index x E/ [i, k] is external to T[Z,k].","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"An invalid span is any span for which our provided tree has a subtree T[Z,k] such that one endpoint of the span is internal to T[Z k] while the other is external to it.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Figure 3 illustrates this definition, while Figure 4 shows the invalid spans induced by a simple dependency tree.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"With these invalid spans in place, the ITG can no longer merge part of a dependency subtree with anything other than another part of the same subtree.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Since all ITG movement can be explained by inversions, this constrained ITG cannot interrupt one dependency phrase with part of another.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Therefore, the phrasal cohesion of the input dependency tree is maintained.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Note that this will not search the exact same alignment space as a cohesion-constrained beam search; instead it uses the union of the cohesion constraint and the weaker ITG constraints (Cherry and Lin, 2006).","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
Transforming this form of the cohesion constraint into a soft constraint is straight-forward.,"{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Instead of overriding the parser so it cannot use invalid English spans, we will note the invalid spans and assign the parser a penalty should it use them.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"The value of this penalty will be determined through discriminative training, as described in Section 4.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"Since the penalty is available within the dynamic programming algorithm, the parser will be able to incorporate it to find a globally optimal alignment.","{'title': '3 Syntax-aware Alignment Search', 'number': '3'}"
"To discriminatively train our alignment systems, we adopt the Support Vector Machine (SVM) for Structured Output (Tsochantaridis et al., 2004).","{'title': '4 Discriminative Training', 'number': '4'}"
"We have selected this system for its high degree of modularity, and because it has an API freely available1.","{'title': '4 Discriminative Training', 'number': '4'}"
"We will summarize the learning mechanism briefly in this section, but readers should refer to (Tsochantaridis et al., 2004) for more details.","{'title': '4 Discriminative Training', 'number': '4'}"
SVM learning is most easily expressed as a constrained numerical optimization problem.,"{'title': '4 Discriminative Training', 'number': '4'}"
"All constraints mentioned in this section are constraints on this optimizer, and have nothing to do with the cohesion constraint from Section 2.","{'title': '4 Discriminative Training', 'number': '4'}"
Traditional SVMs attempt to find a linear separator that creates the largest possible margin between two classes of vectors.,"{'title': '4 Discriminative Training', 'number': '4'}"
"Structured output SVMs attempt to separate the correct structure from all incorrect structures by the largest possible margin, for all training instances.","{'title': '4 Discriminative Training', 'number': '4'}"
"This may sound like a much more difficult problem, but with a few assumptions in place, the task begins to look very similar to a traditional SVM.","{'title': '4 Discriminative Training', 'number': '4'}"
"As in most discriminative training methods, we begin by assuming that a candidate structure y, built for an input instance x, can be adequately described using a feature vector IF(x, y).","{'title': '4 Discriminative Training', 'number': '4'}"
"We also assume that our 1F(x, y) decomposes in such a way that the features can guide a search to recover the structure y from x.","{'title': '4 Discriminative Training', 'number': '4'}"
"That is: is computable, where Y is the set of all possible structures, and w is a vector that assigns weights to each component of IF(x, y). w is the parameter vector we will learn using our SVM.","{'title': '4 Discriminative Training', 'number': '4'}"
"Now the learning task begins to look straightforward: we are working with vectors, and the task of building a structure y has been recast as an argmax operator.","{'title': '4 Discriminative Training', 'number': '4'}"
"Our learning goal is to find a w so that the correct structure is found: Vi, Vy E Y \ yi : (w, 'Pi(yi)) > (w, 'Pi(y)) (2) where xi is the ith training example, yi is its correct structure, and 1&i(y) is short-hand for IF(xi, y).","{'title': '4 Discriminative Training', 'number': '4'}"
"As several w will fulfill (2) in a linearly separable training set, the unique max-margin objective is defined to be the w that maximizes the minimum distance between yi and the incorrect structures in Y.","{'title': '4 Discriminative Training', 'number': '4'}"
This learning framework also incorporates a notion of structured loss.,"{'title': '4 Discriminative Training', 'number': '4'}"
"In a standard vector classification problem, there is 0-1 loss: a vector is either classified correctly or it is not.","{'title': '4 Discriminative Training', 'number': '4'}"
"In the structured case, some incorrect structures can be better than others.","{'title': '4 Discriminative Training', 'number': '4'}"
"For example, having the argmax select an alignment missing only one link is better than selecting one with no correct links and a dozen wrong ones.","{'title': '4 Discriminative Training', 'number': '4'}"
"A loss function A(yi, y) quantifies just how incorrect a particular structure y is.","{'title': '4 Discriminative Training', 'number': '4'}"
"Though Tsochantaridis et al. (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al., 2005).","{'title': '4 Discriminative Training', 'number': '4'}"
"In margin re-scaling, high loss structures must be separated from the correct structure by a larger margin than low loss structures.","{'title': '4 Discriminative Training', 'number': '4'}"
"To allow some misclassifications during training, a soft-margin requirement replaces our maxmargin objective.","{'title': '4 Discriminative Training', 'number': '4'}"
"A slack variable �i is introduced for each training example xi, to allow the learner to violate the margin at a penalty.","{'title': '4 Discriminative Training', 'number': '4'}"
"The magnitude of this penalty to determined by a hand-tuned parameter C. After a few transformations (Tsochantaridis et al., 2004), the soft-margin learning objective can be formulated as a quadratic program: Note how the slack variables �i allow some incorrect structures to be built.","{'title': '4 Discriminative Training', 'number': '4'}"
"Also note that the loss A(yi, y) determines the size of the margin between structures.","{'title': '4 Discriminative Training', 'number': '4'}"
"Unfortunately, (4) provides one constraint for every possible structure for every training example.","{'title': '4 Discriminative Training', 'number': '4'}"
"Enumerating these constraints explicitly is infeasible, but in reality, only a subset of these constraints are necessary to achieve the same objective.","{'title': '4 Discriminative Training', 'number': '4'}"
"Re-organizing (4) produces: where costi is defined as: Provided that the max cost structure can be found in polynomial time, we have all the components needed for a constraint generation approach to this optimization problem.","{'title': '4 Discriminative Training', 'number': '4'}"
Constraint generation places an outer loop around an optimizer that minimizes (3) repeatedly for a growing set of constraints.,"{'title': '4 Discriminative Training', 'number': '4'}"
It begins by minimizing (3) with an empty constraint set in place of (4).,"{'title': '4 Discriminative Training', 'number': '4'}"
This provides values for w~ and ~ξ.,"{'title': '4 Discriminative Training', 'number': '4'}"
The max cost structure is found for i = 1 with the current ~w.,"{'title': '4 Discriminative Training', 'number': '4'}"
"If the resulting costi(y; ~w) is greater than the current value of ξi, then this represents a violated constraint2 in our complete objective, and a new constraint of the form ξi ≥ costi(y; ~w) is added to the constraint set.","{'title': '4 Discriminative Training', 'number': '4'}"
"The algorithm then iterates: the optimizer minimizes (3) again with the new constraint set, and solves the max cost problem for i = i + 1 with the new ~w, growing the constraint set if necessary.","{'title': '4 Discriminative Training', 'number': '4'}"
"Note that the constraints on ξ change with ~w, as cost is a function of ~w.","{'title': '4 Discriminative Training', 'number': '4'}"
"Once the end of the training set is reached, the learner loops back to the beginning.","{'title': '4 Discriminative Training', 'number': '4'}"
Learning ends when the entire training set can be processed without needing to add any constraints.,"{'title': '4 Discriminative Training', 'number': '4'}"
"It can be shown that this will occur within a polynomial number of iterations (Tsochantaridis et al., 2004).","{'title': '4 Discriminative Training', 'number': '4'}"
"With this framework in place, one need only fill in the details to create an SVM for a new structured output space: Using the Structured SVM API, we have created two SVM word aligners: a baseline that uses weighted maximum matching for its argmax operator, and a dependency-augmented ITG that will satisfy our requirements for an aligner with a soft cohesion constraint.","{'title': '4 Discriminative Training', 'number': '4'}"
"Our x becomes a bilingual sentence-pair, while our y becomes an alignment, represented by a set of links.","{'title': '4 Discriminative Training', 'number': '4'}"
"Given a bipartite graph with edge values, the weighted maximum matching algorithm (West, 2001) will find the matching with maximum summed edge values.","{'title': '4 Discriminative Training', 'number': '4'}"
"To create a matching alignment solution, we reproduce the approach of (Taskar et al., 2005) within the framework described in Section 4.1: where co is an omission penalty and c, is a commission penalty.","{'title': '4 Discriminative Training', 'number': '4'}"
"∀l ∈/ y : v(l) ← h~w, ψ(l)i + c, ∀l ∈ y : v(l) ← h~w, ψ(l)i − co Note that our max cost search could not have been implemented as loss-augmented matching had we selected one of the other loss objectives presented in (Tsochantaridis et al., 2004) in place of margin rescaling.","{'title': '4 Discriminative Training', 'number': '4'}"
"We use the same feature representation ψ(l) as (Taskar et al., 2005), with some small exceptions.","{'title': '4 Discriminative Training', 'number': '4'}"
"Let l = (Ej, Fk) be a potential link between the jth word of English sentence E and the kth word of Foreign sentence F. To measure correlation between Ej and Fk we use conditional link probability (Cherry and Lin, 2003) in place of the Dice coefficient: where the link counts are determined by wordaligning 50K sentence pairs with another matching SVM that uses the φ2 measure (Gale and Church, 1991) in place of Dice.","{'title': '4 Discriminative Training', 'number': '4'}"
"The 02 measure requires only co-occurrence counts. d is an absolute discount parameter as in (Moore, 2005).","{'title': '4 Discriminative Training', 'number': '4'}"
"Also, we omit the IBM Model 4 Prediction features, as we wish to know how well we can do without resorting to traditional word alignment techniques.","{'title': '4 Discriminative Training', 'number': '4'}"
"Otherwise, the features remain the same, including distance features that measure abs ( E − k ); orthographic features; word frequencies; common-word features; a bias term set always to 1; and an HMM approximation cor(Ej+1, Fk+1).","{'title': '4 Discriminative Training', 'number': '4'}"
"Because of the modularity of the structured output SVM, our SVM ITG re-uses a large amount infrastructure from the matching solution.","{'title': '4 Discriminative Training', 'number': '4'}"
"We essentially plug an ITG parser in the place of the matching algorithm, and add features to take advantage of information made available by the parser. x remains a sentence pair, and y becomes an ITG parse tree that decomposes x and specifies an alignment.","{'title': '4 Discriminative Training', 'number': '4'}"
Our required components are as follows: The OT vector has two new features in addition to those present in the matching system’s O.,"{'title': '4 Discriminative Training', 'number': '4'}"
"These features can be active only for non-terminal productions, which have the form A —* [AA]  |(AA).","{'title': '4 Discriminative Training', 'number': '4'}"
"One feature indicates an inverted production A —* (AA), while the other indicates the use of an invalid span according to a provided English dependency tree, as described in Section 3.2.","{'title': '4 Discriminative Training', 'number': '4'}"
These are the only features that can be active for nonterminal productions.,"{'title': '4 Discriminative Training', 'number': '4'}"
A terminal production rl that corresponds to a link l is given that link’s features from the matching system: OT (rl) = O(l).,"{'title': '4 Discriminative Training', 'number': '4'}"
Terminal productions ro corresponding to unaligned tokens are given blank feature vectors: OT(ro) = 0.,"{'title': '4 Discriminative Training', 'number': '4'}"
The SVM requires complete IF vectors for the correct training structures.,"{'title': '4 Discriminative Training', 'number': '4'}"
"Unfortunately, our training set contains gold standard alignments, not ITG parse trees.","{'title': '4 Discriminative Training', 'number': '4'}"
"The gold standard is divided into sure and possible link sets 5 and P (Och and Ney, 2003).","{'title': '4 Discriminative Training', 'number': '4'}"
"Links in 5 must be included in a correct alignment, while P links are optional.","{'title': '4 Discriminative Training', 'number': '4'}"
"We create ITG trees from the gold standard using the following sorted priorities during tree construction: This creates trees that represent high scoring alignments, using a minimal number of invalid spans.","{'title': '4 Discriminative Training', 'number': '4'}"
"Only the span and inversion counts of these trees will be used in training, so we need not achieve a perfect tree structure.","{'title': '4 Discriminative Training', 'number': '4'}"
We still evaluate all methods with the original alignment gold standard.,"{'title': '4 Discriminative Training', 'number': '4'}"
We conduct two experiments.,"{'title': '5 Experiments and Results', 'number': '5'}"
The first tests the dependency-augmented ITG described in Section 3.2 as an aligner with hard cohesion constraints.,"{'title': '5 Experiments and Results', 'number': '5'}"
The second tests our discriminative ITG with soft cohesion constraints against two strong baselines.,"{'title': '5 Experiments and Results', 'number': '5'}"
We conduct our experiments using French-English Hansard data.,"{'title': '5 Experiments and Results', 'number': '5'}"
"Our 02 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs.","{'title': '5 Experiments and Results', 'number': '5'}"
"Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003).","{'title': '5 Experiments and Results', 'number': '5'}"
"For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003).","{'title': '5 Experiments and Results', 'number': '5'}"
SVM learning parameters are tuned using the 37-pair development set provided with this data.,"{'title': '5 Experiments and Results', 'number': '5'}"
"English dependency trees are provided by Minipar (Lin, 1994).","{'title': '5 Experiments and Results', 'number': '5'}"
The goal of this experiment is to empirically confirm that the English spans marked invalid by Section 3.2’s dependency-augmented ITG provide useful guidance to an aligner.,"{'title': '5 Experiments and Results', 'number': '5'}"
"To do so, we compare an ITG with hard cohesion constraints, an unconstrained ITG, and a weighted maximum matching aligner.","{'title': '5 Experiments and Results', 'number': '5'}"
All aligners use the same simple objective function.,"{'title': '5 Experiments and Results', 'number': '5'}"
"They maximize summed link values v(l), where v(l) is defined as follows for an l = (Ej, Fk): All three aligners link based on 02 correlation scores, breaking ties in favor of closer pairs.","{'title': '5 Experiments and Results', 'number': '5'}"
This allows us to evaluate the hard constraints outside the context of supervised learning.,"{'title': '5 Experiments and Results', 'number': '5'}"
Table 1 shows the results of this experiment.,"{'title': '5 Experiments and Results', 'number': '5'}"
We can see that switching the search method from weighted maximum matching to a cohesionconstrained ITG (D-ITG) has produced a 34% relative reduction in alignment error rate.,"{'title': '5 Experiments and Results', 'number': '5'}"
"The bulk of this improvement results from a substantial increase in precision, though recall has also gone up.","{'title': '5 Experiments and Results', 'number': '5'}"
This indicates that these cohesion constraints are a strong alignment feature.,"{'title': '5 Experiments and Results', 'number': '5'}"
"The ITG row shows that the weaker ITG constraints are also valuable, but the cohesion constraint still improves on them.","{'title': '5 Experiments and Results', 'number': '5'}"
"We now test the performance of our SVM ITG with soft cohesion constraint, or SD-ITG, which is described in Section 4.2.2.","{'title': '5 Experiments and Results', 'number': '5'}"
We will test against two strong baselines.,"{'title': '5 Experiments and Results', 'number': '5'}"
"The first baseline, matching is the matching SVM described in Section 4.2.1, which is a re-implementation of the state-of-theart work in (Taskar et al., 2005)3.","{'title': '5 Experiments and Results', 'number': '5'}"
"The second baseline, D-ITG is an ITG aligner with hard cohesion constraints, but which uses the weights trained by the matching SVM to assign link values.","{'title': '5 Experiments and Results', 'number': '5'}"
This is the most straight-forward way to combine discriminative training with the hard syntactic constraints.,"{'title': '5 Experiments and Results', 'number': '5'}"
The results are shown in Table 2.,"{'title': '5 Experiments and Results', 'number': '5'}"
"The first thing to note is that our Matching baseline is achieving scores in line with (Taskar et al., 2005), which reports an AER of 0.107 using similar features and the same training and test sets.","{'title': '5 Experiments and Results', 'number': '5'}"
The effect of the hard cohesion constraint has been greatly diminished after discriminative training.,"{'title': '5 Experiments and Results', 'number': '5'}"
"Matching and D-ITG correspond to the the entries of the same name in Table 1, only with a much stronger, learned value function v(l).","{'title': '5 Experiments and Results', 'number': '5'}"
"However, in place of a 34% relative error reduction, the hard constraints in the D-ITG produce only a 9% reduction from 0.110 to 0.100.","{'title': '5 Experiments and Results', 'number': '5'}"
Also note that this time the hard constraints result in a reduction in recall.,"{'title': '5 Experiments and Results', 'number': '5'}"
"This indicates that the hard cohesion constraint is providing little guidance not provided by other features, and that it is actually eliminating more sure links than it is helping to find.","{'title': '5 Experiments and Results', 'number': '5'}"
"The soft-constrained SD-ITG, which has access to the D-ITG’s invalid spans as a feature during SVM training, is fairing substantially better.","{'title': '5 Experiments and Results', 'number': '5'}"
Its AER of 0.086 represents a 22% relative error reduction compared to the matching system.,"{'title': '5 Experiments and Results', 'number': '5'}"
The improved error rate is caused by gains in both precision and recall.,"{'title': '5 Experiments and Results', 'number': '5'}"
"This indicates that the invalid span feature is doing more than just ruling out links; perhaps it is de-emphasizing another, less accurate feature’s role.","{'title': '5 Experiments and Results', 'number': '5'}"
"The SD-ITG overrides the cohesion constraint in only 41 of the 347 test sentences, so we can see that it is indeed a soft constraint: it is obeyed nearly all the time, but it can be broken when necessary.","{'title': '5 Experiments and Results', 'number': '5'}"
"The SD-ITG achieves by far the strongest ITG alignment result reported on this French-English set; surpassing the 0.16 AER reported in (Zhang and Gildea, 2004).","{'title': '5 Experiments and Results', 'number': '5'}"
"Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training set needs to be iteratively aligned.","{'title': '5 Experiments and Results', 'number': '5'}"
"Our matching SVM trains in minutes on a single-processor machine, while the SD-ITG trains in roughly one hour.","{'title': '5 Experiments and Results', 'number': '5'}"
"The ITG is the bottleneck, so training time could be improved by optimizing the parser.","{'title': '5 Experiments and Results', 'number': '5'}"
Several other aligners have used discriminative training.,"{'title': '6 Related Work', 'number': '6'}"
"Our work borrows heavily from (Taskar et al., 2005), which uses a max-margin approach with a weighted maximum matching aligner.","{'title': '6 Related Work', 'number': '6'}"
"(Moore, 2005) uses an averaged perceptron for training with a customized beam search.","{'title': '6 Related Work', 'number': '6'}"
"(Liu et al., 2005) uses a log-linear model with a greedy search.","{'title': '6 Related Work', 'number': '6'}"
"To our knowledge, ours is the first alignment approach to use this highly modular structured SVM, and the first discriminative method to use an ITG for the base aligner.","{'title': '6 Related Work', 'number': '6'}"
"(Gildea, 2003) presents another aligner with a soft syntactic constraint.","{'title': '6 Related Work', 'number': '6'}"
"This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001).","{'title': '6 Related Work', 'number': '6'}"
This allows subtrees to move during translation.,"{'title': '6 Related Work', 'number': '6'}"
"As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here.","{'title': '6 Related Work', 'number': '6'}"
"In (Zhang and Gildea, 2004), this model was tested on the same annotated French-English sentence pairs that we divided into training and test sets for our experiments; it achieved an AER of 0.15.","{'title': '6 Related Work', 'number': '6'}"
"We have presented a discriminative, syntactic word alignment method.","{'title': '7 Conclusion', 'number': '7'}"
"Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum matching baseline.","{'title': '7 Conclusion', 'number': '7'}"
"An ITG parser is used for the alignment search, exposing two syntactic features: the use of inverted productions, and the use of spans that would not be available in a tree-to-string system.","{'title': '7 Conclusion', 'number': '7'}"
This second feature creates a soft phrasal cohesion constraint.,"{'title': '7 Conclusion', 'number': '7'}"
Discriminative training allows us to maintain all of the features that are useful to the maximum matching baseline in addition to the new syntactic features.,"{'title': '7 Conclusion', 'number': '7'}"
We have shown that these features produce a 22% relative reduction in error rate with respect to a strong flat-string model.,"{'title': '7 Conclusion', 'number': '7'}"
