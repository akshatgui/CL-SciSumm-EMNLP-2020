col1,col2
"We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems).",Introduction
In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds.,Introduction
"We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.",Introduction
We are exploring empirical methods of determining semantic relationships between constituents in natural language.,Experiment/Discussion
"Our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make inferences about propositions that hold between scientific concepts within biomedical texts (Swanson and Smalheiser, 1994).",Experiment/Discussion
"One of the important challenges of biomedical text, along with most other technical text, is the proliferation of noun compounds.",Experiment/Discussion
"A typical article title is shown below; it consists a cascade of four noun phrases linked by prepositions: Open-labeled long-term study of the efficacy, safety, and tolerability of subcutaneous sumatriptan in acute migraine treatment.",Experiment/Discussion
"The real concern in analyzing such a title is in determining the relationships that hold between different concepts, rather than on finding the appropriate attachments (which is especially difficult given the lack of a verb).",Experiment/Discussion
"And before we tackle the prepositional phrase attachment problem, we must find a way to analyze the meanings of the noun compounds.",Experiment/Discussion
"Our goal is to extract propositional information from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them.",Experiment/Discussion
"For example, we want to characterize the treatment-for-disease relationship between the words of migraine treatment versus the method-of-treatment relationship between the words of aerosol treatment.",Experiment/Discussion
"These relations are intended to be combined to produce larger propositions that can then be used in a variety of interpretation paradigms, such as abductive reasoning (Hobbs et al., 1993) or inductive logic programming (Ng and Zelle, 1997).",Experiment/Discussion
"Note that because we are concerned with the semantic relations that hold between the concepts, as opposed to the more standard, syntax-driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classification, amenable to standard machine learning classification techniques.",Experiment/Discussion
We have found that we can use such algorithms to classify relationships between two-word noun compounds with a surprising degree of accuracy.,Experiment/Discussion
A one-out-of-eighteen classification using a neural net achieves accuracies as high as 62%.,Experiment/Discussion
"By taking advantage of lexical ontologies, we achieve strong results on noun compounds for which neither word is present in the training set.",Experiment/Discussion
"Thus, we think this is a promising approach for a variety of semantic labeling tasks.",Experiment/Discussion
"The reminder of this paper is organized as follows: Section 2 describes related work, Section 3 describes the semantic relations and how they were chosen, and Section 4 describes the data collection and ontologies.",Experiment/Discussion
"In Section 5 we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method.",Experiment/Discussion
Section 6 concludes the paper and discusses future work.,Experiment/Discussion
Several approaches have been proposed for empirical noun compound interpretation.,Experiment/Discussion
"Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics.",Experiment/Discussion
"Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured.",Experiment/Discussion
"We are interested in the third task, interpretation of the underlying semantics.",Experiment/Discussion
Most related work relies on hand-written rules of one kind or another.,Experiment/Discussion
"Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules.",Experiment/Discussion
"Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation.",Experiment/Discussion
Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text.,Experiment/Discussion
"Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest.",Experiment/Discussion
"These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates.",Experiment/Discussion
"Our goals are more general than those of information extraction, and so this work should be helpful for that task.",Experiment/Discussion
"However, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks.",Experiment/Discussion
"There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment.",Experiment/Discussion
"The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with.",Experiment/Discussion
"Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb.",Experiment/Discussion
It is not clear how to use the results of such analysis after they are found; the semantics of the relationship between the terms must still be determined.,Experiment/Discussion
"In our framework we would cast this problem as finding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, and then seeing if the categorization algorithm determines their exists any relationship R'(n1, R(p, n2)) or R'(v,R(p,n2)).",Experiment/Discussion
The algorithms used in the related work reflect the fact that they condition probabilities on a particular verb and noun.,Experiment/Discussion
"Resnik (1993; 1995) use classes in Wordnet (Fellbaum, 1998) and a measure of conceptual association to generalize over the nouns.",Experiment/Discussion
Brill and Resnik (1994) use Brill’s transformation-based algorithm along with simple counts within a lexical hierarchy in order to generalize over individual words.,Experiment/Discussion
"Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results.",Experiment/Discussion
Our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns.,Experiment/Discussion
"In this work we aim for a representation that is intermediate in generality between standard case roles (such as Agent, Patient, Topic, Instrument), and the specificity required for information extraction.",Experiment/Discussion
"We have created a set of relations that are sufficiently general to cover a significant number of noun compounds, but that can be domain specific enough to be useful in analysis.",Experiment/Discussion
"We want to support relationships between entities that are shown to be important in cognitive linguistics, in particular we intend to support the kinds of inferences that arise from Talmy’s force dynamics (Talmy, 1985).",Experiment/Discussion
"It has been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990).",Experiment/Discussion
"In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel.",Experiment/Discussion
The problem remains of determining what the appropriate kinds of relations are.,Experiment/Discussion
"In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compounds (NCs).",Experiment/Discussion
Levi (1978) argues that there exists a small set of semantic relationships that NCs may imply.,Experiment/Discussion
Downing (1977) argues that the semantics of NCs cannot be exhausted by any finite listing of relationships.,Experiment/Discussion
Between these two extremes lies Warren’s (1978) taxonomy of six major semantic relations organized into a hierarchical structure.,Experiment/Discussion
We have identified the 38 relations shown in Table 1.,Experiment/Discussion
"We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren, but in many cases these are inappropriate.",Experiment/Discussion
"Levi’s classes are too general for our purposes; for example, she collapses the “location” and “time” relationships into one single class “In” and therefore field mouse and autumnal rain belong to the same class.",Experiment/Discussion
"Warren’s classification schema is much more detailed, and there is some overlap between the top levels of Warren’s hierarchy and our set of relations.",Experiment/Discussion
"For example, our “Cause (2-1)” for flu virus corresponds to her “Causer-Result” of hay fever, and our “Person Afflicted” (migraine patient) can be thought as Warren’s “Belonging-Possessor” of gunman.",Experiment/Discussion
"Warren differentiates some classes also on the basis of the semantics of the constituents, so that, for example, the “Time” relationship is divided up into “Time-Animate Entity” of weekend guests and “Time-Inanimate Entity” of Sunday paper.",Experiment/Discussion
Our classification is based on the kind of relationships that hold between the constituent nouns rather than on the semantics of the head nouns.,Experiment/Discussion
"For the automatic classification task, we used only the 18 relations (indicated in bold in Table 1) for which an adequate number of examples were found in the current collection.",Experiment/Discussion
"Many NCs were ambiguous, in that they could be described by more than one semantic relationship.",Experiment/Discussion
"In these cases, we simply multi-labeled them: for example, cell growth is both “Activity” and “Change”, tumor regression is “Ending/reduction” and “Change” and bladder dysfunction is “Location” and “Defect”.",Experiment/Discussion
Our approach handles this kind of multi-labeled classification.,Experiment/Discussion
Two relation types are especially problematic.,Experiment/Discussion
"Some compounds are non-compositional or lexicalized, such as vitamin k and e2 protein; others defy classification because the nouns are subtypes of one another.",Experiment/Discussion
"This group includes migraine headache, guinea pig, and hbv carrier.",Experiment/Discussion
We placed all these NCs in a catch-all category.,Experiment/Discussion
We also included a “wrong” category containing word pairs that were incorrectly labeled as NCs.2 The relations were found by iterative refinement based on looking at 2245 extracted compounds (described in the next section) and finding commonalities among them.,Experiment/Discussion
Labeling was done by the authors of this paper and a biology student; the NCs were classified out of context.,Experiment/Discussion
"We expect to continue development and refinement of these relationship types, based on what ends up clearly being use2The percentage of the word pairs extracted that were not true NCs was about 6%; some examples are: treat migraine, ten patient, headache more.",Experiment/Discussion
"We do not know, however, how many NCs we missed.",Experiment/Discussion
The errors occurred when the wrong label was assigned by the tagger (see Section 4). ful “downstream” in the analysis.,Experiment/Discussion
"The end goal is to combine these relationships in NCs with more that two constituent nouns, like in the example intranasal migraine treatment of Section 1.",Experiment/Discussion
"To create a collection of noun compounds, we performed searches from MedLine, which contains references and abstracts from 4300 biomedical journals.",Experiment/Discussion
"We used several query terms, intended to span across different subfields.",Experiment/Discussion
We retained only the titles and the abstracts of the retrieved documents.,Experiment/Discussion
"On these titles and abstracts we ran a part-of-speech tagger (Cutting et al., 1991) and a program that extracts only sequences of units tagged as nouns.",Experiment/Discussion
"We extracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents.",Experiment/Discussion
"The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine (Humphreys et al., 1998).",Experiment/Discussion
"We use the MetaThesaurus component to map lexical items into unique concept IDs (CUIs).3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy (Lowe and Barnett, 1994); we mapped the CUIs into MeSH terms.",Experiment/Discussion
"There are about 19,000 unique main terms in MeSH, as well as additional modifiers.",Experiment/Discussion
"There are 15 main subhierarchies (trees) in MeSH, each corresponding to a major branch of medical ontology.",Experiment/Discussion
"For example, tree A corresponds to Anatomy, tree B to Organisms, and so on.",Experiment/Discussion
"The longer the name of the MeSH term, the longer the path from the root and the more precise the description.",Experiment/Discussion
"For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous System Diseases) and so on.",Experiment/Discussion
We use the MeSH hierarchy for generalization across classes of nouns; we use it instead of the other resources in the UMLS primarily because of MeSH’s hierarchical structure.,Experiment/Discussion
"For these experiments, we considered only those noun compounds for which both nouns can be mapped into MeSH terms, resulting in a total of 2245 NCs.",Experiment/Discussion
"Because we have defined noun compound relation determination as a classification problem, we can make use of standard classification algorithms.",Experiment/Discussion
"In particular, we used neural networks to classify across all relations simultaneously. shown in boldface are those used in the experiments reported on here.",Experiment/Discussion
Relation ID numbers are shown in parentheses by the relation names.,Experiment/Discussion
The second column shows the number of labeled examples for each class; the last row shows a class consisting of compounds that exhibit more than one relation.,Experiment/Discussion
The notation (1-2) and (2-1) indicates the directionality of the relations.,Experiment/Discussion
"For example, Cause (1-2) indicates that the first noun causes the second, and Cause (2-1) indicates the converse.",Experiment/Discussion
We ran the experiments creating models that used different levels of the MeSH hierarchy.,Experiment/Discussion
"For example, for the NC flu vaccination, flu maps to the MeSH term D4.808.54.79.429.154.349 and vaccination to G3.770.670.310.890.",Experiment/Discussion
Flu vaccination for Model 4 would be represented by a vector consisting of the concatenation of the two descriptors showing only the first four levels: D4.808.54.79 G3.770.670.310 (see Table 2).,Experiment/Discussion
"When a word maps to a general MeSH term (like treatment, Y11) zeros are appended to the end of the descriptor to stand in place of the missing values (so, for example, treatment in Model 3 is Y 11 0, and in Model 4 is Y 11 0 0, etc.).",Experiment/Discussion
The numbers in the MeSH descriptors are categorical values; we represented them with indicator variables.,Experiment/Discussion
"That is, for each variable we calculated the number of possible categories c and then represented an observation of the variable as a sequence of c binary variables in which one binary variable was one and the remaining c − 1 binary variables were zero.",Experiment/Discussion
We also used a representation in which the words themselves were used as categorical input variables (we call this representation “lexical”).,Experiment/Discussion
For this collection of NCs there were 1184 unique nouns and therefore the feature vector for each noun had 1184 components.,Experiment/Discussion
In Table 3 we report the length of the feature vectors for one noun for each model.,Experiment/Discussion
The entire NC was described by concatenating the feature vectors for the two nouns in sequence.,Experiment/Discussion
The NCs represented in this fashion were used as input to a neural network.,Experiment/Discussion
We used a feed-forward network trained with conjugate gradient descent. number corresponds to the level of the MeSH hierarchy used for classification.,Experiment/Discussion
Lexical NN is Neural Network on Lexical and Lexical: Log Reg is Logistic Regression on NN.,Experiment/Discussion
"Acc1 refers to how often the correct relation is the top-scoring relation, Acc2 refers to how often the correct relation is one of the top two according to the neural net, and so on.",Experiment/Discussion
Guessing would yield a result of 0.077.,Experiment/Discussion
"The network had one hidden layer, in which a hyperbolic tangent function was used, and an output layer representing the 18 relations.",Experiment/Discussion
"A logistic sigmoid function was used in the output layer to map the outputs into the interval (0, 1).",Experiment/Discussion
The number of units of the output layer was the number of relations (18) and therefore fixed.,Experiment/Discussion
The network was trained for several choices of numbers of hidden units; we chose the best-performing networks based on training set error for each of the models.,Experiment/Discussion
We subsequently tested these networks on held-out testing data.,Experiment/Discussion
We compared the results with a baseline in which logistic regression was used on the lexical features.,Experiment/Discussion
"Given the indicator variable representation of these features, this logistic regression essentially forms a table of log-odds for each lexical item.",Experiment/Discussion
We also compared to a method in which the lexical indicator variables were used as input to a neural network.,Experiment/Discussion
"This approach is of interest to see to what extent, if any, the MeSH-based features affect performance.",Experiment/Discussion
Note also that this lexical neural-network approach is feasible in this setting because the number of unique words is limited (1184) – such an approach would not scale to larger problems.,Experiment/Discussion
In Table 4 and in Figure 1 we report the results from these experiments.,Experiment/Discussion
Neural network using lexical features only yields 62% accuracy on average across all 18 relations.,Experiment/Discussion
A neural net trained on Model 6 using the MeSH terms to represent the nouns yields an accuracy of 61% on average across all 18 relations.,Experiment/Discussion
"Note that reasonable performance is also obtained for Model 2, which is a much more general representation.",Experiment/Discussion
Table 4 shows that both methods achieve up to 78% accuracy at including the correct relation among the top three hypothesized.,Experiment/Discussion
"Multi-class classification is a difficult problem (Vapnik, 1998).",Experiment/Discussion
"In this problem, a baseline in which Testing set performance on the best models for each MeSH level Levels of the MeSH Hierarchy the algorithm guesses yields about 5% accuracy.",Experiment/Discussion
"We see that our method is a significant improvement over the tabular logistic-regression-based approach, which yields an accuracy of only 31 percent.",Experiment/Discussion
"Additionally, despite the significant reduction in raw information content as compared to the lexical representation, the MeSH-based neural network performs as well as the lexical-based neural network.",Experiment/Discussion
(And we again stress that the lexical-based neural network is not a viable option for larger domains.),Experiment/Discussion
Figure 2 shows the results for each relation.,Experiment/Discussion
"MeSH-based generalization does better on some relations (for example 14 and 15) and Lexical on others (7, 22).",Experiment/Discussion
It turns out that the test set for relationship 7 (“Produces on a genetic level”) is dominated by NCs containing the words alleles and mrna and that all the NCs in the training set containing these words are assigned relation label 7.,Experiment/Discussion
"A similar situation is seen for relation 22, “Time(2-1)”.",Experiment/Discussion
"In the test set examples the second noun is either recurrence, season or time.",Experiment/Discussion
"In the training set, these nouns appear only in NCs that have been labeled as belonging to relation 22.",Experiment/Discussion
"On the other hand, if we look at relations 14 and 15, we find a wider range of words, and in some cases the words in the test set are not present in the training set.",Experiment/Discussion
"In relationship 14 (“Purpose”), for example, vaccine appears 6 times in the test set (e.g., varicella vaccine).",Experiment/Discussion
"In the training set, NCs with vaccine in it have also been classified as “Instrument” (antigen vaccine, polysaccharide vaccine), as “Object” (vaccine development), as “Subtype of” (opv vaccine) and as “Wrong” (vaccines using).",Experiment/Discussion
"Other words in the test set for 14 are varicella which is present in the trainig set only in varicella serology labeled as “Attribute of clinical study”, drainage which is in the training set only as “Location” (gallbladder drainage and tract drainage) and “Activity” (bile drainage).",Experiment/Discussion
Other test set words such as immunisation and carcinogen do not appear in the training set at all.,Experiment/Discussion
"In other words, it seems that the MeSHk-based categorization does better when generalization is required.",Experiment/Discussion
"Additionally, this data set is “dense” in the sense that very few testing words are not present in the training data.",Experiment/Discussion
This is of course an unrealistic situation and we wanted to test the robustness of the method in a more realistic setting.,Experiment/Discussion
The results reported in Table 4 and in Figure 1 were obtained splitting the data into 50% training and 50% testing for each relation and we had a total of 855 training points and 805 test points.,Experiment/Discussion
"Of these, only 75 examples in the testing set consisted of NCs in which both words were not present in the training set.",Experiment/Discussion
We decided to test the robustness of the MeSHbased model versus the lexical model in the case of unseen words; we are also interested in seeing the relative importance of the first versus the second noun.,Experiment/Discussion
"Therefore, we split the data into 5% training (73 data points) and 95% testing (1587 data points) and partitioned the testing set into 4 subsets as follows (the numbers in parentheses are the numbers of points for each case): Table 5 and Figures 3 and 4 present the accuracies for these test set partitions.",Experiment/Discussion
Figure 3 shows that the MeSH-based models are more robust than the lexical when the number of unseen words is high and when the size of training set is (very) small.,Experiment/Discussion
"In this more realistic situation, the MeSH models are able to generalize over previously unseen words.",Experiment/Discussion
"For unseen words, lexical reduces to guessing.4 Figure 4 shows the accuracy for the MeSH basedmodel for the the four cases of Table 5.",Experiment/Discussion
It is interesting to note that the accuracy for Case 1 (first noun not present in the training set) is much higher than the accuracy for Case 2 (second noun not present in the training set).,Experiment/Discussion
This seems to indicate that the second noun is more important for the classification that the first one.,Experiment/Discussion
We have presented a simple approach to corpusbased assignment of semantic relations for noun compounds.,Results/Conclusion
The main idea is to define a set of relations that can hold between the terms and use standard machine learning techniques and a lexical hierarchy to generalize from training instances to new examples.,Results/Conclusion
The initial results are quite promising.,Results/Conclusion
In this task of multi-class classification (with 18 classes) we achieved an accuracy of about 60%.,Results/Conclusion
"These results can be compared with Vanderwende ¢Note that for unseen words, the baseline lexical-based logistic regression approach, which essentially builds a tabular representation of the log-odds for each class, also reduces to random guessing.",Results/Conclusion
Testing set performances for different partitions on the test set Levels of the MeSH Hierarchy els accuracies (for the entire test set and for case 4) and the dashed lines represent the corresponding lexical accuracies.,Results/Conclusion
"The accuracies are smaller than the previous case of Table 4 because the training set is much smaller, but the point of interest is the difference in the performance of MeSH vs. lexical in this more difficult setting.",Results/Conclusion
Note that lexical for case 4 reduces to random guessing.,Results/Conclusion
Testing set performances for different partitions on the test set for the MeSH−based model Levels of the MeSH Hierarchy (1994) who reports an accuracy of 52% with 13 classes and Lapata (2000) whose algorithm achieves about 80% accuracy for a much simpler binary classification.,Results/Conclusion
We have shown that a class-based representation performes as well as a lexical-based model despite the reduction of raw information content and despite a somewhat errorful mapping from terms to concepts.,Results/Conclusion
We have also shown that representing the nouns of the compound by a very general representation (Model 2) achieves a reasonable performance of aout 52% accuracy on average.,Results/Conclusion
This is particularly important in the case of larger collections with a much bigger number of unique words for which the lexical-based model is not a viable option.,Results/Conclusion
Our results seem to indicate that we do not lose much in terms of accuracy using the more compact MeSH representation.,Results/Conclusion
We have also shown how MeSH-besed models out perform a lexical-based approach when the number of training points is small and when the test set consists of words unseen in the training data.,Results/Conclusion
This indicates that the MeSH models can generalize successfully over unseen words.,Results/Conclusion
Our approach handles “mixed-class” relations naturally.,Results/Conclusion
"For the mixed class Defect in Location, the algorithm achieved an accuracy around 95% for both “Defect” and “Location” simultaneously.",Results/Conclusion
Our results also indicate that the second noun (the head) is more important in determining the relationships than the first one.,Results/Conclusion
In future we plan to train the algorithm to allow different levels for each noun in the compound.,Results/Conclusion
"We also plan to compare the results to the tree cut algorithm reported in (Li and Abe, 1998), which allows different levels to be identified for different subtrees.",Results/Conclusion
We also plan to tackle the problem of noun compounds containing more than two terms.,Results/Conclusion
We would like to thank Nu Lai for help with the classification of the noun compound relations.,Results/Conclusion
This work was supported in part by NSF award number IIS-9817353.,Results/Conclusion
