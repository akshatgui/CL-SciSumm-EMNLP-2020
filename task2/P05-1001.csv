col1,col2
"In machine learning, whether one can build a more accurate classifier by using data is an important issue.",{}
"Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.",{}
This paper presents a novel semi-supervised method that employs a learning paradigm which we call The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.,{}
"By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.",{}
The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).,{}
"In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain.","{'title': '1 Introduction', 'number': '1'}"
"Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.","{'title': '1 Introduction', 'number': '1'}"
This problem is often referred to as semi-supervised learning.,"{'title': '1 Introduction', 'number': '1'}"
"Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.","{'title': '1 Introduction', 'number': '1'}"
"For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).","{'title': '1 Introduction', 'number': '1'}"
A related idea is to use Expectation Maximization (EM) to impute labels.,"{'title': '1 Introduction', 'number': '1'}"
"Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.","{'title': '1 Introduction', 'number': '1'}"
Merialdo (1994)).,"{'title': '1 Introduction', 'number': '1'}"
A number of bootstrapping methods have been proposed for NLP tasks (e.g.,"{'title': '1 Introduction', 'number': '1'}"
"Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).","{'title': '1 Introduction', 'number': '1'}"
But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available.,"{'title': '1 Introduction', 'number': '1'}"
Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available.,"{'title': '1 Introduction', 'number': '1'}"
It is exactly this important and difficult problem that we tackle here.,"{'title': '1 Introduction', 'number': '1'}"
"This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e. what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.","{'title': '1 Introduction', 'number': '1'}"
"That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated ‘training data’.","{'title': '1 Introduction', 'number': '1'}"
"We learn the commonality (or structure) of such many classifiers relevant to the task, and use it to improve performance on the target task.","{'title': '1 Introduction', 'number': '1'}"
"One example of such auxiliary problems for chunking tasks is to ‘mask’ a word and predict whether it is “people” or not from the context, like language modeling.","{'title': '1 Introduction', 'number': '1'}"
Another example is to predict the prediction of some classifier trained for the target task.,"{'title': '1 Introduction', 'number': '1'}"
"These auxiliary classifiers can be adequately learned since we have very large amounts of ‘training data’ for them, which we automatically generate from a very large amount of unlabeled data.","{'title': '1 Introduction', 'number': '1'}"
The contributions of this paper are two-fold.,"{'title': '1 Introduction', 'number': '1'}"
"First, we present a novel robust semi-supervised method based on a new learning model and its application to chunking tasks.","{'title': '1 Introduction', 'number': '1'}"
"Second, we report higher performance than the previous best results on syntactic chunking (the CoNLL’00 corpus) and named entity chunking (the CoNLL’03 English and German corpora).","{'title': '1 Introduction', 'number': '1'}"
"In particular, our results are obtained by using unlabeled data as the only additional resource while many of the top systems rely on hand-crafted resources such as large name gazetteers or even rulebased post-processing.","{'title': '1 Introduction', 'number': '1'}"
This work uses a linear formulation of structural learning.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
We first briefly review a standard linear prediction model and then extend it for structural learning.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
We sketch an optimization algorithm using SVD and compare it to related methods.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"In the standard formulation of supervised learning, we seek a predictor that maps an input vector x E X to the corresponding output y E Y.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Linear prediction models are based on real-valued predictors of the form f (x) = wTx, where w is called a weight vector.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"For binary problems, the sign of the linear prediction gives the class label.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"For k-way classification (withk>2), a typical method is winner takes all, where we train one predictor per class and choose the class with the highest output value.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"A frequently used method for finding an accurate predictor�fis regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on thentraining examples{(Xi;Yi)}: L(f(Xi);Yi)+r(f)): L(.) is a loss function to quantify the difference between the prediction f (Xi) and the true output Yi, andr(.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
)is a regularization term to control the model complexity.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Kudoh and Matsumoto (2001), Zhang and Johnson (2003)).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"We present a linear prediction model for structural learning, which extends the traditional model to multiple problems.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
We seek to discover this structure through joint empirical risk minimization over the multiple problems.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
Consider m problems indexed by ` E { 1 each with nt samples (Xti; Yt) indexed by i E {1; ::: ; nt}.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"In our joint linear model, a predictor for problem ` takes the following form where we use I to denote the identity matrix.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
Matrix O (whose rows are orthonormal) is the common structure parameter shared by all the problems; wt and vt are weight vectors specific to each prediction problem `.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
The idea of this model is to discover a common low-dimensional predictive structure (shared by the m problems) parameterized by the projection matrix O.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"In this setting, the goal of structural learning may also be regarded as learning a good feature map Ox — a low-dimensional feature vector parameterized by O.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"In joint ERM, we seekO(and weight vectors) that minimizes the empirical risk summed over all the problems: It can be shown that using joint ERM, we can reliably estimate the optimal joint parameterOas long asmis large (even when eachntis small).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
This is the key reason why structural learning is effective.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"A formal PAC-style analysis can be found in (Ando and Zhang, 2004).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"The optimization problem (2) has a simple solution using SVD when we choose square regularization: r (ft) = A I I wt I I 2 , where the regularization parameter A is given.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"For clarity, let ut be a weight vector for problemfsuch that:ut=wt+OTvt.Then, (2) becomes the minimization of the joint empirical risk written as: This minimization can be approximately solved by the following alternating optimization procedure: •Fix(O,{vt}), and findmpredictors{ut}that minimizes the joint empirical risk (3).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"•Fixmpredictors{ut}, and find(O,{vt})that minimizes the joint empirical risk (3).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
•Iterate until a convergence criterion is met.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"In the first step, we train m predictors independently.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
It is the second step that couples all the problems.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Its solution is given by the SVD (singular value decomposition) of the predictor matrix U = [u 1, ... , ur,, ] : the rows of the optimum O are given by the most significant left singular vectors1 of U.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Intuitively, the optimum O captures the maximal commonality of the m predictors (each derived from ut).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"These m predictors are updated using the new structure matrix O in the next iteration, and the process repeats.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"The formal derivation can be found in (Ando and Zhang, 2004).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data spaceX.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
This is possible because we observe multiple predictors from multiple learning tasks.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"If we regard the observed predictors as sample points of the predictor distribution in the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the “principle components” (or commonality) of these predictors (i.e., “what good predictors are like”).","{'title': '2 A Model for Learning Structures', 'number': '2'}"
Consequently the method directly looks for low-dimensional structures with the highest predictive power.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
The above argument also applies to the feature generation from unlabeled data using LSI (e.g.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
Ando (2004)).,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Similarly, Miller et al. (2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"Since Miller et al. (2004)’s experiments used a proprietary corpus, direct performance comparison is not possible.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
"As we will see, our starting performance is already high.","{'title': '2 A Model for Learning Structures', 'number': '2'}"
Therefore the additional information discovered by SVD-ASO appears crucial to achieve appreciable improvements.,"{'title': '2 A Model for Learning Structures', 'number': '2'}"
"For semi-supervised learning, the idea is to create many auxiliary prediction problems (relevant to the task) from unlabeled data so that we can learn the shared structureO(useful for the task) using the ASO algorithm.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"In particular, we want to create auxiliary problems with the following properties: .Automatic labeling: we need to automatically generate various “labeled” data for the auxiliary problems from unlabeled data.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
•Relevancy: auxiliary problems should be related to the target problem.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"That is, they should share a certain predictive structure.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"The final classifier for the target task is in the form of (1), a linear predictor for structural learning.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
We fixO(learned from unlabeled data through auxiliary problems) and optimize weight vectorswandv on the given labeled data.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"We summarize this semisupervised learning procedure below. f = arg min f E ��1 L(f(o Xi)>Yi) + ~11w1122, where f (O; x) = wTx + vTOx as in (1).","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
The idea is to discover useful features (which do not necessarily appear in the labeled data) from the unlabeled data through learning auxiliary problems.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"Clearly, auxiliary problems more closely related to the target problem will be more beneficial.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"However, even if some problems are less relevant, they will not degrade performance severely since they merely result in some irrelevant features (originated from irrelevantO-components), which ERM learners can cope with.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"On the other hand, potential gains from relevant auxiliary problems can be significant.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"In this sense, our method is robust.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"We present two general strategies for generating useful auxiliary problems: one in a completely unsupervised fashion, and the other in a partiallysupervised fashion.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"In the first strategy, we regard some observable substructures of the input dataXas auxiliary class labels, and try to predict these labels using other parts of the input data.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
Ex 3.1 Predict words.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"Create auxiliary problems by regarding the word at each position as an auxiliary label, which we want to predictfrom the context.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"For instance, predict whether a word is “Smith” or not from its context.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"This problem is relevant to, for instance, named entity chunking since knowing a word is “Smith” helps to predict whether it is part ofa name.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"One binary classification problem can be created for each possible word value (e.g., “IBM”, “he”, “get”,••J.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"Hence, many auxiliary problems can be obtained using this idea.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"More generally, given a feature representation of the input data, we may mask some features as unobserved, and learn classifiers to predict these ‘masked’ features based on other features that are not masked.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
The automatic-labeling requirement is satisfied since the auxiliary labels are observable to us.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"To create relevant problems, we should choose to (mask and) predict features that have good correlation to the target classes, such as words on text tagging/chunking tasks.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
The second strategy is motivated by co-training.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
We use two (or more) distinct feature maps:-P1 and 'P2.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"First, we train a classifier F1 for the target task, using the feature map -P1 and the labeled data.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"The auxiliary tasks are to predict the behavior of this classifier F1 (such as predicted labels) on the unlabeled data, by using the other feature map P2.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"Note that unlike co-training, we only use the classifier as a means of creating auxiliary problems that meet the relevancy requirement, instead of using it to bootstrap labels.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
Ex 3.2 Predict the top-k choices of the classifier.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
Predict the combination of k (a few) classes to which F1 assigns the highest output (confidence) values.,"{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"For instance, predict whether F1 assigns the highest confidence values to CLASS1 and CLASS2 in this order.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"By setting k = 1, the auxiliary task is simply to predict the label prediction of classifier F1.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"By setting k > 1, fine-grained distinctions (related to intrinsic sub-classes of target classes) can be learned.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"From a c-way classification problem, c!=(c — k)! binary prediction problems can be created.","{'title': '3 Semi-supervised Learning Method', 'number': '3'}"
"Using auxiliary problems introduced above, we study the performance of our semi-supervised learning method on named entity chunking and syntactic chunking.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
This section describes the algorithmic aspects of the experimental framework.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The taskspecific setup is described in Sections 5 and 6.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"In our experiments, we use an extension of SVDASO.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"In NLP applications, features have natural grouping according to their types/origins such as ‘current words’, ‘parts-of-speech on the right’, and so forth.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
It is desirable to perform a localized optimization for each of such natural feature groups.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"Hence, we associate each feature group with a submatrix of structure matrixO.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"See (Ando and Zhang, 2004) for the precise formulation.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"In addition, we regularize only those components of wt which correspond to the non-negative part of ut.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing ‘the others’.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"The resulting extension, in effect, only uses the positive components ofUin the SVD computation.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of sequential word tagging.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
We perform Viterbistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"In all settings (including baseline methods), the loss function is a modification of the Huber’s robust loss for regression: L(p, y) = max (0,1 — py)2 if py > —1; and —4py otherwise; with square regularization (A=10-4).","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
One may select other loss functions such as SVM or logistic regression.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The specific choice is not important for the purpose of this paper.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations (Zhang, 2004).","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"As we will show in Section 7.3, our formulation is relatively insensitive to the change inh(rowdimension of the structure matrix).","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"We fixh(for each feature group) to 50, and use it in all settings.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The most time-consuming process is the training ofmauxiliary predictors on the unlabeled data (computingUin Figure 1).,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"Fixing the number of iterations to a constant, it runs in linear tomand the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"Supervised classifier For comparison, we train a classifier using the same features and algorithm, but without unlabeled data (O=0in effect).","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
Co-training We test co-training since our idea of partially-supervised auxiliary problems is motivated by co-training.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"Our implementation follows the original work (Blum and Mitchell, 1998).","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The two (or more) classifiers (with distinct feature maps) are trained with labeled data.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
We maintain a pool ofq unlabeled instances by random selection.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The classifier proposes labels for the instances in this pool.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"We choosesinstances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
The process is then repeated.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"We exploreq=50K, 100K,s=50,100,500,1K, and commonly-used feature splits: ‘current vs. context’ and ‘current+left-context vs. current+right-context’.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
Self-training Single-view bootstrapping is sometimes called self-training.,"{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
"We test the basic selftraining2, which replaces multiple classifiers in the co-training procedure with a single classifier that employs all the features. co/self-training oracle performance To avoid the issue of parameter selection for the co- and selftraining, we report their best possible oracle performance, which is the best F-measure number among all the co- and self-training parameter settings including the choice of the number of iterations. words, parts-of-speech (POS), character types, 4 characters at the beginning/ending in a 5-word window. words in a 3-syntactic chunk window. labels assigned to two words on the left. bi-grams of the current word and the label on the left. labels assigned to previous occurrences of the current word.","{'title': '4 Algorithms Used in Experiments', 'number': '4'}"
We report named entity chunking performance on the CoNLL’03 shared-task3 corpora (English and German).,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
We choose this task because the original intention of this shared task was to test the effectiveness of semi-supervised learning methods.,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"However, it turned out that none of the top performing systems used unlabeled data.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"The likely reason is that the number of labeled data is relatively large (>200K), making it hard to benefit from unlabeled data.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"In particular, we do not use any gazetteer information, which was used in all other systems.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., “World Cup”).","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
We use the official training/development/test splits.,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
They were chosen from the same sources – Reuters and ECI Multilingual Text Corpus – as the provided corpora but disjoint from them.,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as shown in Figure 2.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
We use POS and syntactic chunk information provided by the organizer.,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"As shown in Figure 3, we experiment with auxiliary problems from Ex 3.1 and 3.2: “Predict current (or previous or next) words”; and “Predict top-2 choices of the classifier” using feature splits ‘left context vs. the others’ and ‘right context vs. the others’.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Also, we leave out all but at most 1000 binary prediction problems of each type that have the largest numbers of positive examples to ensure that auxiliary predictors can be adequately learned with a sufficiently large number of examples.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"The results we report are obtained by using all the problems in Figure 3 unless otherwise specified. with one of three sets of labeled training examples: a small English set (10K examples randomly chosen), the entire English training set (204K), and the entire German set (207K), tested on either the development set or test set.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"ASO-semi significantly improves both precision and recall in all the six configurations, resulting in improved F-measures over the supervised baseline by +2.62% to +10.10%.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Co- and self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low:—0.05% to +1.63%.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Comparison with top systems As shown in Figure 5, ASO-semi achieves higher performance than the top systems on both English and German data.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
Most of the top systems boost performance by external hand-crafted resources such as: large gazetteers4; a large amount (2 million words) of labeled data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03).,"{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging.","{'title': '5 Named Entity Chunking Experiments', 'number': '5'}"
"Next, we report syntactic chunking performance on the CoNLL’00 shared-task5 corpus.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 15–18 (212K words) and section 20, respectively.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
They are annotated with eleven types of syntactic chunks such as noun phrases.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
We uni- and bi-grams of words and POS in a 5-token window. word-POS bi-grams in a 3-token window.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
POS tri-grams on the left and right. labels of the two words on the left and their bi-grams. bi-grams of the current word and two labels on the left. use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
We use the POS information provided by the organizer.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
The types of auxiliary problems are the same as in the named entity experiments.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"For word predictions, we exclude instances of punctuation symbols.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"As shown in Figure 7, ASO-semi improves both precision and recall over the supervised baseline.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"It achieves 94.39% in F-measure, which outperforms the supervised baseline by 0.79%.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"Co- and selftraining again slightly improve recall but slightly degrade precision at their oracle performance, which demonstrates that it is not easy to benefit from unlabeled data on this task.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"Unlike named entity chunking, the use of external resources on this task is rare.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
"An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use.","{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
KM01 and CM03 boost performance by classifier combinations.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
SP03 trains conditional random fields for NP (noun phrases) only.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
ASO-semi produces higher NP chunking performance than the others.,"{'title': '6 Syntactic Chunking Experiments', 'number': '6'}"
Figure 9 shows F-measure obtained by computing O from individual types of auxiliary problems on named entity chunking.,"{'title': '7 Empirical Analysis', 'number': '7'}"
"Both types – “Predict words” and “Predict top-2 choices of the classifier” – are useful, producing significant performance improvements over the supervised baseline.","{'title': '7 Empirical Analysis', 'number': '7'}"
The best performance is achieved when O is produced from all of the auxiliary problems.,"{'title': '7 Empirical Analysis', 'number': '7'}"
"To gain insights into the information obtained from unlabeled data, we examine the O entries associated with the feature ‘current words’, computed for the English named entity task.","{'title': '7 Empirical Analysis', 'number': '7'}"
"Figure 10 shows the features associated with the entries ofOwith the largest values, computed from the 2000 unsupervised auxiliary problems: “Predict previous words” and “Predict next words”.","{'title': '7 Empirical Analysis', 'number': '7'}"
"For clarity, the figure only shows words beginning with upper-case letters (i.e., likely to be names in English).","{'title': '7 Empirical Analysis', 'number': '7'}"
Our method captures the spirit of predictive word-clustering but is more general and effective on our tasks.,"{'title': '7 Empirical Analysis', 'number': '7'}"
It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.,"{'title': '7 Empirical Analysis', 'number': '7'}"
The intuition is as follows.,"{'title': '7 Empirical Analysis', 'number': '7'}"
Suppose we split the features into two parts -P1 and 'P2 and predict -P1 based on 'P2.,"{'title': '7 Empirical Analysis', 'number': '7'}"
Suppose features in -P1 are correlated to the class labels (but not necessarily correlated among themselves).,"{'title': '7 Empirical Analysis', 'number': '7'}"
"Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.","{'title': '7 Empirical Analysis', 'number': '7'}"
"Under some conditions, it can be shown that features in 'P2 with similar predictive performance tend to map to similar low-dimensional vectors through O.","{'title': '7 Empirical Analysis', 'number': '7'}"
This effect can be empirically observed in Figure 10 and will be formally shown elsewhere.,"{'title': '7 Empirical Analysis', 'number': '7'}"
"Recall that throughout the experiments, we fix the row-dimension ofO(for each feature group) to 50.","{'title': '7 Empirical Analysis', 'number': '7'}"
"Figure 11 plots F-measure in relation to the rowdimension ofO, which shows that the method is relatively insensitive to the change of this parameter, at least in the range which we consider.","{'title': '7 Empirical Analysis', 'number': '7'}"
We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.,"{'title': '8 Conclusion', 'number': '8'}"
"On CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.","{'title': '8 Conclusion', 'number': '8'}"
The key idea is to create auxiliary problems automatically from unlabeled data so that predictive structures can be learned from that data.,"{'title': '8 Conclusion', 'number': '8'}"
"In practice, it is desirable to create as many auxiliary problems as possible, as long as there is some reason to believe in their relevancy to the task.","{'title': '8 Conclusion', 'number': '8'}"
This is because the risk is relatively minor while the potential gain from relevant problems is large.,"{'title': '8 Conclusion', 'number': '8'}"
"Moreover, the auxiliary problems used in our experiments are merely possible examples.","{'title': '8 Conclusion', 'number': '8'}"
One advantage of our approach is that one may design a variety of auxiliary problems to learn various aspects of the target problem from unlabeled data.,"{'title': '8 Conclusion', 'number': '8'}"
Structural learning provides a framework for carrying out possible new ideas.,"{'title': '8 Conclusion', 'number': '8'}"
Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.,"{'title': 'Acknowledgments', 'number': '9'}"
