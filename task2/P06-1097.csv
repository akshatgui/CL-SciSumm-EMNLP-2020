col1,col2
"We introduce a semi-supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word-alignment quality on a small, manually word-aligned sub-corpus.",{}
We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality.,{}
"The most widely applied training procedure for statistical machine translation — IBM model 4 (Brown et al., 1993) unsupervised training followed by post-processing with symmetrization heuristics (Och and Ney, 2003) — yields low quality word alignments.","{'title': '1 Introduction', 'number': '1'}"
"When compared with gold standard parallel data which was manually aligned using a high-recall/precision methodology (Melamed, 1998), the word-level alignments produced automatically have an F-measure accuracy of 64.6 and 76.4% (see Section 2 for details).","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we improve word alignment and, subsequently, MT accuracy by developing a range of increasingly sophisticated methods: 1.","{'title': '1 Introduction', 'number': '1'}"
"We first recast the problem of estimating the IBM models (Brown et al., 1993) in a discriminative framework, which leads to an initial increase in word-alignment accuracy.","{'title': '1 Introduction', 'number': '1'}"
2.,"{'title': '1 Introduction', 'number': '1'}"
"We extend the IBM models with new (sub)models, which leads to additional increases in word-alignment accuracy.","{'title': '1 Introduction', 'number': '1'}"
"In the process, we also show that these improvements are explained not only by the power of the new models, but also by a novel search procedure for the alignment of highest probability.","{'title': '1 Introduction', 'number': '1'}"
3.,"{'title': '1 Introduction', 'number': '1'}"
"Finally, we propose a training procedure that interleaves discriminative training with maximum likelihood training.","{'title': '1 Introduction', 'number': '1'}"
"These steps lead to word alignments of higher accuracy which, in our case, correlate with higher MT accuracy.","{'title': '1 Introduction', 'number': '1'}"
The rest of the paper is organized as follows.,"{'title': '1 Introduction', 'number': '1'}"
"In Section 2, we review the data sets we use to validate experimentally our algorithms and the associated baselines.","{'title': '1 Introduction', 'number': '1'}"
"In Section 3, we present iteratively our contributions that eventually lead to absolute increases in alignment quality of 4.8% for French/English and 4.8% for Arabic/English, as measured using F-measure for large word alignment tasks.","{'title': '1 Introduction', 'number': '1'}"
"These contributions pertain to the casting of the training procedure in the discriminative framework (Section 3.1); the IBM model extensions and modified search procedure for the Viterbi alignments (Section 3.2); and the interleaved, minimum error/maximum likelihood, training algorithm (Section 4).","{'title': '1 Introduction', 'number': '1'}"
"In Section 5, we assess the impact that our improved alignments have on MT quality.","{'title': '1 Introduction', 'number': '1'}"
We conclude with a comparison of our work with previous research on discriminative training for word alignment and a short discussion of semi-supervised learning.,"{'title': '1 Introduction', 'number': '1'}"
We conduct experiments on alignment and translation tasks using Arabic/English and French/English data sets (see Table 1 for details).,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"Both sets have training data and two gold standard word alignments for small samples of the training data, which we use as the alignment discriminative training set and alignment test set.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
Translation quality is evaluated by translating a held-out translation test set.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"An additional translation set called the Maximum BLEU set is employed by the SMT system to train the weights associated with the components of its log-linear model (Och, 2003).","{'title': '2 Data Sets and Baseline', 'number': '2'}"
The training corpora are publicly available: both the Arabic/English data and the French/English Hansards were released by LDC.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"We created the manual word alignments ourselves, following the Blinker guidelines (Melamed, 1998).","{'title': '2 Data Sets and Baseline', 'number': '2'}"
To train our baseline systems we follow a standard procedure.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"The models were trained two times, first using French or Arabic as the source language and then using English as the source language.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
"For each training direction, we run GIZA++ (Och and Ney, 2003), specifying 5 iterations of Model 1, 4 iterations of the HMM model (Vogel et al., 1996), and 4 iterations of Model 4.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
We quantify the quality of the resulting hypothesized alignments with F-measure using the manually aligned sets.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
We present the results for three different conditions in Table 2.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"For the “F to E” direction the models assign non-zero probability to alignments consisting of links from one Foreign word to zero or more English words, while for “E to F” the models assign non-zero probability to alignments consisting of links from one English word to zero or more Foreign words.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
It is standard practice to improve the final alignments by combining the “F to E” and “E to F” directions using symmetrization heuristics.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
"We use the “union”, “refined” and “intersection” heuristics defined in (Och and Ney, 2003) which are used in conjunction with IBM Model 4 as the baseline in virtually all recent work on word alignment.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
"In Table 2, we report the best symmetrized results.","{'title': '2 Data Sets and Baseline', 'number': '2'}"
The low F-measure scores of the baselines motivate our work.,"{'title': '2 Data Sets and Baseline', 'number': '2'}"
We reinterpret the five groups of parameters of Model 4 listed in the first five lines of Table 3 as sub-models of a log-linear model (see Equation 1).,"{'title': '3 Improving Word Alignments', 'number': '3'}"
Each sub-model hm has an associated weight am.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"Given a vector of these weights a, the alignment search problem, i.e. the search to return the best alignment a� of the sentences e and f according to the model, is specified by Equation 2.","{'title': '3 Improving Word Alignments', 'number': '3'}"
"Log-linear models are often trained to maximize entropy, but we will train our model directly on the final performance criterion.","{'title': '3 Improving Word Alignments', 'number': '3'}"
"We use 1−F-measure as our error function, comparing hypothesized word alignments for the discriminative training set with the gold standard.","{'title': '3 Improving Word Alignments', 'number': '3'}"
Och (2003) has described an efficient exact one-dimensional error minimization technique for a similar search problem in machine translation.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"The technique involves calculating a piecewise constant function f,,,(x) which evaluates the error of the hypotheses which would be picked by equation 2 from a set of hypotheses if we hold all weights constant, except for the weight An (which is set to x).","{'title': '3 Improving Word Alignments', 'number': '3'}"
"The discriminative reranking algorithm is initialized with the parameters of the sub-models 0, an initial choice of the A vector, gold standard word alignments (labels) for the alignment discriminative training set, the constant N specifying the N-best list size used', and an empty master set of hypothesized alignments.","{'title': '3 Improving Word Alignments', 'number': '3'}"
The algorithm is a three step loop: We define new sub-models to model factors not captured by Model 4.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"These are lines 6 to 16 of Table 3, where we use the “E to F” alignment direction as an example.","{'title': '3 Improving Word Alignments', 'number': '3'}"
"We use word-level translation tables informed by both the “E to F” and the “F to E” translation directions derived using the three symmetrization heuristics, the “E to F” translation table from the final iteration of the HMM model and an “E to F” translation table derived using approximative stemming.","{'title': '3 Improving Word Alignments', 'number': '3'}"
The approximative stemming sub-model (sub-model 9) uses the first 4 letters of each vocabulary item as the stem for English and French while for Arabic we use the full word as the stem.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"We also use submodels for backed off fertility, and direct penalization of unaligned English words (“zero fertility”) and aligned English words, and unaligned Foreign words (“NULL-generated” words) and aligned Foreign words.","{'title': '3 Improving Word Alignments', 'number': '3'}"
This is a small sampling of the kinds of knowledge sources we can use in this framework; many others have been proposed in the literature.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
Table 4 shows an evaluation of discriminative reranking.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
We observe: 1.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"The first line is the starting point, which is the Viterbi alignment of the 4th iteration of HMM training.","{'title': '3 Improving Word Alignments', 'number': '3'}"
2.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
The 1-to-many alignments generated by discriminatively reranking Model 4 are better than the 1-to-many alignments of four iterations of Model 4.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
3.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
The 1-to-many alignments of the discriminatively reranked extended model are much better than four iterations of Model 4.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
4.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"The discriminatively reranked extended model outperforms four iterations of Model 4 in both cases with the best heuristic symmetrization, but some of the gain is lost as we are optimizing the F-measure of the 1-to-many alignments rather than the F-measure of the many-to-many alignments directly.","{'title': '3 Improving Word Alignments', 'number': '3'}"
"Overall, the results show our approach is better than or competitive with running four iterations of unsupervised Model 4 training.","{'title': '3 Improving Word Alignments', 'number': '3'}"
Brown et al. (1993) introduced operations defining a hillclimbing search appropriate for Model 4.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"Their search starts with a complete hypothesis and exhaustively applies two operations to it, selecting the best improved hypothesis it can find (or terminating if no improved hypothesis is found).","{'title': '3 Improving Word Alignments', 'number': '3'}"
This search makes many search errors2.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"We developed a new alignment algorithm to reduce search errors: The first two improvements are related to the well-known Tabu local search algorithm (Glover, 2A search error in a word aligner is a failure to find the best alignment according to the model, i.e. in our case a failure to maximize Equation 2.","{'title': '3 Improving Word Alignments', 'number': '3'}"
1986).,"{'title': '3 Improving Word Alignments', 'number': '3'}"
The third improvement is important for restricting total time used when producing alignments for large training corpora.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
We performed two experiments.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
The first evaluates the number of search errors.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"For each corpus we sampled 1000 sentence pairs randomly, with no sentence length restriction.","{'title': '3 Improving Word Alignments', 'number': '3'}"
Model 4 parameters are estimated from the final HMM Viterbi alignment of these sentence pairs.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"We then search to try to find the Model 4 Viterbi alignment with both the new and old algorithms, allowing them both to process for the same amount of time.","{'title': '3 Improving Word Alignments', 'number': '3'}"
The percentage of known search errors is the percentage of sentences from our sample in which we were able to find a more probable candidate by applying our new algorithm using 24 hours of computation for just the 1000 sample sentences.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"Table 5 presents the results, showing that our new algorithm reduced search errors in all cases, but further reduction could be obtained.","{'title': '3 Improving Word Alignments', 'number': '3'}"
The second experiment shows the impact of the new search on discriminative reranking of Model 4 (see Table 6).,"{'title': '3 Improving Word Alignments', 'number': '3'}"
Reduced search errors lead to a better fit of the discriminative training corpus.,"{'title': '3 Improving Word Alignments', 'number': '3'}"
"Intuitively, in approximate EM training for Model 4 (Brown et al., 1993), the E-step corresponds to calculating the probability of all alignments according to the current model estimate, while the M-step is the creation of a new model estimate given a probability distribution over alignments (calculated in the E-step).","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"In the E-step ideally all possible alignments should be enumerated and labeled with p(a|e, f), but this is intractable.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"For the M-step, we would like to count over all possible alignments for each sentence pair, weighted by their probability according to the model estimated at the previous step.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"Because this is not tractable, we make the assumption that the single assumed Viterbi alignment can be used to update our estimate in the Mstep.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
This approximation is called Viterbi training.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
Neal and Hinton (1998) analyze approximate EM training and motivate this type of variant.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
We extend approximate EM training to perform a new type of training which we call Minimum Error / Maximum Likelihood Training.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
The intuition behind this approach to semi-supervised training is that we wish to obtain the advantages of both discriminative training (error minimization) and approximate EM (which allows us to estimate a large numbers of parameters even though we have very few gold standard word alignments).,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"We introduce the EMD algorithm, in which discriminative training is used to control the contributions of sub-models (thereby minimizing error), while a procedure similar to one step of approximate EM is used to estimate the large number of sub-model parameters.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
A brief sketch of the EMD algorithm applied to our extended model is presented in Figure 1.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"Parameters have a superscript t representing their value at iteration t. We initialize the algorithm with the gold standard word alignments (labels) of the word alignment discriminative training set, an initial A, N, and the starting alignments (the iteration 4 HMM Viterbi alignment).","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"In line 2, we make iteration 0 estimates of the 5 sub-models of Model 4 and the 6 heuristic sub-models which are iteration dependent.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"In line 3, we run discriminative training using the algorithm from Section 3.1.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"In line 4, we measure the error of the resulting A vector.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"In the main loop in line 7 we align the full training set (similar to the E-step of EM), while in line 8 we estimate the iteration-dependent sub-models (similar to the M-step of EM).","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
Then we perform discriminative reranking in line 9 and check for convergence in lines 10 and 11 (convergence means that error was not decreased from the previous iteration).,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
The output of the algorithm is new hypothesized alignments of the training corpus.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
Table 7 evaluates the EMD semi-supervised training algorithm.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"We observe: improvement for the first, second and third iterations.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
We also performed an additional experiment for French/English aimed at understanding the potential contribution of the word aligned data without the new algorithm4.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"Like Ittycheriah and Roukos (2005), we converted the alignment discriminative training corpus links into a special corpus consisting of parallel sentences where each sentence consists only of a single word involved in the link.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
We found that the information in the links was “washed out” by the rest of the data and resulted in no change in the alignment test set’s F-Measure.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
Callison-Burch et al. (2004) showed in their work on combining alignments of lower and higher quality that the alignments of higher quality should be given a much higher weight than the lower quality alignments.,"{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"Using this insight, we found that adding 10,000 copies of the special corpus to our training data resulted in the highest alignment test set gain, which was a small gain of 0.6 F-Measure.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"This result suggests that while the link information is useful for improving FMeasure, our improved methods for training are producing much larger improvements.","{'title': '4 Semi-Supervised Training for Word Alignments', 'number': '4'}"
"The symmetrized alignments from the last iteration of EMD were used to build phrasal SMT systems, as were the symmetrized Model 4 alignments (the baseline).","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"Aside from the final alignment, all other resources were held constant between the baseline and contrastive SMT systems, including those based on lower level alignments models such as IBM Model 1.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"For all of our experiments, we use two language models, one built using the English portion of the training data and the other built using additional English news data.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"We run Maximum BLEU (Och, 2003) for 25 iterations individually for each system.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
Table 8 shows our results.,"{'title': '5 Improvement of MT Quality', 'number': '5'}"
"We report BLEU (Papineni et al., 2001) multiplied by 100.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
We also show the F-measure after heuristic symmetrization of the alignment test sets.,"{'title': '5 Improvement of MT Quality', 'number': '5'}"
The table shows that 4We would like to thank an anonymous reviewer for suggesting that this experiment would be useful even when using a small discriminative training corpus. our algorithm produces heuristically symmetrized final alignments of improved F-measure.,"{'title': '5 Improvement of MT Quality', 'number': '5'}"
"Using these alignments in our phrasal SMT system, we produced a statistically significant BLEU improvement (at a 95% confidence interval a gain of 0.78 is necessary) on the French/English task and a statistically significant BLEU improvement on the Arabic/English task (at a 95% confidence interval a gain of 1.2 is necessary).","{'title': '5 Improvement of MT Quality', 'number': '5'}"
The error criterion we used for all experiments is 1 − F-measure.,"{'title': '5 Improvement of MT Quality', 'number': '5'}"
The formula for F-measure is shown in Equation 3.,"{'title': '5 Improvement of MT Quality', 'number': '5'}"
"(Fraser and Marcu, 2006) established that tuning the trade-off between Precision and Recall in the F-Measure formula will lead to the best BLEU results.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"We tuned α by building a collection of alignments using our baseline system, measuring Precision and Recall against the alignment discriminative training set, building SMT systems and measuring resulting BLEU scores, and then searching for an appropriate α setting.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"We searched α = 0.1, 0.2, ..., 0.9 and set α so that the resulting F-measure tracks BLEU to the best extent possible.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"The best settings were α = 0.2 for Arabic/English and α = 0.7 for French/English, and these settings of α were used for every result reported in this paper.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
"See (Fraser and Marcu, 2006) for further details.","{'title': '5 Improvement of MT Quality', 'number': '5'}"
Previous work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word-alignment as a supervised task.,"{'title': '6 Previous Research', 'number': '6'}"
"Examples of this perspective include (Liu et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005).","{'title': '6 Previous Research', 'number': '6'}"
"All of these also used knowledge from one of the IBM Models in order to obtain competitive results with the baseline (with the exception of (Moore, 2005)).","{'title': '6 Previous Research', 'number': '6'}"
We interleave discriminative training with EM and are therefore performing semi-supervised training.,"{'title': '6 Previous Research', 'number': '6'}"
We show that semi-supervised training leads to better word alignments than running unsupervised training followed by discriminative training.,"{'title': '6 Previous Research', 'number': '6'}"
Another important difference with previous work is that we are concerned with generating many-to-many word alignments.,"{'title': '6 Previous Research', 'number': '6'}"
"Cherry and Lin (2003) and Taskar et al. (2005) compared their results with Model 4 using “intersection” by looking at AER (with the “Sure” versus “Possible” link distinction), and restricted themselves to considering 1-to-1 alignments.","{'title': '6 Previous Research', 'number': '6'}"
"However, “union” and “refined” alignments, which are many-to-many, are what are used to build competitive phrasal SMT systems, because “intersection” performs poorly, despite having been shown to have the best AER scores for the French/English corpus we are using (Och and Ney, 2003).","{'title': '6 Previous Research', 'number': '6'}"
"(Fraser and Marcu, 2006) recently found serious problems with AER both empirically and analytically, which explains why optimizing AER frequently results in poor machine translation performance.","{'title': '6 Previous Research', 'number': '6'}"
"Finally, we show better MT results by using Fmeasure with a tuned α value.","{'title': '6 Previous Research', 'number': '6'}"
"The only previous discriminative approach which has been shown to produce translations of similar or better quality to those produced by the symmetrized baseline was (Ittycheriah and Roukos, 2005).","{'title': '6 Previous Research', 'number': '6'}"
"They had access to 5000 gold standard word alignments, considerably more than the 100 or 110 gold standard word alignments used here.","{'title': '6 Previous Research', 'number': '6'}"
"They also invested significant effort in sub-model engineering (producing both sub-models specific to Arabic/English alignment and sub-models which would be useful for other language pairs), while we use sub-models which are simple extensions of Model 4 and language independent.","{'title': '6 Previous Research', 'number': '6'}"
"The problem of semi-supervised learning is often defined as “using unlabeled data to help supervised learning” (Seeger, 2000).","{'title': '6 Previous Research', 'number': '6'}"
Most work on semi-supervised learning uses underlying distributions with a relatively small number of parameters.,"{'title': '6 Previous Research', 'number': '6'}"
"An initial model is estimated in a supervised fashion using the labeled data, and this supervised model is used to attach labels (or a probability distribution over labels) to the unlabeled data, then a new supervised model is estimated, and this is iterated.","{'title': '6 Previous Research', 'number': '6'}"
"If these techniques are applied when there are a small number of labels in relation to the number of parameters used, they will suffer from the “overconfident pseudo-labeling problem” (Seeger, 2000), where the initial labels of poor quality assigned to the unlabeled data will dominate the model estimated in the M-step.","{'title': '6 Previous Research', 'number': '6'}"
"However, there are tasks with large numbers of parameters where there are sufficient labels.","{'title': '6 Previous Research', 'number': '6'}"
Nigam et al. (2000) addressed a text classification task.,"{'title': '6 Previous Research', 'number': '6'}"
"They estimate a Naive Bayes classifier over the labeled data and use it to provide initial MAP estimates for unlabeled documents, followed by EM to further refine the model.","{'title': '6 Previous Research', 'number': '6'}"
"Callison-Burch et al. (2004) examined the issue of semi-supervised training for word alignment, but under a scenario where they simulated sufficient gold standard word alignments to follow an approach similar to Nigam et al.","{'title': '6 Previous Research', 'number': '6'}"
(2000).,"{'title': '6 Previous Research', 'number': '6'}"
We do not have enough labels for this approach.,"{'title': '6 Previous Research', 'number': '6'}"
We are aware of two approaches to semisupervised learning which are more similar in spirit to ours.,"{'title': '6 Previous Research', 'number': '6'}"
Ivanov et al. (2001) used discriminative training in a reinforcement learning context in a similar way to our adding of a discriminative training step to an unsupervised context.,"{'title': '6 Previous Research', 'number': '6'}"
A large body of work uses semi-supervised learning for clustering by imposing constraints on clusters.,"{'title': '6 Previous Research', 'number': '6'}"
"For instance, in (Basu et al., 2004), the clustering system was supplied with pairs of instances labeled as belonging to the same or different clusters.","{'title': '6 Previous Research', 'number': '6'}"
"We presented a semi-supervised algorithm based on IBM Model 4, with modeling and search extensions, which produces alignments of improved F-measure over unsupervised Model 4 training.","{'title': '7 Conclusion', 'number': '7'}"
We used these alignments to produce translations of higher quality.,"{'title': '7 Conclusion', 'number': '7'}"
"The semi-supervised learning literature generally addresses augmenting supervised learning tasks with unlabeled data (Seeger, 2000).","{'title': '7 Conclusion', 'number': '7'}"
"In contrast, we augmented an unsupervised learning task with labeled data.","{'title': '7 Conclusion', 'number': '7'}"
We hope that Minimum Error / Maximum Likelihood training using the EMD algorithm can be used for a wide diversity of tasks where there is not enough labeled data to allow supervised estimation of an initial model of reasonable quality.,"{'title': '7 Conclusion', 'number': '7'}"
"This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.","{'title': '8 Acknowledgments', 'number': '8'}"
HR001106-C-0022.,"{'title': '8 Acknowledgments', 'number': '8'}"
We would like to thank the USC Center for High Performance Computing and Communications.,"{'title': '8 Acknowledgments', 'number': '8'}"
