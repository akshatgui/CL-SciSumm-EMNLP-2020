col1,col2
Letter-to-phoneme conversion generally requires aligned training data of letters and phonemes.,{}
"Typically, the alignments are limited to one-to-one alignments.",{}
We present a novel technique of training with many-to-many alignments.,{}
A letter chunking bigram prediction manages double letters and double phonemes automatically as opposed to preprocessing with fixed lists.,{}
We also apply an HMM method in conjunction with a local classification model to predict a global phoneme sequence given a word.,{}
The many-to-many alignments result in significant improvements over the traditional one-to-one approach.,{}
Our system achieves state-of-the-art performance on several languages and data sets.,{}
Letter-to-phoneme (L2P) conversion requires a system to produce phonemes that correspond to a given written word.,"{'title': '1 Introduction', 'number': '1'}"
"Phonemes are abstract representations of how words should be pronounced in natural speech, while letters or graphemes are representations of words in written language.","{'title': '1 Introduction', 'number': '1'}"
"For example, the phonemes for the word phoenix are [ f i n ■ k s ].","{'title': '1 Introduction', 'number': '1'}"
"The L2P task is a crucial part of speech synthesis systems, as converting input text (graphemes) into phonemes is the first step in representing sounds.","{'title': '1 Introduction', 'number': '1'}"
"L2P conversion can also help improve performance in spelling correction (Toutanova and Moore, 2001).","{'title': '1 Introduction', 'number': '1'}"
"Unfortunately, proper nouns and unseen words prevent a table look-up approach.","{'title': '1 Introduction', 'number': '1'}"
It is infeasible to construct a lexical database that includes every word in the written language.,"{'title': '1 Introduction', 'number': '1'}"
"Likewise, orthographic complexity of many languages prevents us from using hand-designed conversion rules.","{'title': '1 Introduction', 'number': '1'}"
There are always exceptional rules that need to be added to cover a large vocabulary set.,"{'title': '1 Introduction', 'number': '1'}"
"Thus, an automatic L2P system is desirable.","{'title': '1 Introduction', 'number': '1'}"
"Many data-driven techniques have been proposed for letter-to-phoneme conversion systems, including pronunciation by analogy (Marchand and Damper, 2000), constraint satisfaction (Van Den Bosch and Canisius, 2006), Hidden Markov Model (Taylor, 2005), decision trees (Black et al., 1998), and neural networks (Sejnowski and Rosenberg, 1987).","{'title': '1 Introduction', 'number': '1'}"
"The training data usually consists of written words and their corresponding phonemes, which are not aligned; there is no explicit information indicating individual letter and phoneme relationships.","{'title': '1 Introduction', 'number': '1'}"
These relationships must be postulated before a prediction model can be trained.,"{'title': '1 Introduction', 'number': '1'}"
"Previous work has generally assumed one-to-one alignment for simplicity (Daelemans and Bosch, 1997; Black et al., 1998; Damper et al., 2005).","{'title': '1 Introduction', 'number': '1'}"
"An expectation maximization (EM) based algorithm (Dempster et al., 1977) is applied to train the aligners.","{'title': '1 Introduction', 'number': '1'}"
"However, there are several problems with this approach.","{'title': '1 Introduction', 'number': '1'}"
"Letter strings and phoneme strings are not typically the same length, so null phonemes and null letters must be introduced to make oneto-one-alignments possible, Furthermore, two letters frequently combine to produce a single phoneme (double letters), and a single letter can sometimes produce two phonemes (double phonemes).","{'title': '1 Introduction', 'number': '1'}"
"To help address these problems, we propose an automatic many-to-many aligner and incorporate it into a generic classification predictor for letter-tophoneme conversion.","{'title': '1 Introduction', 'number': '1'}"
"Our many-to-many aligner automatically discovers double phonemes and double letters, as opposed to manually preprocessing data by merging phonemes using fixed lists.","{'title': '1 Introduction', 'number': '1'}"
"To our knowledge, applying many-to-many alignments to letter-to-phoneme conversion is novel.","{'title': '1 Introduction', 'number': '1'}"
"Once we have our many-to-many alignments, we use that data to train a prediction model.","{'title': '1 Introduction', 'number': '1'}"
"Many phoneme prediction systems are based on local prediction methods, which focus on predicting an individual phoneme given each letter in a word.","{'title': '1 Introduction', 'number': '1'}"
"Conversely, a method like pronunciation by analogy (PbA) (Marchand and Damper, 2000) is considered a global prediction method: predicted phoneme sequences are considered as a whole.","{'title': '1 Introduction', 'number': '1'}"
"Recently, Van Den Bosch and Canisius (2006) proposed trigram class prediction, which incorporates a constraint satisfaction method to produce a global prediction for letter-to-phoneme conversion.","{'title': '1 Introduction', 'number': '1'}"
"Both PbA and trigram class prediction show improvement over predicting individual phonemes, confirming that L2P systems can benefit from incorporating the relationship between phonemes in a sequence.","{'title': '1 Introduction', 'number': '1'}"
"In order to capitalize on the information found in phoneme sequences, we propose to apply an HMM method after a local phoneme prediction process.","{'title': '1 Introduction', 'number': '1'}"
"Given a candidate list of two or more possible phonemes, as produced by the local predictor, the HMM will find the best phoneme sequence.","{'title': '1 Introduction', 'number': '1'}"
"Using this approach, our system demonstrates an improvement on several language data sets.","{'title': '1 Introduction', 'number': '1'}"
The rest of the paper is structured as follows.,"{'title': '1 Introduction', 'number': '1'}"
We describe the letter-phoneme alignment methods including a standard one-to-one alignment method and our many-to-many approach in Section 2.,"{'title': '1 Introduction', 'number': '1'}"
The alignment methods are used to align graphemes and phonemes before the phoneme prediction models can be trained from the training examples.,"{'title': '1 Introduction', 'number': '1'}"
"In Section 3, we present a letter chunk prediction method that automatically discovers double letters in grapheme sequences.","{'title': '1 Introduction', 'number': '1'}"
It incorporates our manyto-many alignments with prediction models.,"{'title': '1 Introduction', 'number': '1'}"
"In Section 4, we present our application of an HMM method to the local prediction results.","{'title': '1 Introduction', 'number': '1'}"
The results of experiments on several language data sets are discussed in Section 5.,"{'title': '1 Introduction', 'number': '1'}"
We conclude and propose future work in Section 6.,"{'title': '1 Introduction', 'number': '1'}"
"There are two main problems with one-to-one alignments: First, consider the double letter problem.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"In most cases when the grapheme sequence is longer than the phoneme sequence, it is because some letters are silent.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For example, in the word abode, pronounced [ a b o d ], the letter a produces a null phoneme (E).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
This is well captured by one-to-one aligners.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"However, the longer grapheme sequence can also be generated by double letters; for example, in the word king, pronounced [ k i ], the letters ng together produce the phoneme [ ].","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"In this case, one-to-one aligners using null phonemes will produce an incorrect alignment.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"This can cause problems for the phoneme prediction model by training it to produce a null phoneme from either of the letters n or g. In the double phoneme case, a new phoneme is introduced to represent a combination of two (or more) phonemes.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For example, in the word fume with phoneme sequence [ f j u m ], the letter u produces both the [ j ] and [ u ] phonemes.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
There are two possible solutions for constructing a oneto-one alignment in this case.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
The first is to create a new phoneme by merging the phonemes [ j ] and [ u ].,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
This requires constructing a fixed list of new phonemes before beginning the alignment process.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
The second solution is to add a null letter in the grapheme sequence.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"However, the null letter not only confuses the phoneme prediction model, but also complicates the the phoneme generation phase.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For comparison with our many-to-many approach, we implement a one-to-one aligner based on the epsilon scattering method (Black et al., 1998).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The method applies the EM algorithm to estimate the probability of mapping a letter l to a phoneme p, P(l, p).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The initial probability table starts by mapping all possible alignments between letters and phonemes for each word in the training data, introducing all possible null phoneme positions.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For example, the word/phoneme-sequence pair abode [ ❅ b o d ] has five possible positions where a null phoneme can be added to make an alignment.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The training process uses the initial probability table P(l, p) to find the best possible alignments for each word using the Dynamic Time Warping (DTW) algorithm (Sankoff and Kruskal, 1999).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"At each iteration, the probability table P(l, p) is re-calculated based on the best alignments found in that iteration.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
Finding the best alignments and re-calculating the probability table continues iteratively until there is no change in the probability table.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The final probability table P(l, p) is used to find one-to-one alignments given graphemes and phonemes.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
We present a many-to-many alignment algorithm that overcomes the limitations of one-to-one aligners.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The training of the many-to-many aligner is an extension of the forward-backward training of a one-to-one stochastic transducer presented in (Ristad and Yianilos, 1998).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Partial counts are counts of all possible mappings from letters to phonemes that are collected in the -y table, while mapping probabilities (initially uniform) are maintained in the S table.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For each grapheme-/phoneme-sequence pair (x, y), the EM-many2many function (Algorithm 1) calls the Expectation-many2many function (Algorithm 2) to collect partial counts.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
T and V are the lengths of x and y respectively.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
The maxX and maxY variables are the maximum lengths of subsequences used in a single mapping operation for x and y.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"(For the task at hand, we set both maxX and maxY to 2.)","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
The Maximization-step function simply normalizes the partial counts to create a probability distribution.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
Normalization can be done over the whole table to create a joint distribution or per grapheme to create a conditional distribution.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The Forward-many2many function (Algorithm 3) fills in the table α, with each entry α(t, v) being the sum of all paths through the transducer that generate the sequence pair (xi, y��).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Analogously, the Backward-many2many function fills in Q, with each entry Q(t, v) being the sum of all paths through the transducer that generate the sequence pair (xt , yr).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
The constants DELX and DELY indicate whether or not deletions are allowed on either side.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"In our system, we allow letter deletions (i.e. mapping of letters to null phoneme), but not phoneme deletions.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Expectation-many2many first calls the two functions to fill the α and Q tables, and then uses the probabilities to calculate partial counts for every possible mapping in the sequence pair.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"The partial count collected at positions t and v in the sequence pair is the sum of all paths that generate the sequence pair and go through (t, v), divided by the sum of all paths that generate the entire sequence pair (α(T, V )).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Once the probabilities are learned, the Viterbi algorithm can be used to produce the most likely alignment as in the following equations.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
Back pointers to maximizing arguments are kept at each step so the alignment can be reconstructed.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Given a set of words and their phonemes, alignments are made across graphemes and phonemes.","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"For example, the word phoenix, with phonemes [ f i n i k s ], is aligned as: ph oe n i x || f i n i ks The letters ph are an example of the double letter problem (mapping to the single phoneme [ f ]), while the letter x is an example of the double phoneme problem (mapping to both [ k ] and [ s ] in the phoneme sequence).","{'title': '2 Letter-phoneme alignment', 'number': '2'}"
These alignments provide more accurate grapheme-to-phoneme relationships for a phoneme prediction model.,"{'title': '2 Letter-phoneme alignment', 'number': '2'}"
"Our new alignment scheme provides more accurate alignments, but it is also more complex — sometimes a prediction model should predict two phonemes for a single letter, while at other times the prediction model should make a prediction based on a pair of letters.","{'title': '3 Letter chunking', 'number': '3'}"
"In order to distinguish between these two cases, we propose a method called “letter chunking”.","{'title': '3 Letter chunking', 'number': '3'}"
"Once many-to-many alignments are built across graphemes and phonemes, each word contains a set of letter chunks, each consisting of one or two letters aligned with phonemes.","{'title': '3 Letter chunking', 'number': '3'}"
Each letter chunk can be considered as a grapheme unit that contains either one or two letters.,"{'title': '3 Letter chunking', 'number': '3'}"
"In the same way, each phoneme chunk can be considered as a phoneme unit consisting of one or two phonemes.","{'title': '3 Letter chunking', 'number': '3'}"
Note that the double letters and double phonemes are implicitly discovered by the alignments of graphemes and phonemes.,"{'title': '3 Letter chunking', 'number': '3'}"
They are not necessarily consistent over the training data but based on the alignments found in each word.,"{'title': '3 Letter chunking', 'number': '3'}"
"In the phoneme generation phase, the system has only graphemes available to predict phonemes, so there is no information about letter chunk boundaries.","{'title': '3 Letter chunking', 'number': '3'}"
We cannot simply merge any two letters that have appeared as a letter chunk in the training data.,"{'title': '3 Letter chunking', 'number': '3'}"
"For example, although the letter pair sh is usually pronounced as a single phoneme in English (e.g. gash [ g ae f ]), this is not true universally (e.g. gasholder [ g ae s h o l d @ r ]).","{'title': '3 Letter chunking', 'number': '3'}"
"Therefore, we implement a letter chunk prediction model to provide chunk boundaries given only graphemes.","{'title': '3 Letter chunking', 'number': '3'}"
"In our system, a bigram letter chunking prediction automatically discovers double letters based on instance-based learning (Aha et al., 1991).","{'title': '3 Letter chunking', 'number': '3'}"
"Since the many-to-many alignments are drawn from 1-0, 1-1, 1-2, 2-0, and 2-1 relationships, each letter in a word can form a chunk with its neighbor or stand alone as a chunk itself.","{'title': '3 Letter chunking', 'number': '3'}"
We treat the chunk prediction as a binary classification problem.,"{'title': '3 Letter chunking', 'number': '3'}"
We generate all the bigrams in a word and determine whether each bigram should be a chunk based on its context.,"{'title': '3 Letter chunking', 'number': '3'}"
Table 1 shows an example of how chunking prediction proceeds for the word longs.,"{'title': '3 Letter chunking', 'number': '3'}"
"Letters li−2, li−1, li+1, and li+2 are the context of the bigram li; chunk = 1 if the letter bigram li is a chunk.","{'title': '3 Letter chunking', 'number': '3'}"
"Otherwise, the chunk simply consists of an individual letter.","{'title': '3 Letter chunking', 'number': '3'}"
"In the example, the word is decomposed as l|o|ng|s, which can be aligned with its pronunciation [ l  |6  |N  |z ].","{'title': '3 Letter chunking', 'number': '3'}"
"If the model happens to predict consecutive overlapping chunks, only the first of the two is accepted.","{'title': '3 Letter chunking', 'number': '3'}"
Most of the previously proposed techniques for phoneme prediction require training data to be aligned in one-to-one alignments.,"{'title': '4 Phoneme prediction', 'number': '4'}"
Those models approach the phoneme prediction task as a classification problem: a phoneme is predicted for each letter independently without using other predictions from the same word.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"These local predictions assume independence of predictions, even though there are clearly interdependencies between predictions.","{'title': '4 Phoneme prediction', 'number': '4'}"
Predicting each phoneme in a word without considering other assignments may not satisfy the main goal of finding a set of phonemes that work together to form a word.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"A trigram phoneme prediction with constraint satisfaction inference (Van Den Bosch and Canisius, 2006) was proposed to improve on local predictions.","{'title': '4 Phoneme prediction', 'number': '4'}"
"From each letter unit, it predicts a trigram class that has the target phoneme in the middle surrounded by its neighboring phonemes.","{'title': '4 Phoneme prediction', 'number': '4'}"
"The phoneme sequence is generated in such a way that it satisfies the trigram, bigram and unigram constraints.","{'title': '4 Phoneme prediction', 'number': '4'}"
The overlapping predictions improve letter-to-phoneme performance mainly by repairing imperfect one-to-one alignments.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"However, the trigram class prediction tends to be more complex as it increases the number of target classes.","{'title': '4 Phoneme prediction', 'number': '4'}"
"For English, there are only 58 unigram phoneme classes but 13,005 tri-gram phoneme classes.","{'title': '4 Phoneme prediction', 'number': '4'}"
The phoneme combinations in the tri-gram classes are potentially confusing to the prediction model because the model has more target classes in its search space while it has access to the same number of local features in the grapheme side.,"{'title': '4 Phoneme prediction', 'number': '4'}"
We propose to apply a supervised HMM method embedded with local classification to find the most likely sequence of phonemes given a word.,"{'title': '4 Phoneme prediction', 'number': '4'}"
An HMM is a statistical model that combines the observation likelihood (probability ofphonemes given letters) and transition likelihood (probability of current phoneme given previous phonemes) to predict each phoneme.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"Our approach differs from a basic Hidden Markov Model for letter-to-phoneme system (Taylor, 2005) that formulates grapheme sequences as observation states and phonemes as hidden states.","{'title': '4 Phoneme prediction', 'number': '4'}"
The basic HMM system for L2P does not provide good performance on the task because it lacks context information on the grapheme side.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"In fact, a pronunciation depends more on graphemes than on the neighboring phonemes; therefore, the transition probability (language model) should affect the prediction decisions only when there is more than one possible phoneme that can be assigned to a letter.","{'title': '4 Phoneme prediction', 'number': '4'}"
"Our approach is to use an instance-based learning technique as a local predictor to generate a set of phoneme candidates for each letter chunk, given its context in a word.","{'title': '4 Phoneme prediction', 'number': '4'}"
The local predictor produces confidence values for Each candidate phoneme.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"We normalize the confidence values into values between 0 and 1, and treat them as the emission probabilities, while the transition probabilities are derived directly from the phoneme sequences in the training data.","{'title': '4 Phoneme prediction', 'number': '4'}"
The pronunciation is generated by considering both phoneme prediction values and transition probabilities.,"{'title': '4 Phoneme prediction', 'number': '4'}"
The optimal phoneme sequence is found with the Viterbi search algorithm.,"{'title': '4 Phoneme prediction', 'number': '4'}"
We limit the size of the context to n = 3 in order to avoid overfitting and minimize the complexity of the model.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"Since the candidate set is from the classifier, the search space is limited to a small number of candidate phonemes (1 to 5 phonemes in most cases).","{'title': '4 Phoneme prediction', 'number': '4'}"
The HMM postprocessing is independent of local predictions from the classifier.,"{'title': '4 Phoneme prediction', 'number': '4'}"
"Instead, it selects the best phoneme sequence from a set of possible local predictions by taking advantage of the phoneme language model, which is trained on the phoneme sequences in the training data.","{'title': '4 Phoneme prediction', 'number': '4'}"
"We evaluated our approaches on CMUDict, Brulex, and German, Dutch and English Celex corpora (Baayen et al., 1996).","{'title': '5 Evaluation', 'number': '5'}"
The corpora (except English Celex) are available as part of the Letterto-Phoneme Conversion PRONALSYL Challenge1.,"{'title': '5 Evaluation', 'number': '5'}"
"For the English Celex data, we removed duplicate words as well as words shorter than four letters.","{'title': '5 Evaluation', 'number': '5'}"
Table 2 shows the number of words and the language of each corpus.,"{'title': '5 Evaluation', 'number': '5'}"
"For all of our experiments, our local classifier for predicting phonemes is the instance-based learning IB1 algorithm (Aha et al., 1991) implemented in the TiMBL package (Daelemans et al., 2004).","{'title': '5 Evaluation', 'number': '5'}"
The HMM technique is applied as post processing to the instance-based learning to provide a sequence prediction.,"{'title': '5 Evaluation', 'number': '5'}"
"In addition to comparing one-toone and many-to-many alignments, we also compare our method to the constraint satisfaction inference method as described in Section 4.","{'title': '5 Evaluation', 'number': '5'}"
"The results are reported in word accuracy rate based on the 10-fold cross validation, with the mean and standard deviation values.","{'title': '5 Evaluation', 'number': '5'}"
Table 3 shows word accuracy performance across a variety of methods.,"{'title': '5 Evaluation', 'number': '5'}"
We show results comparing the one-to-one aligner described in Section 2.1 and the one-to-one aligner provided by the PRONALSYL challenge.,"{'title': '5 Evaluation', 'number': '5'}"
"The PRONALSYS one-to-one alignments are taken directly from the PRONALSYL challenge, whose method is based on an EM algorithm.","{'title': '5 Evaluation', 'number': '5'}"
"For both alignments, we use instancebased learning as the prediction model.","{'title': '5 Evaluation', 'number': '5'}"
"Overall, our one-to-one alignments outperform the alignments provided by the data sets for all corpora.","{'title': '5 Evaluation', 'number': '5'}"
The main difference between the PRONALSYS one-to-one alignment and our one-to-one alignment is that our aligner does not allow a null letter on the grapheme side.,"{'title': '5 Evaluation', 'number': '5'}"
Consider the word abomination [ a b n m i n e f a n ]: the first six letters and phonemes are aligned the same way by both aligners (abomin- [ a b n m i n ]).,"{'title': '5 Evaluation', 'number': '5'}"
"However, the two aligners produce radically different alignments for the last five letters.","{'title': '5 Evaluation', 'number': '5'}"
"The alignment provided by the PRONALSYS one-to-one alignments is: e f a n Clearly, the latter alignment provides more information on how the graphemes map to the phonemes.","{'title': '5 Evaluation', 'number': '5'}"
Table 3 also shows that impressive improvements for all evaluated corpora are achieved by using many-to-many alignments rather than one-to-one alignments (1-1 align vs. M-M align).,"{'title': '5 Evaluation', 'number': '5'}"
"The significant improvements, ranging from 2.7% to 7.6% in word accuracy, illustrate the importance of having more precise alignments.","{'title': '5 Evaluation', 'number': '5'}"
"For example, we can now obtain the correct alignment for the second part of the word abomination: Instead of adding a null phoneme in the phoneme sequence, the many-to-many aligner maps the letter chunk ti to a single phoneme.","{'title': '5 Evaluation', 'number': '5'}"
"The HMM approach is based on the same hypothesis as the constraint satisfaction inference (CSInf) (Van Den Bosch and Canisius, 2006).","{'title': '5 Evaluation', 'number': '5'}"
"The results in Table 3 (1-1+CSInf vs. 1-1+HMM) show that the HMM approach consistently improves performance over the baseline system (1-1 align), while the CSInf degrades performance on the Brulex data set.","{'title': '5 Evaluation', 'number': '5'}"
"For the CSInf method, most errors are caused by trigram confusion in the prediction phase.","{'title': '5 Evaluation', 'number': '5'}"
"The results of our best system, which combines the HMM method with the many-to-many alignments (M-M+HMM), are better than the results reported in (Black et al., 1998) on both the CMUDict and German Celex data sets.","{'title': '5 Evaluation', 'number': '5'}"
"This is true even though Black et al. (1998) use explicit lists of letterphoneme mappings during the alignment process, while our approach is a fully automatic system that does not require any handcrafted list.","{'title': '5 Evaluation', 'number': '5'}"
We presented a novel technique of applying manyto-many alignments to the letter-to-phoneme conversion problem.,"{'title': '6 Conclusion and future work', 'number': '6'}"
The many-to-many alignments relax the constraint assumptions of the traditional one-toone alignments.,"{'title': '6 Conclusion and future work', 'number': '6'}"
Letter chunking bigram prediction incorporates many-to-many alignments into the conventional phoneme prediction models.,"{'title': '6 Conclusion and future work', 'number': '6'}"
"Finally, the HMM technique yields global phoneme predictions based on language models.","{'title': '6 Conclusion and future work', 'number': '6'}"
Impressive word accuracy improvements are achieved when the many-to-many alignments are applied over the baseline system.,"{'title': '6 Conclusion and future work', 'number': '6'}"
"On several languages and data sets, using the many-to-many alignments, word accuracy improvements ranged from 2.7% to 7.6%, as compared to one-to-one alignments.","{'title': '6 Conclusion and future work', 'number': '6'}"
The HMM cooperating with the local predictions shows slight improvements when it is applied to the manyto-many alignments.,"{'title': '6 Conclusion and future work', 'number': '6'}"
We illustrated that the HMM technique improves the word accuracy more consistently than the constraint-based approach.,"{'title': '6 Conclusion and future work', 'number': '6'}"
"Moreover, the HMM can be easily incorporated into the many-to-many alignment approach.","{'title': '6 Conclusion and future work', 'number': '6'}"
We are investigating the possibility of integrating syllabification information into our system.,"{'title': '6 Conclusion and future work', 'number': '6'}"
"It has been reported that syllabification can potentially improve pronunciation performance in English (Marchand and Damper, 2005).","{'title': '6 Conclusion and future work', 'number': '6'}"
"We plan to explore other sequence prediction approaches, such as discriminative training methods (Collins, 2004), and sequence tagging with Support Vector Machines (SVM-HMM) (Altun et al., 2003) to incorporate more features (context information) into the phoneme generation model.","{'title': '6 Conclusion and future work', 'number': '6'}"
We are also interested in applying our approach to other related areas such as morphology and transliteration.,"{'title': '6 Conclusion and future work', 'number': '6'}"
"We would like to thank Susan Bartlett, Colin Cherry, and other members of the Natural Language Processing research group at University of Alberta for their helpful comments and suggestions.","{'title': 'Acknowledgements', 'number': '7'}"
This research was supported by the Natural Sciences and Engineering Research Council of Canada.,"{'title': 'Acknowledgements', 'number': '7'}"
