col1,col2
We present a novel translation model on alignment template (TAT) which describes the alignment between a source parse tree and a target string.,{}
A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.,{}
"The model is linguistically syntaxbased because TATs are extracted automatically from word-aligned, source side parsed parallel texts.",{}
"To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.",{}
"Our experiments show that the TAT-based model significantly outperforms Pharaoh, a state-of-the-art decoder for phrase-based models.",{}
"Phrase-based translation models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004), which go beyond the original IBM translation models (Brown et al., 1993) 1 by modeling translations of phrases rather than individual words, have been suggested to be the state-of-theart in statistical machine translation by empirical evaluations.","{'title': '1 Introduction', 'number': '1'}"
"In phrase-based models, phrases are usually strings of adjacent words instead of syntactic constituents, excelling at capturing local reordering and performing translations that are localized to from that paper: a source string fJ1 = f1, ... ,fj, ... , fJ is to be translated into a target string eI1 = el, ... , ei, ... , eI.","{'title': '1 Introduction', 'number': '1'}"
"Here, I is the length of the target string, and J is the length of the source string. substrings that are common enough to be observed on training data.","{'title': '1 Introduction', 'number': '1'}"
"However, a key limitation of phrase-based models is that they fail to model reordering at the phrase level robustly.","{'title': '1 Introduction', 'number': '1'}"
"Typically, phrase reordering is modeled in terms of offset positions at the word level (Koehn, 2004; Och and Ney, 2004), making little or no direct use of syntactic information.","{'title': '1 Introduction', 'number': '1'}"
Recent research on statistical machine translation has lead to the development of syntax-based models.,"{'title': '1 Introduction', 'number': '1'}"
"Wu (1997) proposes Inversion Transduction Grammars, treating translation as a process of parallel parsing of the source and target language via a synchronized grammar.","{'title': '1 Introduction', 'number': '1'}"
Alshawi et al. (2000) represent each production in parallel dependency tree as a finite transducer.,"{'title': '1 Introduction', 'number': '1'}"
Melamed (2004) formalizes machine translation problem as synchronous parsing based on multitext grammars.,"{'title': '1 Introduction', 'number': '1'}"
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers.,"{'title': '1 Introduction', 'number': '1'}"
"Chiang (2005) presents a hierarchical phrasebased model that uses hierarchical phrase pairs, which are formally productions of a synchronous context-free grammar.","{'title': '1 Introduction', 'number': '1'}"
"Ding and Palmer (2005) propose a syntax-based translation model based on a probabilistic synchronous dependency insert grammar, a version of synchronous grammars defined on dependency trees.","{'title': '1 Introduction', 'number': '1'}"
"All these approaches, though different in formalism, make use of synchronous grammars or tree-based transduction rules to model both source and target languages.","{'title': '1 Introduction', 'number': '1'}"
"Another class of approaches make use of syntactic information in the target language alone, treating the translation problem as a parsing problem.","{'title': '1 Introduction', 'number': '1'}"
Yamada and Knight (2001) use a parser in the target language to train probabilities on a set of operations that transform a target parse tree into a source string.,"{'title': '1 Introduction', 'number': '1'}"
"Paying more attention to source language analysis, Quirk et al. (2005) employ a source language dependency parser, a target language word segmentation component, and an unsupervised word alignment component to learn treelet translations from parallel corpus.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we propose a statistical translation model based on tree-to-string alignment template which describes the alignment between a source parse tree and a target string.","{'title': '1 Introduction', 'number': '1'}"
A TAT is capable of generating both terminals and non-terminals and performing reordering at both low and high levels.,"{'title': '1 Introduction', 'number': '1'}"
"The model is linguistically syntax-based because TATs are extracted automatically from word-aligned, source side parsed parallel texts.","{'title': '1 Introduction', 'number': '1'}"
"To translate a source sentence, we first employ a parser to produce a source parse tree and then apply TATs to transform the tree into a target string.","{'title': '1 Introduction', 'number': '1'}"
"One advantage of our model is that TATs can be automatically acquired to capture linguistically motivated reordering at both low (word) and high (phrase, clause) levels.","{'title': '1 Introduction', 'number': '1'}"
"In addition, the training of TAT-based model is less computationally expensive than tree-to-tree models.","{'title': '1 Introduction', 'number': '1'}"
"Similarly to (Galley et al., 2004), the tree-to-string alignment templates discussed in this paper are actually transformation rules.","{'title': '1 Introduction', 'number': '1'}"
The major difference is that we model the syntax of the source language instead of the target side.,"{'title': '1 Introduction', 'number': '1'}"
"As a result, the task of our decoder is to find the best target string while Galley’s is to seek the most likely target tree.","{'title': '1 Introduction', 'number': '1'}"
"A tree-to-string alignment template z is a triple T, 5, A), which describes the alignment A between a source parse tree T = T(FJ' 1 ) 2 and a target string 5 = E�' is also composed of both terminals (target words) and non-terminals (placeholders).","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
An alignment A is defined as a subset of the Cartesian product of source and target symbol positions: 2We use T(·) to denote a parse tree.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"To reduce notational overhead, we use T(z) to represent the parse tree in z.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"Similarly, S(z) denotes the string in z.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"In the following, we formally describe how to introduce tree-to-string alignment templates into probabilistic dependencies to model Pr(ei|fJ1 ) 3.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"In a first step, we introduce the hidden variable T(fJ1 ) that denotes a parse tree of the source senNext, another hidden variable D is introduced to detach the source parse tree T (fJ1 ) into a sequence of K subtrees T 1K with a preorder transversal.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
We assume that each subtree Tk produces a target string 5k.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"As a result, the sequence of subtrees T�1K produces a sequence of target strings �5K1 , which can be combined serially to generate the target sentence ei.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"We assume that Pr(e,|D,T(fJ1 ),fJ1) = Pr(�5K1|T�1K) because ei is actually generated by the derivation of �5K1 .","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
Note that we omit an explicit dependence on the detachment D to avoid notational overhead.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
3The notational convention will be as follows.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
We use the symbol Pr(·) to denote general probability distribution with no specific assumptions.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"In contrast, for model-based probability distributions, we use generic symbol p(·).","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"To further decompose Pr(˜S |T˜), the tree-tostring alignment template, denoted by the variable z, is introduced as a hidden variable.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"⇒ X3 X4 of China ⇒ economic X4 of China ⇒ economic development of China Following Och and Ney (2002), we base our model on log-linear framework.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"Hence, all knowledge sources are described as feature functions that include the given source string fJ1 , the target string eI1, and hidden variables.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
The hidden variable T(fJ1 ) is omitted because we usually make use of only single best output of a parser.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"As we assume that all detachment have the same probability, the hidden variable D is also omitted.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"As a result, the model we actually adopt for experiments is limited because the parse, detachment, and TAT application sub-models are simplified.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"For our experiments we use the following seven feature functions 4 that are analogous to default feature set of Pharaoh (Koehn, 2004).","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"To simplify the notation, we omit the dependence on the hidden variables of the model.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"Therefore, the TAT-based translation model can be decomposed into four sub-models: Figure 2 shows how TATs work to perform translation.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"First, the input source sentence is parsed.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"Next, the parse tree is detached into five subtrees with a preorder transversal.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"For each subtree, a TAT is selected and applied to produce a string.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"Finally, these strings are combined serially to generate the translation (we use X to denote the non-terminal): 4When computing lexical weighting features (Koehn et al., 2003), we take only terminals into account.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"If there are no terminals, we set the feature value to 1.","{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
We use lex(·) to denote lexical weighting.,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
We denote the number of TATs used for decoding by K and the length of target string by L,"{'title': '2 Tree-to-String Alignment Template', 'number': '2'}"
"To extract tree-to-string alignment templates from a word-aligned, source side parsed sentence pair hT (fJ1 ), eI1, Ai, we need first identify TSAs (TreeString-Alignment) using similar criterion as suggested in (Och and Ney, 2004).","{'title': '3 Training', 'number': '3'}"
"A TSA is a triple Usually, we can extract a very large amount of TATs from training data using the above rules, making both training and decoding very slow.","{'title': '3 Training', 'number': '3'}"
"Therefore, we impose three restrictions to reduce the magnitude of extracted TATs: This constraint requires that both the first and last symbols in the target string must be aligned to some source symbols.","{'title': '3 Training', 'number': '3'}"
Table 1 shows the TATs extracted from the TSA in Figure 3 with h = 2 and c = 2.,"{'title': '3 Training', 'number': '3'}"
"As we restrict that T(fj2 j1 ) must be a subtree of T(fJ1 ), TATs may be treated as syntactic hierarchical phrase pairs (Chiang, 2005) with tree structure on the source side.","{'title': '3 Training', 'number': '3'}"
"At the same time, we face the risk of losing some useful non-syntactic phrase pairs.","{'title': '3 Training', 'number': '3'}"
"For example, the phrase pair +{' AOL ALA H President Bush made can never be obtained in form of TAT from the TSA in Figure 3 because there is no subtree for that source string.","{'title': '3 Training', 'number': '3'}"
We approach the decoding problem as a bottom-up beam search.,"{'title': '4 Decoding', 'number': '4'}"
"To translate a source sentence, we employ a parser to produce a parse tree.","{'title': '4 Decoding', 'number': '4'}"
"Moving bottomup through the source parse tree, we compute a list of candidate translations for the input subtree rooted at each node with a postorder transversal.","{'title': '4 Decoding', 'number': '4'}"
Candidate translations of subtrees are placed in stacks.,"{'title': '4 Decoding', 'number': '4'}"
Figure 4 shows the organization of candidate translation stacks.,"{'title': '4 Decoding', 'number': '4'}"
"A candidate translation contains the following information: A TAT z is usable to a parse tree T if and only if T(z) is rooted at the root of T and covers part of nodes of T. Given a parse tree T, we find all usable TATs.","{'title': '4 Decoding', 'number': '4'}"
"Given a usable TAT z, if T(z) is equal to T, then 5(z) is a candidate translation of T. If T(z) covers only a portion of T, we have to compute a list of candidate translations for T by replacing the non-terminals of 5(z) with candidate translations of the corresponding uncovered subtrees.","{'title': '4 Decoding', 'number': '4'}"
"For example, when computing the candidate translations for the tree rooted at node 8, the TAT used in Figure 5 covers only a portion of the parse tree in Figure 4.","{'title': '4 Decoding', 'number': '4'}"
There are two uncovered subtrees that are rooted at node 2 and node 7 respectively.,"{'title': '4 Decoding', 'number': '4'}"
"Hence, we replace the third symbol with the candidate translations in stack 2 and the first symbol with the candidate translations in stack 7.","{'title': '4 Decoding', 'number': '4'}"
"At the same time, the feature values and probabilities are also accumulated for the new candidate translations.","{'title': '4 Decoding', 'number': '4'}"
"To speed up the decoder, we limit the search space by reducing the number of TATs used for each input node.","{'title': '4 Decoding', 'number': '4'}"
"There are two ways to limit the TAT table size: by a fixed limit (tatTable-limit) of how many TATs are retrieved for each input node, and by a probability threshold (tatTable-threshold) that specify that the TAT probability has to be above some value.","{'title': '4 Decoding', 'number': '4'}"
"On the other hand, instead of keeping the full list of candidates for a given node, we keep a top-scoring subset of the candidates.","{'title': '4 Decoding', 'number': '4'}"
This can also be done by a fixed limit (stack-limit) or a threshold (stack-threshold).,"{'title': '4 Decoding', 'number': '4'}"
"To perform recombination, we combine candidate translations that share the same leading and trailing bigrams in each stack.","{'title': '4 Decoding', 'number': '4'}"
Our experiments were on Chinese-to-English translation.,"{'title': '5 Experiments', 'number': '5'}"
"The training corpus consists of 31,149 sentence pairs with 843,256 Chinese words and 949, 583 English words.","{'title': '5 Experiments', 'number': '5'}"
"For the language model, we used SRI Language Modeling Toolkit (Stolcke, 2002) to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the 31,149 English sentences.","{'title': '5 Experiments', 'number': '5'}"
"We selected 571 short sentences from the 2002 NIST MT Evaluation test set as our development corpus, and used the 2005 NIST MT Evaluation test set as our test corpus.","{'title': '5 Experiments', 'number': '5'}"
"We evaluated the translation quality using the BLEU metric (Papineni et al., 2002), as calculated by mteval-v11b.pl with its default setting except that we used case-sensitive matching of n-grams.","{'title': '5 Experiments', 'number': '5'}"
"The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), a freely available decoder for phrase-based translation models: We ran GIZA++ (Och and Ney, 2000) on the training corpus in both directions using its default setting, and then applied the refinement rule “diagand” described in (Koehn et al., 2003) to obtain a single many-to-many word alignment for each sentence pair.","{'title': '5 Experiments', 'number': '5'}"
"After that, we used some heuristics, which including rule-based translation of numbers, dates, and person names, to further improve the alignment accuracy.","{'title': '5 Experiments', 'number': '5'}"
"Given the word-aligned bilingual corpus, we obtained 1, 231, 959 bilingual phrases (221, 453 used on test corpus) using the training toolkits publicly released by Philipp Koehn with its default setting.","{'title': '5 Experiments', 'number': '5'}"
"To perform minimum error rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on development set, we used optimizeV5IBMBLEU.m (Venugopal and Vogel, 2005).","{'title': '5 Experiments', 'number': '5'}"
We used default pruning settings for Pharaoh except that we set the distortion limit to 4.,"{'title': '5 Experiments', 'number': '5'}"
"On the same word-aligned training data, it took us about one month to parse all the 31,149 Chinese sentences using a Chinese parser written by Deyi Xiong (Xiong et al., 2005).","{'title': '5 Experiments', 'number': '5'}"
The parser was trained on articles 1 − 270 of Penn Chinese Treebank version 1.0 and achieved 79.4% (F1 measure) as well as a 4.4% relative decrease in error rate.,"{'title': '5 Experiments', 'number': '5'}"
"Then, we performed TAT extraction described in section 3 with h = 3 and c = 5 and obtained 350,575 TATs (88,066 used on test corpus).","{'title': '5 Experiments', 'number': '5'}"
"To run our decoder Lynx on development and test corpus, we set tatTable-limit = 20, tatTable-threshold = 0, stack-limit = 100, and stack-threshold = 0.00001.","{'title': '5 Experiments', 'number': '5'}"
Table 2 shows the results on test set using Pharaoh and Lynx with different feature settings.,"{'title': '5 Experiments', 'number': '5'}"
"The 95% confidence intervals were computed using Zhang’s significance tester (Zhang et al., 2004).","{'title': '5 Experiments', 'number': '5'}"
We modified it to conform to NIST’s current definition of the BLEU brevity penalty.,"{'title': '5 Experiments', 'number': '5'}"
"For Pharaoh, eight features were used: distortion model d, a trigram language model lm, phrase translation probabilities O(f|e) and O(e|f), lexical weightings lex(f|e) and lex(e|f), phrase penalty pp, and word penalty wp.","{'title': '5 Experiments', 'number': '5'}"
"For Lynx, seven features described in section 2 were used.","{'title': '5 Experiments', 'number': '5'}"
We find that Lynx outperforms Pharaoh with all feature settings.,"{'title': '5 Experiments', 'number': '5'}"
"With full features, Lynx achieves an absolute improvement of 0.006 over Pharaoh (3.1% relative).","{'title': '5 Experiments', 'number': '5'}"
This difference is statistically significant (p < 0.01).,"{'title': '5 Experiments', 'number': '5'}"
"Note that Lynx made use of only 88,066 TATs on test corpus while 221, 453 bilingual phrases were used for Pharaoh.","{'title': '5 Experiments', 'number': '5'}"
The feature weights obtained by minimum error rate training for both Pharaoh and Lynx are shown in Table 3.,"{'title': '5 Experiments', 'number': '5'}"
We find that φ(f|e) (i.e. h2) is not a helpful feature for Lynx.,"{'title': '5 Experiments', 'number': '5'}"
The reason is that we use only a single non-terminal symbol instead of assigning phrasal categories to the target string.,"{'title': '5 Experiments', 'number': '5'}"
"In addition, we allow the target string consists of only non-terminals, making translation decisions not always based on lexical evidence.","{'title': '5 Experiments', 'number': '5'}"
It is interesting to use bilingual phrases to strengthen the TAT-based model.,"{'title': '5 Experiments', 'number': '5'}"
"As we mentioned before, some useful non-syntactic phrase pairs can never be obtained in form of TAT because we restrict that there must be a corresponding parse tree for the source phrase.","{'title': '5 Experiments', 'number': '5'}"
"Moreover, it takes more time to obtain TATs than bilingual phrases on the same training data because parsing is usually very time-consuming.","{'title': '5 Experiments', 'number': '5'}"
"Given an input subtree T (Fj� j� ), if Fj� j� is a string of terminals, we find all bilingual phrases that the source phrase is equal to Fj� j� .","{'title': '5 Experiments', 'number': '5'}"
"Then we build a TAT for each bilingual phrase (fJ, 1 , A): the tree of the TAT is T (Fj� j� ), the string is eI, 1 , and the alignment is A.","{'title': '5 Experiments', 'number': '5'}"
"If a TAT built from a bilingual phrase is the same with a TAT in the TAT table, we prefer to the greater translation probabilities.","{'title': '5 Experiments', 'number': '5'}"
Table 4 shows the effect of using bilingual phrases for Lynx.,"{'title': '5 Experiments', 'number': '5'}"
Note that these bilingual phrases are the same with those used for Pharaoh.,"{'title': '5 Experiments', 'number': '5'}"
We also conducted an experiment on large data to further examine our design philosophy.,"{'title': '5 Experiments', 'number': '5'}"
The training corpus contains 2.6 million sentence pairs.,"{'title': '5 Experiments', 'number': '5'}"
We used all the data to extract bilingual phrases and a portion of 800K pairs to obtain TATs.,"{'title': '5 Experiments', 'number': '5'}"
Two trigram language models were used for Lynx.,"{'title': '5 Experiments', 'number': '5'}"
One was trained on the 2.6 million English sentences and another was trained on the first 1/3 of the Xinhua portion of Gigaword corpus.,"{'title': '5 Experiments', 'number': '5'}"
"We also included rule-based translations of named entities, dates, and numbers.","{'title': '5 Experiments', 'number': '5'}"
"By making use of these data, Lynx achieves a BLEU score of 0.2830 on the 2005 NIST Chinese-to-English MT evaluation test set, which is a very promising result for linguistically syntax-based models.","{'title': '5 Experiments', 'number': '5'}"
"In this paper, we introduce tree-to-string alignment templates, which can be automatically learned from syntactically-annotated training data.","{'title': '6 Conclusion', 'number': '6'}"
The TAT-based translation model improves translation quality significantly compared with a stateof-the-art phrase-based decoder.,"{'title': '6 Conclusion', 'number': '6'}"
"Treated as special TATs without tree on the source side, bilingual phrases can be utilized for the TAT-based model to get further improvement.","{'title': '6 Conclusion', 'number': '6'}"
It should be emphasized that the restrictions we impose on TAT extraction limit the expressive power of TAT.,"{'title': '6 Conclusion', 'number': '6'}"
"Preliminary experiments reveal that removing these restrictions does improve translation quality, but leads to large memory requirements.","{'title': '6 Conclusion', 'number': '6'}"
We feel that both parsing and word alignment qualities have important effects on the TATbased model.,"{'title': '6 Conclusion', 'number': '6'}"
"We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al., 2005).","{'title': '6 Conclusion', 'number': '6'}"
This work is supported by National High Technology Research and Development Program contract “Generally Technical Research and Basic Database Establishment of Chinese Platform”(Subject No.,"{'title': 'Acknowledgement', 'number': '7'}"
2004AA114010).,"{'title': 'Acknowledgement', 'number': '7'}"
We are grateful to Deyi Xiong for providing the parser and Haitao Mi for making the parser more efficient and robust.,"{'title': 'Acknowledgement', 'number': '7'}"
Thanks to Dr. Yajuan Lv for many helpful comments on an earlier draft of this paper.,"{'title': 'Acknowledgement', 'number': '7'}"
