col1,col2
We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.,Introduction
"Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.",Introduction
"For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.",Introduction
"Over the past ten years, statistical machine translation has seen many exciting developments.",Experiment/Discussion
"Phrasebased systems (Och, 2002; Koehn et.al., 2003; Och and Ney, 2004) advanced the machine translation field by allowing translations of word sequences (a.k.a., phrases) instead of single words.",Experiment/Discussion
This approach has since been the state-of-the-art because of its robustness in modeling local word reordering and the existence of an efficient dynamic programming decoding algorithm.,Experiment/Discussion
"However, when phrase-based systems are used between languages with very different word orders, such as between subject-verb-object (SVO) and subject-object-verb (SOV) languages, long distance reordering becomes one of the key weaknesses.",Experiment/Discussion
Many reordering methods have been proposed in recent years to address this problem in different aspects.,Experiment/Discussion
The first class of approaches tries to explicitly model phrase reordering distances.,Experiment/Discussion
"Distance based distortion model (Och, 2002; Koehn et.al., 2003) is a simple way of modeling phrase level reordering.",Experiment/Discussion
It penalizes non-monotonicity by applying a weight to the number of words between two source phrases corresponding to two consecutive target phrases.,Experiment/Discussion
"Later on, this model was extended to lexicalized phrase reordering (Tillmann, 2004; Koehn, et.al., 2005; Al-Onaizan and Papineni, 2006) by applying different weights to different phrases.",Experiment/Discussion
"Most recently, a hierarchical phrase reordering model (Galley and Manning, 2008) was proposed to dynamically determine phrase boundaries using efficient shift-reduce parsing.",Experiment/Discussion
"Along this line of research, discriminative reordering models based on a maximum entropy classifier (Zens and Ney, 2006; Xiong, et.al., 2006) also showed improvements over the distance based distortion model.",Experiment/Discussion
"None of these reordering models changes the word alignment step in SMT systems, therefore, they can not recover from the word alignment errors.",Experiment/Discussion
These models are also limited by a maximum allowed reordering distance often used in decoding.,Experiment/Discussion
The second class of approaches puts syntactic analysis of the target language into both modeling and decoding.,Experiment/Discussion
"It has been shown that direct modeling of target language constituents movement in either constituency trees (Yamada and Knight, 2001; Galley et.al., 2006; Zollmann et.al., 2008) or dependency trees (Quirk, et.al., 2005) can result in significant improvements in translation quality for translating languages like Chinese and Arabic into English.",Experiment/Discussion
"A simpler alternative, the hierarchical phrase-based approach (Chiang, 2005; Wu, 1997) also showed promising results for translating Chinese to English.",Experiment/Discussion
"Similar to the distance based reordering models, the syntactical or hierarchical approaches also rely on other models to get word alignments.",Experiment/Discussion
"These models typically combine machine translation decoding with chart parsing, therefore significantly increase the decoding complexity.",Experiment/Discussion
"Even though some recent work has shown great improvements in decoding efficiency for syntactical and hierarchical approaches (Huang and Chiang, 2007), they are still not as efficient as phrase-based systems, especially when higher order language models are used.",Experiment/Discussion
"Finally, researchers have also tried to put source language syntax into reordering in machine translation.",Experiment/Discussion
"Syntactical analysis of source language can be used to deterministically reorder input sentences (Xia and McCord, 2004; Collins et.al., 2005; Wang et.al., 2007; Habash, 2007), or to provide multiple orderings as weighted options (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).",Experiment/Discussion
"In these approaches, input source sentences are reordered based on syntactic analysis and some reordering rules at preprocessing step.",Experiment/Discussion
The reordering rules can be either manually written or automatically extracted from data.,Experiment/Discussion
"Deterministic reordering based on syntactic analysis for the input sentences provides a good way of resolving long distance reordering, without introducing complexity to the decoding process.",Experiment/Discussion
"Therefore, it can be efficiently incorporated into phrase-based systems.",Experiment/Discussion
"Furthermore, when the same preprocessing reordering is performed for the training data, we can still apply other reordering approaches, such as distance based reordering and hierarchical phrase reordering, to capture additional local reordering phenomena that are not captured by the preprocessing reordering.",Experiment/Discussion
The work presented in this paper is largely motivated by the preprocessing reordering approaches.,Experiment/Discussion
"In the rest of the paper, we first introduce our dependency parser based reordering approach based on the analysis of the key issues when translating SVO languages to SOV languages.",Experiment/Discussion
"Then, we show experimental results of applying this approach to phrasebased SMT systems for translating from English to five SOV languages (Korean, Japanese, Hindi, Urdu and Turkish).",Experiment/Discussion
"After showing that this approach can also be beneficial for hierarchical phrase-based systems, we will conclude the paper with future research directions.",Experiment/Discussion
"In linguistics, it is possible to define a basic word order in terms of the verb (V) and its arguments, subject (S) and object (O).",Experiment/Discussion
"Among all six possible permutations, SVO and SOV are the most common.",Experiment/Discussion
"Therefore, translating between SVO and SOV languages is a very important area to study.",Experiment/Discussion
We use English as a representative of SVO languages and Korean as a representative for SOV languages in our discussion about the word orders.,Experiment/Discussion
"Figure 1 gives an example sentence in English and its corresponding translation in Korean, along with the alignments between the words.",Experiment/Discussion
"Assume that we split the sentences into four phrases: (John , @), (can hit , -IJ T  ), (the ball , oD) and (.",Experiment/Discussion
", .).",Experiment/Discussion
"Since a phrase-based decoder generates the translation from left to right, the following steps need to happen when we translate from English to Korean: It is clear that in order for the phrase-based decoder to successfully carry out all of the reordering steps, a very strong reordering model is required.",Experiment/Discussion
"When the sentence gets longer with more complex structure, the number of words to move over during decoding can be quite high.",Experiment/Discussion
Imagine when we translate the sentence “English is used as the first or second language in many countries around the world .”.,Experiment/Discussion
The decoder needs to make a jump of 13 words in order to put the translation of “is used” at the end of the translation.,Experiment/Discussion
"Normally in a phrase-based decoder, very long distance reordering is not allowed because of efficiency considerations.",Experiment/Discussion
"Therefore, it is very difficult in general to translate English into Korean with proper word order.",Experiment/Discussion
"However, knowing the dependency parse trees of the English sentences may simplify the reordering problem significantly.",Experiment/Discussion
"In the simple example in Figure 1, if we analyze the English sentence and know that “John” is the subject, “can hit” is the verb and “the ball” is the object, we can reorder the English into SOV order.",Experiment/Discussion
The resulting sentence “John the ball can hit .” will only need monotonic translation.,Experiment/Discussion
This motivates us to use a dependency parser for English to perform the reordering.,Experiment/Discussion
Figure 2 shows the dependency tree for the example sentence in the previous section.,Experiment/Discussion
"In this parse, the verb “hit” has four children: a subject noun “John”, an auxiliary verb “can”, an object noun “ball” and a punctuation “.”.",Experiment/Discussion
"When transforming the sentence to SOV order, we need to move the object noun and the subtree rooted at it to the front of the head verb, but after the subject noun.",Experiment/Discussion
We can have a simple rule to achieve this.,Experiment/Discussion
"However, in reality, there are many possible children for a verb.",Experiment/Discussion
These children have some relative ordering that is typically fixed for SOV languages.,Experiment/Discussion
"In order to describe this kind of ordering, we propose precedence reordering rules based on a dependency parse tree.",Experiment/Discussion
"All rules here are based English and Korean examples, but they also apply to other SOV languages, as we will show later empirically.",Experiment/Discussion
"A precedence reordering rule is a mapping from T to a set of tuples {(L, W, O)}, where T is the part-of-speech (POS) tag of the head in a dependency parse tree node, L is a dependency label for a child node, W is a weight indicating the order of that child node and O is the type of order (either NORMAL or REVERSE).",Experiment/Discussion
"The type of order is only used when we have multiple children with the same weight, while the weight is used to determine the relative order of the children, going from largest to smallest.",Experiment/Discussion
The weight can be any real valued number.,Experiment/Discussion
"The order type NORMAL means we preserve the original order of the children, while REVERSE means we flip the order.",Experiment/Discussion
"We reserve a special label self to refer to the head node itself so that we can apply a weight to the head, too.",Experiment/Discussion
We will call this tuple a precedence tuple in later discussions.,Experiment/Discussion
"In this study, we use manually created rules only.",Experiment/Discussion
"Suppose we have a precedence rule: VB → (nsubj, 2, NORMAL), (dobj, 1, NORMAL), (self, 0, NORMAL).",Experiment/Discussion
"For the example shown in Figure 2, we would apply it to the ROOT node and result in “John the ball can hit .”.",Experiment/Discussion
"Given a set of rules, we apply them in a dependency tree recursively starting from the root node.",Experiment/Discussion
"If the POS tag of a node matches the left-hand-side of a rule, the rule is applied and the order of the sentence is changed.",Experiment/Discussion
We go through all children of the node and get the precedence weights for them from the set of precedence tuples.,Experiment/Discussion
"If we encounter a child node that has a dependency label not listed in the set of tuples, we give it a default weight of 0 and default order type of NORMAL.",Experiment/Discussion
"The children nodes are sorted according to their weights from highest to lowest, and nodes with the same weights are ordered according to the type of order defined in the rule.",Experiment/Discussion
Verb movement is the most important movement when translating from English (SVO) to Korean (SOV).,Experiment/Discussion
"In a dependency parse tree, a verb node can potentially have many children.",Experiment/Discussion
"For example, auxiliary and passive auxiliary verbs are often grouped together with the main verb and moved together with it.",Experiment/Discussion
"The order, however, is reversed after the movement.",Experiment/Discussion
"In the example of Figure 2, the correct Korean word order is “�� (hit) T L}(can) .",Experiment/Discussion
Other categories that are in the same group are phrasal verb particle and negation.,Experiment/Discussion
"If the verb in an English sentence has a prepositional phrase as a child, the prepositional phrase is often placed before the direct object in the Korean counterpart.",Experiment/Discussion
"As shown in Figure 3, “��dl P, ” (“with a bat”) is actually between “ L” (“John”) and “� ��” (“the ball”).",Experiment/Discussion
Another common reordering phenomenon is when a verb has an adverbial clause modifier.,Experiment/Discussion
"In that case, the whole adverbial clause is moved together to be in front of the subject of the main sentence.",Experiment/Discussion
"Inside the adverbial clause, the ordering follows the same verb reordering rules, so we recursively reorder the clause.",Experiment/Discussion
"Our verb precedence rule, as in Table 1, can cover all of the above reordering phenomena.",Experiment/Discussion
"One way to interpret this rule set is as follows: for any node whose POS tag is matches VB* (VB, VBZ, VBD, VBP, VBN, VBG), we group the children node that are phrasal verb particle (prt), auxiliary verb (aux), passive auxiliary verb (auxpass), negation (neg) and the verb itself (self) together and reverse them.",Experiment/Discussion
This verb group is moved to the end of the sentence.,Experiment/Discussion
"We move adverbial clause modifier to the beginning of the sentence, followed by a group of noun subject (nsubj), preposition modifier and anything else not listed in the table, in their original order.",Experiment/Discussion
"Right before the verb group, we put the direct object (dobj).",Experiment/Discussion
Note that all of the children are optional.,Experiment/Discussion
"Similar to the verbs, adjectives can also take an auxiliary verb, a passive auxiliary verb and a negation as modifiers.",Experiment/Discussion
"In such cases, the change in order from English to Korean is similar to the verb rule, except that the head adjective itself should be in front of the verbs.",Experiment/Discussion
"Therefore, in our adjective precedence rule in the second panel of Table 1, we group the auxiliary verb, the passive auxiliary verb and the negation and move them together after reversing their order.",Experiment/Discussion
"They are moved to right after the head adjective, which is put after any other modifiers.",Experiment/Discussion
"For both verb and adjective precedence rules, we also apply some heuristics to prevent excessive movements.",Experiment/Discussion
"In order to do this, we disallow any movement across punctuation and conjunctions.",Experiment/Discussion
"Therefore, for sentences like “John hit the ball but Sam threw the ball”, the reordering result would be “John the ball hit but Sam the ball threw”, instead of “John the ball but Sam the ball threw hit”.",Experiment/Discussion
"In Korean, when a noun is modified by a prepositional phrase, such as in “the way to happiness”, the prepositional phrase is usually moved in front of the noun, resulting in “q (happiness) ° 71at= 7J (to the way)” .",Experiment/Discussion
"Similarly for relative clause modifier, it is also reordered to the front of the head noun.",Experiment/Discussion
"For preposition head node with an object modifier, € %1901 9 ZL tig N 4 °1844 . the order is the object first and the preposition last.",Experiment/Discussion
One example is “with a bat” in Figure 3.,Experiment/Discussion
It corresponds to “4ol-01 (a bat) (with)”.,Experiment/Discussion
We handle these types of reordering by the noun and preposition precedence rules in the third and fourth panel of Table 1.,Experiment/Discussion
"With the rules defined in Table 1, we now show a more complex example in Figure 4.",Experiment/Discussion
"First, the ROOT node matches an adjective rule, with four children nodes labeled as (csubj, cop, advcl, p), and with precedence weights of (0, -2, 1, 0).",Experiment/Discussion
The ROOT node itself has a weight of -1.,Experiment/Discussion
"After reordering, the sentence becomes: “because we do n’t know what the future has Living exciting is .”.",Experiment/Discussion
Note that the whole adverbial phrase rooted at “know” is moved to the beginning of the sentence.,Experiment/Discussion
"After that, we see that the child node rooted at “know” matches a verb rule, with five children nodes labeled as (mark, nsubj, aux, neg, ccomp), with weights (0, 0, -2, -2, 0).",Experiment/Discussion
"In this case, the verb itself also has weight -2.",Experiment/Discussion
"Now we have two groups of nodes, with weight 0 and -2, respectively.",Experiment/Discussion
The first group has a NORMAL order and the second group has a REVERSE order.,Experiment/Discussion
"After reordering, the sentence becomes: “because we what the future has know n’t do Living exciting is .”.",Experiment/Discussion
"Finally, we have another node rooted at “has” that matches the verb rule again.",Experiment/Discussion
"After the final reordering, we end up with the sentence: “because we the future what has know n’t do Living exciting is .”.",Experiment/Discussion
We can see in Figure 4 that this sentence has an almost monotonic alignment with a reasonable Korean translation shown in the figure1.,Experiment/Discussion
"As we mentioned in our introduction, there have been several studies in applying source sentence reordering using syntactical analysis for statistical machine translation.",Experiment/Discussion
"Our precedence reordering approach based on a dependency parser is motivated by those previous works, but we also distinguish from their studies in various ways.",Experiment/Discussion
"Several approaches use syntactical analysis to provide multiple source sentence reordering options through word lattices (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).",Experiment/Discussion
"A key difference between 1We could have improved the rules by using a weight of -3 for the label “mark”, but it was not in our original set of rules. their approaches and ours is that they do not perform reordering during training.",Experiment/Discussion
"Therefore, they would need to rely on reorder units that are likely not violating “phrase” boundaries.",Experiment/Discussion
"However, since we reorder both training and test data, our system operates in a matched condition.",Experiment/Discussion
"They also focus on either Chinese to English (Zhang et.al., 2007; Li et.al., 2007) or English to Danish (Elming, 2008), which arguably have less long distance reordering than between English and SOV languages.",Experiment/Discussion
"Studies most similar to ours are those preprocessing reordering approaches (Xia and McCord, 2004; Collins et.al., 2005; Wang et.al., 2007; Habash, 2007).",Experiment/Discussion
"They all perform reordering during preprocessing based on either automatically extracted syntactic rules (Xia and McCord, 2004; Habash, 2007) or manually written rules (Collins et.al., 2005; Wang et.al., 2007).",Experiment/Discussion
"Compared to these approaches, our work has a few differences.",Experiment/Discussion
"First of all, we study a wide range of SOV languages using manually extracted precedence rules, not just for one language like in these studies.",Experiment/Discussion
"Second, as we will show in the next section, we compare our approach to a very strong baseline with more advanced distance based reordering model, not just the simplest distortion model.",Experiment/Discussion
"Third, our precedence reordering rules, like those in Habash, 2007, are more flexible than those other rules.",Experiment/Discussion
"Using just one verb rule, we can perform the reordering of subject, object, preposition modifier, auxiliary verb, negation and the head verb.",Experiment/Discussion
"Although we use manually written rules in this study, it is possible to learn our rules automatically from alignments, similarly to Habash, 2007.",Experiment/Discussion
"However, unlike Habash, 2007, our manually written rules handle unseen children and their order naturally because we have a default precedence weight and order type, and we do not need to match an often too specific condition, but rather just treat all children independently.",Experiment/Discussion
"Therefore, we do not need to use any backoff scheme in order to have a broad coverage.",Experiment/Discussion
"Fourth, we use dependency parse trees rather than constituency trees.",Experiment/Discussion
"There has been some work on syntactic word order model for English to Japanese machine translation (Chang and Toutanova, 2007).",Experiment/Discussion
"In this work, a global word order model is proposed based on features including word bigram of the target sentence, displacements and POS tags on both source and target sides.",Experiment/Discussion
They build a log-linear model using these features and apply the model to re-rank N-best lists from a baseline decoder.,Experiment/Discussion
"Although we also study the reordering problem in English to Japanese translation, our approach is to incorporate the linguistically motivated reordering directly into modeling and decoding.",Experiment/Discussion
We carried out all our experiments based on a stateof-the-art phrase-based statistical machine translation system.,Results/Conclusion
"When training a system for English to any of the 5 SOV languages, the word alignment step includes 3 iterations of IBM Model-1 training and 2 iterations of HMM training.",Results/Conclusion
We do not use Model-4 because it is slow and it does not add much value to our systems in a pilot study.,Results/Conclusion
"We use the standard phrase extraction algorithm (Koehn et.al., 2003) to get all phrases up to length 5.",Results/Conclusion
"In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding.",Results/Conclusion
"In this model, we use 4 reordering classes (+1, > 1, −1, < −1) and words from both source and target as features.",Results/Conclusion
"For source words, we use the current aligned word, the word before the current aligned word and the next aligned word; for target words, we use the previous two words in the immediate history.",Results/Conclusion
"Using this type of features makes it possible to directly use the maximum entropy model in the decoding process (Zens and Ney, 2006).",Results/Conclusion
"The maximum entropy models are trained on all events extracted from training data word alignments using the LBFGS algorithm (Malouf, 2002).",Results/Conclusion
"Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003), with an implementation based on the lattice MERT (Macherey et.al., 2008).",Results/Conclusion
"For parallel training data, we use an in-house collection of parallel documents.",Results/Conclusion
They come from various sources with a substantial portion coming from the web after using simple heuristics to identify potential document pairs.,Results/Conclusion
"Therefore, for some documents in the training data, we do not necessarily have the exact clean translations.",Results/Conclusion
Table 2 shows the actual statistics about the training data for all five languages we study.,Results/Conclusion
"For all 5 SOV languages, we use the target side of the parallel data and some more monolingual text from crawling the web to build 4gram language models.",Results/Conclusion
We also collected about 10K English sentences from the web randomly.,Results/Conclusion
"Among them, 9.5K are used as evaluation data.",Results/Conclusion
Those sentences were translated by humans to all 5 SOV languages studied in this paper.,Results/Conclusion
Each sentence has only one reference translation.,Results/Conclusion
"We split them into 3 subsets: dev contains 3,500 sentences, test contains 1,000 sentences and the rest of 5,000 sentences are used in a blindtest set.",Results/Conclusion
"The dev set is used to perform MERT training, while the test set is used to select trained weights due to some nondeterminism of MERT training.",Results/Conclusion
"We use IBM BLEU (Papineni et al., 2002) to evaluate our translations and use character level BLEU for Korean and Japanese.",Results/Conclusion
We first compare our precedence rules based preprocessing reordering with the maximum entropy based lexicalized reordering model.,Results/Conclusion
"In Table 3, Baseline is our system with both a distance distortion model and the maximum entropy based lexicalized reordering model.",Results/Conclusion
"For all results reported in this section, we used a maximum allowed reordering distance of 10.",Results/Conclusion
"In order to see how the lexicalized reordering model performs, we also included systems with and without it (-LR means without it).",Results/Conclusion
PR is our proposed approach in this paper.,Results/Conclusion
"Note that since we apply precedence reordering rules during preprocessing, we can combine this approach with any other reordering models used during decoding.",Results/Conclusion
"The only difference is that with the precedence reordering, we would have a different phrase table and in the case of LR, different maximum entropy models.",Results/Conclusion
"In order to implement the precedence rules, we need a dependency parser.",Results/Conclusion
"We choose to use a deterministic inductive dependency parser (Nivre and Scholz, 2004) for its efficiency and good accuracy.",Results/Conclusion
Our implementation of the deterministic dependency parser using maximum entropy models as the underlying classifiers achieves 87.8% labeled attachment score and 88.8% unlabeled attachment score on standard Penn Treebank evaluation.,Results/Conclusion
"As our results in Table 3 show, for all 5 languages, by using the precedence reordering rules as described in Table 1, we achieve significantly better BLEU scores compared to the baseline system.",Results/Conclusion
"In the table, We use two stars (**) to mean that the statistical significance test using the bootstrap method (Koehn, 2004) gives an above 95% significance level when compared to the baselie.",Results/Conclusion
We measured the statistical significance level only for the blindtest data.,Results/Conclusion
"Note that for Korean and Japanese, our precedence reordering rules achieve better absolute BLEU score improvements than for Hindi, Urdu and Turkish.",Results/Conclusion
"Since we only analyzed English and Korean sentences, it is possible that our rules are more geared toward Korean.",Results/Conclusion
"Japanese has almost exactly the same word order as Korean, so we could assume the benefits can carry over to Japanese.",Results/Conclusion
One of our motivations of using the precedence reordering rules is that English will look like SOV languages in word order after reordering.,Results/Conclusion
"Therefore, even monotone decoding should be able to produce better translations.",Results/Conclusion
"To see this, we carried out a controlled experiment, using Korean as an example.",Results/Conclusion
"Clearly, after applying the precedence reordering rules, our English to Korean system is not sensitive to the maximum allowed reordering distance anymore.",Results/Conclusion
"As shown in Figure 5, without the rules, the blindtest BLEU scores improve monotonically as the allowed reordering distance increases.",Results/Conclusion
This indicates that the order difference between English and Korean is very significant.,Results/Conclusion
"Since smaller allowed reordering distance directly corresponds to decoding time, we can see that with the same decoding speed, our proposed approach can achieve almost 5% BLEU score improvements on blindtest set.",Results/Conclusion
"The hierarchical phrase-based approach has been successfully applied to several systems (Chiang, 2005; Zollmann et.al., 2008).",Results/Conclusion
"Since hierarchical phrase-based systems can capture long distance reordering by using a PSCFG model, we expect it to perform well in English to SOV language systems.",Results/Conclusion
We use the same training data as described in the previous sections for building hierarchical systems.,Results/Conclusion
The same 4-gram language models are also used for the 5 SOV languages.,Results/Conclusion
"We adopt the SAMT package (Zollmann and Venugopal, 2006) and follow similar settings as Zollmann et.al., 2008.",Results/Conclusion
"We allow each rule to have at most 6 items on the source side, including nonterminals and extract rules from initial phrases of maximum length 12.",Results/Conclusion
"During decoding, we allow application of all rules of the grammar for chart items spanning up to 12 source words.",Results/Conclusion
"Since our precedence reordering applies at preprocessing step, we can train a hierarchical system after applying the reordering rules.",Results/Conclusion
"When doing so, we use exactly the same settings as a regular hierarchical system.",Results/Conclusion
"The results for both hierarchical systems and those combined with the precedence reordering are shown in Table 4, together with the best normal phrase-based systems we copy from Table 3.",Results/Conclusion
"Here again, we mark any blindtest BLEU score that is better than the corresponding hierarchical system with confidence level above 95%.",Results/Conclusion
Note that the hierarchical systems can not use the maximum entropy based lexicalized phrase reordering models.,Results/Conclusion
"Except for Hindi, applying the precedence reordering rules in a hierarchical system can achieve statistically significant improvements over a normal hierarchical system.",Results/Conclusion
We conjecture that this may be because of the simplicity of our reordering rules.,Results/Conclusion
"Other than the reordering phenomena covered by our rules in Table 1, there could be still some local or long distance reordering.",Results/Conclusion
"Therefore, using a hierarchical phrase-based system can improve those cases.",Results/Conclusion
"Another possible reason is that after the reordering rules apply in preprocessing, English sentences in the training data are very close to the SOV order.",Results/Conclusion
"As a result, EM training becomes much easier and word alignment quality becomes better.",Results/Conclusion
"Therefore, a hierarchical phrase-based system can extract better rules and hence achievesbetter translation quality.",Results/Conclusion
We also point out that hierarchical phrase-based systems require a chart parsing algorithm during decoding.,Results/Conclusion
"Compared to the efficient dynamic programming in phrase-based systems, it is much slower.",Results/Conclusion
This makes our approach more appealing in a realtime statistical machine translation system.,Results/Conclusion
"In this paper, we present a novel precedence reordering approach based on a dependency parser.",Results/Conclusion
"We successfully applied this approach to systems translating English to 5 SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.",Results/Conclusion
"For all 5 languages, we achieve statistically significant improvements in BLEU scores over a state-of-the-art phrasebased baseline system.",Results/Conclusion
"The amount of training data for the 5 languages varies from around 17M to more than 350M words, including some noisy data from the web.",Results/Conclusion
Our proposed approach has shown to be robust and versatile.,Results/Conclusion
"For 4 out of the 5 languages, our approach can even significantly improve over a hierarchical phrase-based baseline system.",Results/Conclusion
"As far as we know, we are the first to show that such reordering rules benefit several SOV languages.",Results/Conclusion
We believe our rules are flexible and can cover many linguistic reordering phenomena.,Results/Conclusion
The format of our rules also makes it possible to automatically extract rules from word aligned corpora.,Results/Conclusion
"In the future, we plan to investigate along this direction and extend the rules to languages other than SOV.",Results/Conclusion
The preprocessing reordering like ours is known to be sensitive to parser errors.,Results/Conclusion
Some preliminary error analysis already show that indeed some sentences suffer from parser errors.,Results/Conclusion
"In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).",Results/Conclusion
"Although there is clearly room for improvements, we also feel that using one reordering during training may not be good enough either.",Results/Conclusion
It would be very interesting to investigate ways to have efficient procedure for training EM models and getting word alignments using word lattices on the source side of the parallel data.,Results/Conclusion
"Along this line of research, we think some kind of tree-to-string model (Liu et.al., 2006) could be interesting directions to pursue.",Results/Conclusion
