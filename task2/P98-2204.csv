col1,col2
I propose a model for determining the hearer's attentional state which depends solely on a list of salient discourse entities (S-list).,{}
The ordering among the elements of the S-list covers also the of the center the centering model.,{}
The ranking criteria for the S-list based on the distinction between entities and incorporate preferences for interand intra-sentential anaphora.,{}
"The model is the basis for an algorithm which operates incrementally, word by word.",{}
I propose a model for determining the hearer's attentional state in understanding discourse.,"{'title': '1 Introduction', 'number': '1'}"
"My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn's (1996) approach for the ranking of the forward-looking center list for German.","{'title': '1 Introduction', 'number': '1'}"
Their approach has been proven as the point of departure for a new model which is valid for English as well.,"{'title': '1 Introduction', 'number': '1'}"
The use of the centering transitions in Brennan et al. 's (1987) algorithm prevents it from being applied incrementally (cf.,"{'title': '1 Introduction', 'number': '1'}"
Kehler (1997)).,"{'title': '1 Introduction', 'number': '1'}"
"In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list).","{'title': '1 Introduction', 'number': '1'}"
"The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube & Hahn's (1996) approach.","{'title': '1 Introduction', 'number': '1'}"
"Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new).","{'title': '1 Introduction', 'number': '1'}"
The S-list is not a local data structure associated with individual utterances.,"{'title': '1 Introduction', 'number': '1'}"
The S-list rather describes the attentional state of the hearer at any given point in processing a discourse.,"{'title': '1 Introduction', 'number': '1'}"
"The S-list is generated incrementally, word by word, and used immediately.","{'title': '1 Introduction', 'number': '1'}"
"Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further specifications for processing complex sentences unnecessary.","{'title': '1 Introduction', 'number': '1'}"
Section 2 describes the centering model as the relevant background for my proposal.,"{'title': '1 Introduction', 'number': '1'}"
"In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm.","{'title': '1 Introduction', 'number': '1'}"
"In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998).","{'title': '1 Introduction', 'number': '1'}"
"The centering model describes the relation between the focus of attention, the choices of referring expressions, and the perceived coherence of discourse.","{'title': '2 A Look Back: Centering', 'number': '2'}"
"The model has been motivated with evidence from preferences for the antecedents of pronouns (Grosz et al., 1983; 1995) and has been applied to pronoun resolution (Brennan et al. (1987), inter alia, whose interpretation differs from the original model).","{'title': '2 A Look Back: Centering', 'number': '2'}"
"The centering model itself consists of two constructs, the backward-looking center and the list of forward-looking centers, and a few rules and constraints.","{'title': '2 A Look Back: Centering', 'number': '2'}"
"Each utterance Ui is assigned a list of forward-looking centers, Cf (Ui), and a unique backward-looking center, Cb(Ui).","{'title': '2 A Look Back: Centering', 'number': '2'}"
A ranking imposed on the elements of the Cf reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui.4.1).,"{'title': '2 A Look Back: Centering', 'number': '2'}"
"The most highly ranked element of C f (Ui) that is realized in U2-F1 (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+i).","{'title': '2 A Look Back: Centering', 'number': '2'}"
"Therefore, the ranking on the Cf plays a crucial role in the model.","{'title': '2 A Look Back: Centering', 'number': '2'}"
Grosz et al. (1995) and Brennan et al.,"{'title': '2 A Look Back: Centering', 'number': '2'}"
"(1987) use grammatical relations to rank the Cf (i.e., subj obj ...) but state that other factors might also play a role.","{'title': '2 A Look Back: Centering', 'number': '2'}"
"For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf.","{'title': '2 A Look Back: Centering', 'number': '2'}"
Table 1 taken from Walker et al. (1994)).,"{'title': '2 A Look Back: Centering', 'number': '2'}"
preferred to RETAIN is preferred to SMOOTHSHIFT is preferred to ROUGH-SHIFT.,"{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
The BFP-algorithm (cf.,"{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"Walker et al. (1994)) consists of three basic steps: To illustrate this algorithm, we consider example (1) (Brennan et al., 1987) which has two different final utterances (1d) and (1 d').","{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"Utterance (1d) contains one pronoun, utterance (id') two pronouns.","{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
We look at the interpretation of (1d) and (1 d').,"{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"After step 2, the algorithm has produced two readings for each variant which are rated by the corresponding transitions in step 3.","{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"In (1d), the pronoun &quot;she&quot; is resolved to &quot;her&quot; (= Brennan) because the CONTINUE transition is ranked higher than SMOOTHSHIFT in the second reading.","{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"In (id'), the pronoun &quot;she&quot; is resolved to &quot;Friedman&quot; because SMOOTHSHIFT is preferred over ROUGH-SHIFT.","{'title': 'CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT', 'number': '3'}"
"The realization and the structure of my model departs significantly from the centering model: In contrast to the centering model, my model does not need a construct which looks back; it does not need transitions and transition ranking criteria.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"Instead of using the Cb to account for local coherence, in my model this is achieved by comparing the first element of the S-list with the preceding state.","{'title': '3 An Alternative to Centering', 'number': '4'}"
Strube & Hahn (1996) rank the Cf according to the information status of discourse entities.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
I here generalize these ranking criteria by redefining them in Prince's (1981; 1992) terms.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"I distinguish between three different sets of expressions, hearer-old discourse entities (OLD), mediated discourse entities (MED), and hearer-new discourse entities (NEW).","{'title': '3 An Alternative to Centering', 'number': '4'}"
"These sets consist of the elements of Prince's familiarity scale (Prince, 1981, p.245).","{'title': '3 An Alternative to Centering', 'number': '4'}"
OLD consists of evoked (E) and unused (U) discourse entities while NEW consists of brand-new (BN) discourse entities.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"MED consists of inferrables (I), containing inferrables (IC) and anchored brand-new (BNA) discourse entities.","{'title': '3 An Alternative to Centering', 'number': '4'}"
These discourse entities are discourse-new but mediated by some hearer-old discourse entity (cf.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Figure 1).,"{'title': '3 An Alternative to Centering', 'number': '4'}"
I do not assume any difference between the elements of each set with respect to their information status.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"E.g., evoked and unused discourse entities have the same information status because both belong to OLD.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"For an operationalization of Prince's terms, I stipulate that evoked discourse entitites are co-referring expressions (pronominal and nominal anaphora, previously mentioned proper names, relative pronouns, appositives).","{'title': '3 An Alternative to Centering', 'number': '4'}"
Unused discourse entities are proper names and titles.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"In texts, brand-new proper names are usually accompanied by a relative clause or an appositive which relates them to the hearer's knowledge.","{'title': '3 An Alternative to Centering', 'number': '4'}"
The corresponding discourse entity is evoked only after this elaboration.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"Whenever these linguistic devices are missing, proper names are treated as unusedl .","{'title': '3 An Alternative to Centering', 'number': '4'}"
I restrict inferrables to the particular subset defined by Hahn et al. (1996).,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Anchored brand-new discourse entities require that the anchor is either evoked or unused.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
I assume the following conventions for the ranking constraints on the elements of the S-list.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"The 3-tuple (x, uttx, pos z) denotes a discourse entity x which is evoked in utterance uttx at the text position posx.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"With respect to any two discourse entities (x, uttx , pas x) and (y, utty, posy), uttx and utty specifying the current utterance Ui or the preceding utterance U2_1, I set up the following ordering constraints on elements in the S-list (Table 2)2.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"For any state of the processor/hearer, the ordering of discourse entities in the S-list that can be derived from the ordering constraints (1) to (3) is denoted by the precedence relation (I) If x E OLD and y E MED, then x y.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"If x E OLD and y E NEW, then x y.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"If x E MED and y E NEW, then x y.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"(2) If x, y E OLD, or x, y E MED, or x, y E NEW, then if utt. utty, then x y, if utt.","{'title': '3 An Alternative to Centering', 'number': '4'}"
= utty and pos.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"< posy, then x y. Summarizing Table 2, I state the following preference ranking for discourse entities in Ui and U2-1: hearer-old discourse entities in U, hearer-old discourse entities in Ui_1, mediated discourse entities in Ui, mediated discourse entities in Ui_i, hearernew discourse entities in U2, hearer-new discourse entities in Ui_1.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"By making the distinction in (2) 'For examples of brand-new proper names and their introduction cf., e.g., the &quot;obituaries&quot; section of the New York Times.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"2The relations >- and = indicate that the utterance containing x follows (>-) the utterance containing y or that x and y are elements of the same utterance (=). between discourse entities in Ui and discourse entities in U2_1, I am able to deal with intra-sentential anaphora.","{'title': '3 An Alternative to Centering', 'number': '4'}"
There is no need for further specifications for complex sentences.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
A finer grained ordering is achieved by ranking discourse entities within each of the sets according to their text position.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Anaphora resolution is performed with a simple look-up in the S-list3.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The elements of the S-list are tested in the given order until one test succeeds.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"Just after an anaphoric expression is resolved, the S-list is updated.","{'title': '3 An Alternative to Centering', 'number': '4'}"
The algorithm processes a text from left to right (the unit of processing is the word): 2.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"If the analysis of utterance U5 is finished, remove all discourse entities from the S-list, which are not realized in U.","{'title': '3 An Alternative to Centering', 'number': '4'}"
The analysis for example (1) is given in Table 36.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
I show only these steps which are of interest for the computation of the S-list and the pronoun resolution.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The preferences for pronouns (in bold font) are given by the S-list immediately above them.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The pronoun &quot;she&quot; in (lb) is resolved to the first element of the S-list.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"When the pronoun &quot;her&quot; in (lc) is encountered, FRIEDMAN is the first element of the S-list since FRIEDMAN is unused and in the current utterance.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"Because of binding restrictions, &quot;her&quot; cannot be resolved to FRIEDMAN but to the second element, BRENNAN.","{'title': '3 An Alternative to Centering', 'number': '4'}"
In both (1d) and (id') the pronoun &quot;she&quot; is resolved to FRIEDMAN.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"3The S-list consists of referring expressions which are specified for text position, agreement, sortal information, and information status.","{'title': '3 An Alternative to Centering', 'number': '4'}"
Coordinated NPs are collected in a set.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"The Slist does not contain predicative NPs, pleonastic &quot;it&quot;, and any elements of direct speech enclosed in double quotes.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"The difference between my algorithm and the BFP-algorithm becomes clearer when the unused discourse entity &quot;Friedman&quot; is replaced by a brandnew discourse entity, e.g., &quot;a professional driver&quot;7 (cf. example (2)).","{'title': '3 An Alternative to Centering', 'number': '4'}"
"In the BFP-algorithm, the ranking of the Cf-list depends on grammatical roles.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"Hence, DRIVER is ranked higher than BRENNAN in the Cft2c).","{'title': '3 An Alternative to Centering', 'number': '4'}"
"In (2d), the pronoun &quot;she&quot; is resolved to BRENNAN because of the preference for CONTINUE over RETAIN.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"In (2d'), &quot;she&quot; is resolved to DRIVER because SMOOTH-SHIFT is preferred over ROUGH-SHIFT.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"In my algorithm, at the end of (2c) the evoked phrase &quot;her&quot; is ranked higher than the brand-new phrase &quot;a professional driver&quot; (cf.","{'title': '3 An Alternative to Centering', 'number': '4'}"
Table 4).,"{'title': '3 An Alternative to Centering', 'number': '4'}"
In both (2d) and (2d') the pronoun &quot;she&quot; is resolved to BRENNAN.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Example (3)8 illustrates how the preferences for intra- and inter-sentential anaphora interact with the information status of discourse entitites (Table 5).,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Sentence (3a) starts a new discourse segment.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The phrase &quot;a judge&quot; is brand-new.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"&quot;Mr Curtis&quot; is mentioned several times before in the text, Hence, 71 owe this variant Andrew Kehler.","{'title': '3 An Alternative to Centering', 'number': '4'}"
—This example can misdirect readers because the phrase &quot;a professional driver&quot; is assigned the &quot;default&quot; gender masculine.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"Anyway, this example — like the original example — seems not to be felicitous English and has only illustrative character.","{'title': '3 An Alternative to Centering', 'number': '4'}"
81n: The New York Tunes.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"Dec. 7, 1997, p.A48 (&quot;Shot in head, suspect goes free, then to college&quot;). the discourse entity CURTIS is evoked and ranked higher than the discourse entity JUDGE.","{'title': '3 An Alternative to Centering', 'number': '4'}"
"In the next step, the ellipsis refers to JUDGE which is evoked then.","{'title': '3 An Alternative to Centering', 'number': '4'}"
The nouns &quot;request&quot; and &quot;prosecutors&quot; are brand-new9.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The pronoun &quot;he&quot; and the possessive pronoun &quot;his&quot; are resolved to CURTIS.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
&quot;Condition&quot; is brand-new but anchored by the possessive pronoun.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
For (3b) and (3c) I show only the steps immediately before the pronouns are resolved.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
In (3b) both &quot;Mr Curtis&quot; and &quot;the judge&quot; are evoked.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"However, &quot;Mr Curtis&quot; is the left-most evoked phrase in this sentence and therefore the most preferred antecedent for the pronoun &quot;him&quot;.","{'title': '3 An Alternative to Centering', 'number': '4'}"
For my experiments I restricted the length of the S-list to five elements.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Therefore &quot;prosecutors&quot; in (3b) is not contained in the S-list.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
The discourse entity SMIRGA is introduced in (3c).,"{'title': '3 An Alternative to Centering', 'number': '4'}"
It becomes evoked after the appositive.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
Hence SM1RGA is the most preferred antecedent for the pronoun &quot;he&quot;.,"{'title': '3 An Alternative to Centering', 'number': '4'}"
"In the first experiment, I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama (1998).","{'title': '4 Some Empirical Data', 'number': '5'}"
Method.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"I use the following guidelines for the hand-simulated analysis (Walker, 1989).","{'title': '4 Some Empirical Data', 'number': '5'}"
I do not assume any world knowledge as part of the anaphora resolution process.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Only agreement criteria, binding and sortal constraints are applied.","{'title': '4 Some Empirical Data', 'number': '5'}"
I do not account for false positives and error chains.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features.","{'title': '4 Some Empirical Data', 'number': '5'}"
"At the beginning of a segment, anaphora resolution is preferentially performed within the same utterance.","{'title': '4 Some Empirical Data', 'number': '5'}"
My algorithm starts with an empty S-list at the beginning of a segment.,"{'title': '4 Some Empirical Data', 'number': '5'}"
The basic unit for which the centering data structures are generated is the utterance U.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"For the BFPalgorithm, I define U as a simple sentence, a complex sentence, or each full clause of a compound sentence.","{'title': '4 Some Empirical Data', 'number': '5'}"
Kameyama's (1998) intra-sentential centering operates at the clause level.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"While tensed clauses are defined as utterances on their own, untensed clauses are processed with the main clause, so that the Cf-list of the main clause contains the elements of the untensed embedded clause.","{'title': '4 Some Empirical Data', 'number': '5'}"
Kameyama distinguishes for tensed clauses further between sequential and hierarchical centering.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Except for reported speech (embedded and inaccessible to the superordinate level), non-report complements, and relative clauses (both embedded but accessible to the superordinate level; less salient than the higher levels), all other types of tensed clauses build a chain of utterances on the same level.","{'title': '4 Some Empirical Data', 'number': '5'}"
"According to the preference for inter-sentential candidates in the centering model, I define the following anaphora resolution strategy for the BEPalgorithm: (1) Test elements of Uj_1.","{'title': '4 Some Empirical Data', 'number': '5'}"
(2) Test elements of Ui left-to-right.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"(3) Test elements of Cf (U2_2), Cf (U_3), ...","{'title': '4 Some Empirical Data', 'number': '5'}"
In my algorithm steps (1) and (2) fall together.,"{'title': '4 Some Empirical Data', 'number': '5'}"
(3) is performed using previous states of the system.,"{'title': '4 Some Empirical Data', 'number': '5'}"
Results.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"The test set consisted of the beginnings of three short stories by Hemingway (2785 words, 153 sentences) and three articles from the New York Times (4546 words, 233 sentences).","{'title': '4 Some Empirical Data', 'number': '5'}"
The results of my experiments are given in Table 6.,"{'title': '4 Some Empirical Data', 'number': '5'}"
The first row gives the number of personal and possessive pronouns.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"The remainder of the Table shows the results for the BFP-algorithm, for the BFPalgorithm extended by Kameyama's intra-sentential specifications, and for my algorithm.","{'title': '4 Some Empirical Data', 'number': '5'}"
The overall error rate of each approach is given in the rows marked with wrong.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"The rows marked with wrong (strat.) give the numbers of errors directly produced by the algorithms' strategy, the rows marked with wrong (ambig.) the number of analyses with ambiguities generated by the BFP-algorithm (my approach does not generate ambiguities).","{'title': '4 Some Empirical Data', 'number': '5'}"
The rows marked with wrong (intra) give the number of errors caused by (missing) specifications for intrasentential anaphora.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Since my algorithm integrates the specifications for intra-sentential anaphora, I count these errors as strategic errors.","{'title': '4 Some Empirical Data', 'number': '5'}"
The rows marked with wrong (chain) give the numbers of errors contained in error chains.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"The rows marked with wrong (other) give the numbers of the remaining errors (consisting of pronouns with split antecedents, errors because of segment boundaries, and missing specifications for event anaphora).","{'title': '4 Some Empirical Data', 'number': '5'}"
Interpretation.,"{'title': '4 Some Empirical Data', 'number': '5'}"
The results of my experiments showed not only that my algorithm performed better than the centering approaches but also revealed insight in the interaction between inter- and intrasentential preferences for anaphoric antecedents.,"{'title': '4 Some Empirical Data', 'number': '5'}"
Kameyama's specifications reduce the complexity in that the Cf-lists in general are shorter after splitting up a sentence into clauses.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Therefore, the BFP-algorithm combined with her specifications has almost no strategic errors while the number of ambiguities remains constant.","{'title': '4 Some Empirical Data', 'number': '5'}"
But this benefit is achieved at the expense of more errors caused by the intra-sentential specifications.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"These errors occur in cases like example (3), in which Kameyama's intrasentential strategy makes the correct antecedent less salient, indicating that a clause-based approach is too fine-grained and that the hierarchical syntactical structure as assumed by Kameyama does not have a great impact on anaphora resolution.","{'title': '4 Some Empirical Data', 'number': '5'}"
"I noted, too, that the BFP-algorithm can generate ambiguous readings for Ui when the pronoun in Ui does not co-specify the Cb(Ui_ ).","{'title': '4 Some Empirical Data', 'number': '5'}"
"In cases, where the C1 (U_1) contains more than one possible antecedent for the pronoun, several ambiguous readings with the same transitions are generated.","{'title': '4 Some Empirical Data', 'number': '5'}"
An examplem: There is no Cb(4a) because no element of the preceding utterance is realized in (4a).,"{'title': '4 Some Empirical Data', 'number': '5'}"
The pronoun &quot;them&quot; in (4b) co-specifies &quot;deer&quot; but the BFP-algorithm generates two readings both of which are marked by a RETAIN transition.,"{'title': '4 Some Empirical Data', 'number': '5'}"
(4) a. Jim pulled the burlap sacks off the deer b. and Liz looked at them.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"In general, the strength of the centering model is that it is possible to use the Cb(U1) as the most preferred antecedent for a pronoun in U.","{'title': '4 Some Empirical Data', 'number': '5'}"
In my model this effect is achieved by the preference for hearer-old discourse entities.,"{'title': '4 Some Empirical Data', 'number': '5'}"
Whenever this preference is misleading both approaches give wrong results.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"Since the Cb is defined strictly local while hearer-old discourse entities are defined global, my model produces less errors.","{'title': '4 Some Empirical Data', 'number': '5'}"
In my model the preference is available immediately while the BFPalgorithm can use its preference not before the second utterance has been processed.,"{'title': '4 Some Empirical Data', 'number': '5'}"
The more global definition of hearer-old discourse entities leads also to shorter error chains.,"{'title': '4 Some Empirical Data', 'number': '5'}"
"— However, the test set is too small to draw final conclusions, but at least for the texts analyzed the preference for hearer-old discourse entities is more appropriate than the preference given by the BFP- algorithm.","{'title': '4 Some Empirical Data', 'number': '5'}"
Kameyama's (1998) version of centering also omits the centering transitions.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
But she uses the Cb and a ranking over simplified transitions preventing the incremental application of her model.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
"The focus model (Sidner, 1983; Suri & McCoy, 1994) accounts for evoked discourse entities explicitly because it uses the discourse focus, which is determined by a successful anaphora resolution.","{'title': '5 Comparison to Related Approaches', 'number': '6'}"
Incremental processing is not a topic of these papers.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
Even models which use salience measures for determining the antecedents of pronoun use the concept of evoked discourse entities.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
Hajieova et al. (1992) assign the highest value to an evoked discourse entity.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
"Also Lappin & Leass (1994), who give the subject of the current sentence the highest weight, have an implicit notion of evokedness.","{'title': '5 Comparison to Related Approaches', 'number': '6'}"
The salience weight degrades from one sentence to another by a factor of two which implies that a repeatedly mentioned discourse entity gets a higher weight than a brand-new subject.,"{'title': '5 Comparison to Related Approaches', 'number': '6'}"
"In this paper, I proposed a model for determining the hearer's attentional state which is based on the distinction between hearer-old and hearer-new discourse entities.","{'title': '6 Conclusions', 'number': '7'}"
"I showed that my model, though it omits the backward-looking center and the centering transitions, does not lose any of the predictive power of the centering model with respect to anaphora resolution.","{'title': '6 Conclusions', 'number': '7'}"
"In contrast to the centering model, my model includes a treatment for intrasentential anaphora and is sufficiently well specified to be applied to real texts.","{'title': '6 Conclusions', 'number': '7'}"
Its incremental character seems to be an answer to the question Kehler (1997) recently raised.,"{'title': '6 Conclusions', 'number': '7'}"
"Furthermore, it neither has the problem of inconsistency Kehler mentioned with respect to the BFP-algorithm nor does it generate unnecessary ambiguities.","{'title': '6 Conclusions', 'number': '7'}"
"Future work will address whether the text position, which is the weakest grammatical concept, is sufficient for the order of the elements of the S-list at the second layer of my ranking constraints.","{'title': '6 Conclusions', 'number': '7'}"
I will also try to extend my model for the analysis of definite noun phrases for which it is necessary to integrate it into a more global model of discourse processing.,"{'title': '6 Conclusions', 'number': '7'}"
Acknowledgments: This work has been funded by a post-doctoral grant from DFG (Str 545/1-1) and is supported by a post-doctoral fellowship award from IRCS.,"{'title': '6 Conclusions', 'number': '7'}"
"I would like to thank Nobo Komagata, Rashmi Prasad, and Matthew Stone who commented on earlier drafts of this paper.","{'title': '6 Conclusions', 'number': '7'}"
"I am grateful for valuable comments by Barbara Grosz, Udo Hahn, Aravind Joshi, Lauri Karttunen, Andrew Kehler, Ellen Prince, and Bonnie Webber.","{'title': '6 Conclusions', 'number': '7'}"
