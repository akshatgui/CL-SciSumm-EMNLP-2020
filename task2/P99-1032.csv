col1,col2
This paper presents a case study of analyzing and improving intercoder reliability in discourse using statistical techniques. corrected tags are formulated and successfully used to guide a revision of the coding manual and develop an automatic classifier.,{}
"This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using the statistical techniques presented in (Bruce and Wiebe, 1998; Bruce and Wiebe, to appear).","{'title': '1 Introduction', 'number': '1'}"
Our approach is data driven: we refine our understanding and presentation of the classification scheme guided by the results of the intercoder analysis.,"{'title': '1 Introduction', 'number': '1'}"
We also present the results of a probabilistic classifier developed on the resulting annotations.,"{'title': '1 Introduction', 'number': '1'}"
Much research in discourse processing has focused on task-oriented and instructional dialogs.,"{'title': '1 Introduction', 'number': '1'}"
"The task addressed here comes to the fore in other genres, especially news reporting.","{'title': '1 Introduction', 'number': '1'}"
The task is to distinguish sentences used to objectively present factual information from sentences used to present opinions and evaluations.,"{'title': '1 Introduction', 'number': '1'}"
"There are many applications for which this distinction promises to be important, including text categorization and summarization.","{'title': '1 Introduction', 'number': '1'}"
This research takes a large step toward developing a reliably annotated gold standard to support experimenting with such applications.,"{'title': '1 Introduction', 'number': '1'}"
This research is also a case study of analyzing and improving manual tagging that is applicable to any tagging task.,"{'title': '1 Introduction', 'number': '1'}"
"We perform a statistical analysis that provides information that complements the information provided by Cohen's Kappa (Cohen, 1960; Carletta, 1996).","{'title': '1 Introduction', 'number': '1'}"
"In particular, we analyze patterns of agreement to identify systematic disagreements that result from relative bias among judges, because they can potentially be corrected automatically.","{'title': '1 Introduction', 'number': '1'}"
The corrected tags serve two purposes in this work.,"{'title': '1 Introduction', 'number': '1'}"
"They are used to guide the revision of the coding manual, resulting in improved Kappa scores, and they serve as a gold standard for developing a probabilistic classifier.","{'title': '1 Introduction', 'number': '1'}"
Using bias-corrected tags as gold-standard tags is one way to define a single best tag when there are multiple judges who disagree.,"{'title': '1 Introduction', 'number': '1'}"
The coding manual and data from our experiments are available at: hap: / /www.cs.nmsu.edur wiebe/projects.,"{'title': '1 Introduction', 'number': '1'}"
"In the remainder of this paper, we describe the classification being performed (in section 2), the statistical tools used to analyze the data and produce the bias-corrected tags (in section 3), the case study of improving intercoder agreement (in section 4), and the results of the classifier for automatic subjectivity tagging (in section 5).","{'title': '1 Introduction', 'number': '1'}"
"We address evidentiality in text (Chafe, 1986), which concerns issues such as what is the source of information, and whether information is being presented as fact or opinion.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"These questions are particularly important in news reporting, in which segments presenting opinions and verbal reactions are mixed with segments presenting objective fact (van Dijk, 1988; Kan et al., 1998).","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"The definitions of the categories in our coding manual are intention-based: &quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"Otherwise, the sentence is subjective.&quot;' We focus on sentences about private states, such as belief, knowledge, emotions, etc.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"(Quirk et al., 1985), and sentences about speech events, such as speaking and writing.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
Such sentences may be either subjective or objective.,"{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"From the coding manual: &quot;Subjective speech-event (and private-state) sentences are used to communicate the speaker's evaluations, opinions, emotions, and speculations.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"The primary intention of objective speech-event (and privatestate) sentences, on the other hand, is to objectively communicate material that is factual to the reporter.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"The speaker, in these cases, is being used as a reliable source of information.&quot; Following are examples of subjective and objective sentences: In sentence 4, there is no uncertainty or evaluation expressed toward the speaking event.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"Thus, from one point of view, one might have considered this sentence to be objective.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
Linguistic categorizations usually do not cover all instances perfectly.,"{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"For example, sentences may fall on the borderline between two categories.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"To allow for uncertainty in the annotation process, the specific tags used in this work include certainty ratings, ranging from 0, for least certain, to 3, for most certain.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"As discussed below in section 3.2, the certainty ratings allow us to investigate whether a model positing additional categories provides a better description of the judges' annotations than a binary model does.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"Subjective and objective categories are potentially important for many text processing applications, such as information extraction and information retrieval, where the evidential status of information is important.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"In generation and machine translation, it is desirable to generate text that is appropriately subjective or objective (Hovy, 1987).","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"In summarization, subjectivity judgments could be included in document profiles, to augment automatically produced document summaries, and to help the user make relevance judgments when using a search engine.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"In addition, they would be useful in text categorization.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"In related work (Wiebe et al., in preparation), we found that article types, such as announcement and opinion piece, are significantly correlated with the subjective and objective classification.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"Our subjective category is related to but differs from the statement-opinion category of the Switchboard-DAMSL discourse annotation project (Jurafsky et al., 1997), as well as the gives opinion category of Bale's (1950) model of small-group interaction.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
"All involve expressions of opinion, but while our category specifications focus on evidentiality in text, theirs focus on how conversational participants interact with one another in dialog.","{'title': '2 The Subjective and Objective Categories', 'number': '2'}"
Table 1 presents data for two judges.,"{'title': '3 Statistical Tools', 'number': '3'}"
The rows correspond to the tags assigned by judge 1 and the columns correspond to the tags assigned by judge 2.,"{'title': '3 Statistical Tools', 'number': '3'}"
"Let nj denote the number of sentences that judge 1 classifies as i and judge 2 classifies as j, and let be the probability that a randomly selected sentence is categorized as i by judge 1 and j by judge 2.","{'title': '3 Statistical Tools', 'number': '3'}"
"Then, the maximum likelihood estimate of fiii is 11-7-71.:+ , where n++ = Eii nii = 504.","{'title': '3 Statistical Tools', 'number': '3'}"
"Table 1 shows a four-category data configuration, in which certainty ratings 0 and 1 are combined and ratings 2 and 3 are combined.","{'title': '3 Statistical Tools', 'number': '3'}"
"Note that the analyses described in this section cannot be performed on the two-category data configuration (in which the certainty ratings are not considered), due to insufficient degrees of freedom (Bishop et al., 1975).","{'title': '3 Statistical Tools', 'number': '3'}"
"Evidence of confusion among the classifications in Table 1 can be found in the marginal totals, ni+ and n+j.","{'title': '3 Statistical Tools', 'number': '3'}"
"We see that judge 1 has a relative preference, or bias, for objective, while judge 2 has a bias for subjective.","{'title': '3 Statistical Tools', 'number': '3'}"
Relative bias is one aspect of agreement among judges.,"{'title': '3 Statistical Tools', 'number': '3'}"
"A second is whether the judges' disagreements are systematic, that is, correlated.","{'title': '3 Statistical Tools', 'number': '3'}"
One pattern of systematic disagreement is symmetric disagreement.,"{'title': '3 Statistical Tools', 'number': '3'}"
"When disagreement is symmetric, the differences between the actual counts, and the counts expected if the judges' decisions were not correlated, are symmetric; that is, Snii for i j, where 5,i, is the difference from independence.","{'title': '3 Statistical Tools', 'number': '3'}"
Our goal is to correct correlated disagreements automatically.,"{'title': '3 Statistical Tools', 'number': '3'}"
We are particularly interested in systematic disagreements resulting from relative bias.,"{'title': '3 Statistical Tools', 'number': '3'}"
We test for evidence of such correlations by fitting probability models to the data.,"{'title': '3 Statistical Tools', 'number': '3'}"
"Specifically, we study bias using the model for marginal homogeneity, and symmetric disagreement using the model for quasisymmetry.","{'title': '3 Statistical Tools', 'number': '3'}"
"When there is such evidence, we propose using the latent class model to correct the disagreements; this model posits an unobserved (latent) variable to explain the correlations among the judges' observations.","{'title': '3 Statistical Tools', 'number': '3'}"
The remainder of this section describes these models in more detail.,"{'title': '3 Statistical Tools', 'number': '3'}"
"All models can be evaluated using the freeware package CoCo, which was developed by Badsberg (1995) and is available at: http: / /web.math.auc.dkr jhb/CoCo.","{'title': '3 Statistical Tools', 'number': '3'}"
A probability model enforces constraints on the counts in the data.,"{'title': '3 Statistical Tools', 'number': '3'}"
The degree to which the counts in the data conform to the constraints is called the fit of the model.,"{'title': '3 Statistical Tools', 'number': '3'}"
"In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993).","{'title': '3 Statistical Tools', 'number': '3'}"
"The higher the G2 value, the poorer the fit.","{'title': '3 Statistical Tools', 'number': '3'}"
"We will consider model fit to be acceptable if its reference significance level is greater than 0.01 (i.e., if there is greater than a 0.01 probability that the data sample was randomly selected from a population described by the model).","{'title': '3 Statistical Tools', 'number': '3'}"
"Bias of one judge relative to another is evidenced as a discrepancy between the marginal totals for the two judges (i.e., ni+ and n+j in Table 1).","{'title': '3 Statistical Tools', 'number': '3'}"
Bias is measured by testing the fit of the model for marginal homogeneity: = 25+i for all i.,"{'title': '3 Statistical Tools', 'number': '3'}"
"The larger the G2 value, the greater the bias.","{'title': '3 Statistical Tools', 'number': '3'}"
The fit of the model can be evaluated as described on pages 293-294 of Bishop et al. (1975).,"{'title': '3 Statistical Tools', 'number': '3'}"
"Judges who show a relative bias do not always agree, but their judgments may still be correlated.","{'title': '3 Statistical Tools', 'number': '3'}"
"As an extreme example, judge 1 may assign the subjective tag whenever judge 2 assigns the objective tag.","{'title': '3 Statistical Tools', 'number': '3'}"
"In this example, there is a kind of symmetry in the judges' responses, but their agreement would be low.","{'title': '3 Statistical Tools', 'number': '3'}"
Patterns of symmetric disagreement can be identified using the model for quasi-symmetry.,"{'title': '3 Statistical Tools', 'number': '3'}"
"This model constrains the off-diagonal counts, i.e., the counts that correspond to disagreement.","{'title': '3 Statistical Tools', 'number': '3'}"
"It states that these counts are the product of a table for independence and a symmetric table, nii = Ai+ x A+i x Aii, such that Aij = ii.In this formula, Ai+ x A+3 is the model for independence and Ai3 is the symmetric interaction term.","{'title': '3 Statistical Tools', 'number': '3'}"
"Intuitively, Aii represents the difference between the actual counts and those predicted by independence.","{'title': '3 Statistical Tools', 'number': '3'}"
This model can be evaluated using CoCo as described on pages 289-290 of Bishop et al. (1975).,"{'title': '3 Statistical Tools', 'number': '3'}"
We use the latent class model to correct symmetric disagreements that appear to result from bias.,"{'title': '3 Statistical Tools', 'number': '3'}"
The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974).,"{'title': '3 Statistical Tools', 'number': '3'}"
"Goodman's procedure is a specialization of the EM algorithm (Dempster et al., 1977), which is implemented in the freeware program CoCo (Badsberg, 1995).","{'title': '3 Statistical Tools', 'number': '3'}"
"Since its development, the latent class model has been widely applied, and is the underlying model in various unsupervised machine learning algorithms, including AutoClass (Cheeseman and Stutz, 1996).","{'title': '3 Statistical Tools', 'number': '3'}"
"The form of the latent class model is that of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable.","{'title': '3 Statistical Tools', 'number': '3'}"
"The latent variable represents the true state of the object, and is the source of the correlations among the observed variables.","{'title': '3 Statistical Tools', 'number': '3'}"
"As applied here, the observed variables are the classifications assigned by the judges.","{'title': '3 Statistical Tools', 'number': '3'}"
"Let B, D, J, and M be these variables, and let L be the latent variable.","{'title': '3 Statistical Tools', 'number': '3'}"
"Then, the latent class model is: (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} .","{'title': '3 Statistical Tools', 'number': '3'}"
"Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges.","{'title': '3 Statistical Tools', 'number': '3'}"
"The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters.","{'title': '3 Statistical Tools', 'number': '3'}"
"For a description of this process, see Goodman (1974), Dawid & Skene (1979), or Pedersen & Bruce (1998).","{'title': '3 Statistical Tools', 'number': '3'}"
"Three versions of the latent class model are considered in this study, one with two latent categories, one with three latent categories, and one with four.","{'title': '3 Statistical Tools', 'number': '3'}"
"We apply these models to three data configurations: one with two categories (subjective and objective with no certainty ratings), one with four categories (subjective and objective with coarse-grained certainty ratings, as shown in Table 1), and one with eight categories (subjective and objective with fine-grained certainty ratings).","{'title': '3 Statistical Tools', 'number': '3'}"
"All combinations of model and data configuration are evaluated, except the four-category latent class model with the twocategory data configuration, due to insufficient degrees of freedom.","{'title': '3 Statistical Tools', 'number': '3'}"
"In all cases, the models fit the data well, as measured by G2.","{'title': '3 Statistical Tools', 'number': '3'}"
"The model chosen as final is the one for which the agreement among the latent categories assigned to the three data configurations is highest, that is, the model that is most consistent across the three data configurations.","{'title': '3 Statistical Tools', 'number': '3'}"
"Our annotation project consists of the following steps:2 bias-corrected tag in many cases, but arguing for his or her own tag in some cases.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Based on the judges' feedback, 22 of the 504 bias-corrected tags are changed, and a second draft of the coding manual is written.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
5.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
A second corpus is annotated by the same four judges according to the new coding manual.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
Each spends about five hours.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
6.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"The results of the second tagging experiment are analyzed using the methods described in section 3, and bias-corrected tags are produced for the second data set.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Two disjoint corpora are used in steps 2 and 5, both consisting of complete articles taken from the Wall Street Journal Treebank Corpus (Marcus et al., 1993).","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"In both corpora, judges assign tags to each non-compound sentence and to each conjunct of each compound sentence, 504 in the first corpus and 500 in the second.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
The segmentation of compound sentences was performed manually before the judges received the data.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Judges J and B, the first two authors of this paper, are NLP researchers.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Judge M is an undergraduate computer science student, and judge D has no background in computer science or linguistics.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Judge J, with help from M, developed the original coding instructions, and Judge J directed the process in step 4.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
The analysis performed in step 3 reveals strong evidence of relative bias among the judges.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
Each pairwise comparison of judges also shows a strong pattern of symmetric disagreement.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
The two-category latent class model produces the most consistent clusters across the data configurations.,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"It, therefore, is used to define the bias-corrected tags.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"In step 4, judge B was excluded from the interactive discussion for logistical reasons.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Discussion is apparently important, because, although B's Kappa values for the first study are on par with the others, B's Kappa values for agreement with the other judges change very little from the first to the second study (this is true across the range of certainty values).","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"In contrast, agreement among the other judges noticeably improves.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Because judge B's poor performance in the second tagging experiment is linked to a difference in procedure, judge B's tags are excluded from our subsequent analysis of the data gathered during the second tagging experiment.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Table 2 shows the changes, from study 1 to study 2, in the Kappa values for pairwise agreement among the judges.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
The best results are clearly for the two who are not authors of this paper (D and M).,"{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"The Kappa value for the agreement between D and M considering all certainty ratings reaches .76, which allows tentative conclusions on Krippendorf's scale (1980).","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"If we exclude the sentences with certainty rating 0, the Kappa values for pairwise agreement between M and D and between J and M are both over .8, which allows definite conclusions on Krippendorf's scale.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Finally, if we only consider sentences with certainty 2 or 3, the pairwise agreements among M, D, and J all have high Kappa values, 0.87 and over.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"We are aware of only one previous project reporting intercoder agreement results for similar categories, the switchboard-DAMSL project mentioned above.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"While their Kappa results are very good for other tags, the opinion-statement tagging was not very successful: &quot;The distinction was very hard to make by labelers, and accounted for a large proportion of our interlabeler error&quot; (Jurafsky et al., 1997).","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"In step 6, as in step 3, there is strong evidence of relative bias among judges D, J and M. Each pairwise comparison of judges also shows a strong pattern of symmetric disagreement.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"The results of this analysis are presented in Table 3.3 Also as in step 3, the two-category latent class model produces the most consistent clusters across the data configurations.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Thus, it is used to define the bias-corrected tags for the second data set as well.","{'title': '4 Improving Agreement in Discourse Tagging', 'number': '4'}"
"Recently, there have been many successful applications of machine learning to discourse processing, such as (Litman, 1996; Samuel et al., 1998).","{'title': '5 Machine Learning Results', 'number': '5'}"
"In this section, we report the results of machine learning experiments, in which we develop probablistic classifiers to automatically perform the subjective and objective classification.","{'title': '5 Machine Learning Results', 'number': '5'}"
"In the method we use for developing classifiers (Bruce and Wiebe, 1999), a search is performed to find a probability model that captures important interdependencies among features.","{'title': '5 Machine Learning Results', 'number': '5'}"
"Because features can be dropped and added during search, the method also performs feature selection.","{'title': '5 Machine Learning Results', 'number': '5'}"
"In these experiments, the system considers naive Bayes, full independence, full interdependence, and models generated from those using forward and backward search.","{'title': '5 Machine Learning Results', 'number': '5'}"
The model selected is the one with the highest accuracy on a held-out portion of the training data. sets.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"On each fold, one set is used for testing, and the other nine are used for training.","{'title': '5 Machine Learning Results', 'number': '5'}"
"Feature selection, model selection, and parameter estimation are performed anew on each fold.","{'title': '5 Machine Learning Results', 'number': '5'}"
The following are the potential features considered on each fold.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not.","{'title': '5 Machine Learning Results', 'number': '5'}"
We also include a binary feature representing whether or not the sentence begins a new paragraph.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).","{'title': '5 Machine Learning Results', 'number': '5'}"
The data consists of the concatenation of the two corpora annotated with bias-corrected tags as described above.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"The baseline accuracy, i.e., the frequency of the more frequent class, is only 51%.","{'title': '5 Machine Learning Results', 'number': '5'}"
The results of the experiments are very promising.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"The average accuracy across all folds is 72.17%, more than 20 percentage points higher than the baseline accuracy.","{'title': '5 Machine Learning Results', 'number': '5'}"
"Interestingly, the system performs better on the sentences for which the judges are certain.","{'title': '5 Machine Learning Results', 'number': '5'}"
"In a post hoc analysis, we consider the sentences from the second data set for which judges M, J, and D rate their certainty as 2 or 3.","{'title': '5 Machine Learning Results', 'number': '5'}"
There are 299/500 such sentences.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"For each fold, we calculate the system's accuracy on the subset of the test set consisting of such sentences.","{'title': '5 Machine Learning Results', 'number': '5'}"
The average accuracy of the subsets across folds is 81.5%.,"{'title': '5 Machine Learning Results', 'number': '5'}"
"Taking human performance as an upper bound, the system has room for improvement.","{'title': '5 Machine Learning Results', 'number': '5'}"
"The average pairwise percentage agreement between D, J, and M and the bias-corrected tags in the entire data set is 89.5%, while the system's percentage agreement with the bias-corrected tags (i.e., its accuracy) is 72.17%.","{'title': '5 Machine Learning Results', 'number': '5'}"
This paper demonstrates a procedure for automatically formulating a single best tag when there are multiple judges who disagree.,"{'title': '6 Conclusion', 'number': '6'}"
The procedure is applicable to any tagging task in which the judges exhibit symmetric disagreement resulting from bias.,"{'title': '6 Conclusion', 'number': '6'}"
"We successfully use bias-corrected tags for two purposes: to guide a revision of the coding manual, and to develop an automatic classifier.","{'title': '6 Conclusion', 'number': '6'}"
"The revision of the coding manual results in as much as a 16 point improvement in pairwise Kappa values, and raises the average agreement among the judges to a Kappa value of over 0.87 for the sentences that can be tagged with certainty.","{'title': '6 Conclusion', 'number': '6'}"
"Using only simple features, the classifier achieves an average accuracy 21 percentage points higher than the baseline, in 10-fold cross validation experiments.","{'title': '6 Conclusion', 'number': '6'}"
"In addition, the average accuracy of the classifier is 81.5% on the sentences the judges tagged with certainty.","{'title': '6 Conclusion', 'number': '6'}"
The strong performance of the classifier and its consistency with the judges demonstrate the value of this approach to developing gold-standard tags.,"{'title': '6 Conclusion', 'number': '6'}"
This research was supported in part by the Office of Naval Research under grant number N00014-95-1-0776.,"{'title': '7 Acknowledgements', 'number': '7'}"
"We are grateful to Matthew T. Bell and Richard A. Wiebe for participating in the annotation study, and to the anonymous reviewers for their comments and suggestions.","{'title': '7 Acknowledgements', 'number': '7'}"
