col1,col2
"We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.",{}
We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.,{}
"We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.",{}
"In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.",{}
Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.,{}
Implicit discourse relations abound in text and readers easily recover the sense of such relations during semantic interpretation.,"{'title': '1 Introduction', 'number': '1'}"
But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing.,"{'title': '1 Introduction', 'number': '1'}"
"Discourse relations, such as causal and contrast relations, are often marked by explicit discourse connectives (also called cue words) such as “because” or “but”.","{'title': '1 Introduction', 'number': '1'}"
"It is not uncommon, though, for a discourse relation to hold between two text spans without an explicit discourse connective, as the example below demonstrates: (1) The 101-year-old magazine has never had to woo advertisers with quite so much fervor before.","{'title': '1 Introduction', 'number': '1'}"
[because] It largely rested on its hard-to-fault demographics.,"{'title': '1 Introduction', 'number': '1'}"
In this paper we address the problem of automatic sense prediction for discourse relations in newspaper text.,"{'title': '1 Introduction', 'number': '1'}"
"For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations.","{'title': '1 Introduction', 'number': '1'}"
"Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007).","{'title': '1 Introduction', 'number': '1'}"
"In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits.","{'title': '1 Introduction', 'number': '1'}"
Explicit relations are easy to identify.,"{'title': '1 Introduction', 'number': '1'}"
"The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).","{'title': '1 Introduction', 'number': '1'}"
So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress.,"{'title': '1 Introduction', 'number': '1'}"
In this paper we investigate the effectiveness of various features designed to capture lexical and semantic regularities for identifying the sense of implicit relations.,"{'title': '1 Introduction', 'number': '1'}"
"Given two text spans, previous work has used the cross-product of the words in the spans as features.","{'title': '1 Introduction', 'number': '1'}"
We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped.,"{'title': '1 Introduction', 'number': '1'}"
"We then introduce several other methods capturing the semantics of the spans (polarity features, semantic classes, tense, etc.) and evaluate their effectiveness.","{'title': '1 Introduction', 'number': '1'}"
This is the first study which reports results on classifying naturally occurring implicit relations in text and uses the natural distribution of the various senses.,"{'title': '1 Introduction', 'number': '1'}"
"Experiments on implicit and explicit relations Previous work has dealt with the prediction of discourse relation sense, but often for explicits and at the sentence level.","{'title': '2 Related Work', 'number': '2'}"
Soricut and Marcu (2003) address the task of parsing discourse structures within the same sentence.,"{'title': '2 Related Work', 'number': '2'}"
"They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988).","{'title': '2 Related Work', 'number': '2'}"
"Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence.","{'title': '2 Related Work', 'number': '2'}"
Such features would not be applicable to the analysis of implicit relations that occur intersententially.,"{'title': '2 Related Work', 'number': '2'}"
"Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations.","{'title': '2 Related Work', 'number': '2'}"
They achieve 81% accuracy in sense disambiguation on this corpus.,"{'title': '2 Related Work', 'number': '2'}"
"However, GraphBank annotations do not differentiate between implicits and explicits, so it is difficult to verify success for implicit relations.","{'title': '2 Related Work', 'number': '2'}"
Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction.,"{'title': '2 Related Work', 'number': '2'}"
"Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations.","{'title': '2 Related Work', 'number': '2'}"
"They delete the connective and use [Arg1, Arg2] as an example of an implicit relation.","{'title': '2 Related Work', 'number': '2'}"
"The approach is tested using binary classification between relations on balanced data, a setting very different from that of any realistic application.","{'title': '2 Related Work', 'number': '2'}"
"For example, a question-answering application that needs to identify causal relations (i.e. as in Girju (2003)), must not only differentiate causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all.","{'title': '2 Related Work', 'number': '2'}"
"In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently.","{'title': '2 Related Work', 'number': '2'}"
"Causal and comparison relations, which are most useful for applications, are less frequent.","{'title': '2 Related Work', 'number': '2'}"
"Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy.","{'title': '2 Related Work', 'number': '2'}"
"Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data.","{'title': '2 Related Work', 'number': '2'}"
"For our experiments, we use the Penn Discourse Treebank (PDTB; Prasad et al., 2008), the largest available annotated corpora of discourse relations.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"The PDTB contains discourse annotations over the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"For each explicit discourse connective (such as “but” or “so”), annotators identified the two text spans between which the relation holds and the sense of the relation.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
The PDTB also provides information about local implicit relations.,"{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"For each pair of adjacent sentences within the same paragraph, annotators selected the explicit discourse connective which best expressed the relation between the sentences and then assigned a sense to the relation.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"In Example (1) above, the annotators identified “because” as the most appropriate connective between the sentences, and then labeled the implicit discourse relation Contingency.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"As mentioned above, each implicit and explicit relation is annotated with a sense.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"In our experiments, we use only the top level of the sense annotations: Comparison, Contingency, Expansion, and Temporal.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
Each relation in the PDTB takes two arguments.,"{'title': '3 Penn Discourse Treebank', 'number': '3'}"
Example (1) can be seen as the predicate Contingency which takes the two sentences as arguments.,"{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2.","{'title': '3 Penn Discourse Treebank', 'number': '3'}"
"Cross product of words Discourse connectives are the most reliable predictors of the semantic sense of the relation (Marcu, 2000; Pitler et al., 2008).","{'title': '4 Word pair features in prior work', 'number': '4'}"
"However, in the absence of explicit markers, the most easily accessible features are the words in the two text spans of the relation.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Intuitively, one would expect that there is some relationship that holds between the words in the two arguments.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Consider for example the following sentences: The recent explosion of country funds mirrors the ”closedend fund mania” of the 1920s, Mr.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Foot says, when narrowly focused funds grew wildly popular.","{'title': '4 Word pair features in prior work', 'number': '4'}"
They fell into oblivion after the 1929 crash.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"The words “popular” and “oblivion” are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Similarly, a pair of words such as (rain, rot) might be indicative of a causal relation.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Indeed, word pairs form the basic feature of most previous work on classifying implicit relations (Marcu and Echihabi, 2001; BlairGoldensohn et al., 2007; Sporleder and Lascarides, 2008) or the simpler task of predicting which connective should be used to express a relation (Lapata and Lascarides, 2004).","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Semantic relations vs. function word pairs If the hypothesis for word pair triggers of discourse relations were true, the analysis of unambiguous relations can be used to discover pairs of words with causal or contrastive relations holding between them.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Yet, feature analysis has not been performed in prior studies to establish or refute this possibility.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"At the same time, feature selection is always necessary for word pairs, which are numerous and lead to data sparsity problems.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Here, we present a meta analysis of the feature selection work in three prior studies.","{'title': '4 Word pair features in prior work', 'number': '4'}"
One approach for reducing the number of features follows the hypothesis of semantic relations between words.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"Marcu and Echihabi (2001) considered only nouns, verbs and and other cue phrases in word pairs.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"They found that even with millions of training examples, prediction results using all words were superior to those based on only pairs of non-function words.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"However, since the learning curve is steeper when function words were removed, they hypothesize that using only non-function words will outperform using all words once enough training data is available.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Verb pairs turned out to be one of the best features, but no useful information was obtained using nouns and adjectives.","{'title': '4 Word pair features in prior work', 'number': '4'}"
Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"They show that (i) stemming, (ii) using a small fixed vocabulary size consisting of only the most frequent stems (which would tend to be dominated by function words) and (iii) a cutoff on the minimum frequency of a feature, all result in improved performance.","{'title': '4 Word pair features in prior work', 'number': '4'}"
They also report that filtering stopwords has a negative impact on the results.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"Given these findings, we expect that pairs of function words are informative features helpful in predicting discourse relation sense.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"In our work that we describe next, we use feature selection to investigate the word pairs in detail.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"For the analysis of word pair features, we use a large collection of automatically extracted explicit examples from the experiments in BlairGoldensohn et al. (2007).","{'title': '4 Word pair features in prior work', 'number': '4'}"
"The data, from now on referred to as TextRels, has explicit contrast and causal relations which were extracted from the English Gigaword Corpus (Graff, 2003) which contains over four million newswire articles.","{'title': '4 Word pair features in prior work', 'number': '4'}"
The explicit cue phrase is removed from each example and the spans are treated as belonging to an implicit relation.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"Besides cause and contrast, the TextRels data include a no-relation category which consists of sentences from the same text that are separated by at least three other sentences.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"To identify features useful for classifying comparison vs other relations, we chose a random sample of 5000 examples for Contrast and 5000 Other relations (2500 each of Cause and No-relation).","{'title': '4 Word pair features in prior work', 'number': '4'}"
"For the complete set of 10,000 examples, word pair features were computed.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"After removing word pairs that appear less than 5 times, the remaining features were ranked by information gain using the MALLET toolkit1.","{'title': '4 Word pair features in prior work', 'number': '4'}"
Table 1 lists the word pairs with highest information gain for the Contrast vs. Other and Cause vs. Other classification tasks.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"All contain very frequent stop words, and interestingly for the Con1mallet.cs.umass.edu trast vs. Other task, most of the word pairs contain discourse connectives.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"This is certainly unexpected, given that word pairs were formed by deleting the discourse connectives from the sentences expressing Contrast.","{'title': '4 Word pair features in prior work', 'number': '4'}"
Word pairs containing “but” as one of their elements in fact signal the presence of a relation that is not Contrast.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"Consider the example shown below: The government says it has reached most isolated townships by now, but because roads are blocked, getting anything but basic food supplies to people remains difficult.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Following Marcu and Echihabi (2001), the pair [The government says it has reached most isolated townships by now, but] and [roads are blocked, getting anything but basic food supplies to people remains difficult.] is created as an example of the Cause relation.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Because of examples like this, “but-but” is a very useful word pair feature indicating Cause, as the but would have been removed for the artifical Contrast examples.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"In fact, the top 17 features for classifying Contrast versus Other all contain the word “but”, and are indications that the relation is Other.","{'title': '4 Word pair features in prior work', 'number': '4'}"
These findings indicate an unexpected anomalous effect in the use of synthetic data.,"{'title': '4 Word pair features in prior work', 'number': '4'}"
"Since relations are created by removing connectives, if an unambiguous connective remains, its presence is a reliable indicator that the example should be classified as Other.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"Such features might work well and lead to high accuracy results in identifying synthetic implicit relations, but are unlikely to be useful in a realistic setting of actual implicits. the-but s-but the-in of-but for-but but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"This ranking explains the observations reported in Blair-Goldensohn et al. (2007) where removing stopwords degraded classifier performance and why using only nouns, verbs or adjectives (Marcu and Echihabi, 2001; Lapata and Lascarides, 2004) is not the best option2.","{'title': '4 Word pair features in prior work', 'number': '4'}"
"The contrast between the “popular”/“oblivion” example we started with above can be analyzed in terms of lexical relations (near antonyms), but also could be explained by different polarities of the two words: “popular” is generally a positive word, while “oblivion” has negative connotations.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"While we agree that the actual words in the arguments are quite useful, we also define several higher-level features corresponding to various semantic properties of the words.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
The words in the two text spans of a relation are taken from the gold-standard annotations in the PDTB.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Polarity Tags: We define features that represent the sentiment of the words in the two spans.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Each word’s polarity was assigned according to its entry in the Multi-perspective Question Answering Opinion Corpus (Wilson et al., 2005).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"In this resource, each sentiment word is annotated as positive, negative, both, or neutral.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"We use the number of negated and non-negated positive, negative, and neutral sentiment words in the two text spans as features.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"If a writer refers to something as “nice” in Arg1, that counts towards the positive sentiment count (Arg1Positive); “not nice” would count towards Arg1NegatePositive.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"A sentiment word is negated if a word with a General Inquirer (Stone et al., 1966) Negate tag precedes it.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We also have features for the cross products of these polarities between Arg1 and Arg2.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We expected that these features could help Comparison examples especially.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Consider the following example: wasn’t a good one.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The venture, formed in 1986, was supposed to be Time’s low-cost, safe entry into women’s magazines.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The word good is annotated with positive polarity, however it is negated.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Safe is tagged as having positive polarity, so this opposition could indicate the Comparison relation between the two sentences.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Inquirer Tags: To get at the meanings of the spans, we look up what semantic categories each word falls into according to the General Inquirer lexicon (Stone et al., 1966).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The General Inquirer has classes for positive and negative polarity, as well as more fine-grained categories such as words related to virtue or vice.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The Inquirer even contains a category called “Comp” that includes words that tend to indicate Comparison, such as “optimal”, “other”, “supreme”, or “ultimate”.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Several of the categories are complementary: Understatement versus Overstatement, Rise versus Fall, or Pleasure versus Pain.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Pairs where one argument contains words that indicate Rise and the other argument indicates Fall might be good evidence for a Comparison relation.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The benefit of using these tags instead of just the word pairs is that we see more observations for each semantic class than for any particular word, reducing the data sparsity problem.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"For example, the pair rose:fell often indicates a Comparison relation when speaking about stocks.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"However, occasionally authors refer to stock prices as “jumping” rather than “rising”.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Since both jump and rise are members of the Rise class, new jump examples can be classified using past rise examples.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Development testing showed that including features for all words’ tags was not useful, so we include the Inquirer tags of only the verbs in the two arguments and their cross-product.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Just as for the polarity features, we include features for both each tag and its negation.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Money/Percent/Num: If two adjacent sentences both contain numbers, dollar amounts, or percentages, it is likely that a comparison relation might hold between the sentences.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"We included a feature for the count of numbers, percentages, and dollar amounts in Arg1 and Arg2.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We also included the number of times each combination of number/percent/dollar occurs in Arg1 and Arg2.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"For example, if Arg1 mentions a percentage and Arg2 has two dollar amounts, the feature Arg1Percent-Arg2Money would have a count of 2.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
This feature is probably genre-dependent.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Numbers and percentages often appear in financial texts but would be less frequent in other genres.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
WSJ-LM: This feature represents the extent to which the words in the text spans are typical of each relation.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"For each sense, we created unigram and bigram language models over the implicit examples in the training set.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We compute each example’s probability according to each of these language models.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
The features are the ranks of the spans’ likelihoods according to the various language models.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"For example, if of the unigram models, the most likely relation to generate this example was Contingency, then the example would include the feature ContingencyUnigram1.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"If the third most likely relation according to the bigram models was Expansion, then it would include the feature ExpansionBigram3.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Expl-LM: This feature ranks the text spans according to language models derived from the explicit examples in the TextRels corpus.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"However, the corpus contains only Cause, Contrast and Norelation, hence we expect the WSJ language models to be more helpful.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Verbs: These features include the number of pairs of verbs in Arg1 and Arg2 from the same verb class.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The intuition behind this feature is that the more related the verbs, the more likely the relation is an Expansion.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"The verb features also include the average length of verb phrases in each argument, as well as the cross product of this feature for the two arguments.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"We hypothesized that verb chunks that contain more words, such as “They [are allowed to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Our final verb features were the part of speech tags (gold-standard from the Penn Treebank) of the main verb.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"One would expect that Expansion would link sentences with the same tense, whereas Contingency and Temporal relations would contain verbs with different tenses.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"First-Last, First3: The first and last words of a relation’s arguments have been found to be particularly useful for predicting its sense (Wellner et al., 2006).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Wellner et al. (2006) suggest that these words are such predictive features because they are often explicit discourse connectives.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"In our experiments on implicits, the first and last words are not connectives.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"In the PDTB, these are annotated as alternatively lexicalized relations (AltLexes).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"To capture such effects, we included the first and last words of Arg1 as features, the first and last words of Arg2, the pair of the first words of Arg1 and Arg2, and the pair of the last words.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We also add two additional features which indicate the first three words of each argument.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Modality: Modal words, such as “can”, “should”, and “may”, are often used to express conditional statements (i.e.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"“If I were a wealthy man, I wouldn’t have to work hard.”) thus signaling a Contingency relation.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"We include a feature for the presence or absence of modals in Arg1 and Arg2, features for specific modal words, and their cross-products.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Context: Some implicit relations appear immediately before or immediately after certain explicit relations far more often than one would expect due to chance (Pitler et al., 2008).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We define a feature indicating if the immediately preceding (or following) relation was an explicit.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"If it was, we include the connective trigger of the relation and its sense as features.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"We use oracle annotations of the connective sense, however, most of the connectives are unambiguous.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
One might expect a different distribution of relation types in the beginning versus further in the middle of a paragraph.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
We capture paragraphposition information using a feature which indicates if Arg1 begins a paragraph.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Word pairs Four variants of word pair models were used in our experiments.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"All the models were eventually tested on implicit examples from the PDTB, but the training set-up was varied.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Wordpairs-TextRels In this setting, we trained a model on word pairs derived from unannotated text (TextRels corpus).","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
Wordpairs-PDTBImpl Word pairs for training were formed from the cross product of words in the textual spans (Arg1 x Arg2) of the PDTB implicit relations.,"{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Wordpairs-selected Here, only word pairs from Wordpairs-PDTBImpl with non-zero information gain on the TextRels corpus were retained.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"Wordpairs-PDTBExpl In this case, the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training.","{'title': '6 Features for sense prediction of implicit discourse relations', 'number': '5'}"
"For all experiments, we used sections 2-20 of the PDTB for training and sections 21-22 for testing.","{'title': '7 Classification Results', 'number': '6'}"
Sections 0-1 were used as a development set for feature design.,"{'title': '7 Classification Results', 'number': '6'}"
We ran four binary classification tasks to identify each of the main relations from the rest.,"{'title': '7 Classification Results', 'number': '6'}"
"As each of the relations besides Expansion are infrequent, we train using equal numbers of positive and negative examples of the target relation.","{'title': '7 Classification Results', 'number': '6'}"
The negative examples were chosen at random.,"{'title': '7 Classification Results', 'number': '6'}"
"We used all of sections 21 and 22 for testing, so the test set is representative of the natural distribution.","{'title': '7 Classification Results', 'number': '6'}"
"The training sets contained: Comparison (1927 positive, 1927 negative), Contingency (3500 each), Expansion3 (6356 each), and Temporal (730 each).","{'title': '7 Classification Results', 'number': '6'}"
"The test set contained: 151 examples of Comparison, 291 examples of Contingency, 986 examples of Expansion, 82 examples of Temporal, and 13 examples of No-relation.","{'title': '7 Classification Results', 'number': '6'}"
"We used Naive Bayes, Maximum Entropy (MaxEnt), and AdaBoost (Freund and Schapire, 1996) classifiers implemented in MALLET.","{'title': '7 Classification Results', 'number': '6'}"
The performance using only our semantically informed features is shown in Table 7.,"{'title': '7 Classification Results', 'number': '6'}"
"Only the Naive Bayes classification results are given, as space is limited and MaxEnt and AdaBoost gave slightly lower accuracies overall.","{'title': '7 Classification Results', 'number': '6'}"
"The table lists the f-score for each of the target relations, with overall accuracy shown in brackets.","{'title': '7 Classification Results', 'number': '6'}"
"Given that the experiments are run on natural distribution of the data, which are skewed towards Expansion relations, the f-score is the more important measure to track.","{'title': '7 Classification Results', 'number': '6'}"
Our random baseline is the f-score one would achieve by randomly assigning classes in proportion to its true distribution in the test set.,"{'title': '7 Classification Results', 'number': '6'}"
"The best results for all four tasks are considerably higher than random prediction, but still low overall.","{'title': '7 Classification Results', 'number': '6'}"
Our features provide 6% to 18% absolute improvements in f-score over the baseline for each of the four tasks.,"{'title': '7 Classification Results', 'number': '6'}"
The largest gain was in the Contingency versus Other prediction task.,"{'title': '7 Classification Results', 'number': '6'}"
The least improvement was for distinguishing Expansion versus Other.,"{'title': '7 Classification Results', 'number': '6'}"
"However, since Expansion forms the largest class of relations, its f-score is still the highest overall.","{'title': '7 Classification Results', 'number': '6'}"
We discuss the results per relation class next.,"{'title': '7 Classification Results', 'number': '6'}"
Comparison We expected that polarity features would be especially helpful for identifying Comparison relations.,"{'title': '7 Classification Results', 'number': '6'}"
"Surprisingly, polarity was actually one of the worst classes of features for Comparison, achieving an f-score of 16.33 (in contrast to using the first, last and first three words of the sentences as features, which leads to an f-score of 21.01).","{'title': '7 Classification Results', 'number': '6'}"
We examined the prevalence of positivenegative or negative-positive polarity pairs in our training set.,"{'title': '7 Classification Results', 'number': '6'}"
"30% of the Comparison examples contain one of these opposite polarity pairs, while 31% of the Other examples contain an opposite polarity pair.","{'title': '7 Classification Results', 'number': '6'}"
"To our knowledge, this is the first study to examine the prevalence of polarity words in the arguments of discourse relations in their natural distributions.","{'title': '7 Classification Results', 'number': '6'}"
"Contrary to popular belief, Comparisons do not tend to have more opposite polarity pairs.","{'title': '7 Classification Results', 'number': '6'}"
"The two most useful classes of features for recognizing Comparison relations were the first, last and first three words in the sentence and the context features that indicate the presence of a paragraph boundary or of an explicit relation just before or just after the location of the hypothesized implicit relation (19.32 f-score).","{'title': '7 Classification Results', 'number': '6'}"
"Contingency The two best features for the Contingency vs. Other distinction were verb information (36.59 f-score) and first, last and first three words in the sentence (36.75 f-score).","{'title': '7 Classification Results', 'number': '6'}"
Context again was one of the features that led to improvement.,"{'title': '7 Classification Results', 'number': '6'}"
"This makes sense, as Pitler et al. (2008) found that implicit contingencies are often found immediately following explicit comparisons.","{'title': '7 Classification Results', 'number': '6'}"
We were surprised that the polarity features were helpful for Contingency but not Comparison.,"{'title': '7 Classification Results', 'number': '6'}"
Again we looked at the prevalence of opposite polarity pairs.,"{'title': '7 Classification Results', 'number': '6'}"
"While for Comparison versus Other there was not a significant difference, for Contingency there are quite a few more opposite polarity pairs (52%) than for not Contingency (41%).","{'title': '7 Classification Results', 'number': '6'}"
The language model features were completely useless for distinguishing contingencies from other relations.,"{'title': '7 Classification Results', 'number': '6'}"
"Expansion As Expansion is the majority class in the natural distribution, recall is less of a problem than precision.","{'title': '7 Classification Results', 'number': '6'}"
The features that help achieve the best f-score are all features that were found to be useful in identifying other relations.,"{'title': '7 Classification Results', 'number': '6'}"
"Polarity tags, Inquirer tags and context were the best features for identifying expansions with f-scores around 70%.","{'title': '7 Classification Results', 'number': '6'}"
"Temporal Implicit temporal relations are relatively rare, making up only about 5% of our test set.","{'title': '7 Classification Results', 'number': '6'}"
Most temporal relations are explicitly marked with a connective like “when” or “after”.,"{'title': '7 Classification Results', 'number': '6'}"
"Yet again, the first and last words of the sentence turned out to be useful indicators for temporal relations (15.93 f-score).","{'title': '7 Classification Results', 'number': '6'}"
The importance of the first and last words for this distinction is clear.,"{'title': '7 Classification Results', 'number': '6'}"
It derives from the fact that temporal implicits often contain words like “yesterday” or “Monday” at the end of the sentence.,"{'title': '7 Classification Results', 'number': '6'}"
Context is the next most helpful feature for temporal relations.,"{'title': '7 Classification Results', 'number': '6'}"
"For Comparison and Contingency, we analyze the behavior of word pair features under several different settings.","{'title': '7 Classification Results', 'number': '6'}"
Specifically we want to address two important related questions raised in recent work by others: (i) is unannotated data from explicits useful for training models that disambiguate implicit discourse relations and (ii) are explicit and implicit relations intrinsically different from each other.,"{'title': '7 Classification Results', 'number': '6'}"
Wordpairs-TextRels is the worst approach.,"{'title': '7 Classification Results', 'number': '6'}"
The best use of word pair features is Wordpairsselected.,"{'title': '7 Classification Results', 'number': '6'}"
This model gives 4% better absolute fscore for Comparison and 14% for Contingency over Wordpairs-TextRels.,"{'title': '7 Classification Results', 'number': '6'}"
"In this setting the TextRels data was used to choose the word pair features, but the probabilities for each feature were estimated using the training portion of the PDTB implicit examples.","{'title': '7 Classification Results', 'number': '6'}"
"We also confirm that even within the PDTB, information from annotated explicit relations (Wordpairs-PDTBExpl) is not as helpful as information from annotated implicit relations (Wordpairs-PDTBImpl).","{'title': '7 Classification Results', 'number': '6'}"
"The absolute difference in f-score between the two models is close to 2% for Comparison, and 6% for Contingency.","{'title': '7 Classification Results', 'number': '6'}"
"Adding other features to word pairs leads to improved performance for Contingency, Expansion and Temporal relations, but not for Comparison.","{'title': '7 Classification Results', 'number': '6'}"
"For contingency detection, the best combination of our features included polarity, verb information, first and last words, modality, context with Wordpairs-selected.","{'title': '7 Classification Results', 'number': '6'}"
"This combination led to a definite improvement, reaching an f-score of 47.13 (16% absolute improvement in f-score over Wordpairs-TextRels).","{'title': '7 Classification Results', 'number': '6'}"
"For detecting expansions, the best combination of our features (polarity+Inquirer tags+context) outperformed Wordpairs-PDTBImpl by a wide margin, close to 13% absolute improvement (fscores of 76.42 and 63.84 respectively).","{'title': '7 Classification Results', 'number': '6'}"
"Our results from the previous section show that classification of implicits benefits from information about nearby relations, and so we expected improvements using a sequence model, rather than classifying each relation independently.","{'title': '7 Classification Results', 'number': '6'}"
"We trained a CRF classifier (Lafferty et al., 2001) over the sequence of implicit examples from all documents in sections 02 to 20.","{'title': '7 Classification Results', 'number': '6'}"
The test set is the same as used for the 2-way classifiers.,"{'title': '7 Classification Results', 'number': '6'}"
We compare against a 6-way4 Naive Bayes classifier.,"{'title': '7 Classification Results', 'number': '6'}"
Only word pairs were used as features for both.,"{'title': '7 Classification Results', 'number': '6'}"
Overall 6-way prediction accuracy is 43.27% for the Naive Bayes model and 44.58% for the CRF model.,"{'title': '7 Classification Results', 'number': '6'}"
"We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions).","{'title': '8 Conclusion', 'number': '7'}"
"Also unlike prior work, we separate the task from the easier task of explicit discourse prediction.","{'title': '8 Conclusion', 'number': '7'}"
"Our experiments demonstrate that features developed to capture word polarity, verb classes and orientation, as well as some lexical features are strong indicators of the type of discourse relation.","{'title': '8 Conclusion', 'number': '7'}"
We analyze word pair features used in prior work that were intended to capture such semantic oppositions.,"{'title': '8 Conclusion', 'number': '7'}"
We show that the features in fact do not capture semantic relation but rather give information about function word co-occurrences.,"{'title': '8 Conclusion', 'number': '7'}"
"However, they are still a useful source of information for discourse relation prediction.","{'title': '8 Conclusion', 'number': '7'}"
"The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations.","{'title': '8 Conclusion', 'number': '7'}"
"Context, in terms of paragraph boundaries and nearby explicit relations, also proves to be useful for the prediction of implicit discourse relations.","{'title': '8 Conclusion', 'number': '7'}"
"It is helpful when added as a feature in a standard, instance-by-instance learning model.","{'title': '8 Conclusion', 'number': '7'}"
A sequence model also leads to over 1% absolute improvement for the task.,"{'title': '8 Conclusion', 'number': '7'}"
"This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.","{'title': '9 Acknowledgments', 'number': '8'}"
We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.,"{'title': '9 Acknowledgments', 'number': '8'}"
