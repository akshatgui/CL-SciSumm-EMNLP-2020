col1,col2
for Recall-Oriented Understudy for Gisting Evaluation.,{}
It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.,{}
"The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.",{}
This paper introduces four different included in the summarization evaluation package and their evaluations.,{}
"Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.",{}
"Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001).","{'title': '1 Introduction', 'number': '1'}"
"However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts.","{'title': '1 Introduction', 'number': '1'}"
This is very expensive and difficult to conduct in a frequent basis.,"{'title': '1 Introduction', 'number': '1'}"
"Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years.","{'title': '1 Introduction', 'number': '1'}"
"For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries.","{'title': '1 Introduction', 'number': '1'}"
"These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence.","{'title': '1 Introduction', 'number': '1'}"
"However, they did not show how the results of these automatic evaluation methods correlate to human judgments.","{'title': '1 Introduction', 'number': '1'}"
"Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations.","{'title': '1 Introduction', 'number': '1'}"
ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.,"{'title': '1 Introduction', 'number': '1'}"
It includes several automatic evaluation methods that measure the similarity between summaries.,"{'title': '1 Introduction', 'number': '1'}"
"We describe ROUGE-N in Section 2, ROUGE-L in Section 3, ROUGE-W in Section 4, and ROUGE-S in Section 5.","{'title': '1 Introduction', 'number': '1'}"
"Section 6 shows how these measures correlate with human judgments using DUC 2001, 2002, and 2003 data.","{'title': '1 Introduction', 'number': '1'}"
Section 7 concludes this paper and discusses future directions.,"{'title': '1 Introduction', 'number': '1'}"
"Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"ROUGE-N is computed as follows: Where n stands for the length of the n-gram, gramn, and Countmatch(gramn) is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
It is clear that ROUGE-N is a recall-related measure because the denominator of the equation is the total sum of the number of n-grams occurring at the reference summary side.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
BLEU measures how well a candidate translation matches a set of reference translations by counting the percentage of n-grams in the candidate translation overlapping with the references.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
Please see Papineni et al. (2001) for details about BLEU.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
Note that the number of n-grams in the denominator of the ROUGE-N formula increases as we add more references.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
This is intuitive and reasonable because there might exist multiple good summaries.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"Every time we add a reference into the pool, we expand the space of alternative summaries.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"By controlling what types of references we add to the reference pool, we can design evaluations that focus on different aspects of summarization.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
Also note that the numerator sums over all reference summaries.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
This effectively gives more weight to matching n-grams occurring in multiple references.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
Therefore a candidate summary that contains words shared by more references is favored by the ROUGE-N measure.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
This is again very intuitive and reasonable because we normally prefer a candidate summary that is more similar to consensus among reference summaries.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"So far, we only demonstrated how to compute ROUGE-N using a single reference.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"When multiple references are used, we compute pairwise summarylevel ROUGE-N between a candidate summary s and every reference, ri, in the reference set.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
We then take the maximum of pairwise summary-level ROUGE-N scores as the final multiple reference ROUGE-N score.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"This can be written as follows: This procedure is also applied to computation of ROUGE-L (Section 3), ROUGE-W (Section 4), and ROUGE-S (Section 5).","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"In the implementation, we use a Jackknifing procedure.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"Given M references, we compute the best score over M sets of M-1 references.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
The final ROUGE-N score is the average of the M ROUGE-N scores using different M-1 references.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference summaries are usually the only human summaries available.,"{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"Using this procedure, we are able to estimate average human performance by averaging M ROUGE-N scores of one reference vs. the rest M-1 references.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"Although the Jackknifing procedure is not necessary when we just want to compute ROUGE scores using multiple references, it is applied in all ROUGE score computations in the ROUGE evaluation package.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"In the next section, we describe a ROUGE measure based on longest common subsequences between two summaries.","{'title': '2 ROUGE-N: N-gram Co-Occurrence Statistics', 'number': '2'}"
"A sequence Z = [z1, z2, ..., zn] is a subsequence of another sequence X = [x1, x2, ..., xm], if there exists a strict increasing sequence [i1, i2, ..., ik] of indices of X such that for all j = 1, 2, ..., k, we have xij = zj (Cormen et al., 1989).","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Given two sequences X and Y, the longest common subsequence (LCS) of X and Y is a common subsequence with maximum length.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
LCS has been used in identifying cognate candidates during construction of N-best translation lexicon from parallel text.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
He used LCS as an approximate string matching algorithm.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Saggion et al. (2002) used normalized pairwise LCS to compare similarity between two texts in automatic summarization evaluation.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"To apply LCS in summarization evaluation, we view a summary sentence as a sequence of words.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"The intuition is that the longer the LCS of two summary sentences is, the more similar the two summaries are.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"We propose using LCS-based Fmeasure to estimate the similarity between two summaries X of length m and Y of length n, assuming X is a reference summary sentence and Y is a candidate summary sentence, as follows: Where LCS(X,Y) is the length of a longest common subsequence of X and Y, and ß = Plcs/Rlcs when ?Flcs/?Rlcs=?Flcs/?Plcs.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"In DUC, ß is set to a very big number (?","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
8 ).,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Therefore, only Rlcs is considered.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"We call the LCS-based F-measure, i.e.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Equation 4, ROUGE-L. Notice that ROUGE-L is 1 when X = Y; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen, 1979).","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
The composite factors are LCS-based recall and precision in this case.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Melamed et al. (2003) used unigram F-measure to estimate machine translation quality and showed that unigram Fmeasure was as good as BLEU.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as n-grams.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
ROUGE-L as defined in Equation 4 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X (reference summary sentence) that are also present in Y (candidate summary sentence); while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all cooccurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"By only awarding credit to in-sequence unigram matches, ROUGE-L also captures sentence level structure in a natural way.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Consider the following example: S1. police killed the gunman S2. police kill the gunman S3. the gunman kill police We only consider ROUGE-2, i.e.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"N=2, for the purpose of explanation.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Using S1 as the reference and S2 and S3 as the candidate summary sentences, S2 and S3 would have the same ROUGE-2 score, since they both have one bigram, i.e.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
“the gunman”.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"However, S2 and S3 have very different meanings.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"In the case of ROUGE-L, S2 has a score of 3/4 = 0.75 and S3 has a score of 2/4 = 0.5, with ß = 1.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Therefore S2 is better than S3 according to ROUGE-L.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
This example also illustrated that ROUGE-L can work reliably at sentence level.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"However, LCS suffers one disadvantage that it only counts the main in-sequence words; therefore, other alternative LCSes and shorter sequences are not reflected in the final score.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"For example, given the following candidate sentence: S4. the gunman police killed Using S1 as its reference, LCS counts either “the gunman” or “police killed”, but not both; therefore, S4 has the same ROUGE-L score as S3.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
ROUGE-2 would prefer S4 than S3.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Previous section described how to compute sentence-level LCS-based F-measure score.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"When applying to summary-level, we take the union LCS matches between a reference summary sentence, ri, and every candidate summary sentence, cj.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Given a reference summary of u sentences containing a total of m words and a candidate summary of v sentences containing a total of n words, the summary-level LCS-based F-measure can be computed as follows: Again ß is set to a very big number (?","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"8 ) in DUC, i.e. only Rlcs is considered.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"LCS∪ (ri, C) is the LCS score of the union longest common subsequence between reference sentence ri and candidate summary C. For example, if ri = w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of ri and c1 is “w1 w2” and the longest common subsequence of ri and c2 is “w1 w3 w5”.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"The union longest common subsequence of ri, c1, and c2 is “w1 w2 w3 w5” and LCS∪ (ri, C) = 4/5.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"The normalized pairwise LCS proposed by Radev et al. (page 51, 2002) between two summaries S1 and S2, LCS(S1 ,S2)MEAD , is written as follows: Assuming S1 has m words and S2 has n words, Equation 8 can be rewritten as Equation 9 due to symmetry: We then define MEAD LCS recall (Rlcs-MEAD) and MEAD LCS precision (Plcs-MEAD) as follows: We can rewrite Equation (9) in terms of Rlcs-MEAD and Plcs-MEAD with a constant parameter ß = 1 as follows: Equation 12 shows that normalized pairwise LCS as defined in Radev et al.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
(2002) and implemented in MEAD is also a F-measure with ß = 1.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Sentencelevel normalized pairwise LCS is the same as ROUGE-L with ß = 1.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
"Besides setting ß = 1, summary-level normalized pairwise LCS is different from ROUGE-L in how a sentence gets its LCS score from its references.","{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
Normalized pairwise LCS takes the best LCS score while ROUGE-L takes the union LCS score.,"{'title': '3 ROUGE-L: Longest Common Subsequence', 'number': '3'}"
LCS has many nice properties as we have described in the previous sections.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: Y1 and Y2 have the same ROUGE-L score.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"However, in this case, Y1 should be the better choice than Y2 because Y1 has consecutive matches.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Given two sentences X and Y, the WLCS score of X and Y can be computed using the following dynamic programming procedure: For (j = 1; j <= n; j++) If xi = yj Then // the length of consecutive matches at // position i-1 and j -1 Where c is the dynamic programming table, c(i,j) stores the WLCS score ending at word xi of X and yj of Y, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j).","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"In other words, consecutive matches are awarded more scores than non-consecutive matches.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"For example, f(k)=ak – b when k >= 0, and a, b > 0.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
This function charges a gap penalty of –b for each non-consecutive n-gram sequences.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
Another possible function family is the polynomial family of the form ka where -a > 1.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"For example, f(k)=k2 has a close form inverse function f -1(k)=k1/2.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"F-measure based on WLCS can be computed as follows, given two sequences X of length m and Y of length n: Where f -1 is the inverse function of f. In DUC, ß is set to a very big number (?","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
8 ).,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Therefore, only Rwlcs is considered.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"We call the WLCS-based Fmeasure, i.e.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Equation 15, ROUGE-W.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Using Equation 15 and f(k)=k2 as the weighting function, the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Therefore, Y1 would be ranked higher than Y2 using WLCS.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
We use the polynomial function of the form ka in the ROUGE evaluation package.,"{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"In the next section, we introduce the skip-bigram co-occurrence statistics.","{'title': '4 ROUGE-W: Weighted Longest Common Subsequence', 'number': '4'}"
"Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
Skip-bigram cooccurrence statistics measure the overlap of skipbigrams between a candidate translation and a set of reference translations.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Using the example given in Section 3.1: each sentence has C(4,2)1 = 6 skip-bigrams.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"For example, S1 has the following skip-bigrams: (“police killed”, “police the”, “police gunman”, “killed the”, “killed gunman”, “the gunman”) S2 has three skip-bigram matches with S1 (“police the”, “police gunman”, “the gunman”), S3 has one skip-bigram match with S1 (“the gunman”), and S4 has two skip-bigram matches with S1 (“police killed”, “the gunman”).","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram-based F-measure as follows: Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y, ß controlling the relative importance of Pskip2 and Rskip2, and C is the combination function.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"We call the skip-bigram-based Fmeasure, i.e.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Equation 18, ROUGE-S.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Using Equation 18 with ß = 1 and S1 as the reference, S2’s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Therefore, S2 is better than S3 and S4, and S4 is better than S3.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
This result is more intuitive than using BLEU-2 and ROUGE-L. One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Applying skip-bigram without any constraint on the distance between the words, spurious matches such as “the the” or “of in” might be counted as valid matches.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"To reduce these spurious matches, we can limit the maximum skip distance, dskip, between two in-order words that is allowed to form a skip-bigram.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"For example, if we set dskip to 0 then ROUGE-S is equivalent to bigram overlap Fmeasure.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"Adjusting Equations 16, 17, and 18 to use maximum skip distance limit is straightforward: we only count the skip-bigram matches, SKIP2(X,Y), within the maximum skip distance and replace denominators of Equations 16, C(m,2), and 17, C(n,2), with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
One potential problem for ROUGE-S is that it does not give any credit to a candidate sentence if the sentence does not have any word pair co-occurring with its references.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"For example, the following sentence has a ROUGE-S score of zero: S5. gunman the killed police S5 is the exact reverse of S1 and there is no skip bigram match between them.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"However, we would like to differentiate sentences similar to S5 from sentences that do not have single word cooccurrence with S1.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"To achieve this, we extend ROUGE-S with the addition of unigram as counting unit.","{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
The extended version is called ROUGE-SU.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
We can also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning of candidate and reference sentences.,"{'title': '5 ROUGE-S: Skip-Bigram Co-Occurrence Statistics', 'number': '5'}"
"To assess the effectiveness of ROUGE measures, we compute the correlation between ROUGE assigned summary scores and human assigned summary scores.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The intuition is that a good evaluation measure should assign a good score to a good summary and a bad score to a bad summary.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The ground truth is based on human assigned scores.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Acquiring human judgments are usually very expensive; fortunately, we have DUC 2001, 2002, and 2003 evaluation data that include human judgments for the following: Besides these human judgments, we also have 3 sets of manual summaries for DUC 2001, 2 sets for DUC 2002, and 4 sets for DUC 2003.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Human judges assigned content coverage scores to a candidate summary by examining the percentage of content overlap between a manual summary unit, i.e. elementary discourse unit or sentence, and the candidate summary using Summary Evaluation Environment3 (SEE) developed by the University of Southern California’s Information Sciences Institute (ISI).","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The overall candidate summary score is the average of the content coverage scores of all the units in the manual summary.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
Note that human judges used only one manual summary in all the evaluations although multiple alternative summaries were available.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"With the DUC data, we computed Pearson’s product moment correlation coefficients, Spearman’s rank order correlation coefficients, and Kendall’s correlation coefficients between systems’ average ROUGE scores and their human assigned average coverage scores using single reference and multiple references.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"To investigate the effect of stemming and inclusion or exclusion of stopwords, we also ran experiments over original automatic and manual summaries (CASE set), stemmed4 version of the summaries (STEM set), and stopped version of the summaries (STOP set).","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"For example, we computed ROUGE scores for the 12 systems participated in the DUC 2001 single document summarization evaluation using the CASE set with single reference and then calculated the three correlation scores for these 12 systems’ ROUGE scores vs. human assigned average coverage scores.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
After that we repeated the process using multiple references and then using STEM and STOP sets.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected for each ROUGE measure and each DUC task.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"To assess the significance of the results, we applied bootstrap resampling technique (Davison and Hinkley, 1997) to estimate 95% confidence intervals for every correlation computation.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"17 ROUGE measures were tested for each run using ROUGE evaluation package v1.2.1: ROUGE-N with N = 1 to 9, ROUGE-L, ROUGE-W with weighting factor a = 1.2, ROUGE-S and ROUGE-SU with maximum skip distance d,1,,o = 1, 4, and 9.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Due to limitation of space, we only report correlation analysis results based on Pearson’s correlation coefficient.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
Correlation analyses based on Spearman’s and Kendall’s correlation coefficients are tracking Pearson’s very closely and will be posted later at the ROUGE website5 for reference.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The critical value6 for Pearson’s correlation is 0.632 at 95% confidence with 8 degrees of freedom.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
Table 1 shows the Pearson’s correlation coefficients of the 17 ROUGE measures vs. human judgments on DUC 2001 and 2002 100 words single document summarization data.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The best values in each column are marked with dark (green) color and statistically equivalent values to the best values are marked with gray.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"We found that correlations were not affected by stemming or removal of stopwords in this data set, ROUGE-2 performed better among the ROUGE-N variants, ROUGE-L, ROUGE-W, and ROUGE-S were all performing well, and using multiple references improved performance though not much.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
All ROUGE measures achieved very good correlation with human judgments in the DUC 2002 data.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
This might due to the double sample size in DUC 2002 (295 vs. 149 in DUC 2001) for each system.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
Table 2 shows the correlation analysis results on the DUC 2003 single document very short summary data.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"We found that ROUGE-1, ROUGE-L, ROUGESU4 and 9, and ROUGE-W were very good measures in this category, ROUGE-N with N > 1 performed significantly worse than all other measures, and exclusion of stopwords improved performance in general except for ROUGE-1.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Due to the large number of samples (624) in this data set, using multiple references did not improve correlations.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"In Table 3 A1, A2, and A3, we show correlation analysis results on DUC 2001, 2002, and 2003 100 words multi-document summarization data.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The results indicated that using multiple references improved correlation and exclusion of stopwords usually improved performance.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"ROUGE-1, 2, and 3 performed fine but were not consistent.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"ROUGE-1, ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGESU9 with stopword removal had correlation above 0.70.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
ROUGE-L and ROUGE-W did not work well in this set of data.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Table 3 C, D1, D2, E1, E2, and F show the correlation analyses using multiple references on the rest of DUC data.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
These results again suggested that exclusion of stopwords achieved better performance especially in multi-document summaries of 50 words.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Better correlations (> 0.70) were observed on long summary tasks, i.e.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
200 and 400 words summaries.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The relative performance of ROUGE measures followed the pattern of the 100 words multi-document summarization task.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"Comparing the results in Table 3 with Tables 1 and 2, we found that correlation values in the multidocument tasks rarely reached high 90% except in long summary tasks.","{'title': '6 Evaluations of ROUGE', 'number': '6'}"
One possible explanation of this outcome is that we did not have large amount of samples for the multi-document tasks.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
In the single document summarization tasks we had over 100 samples; while we only had about 30 samples in the multi-document tasks.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
The only tasks that had over 30 samples was from DUC 2002 and the correlations of ROUGE measures with human judgments on the 100 words summary task were much better and more stable than similar tasks in DUC 2001 and 2003.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
Statistically stable human judgments of system performance might not be obtained due to lack of samples and this in turn caused instability of correlation analyses.,"{'title': '6 Evaluations of ROUGE', 'number': '6'}"
"In this paper, we introduced ROUGE, an automatic evaluation package for summarization, and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data.","{'title': '7 Conclusions', 'number': '7'}"
"To check the significance of the results, we estimated confidence intervals of correlations using bootstrap resampling.","{'title': '7 Conclusions', 'number': '7'}"
"We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.","{'title': '7 Conclusions', 'number': '7'}"
"In summary, we showed that the ROUGE package could be used effectively in automatic evaluation of summaries.","{'title': '7 Conclusions', 'number': '7'}"
"In a separate study (Lin and Och, 2004), ROUGE-L, W, and S were also shown to be very effective in automatic evaluation of machine translation.","{'title': '7 Conclusions', 'number': '7'}"
"The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004).","{'title': '7 Conclusions', 'number': '7'}"
"However, how to achieve high correlation with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic.","{'title': '7 Conclusions', 'number': '7'}"
"The author would like to thank the anonymous reviewers for their constructive comments, Paul Over at NIST, U.S.A, and ROUGE users around the world for testing and providing useful feedback on earlier versions of the ROUGE evaluation package, and the DARPA TIDES project for supporting this research.","{'title': '8 Acknowledgements', 'number': '8'}"
