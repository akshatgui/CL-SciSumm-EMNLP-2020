col1,col2
"first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).",{}
"Next, by annealing the free parameter that controls this bias, we achieve further improvements.",{}
"We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.",{}
"We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this to date.",{}
"Our method, is a general technique with broad applicability to hidden-structure discovery problems.",{}
Inducing a weighted context-free grammar from flat text is a hard problem.,"{'title': '1 Introduction', 'number': '1'}"
"A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979).","{'title': '1 Introduction', 'number': '1'}"
EM’s mediocre performance (Table 1) reflects two problems.,"{'title': '1 Introduction', 'number': '1'}"
"First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure.","{'title': '1 Introduction', 'number': '1'}"
"Second, the likelihood surface is not globally concave, and learners such as the EM algorithm can get trapped on local maxima (Charniak, 1993).","{'title': '1 Introduction', 'number': '1'}"
"We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3).","{'title': '1 Introduction', 'number': '1'}"
"We then explore how gradually changing S over time affects learning (§4): we start out with a strong preference for short dependencies, then relax the preference.","{'title': '1 Introduction', 'number': '1'}"
"The new approach, structural annealing, often gives superior performance.","{'title': '1 Introduction', 'number': '1'}"
An alternative structural bias is explored in §5.,"{'title': '1 Introduction', 'number': '1'}"
"This approach views a sentence as a sequence of one or more yields of separate, independent trees.","{'title': '1 Introduction', 'number': '1'}"
"The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically.","{'title': '1 Introduction', 'number': '1'}"
This allows the learner to accept hypotheses that explain the sentences as independent pieces.,"{'title': '1 Introduction', 'number': '1'}"
"In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias.","{'title': '1 Introduction', 'number': '1'}"
In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).,"{'title': '2 Task and Model', 'number': '2'}"
"The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997).","{'title': '2 Task and Model', 'number': '2'}"
"Let x = (x1, x2, ..., xn) be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence.","{'title': '2 Task and Model', 'number': '2'}"
"A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its sets of left and right dependents, respectively.","{'title': '2 Task and Model', 'number': '2'}"
"The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles or crossing dependencies.2 yleft(0) is taken to be empty, and yright(0) contains the sentence’s single head.","{'title': '2 Task and Model', 'number': '2'}"
Let yi denote the subtree rooted at position i.,"{'title': '2 Task and Model', 'number': '2'}"
"The probability P(yi  |xi) of generating this subtree, given its head word xi, is defined recursively: where firsty(j) is a predicate defined to be true iff xj is the closest child (on either side) to its parent xi.","{'title': '2 Task and Model', 'number': '2'}"
"The probability of the entire tree is given by pe(x, y) = P(y0  |$).","{'title': '2 Task and Model', 'number': '2'}"
The parameters O are the conditional distributions pstop and pchild.,"{'title': '2 Task and Model', 'number': '2'}"
Experimental baseline: EM.,"{'title': '2 Task and Model', 'number': '2'}"
"Following common practice, we always replace words by part-ofspeech (POS) tags before training or testing.","{'title': '2 Task and Model', 'number': '2'}"
We used the EM algorithm to train this model on POS sequences in six languages.,"{'title': '2 Task and Model', 'number': '2'}"
Complete experimental details are given in the appendix.,"{'title': '2 Task and Model', 'number': '2'}"
Performance with unsupervised and supervised model selection across different λ values in add-λ smoothing and three initializers O(0) is reported in Table 1.,"{'title': '2 Task and Model', 'number': '2'}"
The supervised-selected model is in the 40–55% F1-accuracy range on directed dependency attachments.,"{'title': '2 Task and Model', 'number': '2'}"
(Here F1 Pz� precision Pz� recall; see appendix.),"{'title': '2 Task and Model', 'number': '2'}"
"Supervised model selection, which uses a small annotated development set, performs almost as well as the oracle, but unsupervised model selection, which selects the model that maximizes likelihood on an unannotated development set, is often much worse.","{'title': '2 Task and Model', 'number': '2'}"
Hidden-variable estimation algorithms— including EM—typically work by iteratively manipulating the model parameters O to improve an objective function F(O).,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"EM explicitly alternates between the computation of a posterior distribution over hypotheses, pp(y  |x) (where y is any tree with yield x), and computing a new parameter estimate O.3 with a locality bias at varying S. Each curve corresponds to a different language and shows performance of supervised model selection within a given S, across A and O1°) values.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
(See Table 3 for performance of models selected across Ss.),"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"We decode with S = 0, though we found that keeping the training-time value of S would have had almost no effect.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
The EM baseline corresponds to S = 0.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
One way to bias a learner toward local explanations is to penalize longer attachments.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
"Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving: where y(i) = yleft(i) U yright(i).","{'title': '3 Locality Bias among Trees', 'number': '3'}"
"Note that if δ = 0, we have the original model.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
"As δ —* −oc, the new model p' will favor parses with shorter dependencies.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
"The dynamic programming algorithms remain the same as before, with the appropriate ea|i−j |factor multiplied in at each attachment between xi and xj.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
"Note that when δ = 0, pe - pe.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
Experiment.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"We applied a locality bias to the same dependency model by setting δ to different we show performance with add-10 smoothing, the all-zero initializer, for three languages with three different initial values 6o.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
Time progresses from left to right.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
Note that it is generally best to start at 6o « 0; note also the importance of picking the right point on the curve to stop.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"See Table 3 for performance of models selected across smoothing, initialization, starting, and stopping choices, in all six languages. values in [−1, 0.2] (see Eq.","{'title': '3 Locality Bias among Trees', 'number': '3'}"
2).,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
The same initializers Off0) and smoothing conditions were tested.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
Performance of supervised model selection among models trained at different δ values is plotted in Fig.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
1.,"{'title': '3 Locality Bias among Trees', 'number': '3'}"
"When a model is selected across all conditions (3 initializers x 6 smoothing values x 7 δs) using annotated development data, performance is notably better than the EM baseline using the same selection procedure (see Table 3, second column).","{'title': '3 Locality Bias among Trees', 'number': '3'}"
The central idea of this paper is to gradually change (anneal) the bias δ.,"{'title': '4 Structural Annealing', 'number': '4'}"
"Early in learning, local dependencies are emphasized by setting δ « 0.","{'title': '4 Structural Annealing', 'number': '4'}"
"Then δ is iteratively increased and training repeated, using the last learned model to initialize.","{'title': '4 Structural Annealing', 'number': '4'}"
"This idea bears a strong similarity to deterministic annealing (DA), a technique used in clustering and classification to smooth out objective functions that are piecewise constant (hence discontinuous) or bumpy (non-concave) (Rose, 1998; Ueda and Nakano, 1998).","{'title': '4 Structural Annealing', 'number': '4'}"
"In unsupervised learning, DA iteratively re-estimates parameters like EM, but begins by requiring that the entropy of the posterior pp(y  |x) be maximal, then gradually relaxes this entropy constraint.","{'title': '4 Structural Annealing', 'number': '4'}"
"Since entropy is concave in O, the initial task is easy (maximize a concave, continuous function).","{'title': '4 Structural Annealing', 'number': '4'}"
"At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective.","{'title': '4 Structural Annealing', 'number': '4'}"
"By the last iteration the objective is the same as in EM, but the annealed search process has acted like a good initializer.","{'title': '4 Structural Annealing', 'number': '4'}"
This method was applied with some success to grammar induction models by Smith and Eisner (2004).,"{'title': '4 Structural Annealing', 'number': '4'}"
"In this work, instead of imposing constraints on the entropy of the model, we manipulate bias toward local hypotheses.","{'title': '4 Structural Annealing', 'number': '4'}"
"As δ increases, we penalize long dependencies less.","{'title': '4 Structural Annealing', 'number': '4'}"
"We call this structural annealing, since we are varying the strength of a soft constraint (bias) on structural hypotheses.","{'title': '4 Structural Annealing', 'number': '4'}"
"In structural annealing, the final objective would be the same as EM if our final δ, δf = 0, but we found that annealing farther (δf > 0) works much better.4 Experiment: Annealing δ.","{'title': '4 Structural Annealing', 'number': '4'}"
We experimented with annealing schedules for δ.,"{'title': '4 Structural Annealing', 'number': '4'}"
"We initialized at δ0 E {−1, −0.4, −0.21, and increased δ by 0.1 (in the first case) or 0.05 (in the others) up to δf = 3.","{'title': '4 Structural Annealing', 'number': '4'}"
Models were trained to convergence at each δepoch.,"{'title': '4 Structural Annealing', 'number': '4'}"
"Model selection was applied over the same initialization and regularization conditions as before, δ0, and also over the choice of δf, with stopping allowed at any stage along the δ trajectory.","{'title': '4 Structural Annealing', 'number': '4'}"
Trajectories for three languages with three different δ0 values are plotted in Fig.,"{'title': '4 Structural Annealing', 'number': '4'}"
2.,"{'title': '4 Structural Annealing', 'number': '4'}"
"Generally speaking, δ0 « 0 performs better.","{'title': '4 Structural Annealing', 'number': '4'}"
"There is consistently an early increase in performance as δ increases, but the stopping δf matters tremendously.","{'title': '4 Structural Annealing', 'number': '4'}"
Selected annealed-δ models surpass EM in all six languages; see the third column of Table 3.,"{'title': '4 Structural Annealing', 'number': '4'}"
Note that structural annealing does not always outperform fixed-δ training (English and Portuguese).,"{'title': '4 Structural Annealing', 'number': '4'}"
"This is because we only tested a few values of δ0, since annealing requires longer runtime.","{'title': '4 Structural Annealing', 'number': '4'}"
A related way to focus on local structure early in learning is to broaden the set of hypotheses to include partial parse structures.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"If x = (x1, x2, ..., xn), the standard approach assumes that x corresponds to the vertices of a single dependency tree.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Instead, we entertain every hypothesis in which x is a sequence of yields from separate, independently-generated trees.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"For example, (x1, x2, x3) is the yield of one tree, (x4, x5) is the with structural annealing on the breakage weight 3.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Here we show performance with add-10 smoothing, the all-zero initializer, for three languages with three different initial values ,Qo.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Time progresses from left (large ,Q) to right.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"See Table 3 for performance of models selected across smoothing, initialization, and stopping choices, in all six languages. yield of a second, and (x6, ..., xn) is the yield of a third.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
One extreme hypothesis is that x is n singlenode trees.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
At the other end of the spectrum is the original set of hypotheses—full trees on x.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Each has a nonzero probability.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Segmented analyses are intermediate representations that may be helpful for a learner to use to formulate notions of probable local structure, without committing to full trees.5 We only allow unobserved breaks, never positing a hard segmentation of the training sentences.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Over time, we increase the bias against broken structures, forcing the learner to commit most of its probability mass to full trees.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
At first glance broadening the hypothesis space to entertain all 2n−1 possible segmentations may seem expensive.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
In fact the dynamic programming computation is almost the same as summing or maximizing over connected dependency trees.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"For the latter, we use an inside-outside algorithm that computes a score for every parse tree by computing the scores of items, or partial structures, through a bottom-up process.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Smaller items are built first, then assembled using a set of rules defining how larger items can be built.6 Now note that any sequence of partial trees over x can be constructed by combining the same items into trees.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"The only difference is that we are willing to consider unassembled sequences of these partial trees as hypotheses, in addition to the fully connected trees.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"One way to accomplish this in terms of yright(0) is to say that the root, $, is allowed to have multiple children, instead of just one.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Here, these children are independent of each other (e.g., generated by a unigram Markov model).","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words—can give guaranteed O(nk2) runtime with little to no loss in accuracy (for simple models).","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"This constraint could lead to highly contrived parse trees, or none at all, for some sentences—both are avoided by the allowance of segmentation into a sequence of trees (each attached to $).","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
The construction of the “vine” (sequence of $’s children) takes only O(n) time once the chart has been assembled.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Our broadened hypothesis model is a probabilistic vine grammar with a unigram model over $’s children.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"We allow (but do not require) segmentation of sentences, where each independent child of $ is the root of one of the segments.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
We do not impose any constraints on dependency length.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Now the total probability of an n-length sentence x, marginalizing over its hidden structures, sums up not only over trees, but over segmentations of x.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"For completeness, we must include a probability model over the number of trees generated, which could be anywhere from 1 to n. The model over the number T of trees given a sentence of length n will take the following log-linear form: where Q E R is the sole parameter.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"When Q = 0, every value of T is equally likely.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"For Q « 0, the model prefers larger structures with few breaks.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"At the limit (Q —* −oc), we achieve the standard learning setting, where the model must explain x using a single tree.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"We start however at Q » 0, where the model prefers smaller trees with more breaks, in the limit preferring each word in x to be its own tree.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"We could describe “brokenness” as a feature in the model whose weight, Q, is chosen extrinsically (and time-dependently), rather than empirically—just as was done with S. Annealing β resembles the popular bootstrapping technique (Yarowsky, 1995), which starts out aiming for high precision, and gradually improves coverage over time.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"With strong bias (β » 0), we seek a model that maintains high dependency precision on (non-$) attachments by attaching most tags to $.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Over time, as this is iteratively weakened (β -* −oo), we hope to improve coverage (dependency recall).","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Bootstrapping was applied to syntax learning by Steedman et al. (2003).,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Our approach differs in being able to remain partly agnostic about each tag’s true parent (e.g., by giving 50% probability to attaching to $), whereas Steedman et al. make a hard decision to retrain on a whole sentence fully or leave it out fully.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"In earlier work, Brill and Marcus (1992) adopted a “local first” iterative merge strategy for discovering phrase structure.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Experiment: Annealing β.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
We experimented with different annealing schedules for β.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"The initial value of β, β0, was one of {−1 , 0, 2 }.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"After EM training, β was diminished by 10;this was repeated down to a value of βf = −3.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Performance after training at each β value is shown in Fig.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"3.7 We see that, typically, there is a sharp increase in performance somewhere during training, which typically lessens as β -* −oo.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
Starting β too high can also damage performance.,"{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"This method, then, is not robust to the choice of λ, β0, or βf, nor does it always do as well as annealing δ, although considerable gains are possible; see the fifth column of Table 3.","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"By testing models trained with afixed value of β (for values in [−1,1]), we ascertained that the performance improvement is due largely to annealing, not just the injection of segmentation bias (fourth vs. fifth column of Table 3).8","{'title': '5 Structural Bias via Segmentation', 'number': '5'}"
"Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by EM.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"CE was found to outperform EM on the task of focus in this paper, when applied to English data (Smith and Eisner, 2005b).","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Here we review the method briefly, show how it performs across languages, and demonstrate that it can be combined effectively with structural bias.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Contrastive training defines for each example xi a class of presumably poor, but similar, instances called the “neighborhood,” N(xi), and seeks to maximize At this point we switch to a log-linear (rather than stochastic) parameterization of the same weighted grammar, for ease of numerical optimization.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"All this means is that O (specifically, pstop and pchild in Eq.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
1) is now a set of nonnegative weights rather than probabilities.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Neighborhoods that can be expressed as finitestate lattices built from xi were shown to give significant improvements in dependency parser quality over EM.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Performance of CE using two of those neighborhoods on the current model and datasets is shown in Table 2.9 0-mean diagonal Gaussian smoothing was applied, with different variances, and model selection was applied over smoothing conditions and the same initializers as before.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Four of the languages have at least one effective CE condition, supporting our previous English results (Smith and Eisner, 2005b), but CE was harmful for Bulgarian and Mandarin.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Perhaps better neighborhoods exist for these languages, or there is some ideal neighborhood that would perform well for all languages.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Our approach of allowing broken trees (§5) is a natural extension of the CE framework.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Contrastive estimation views learning as a process of moving posterior probability mass from (implicit) negative examples to (explicit) positive examples.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"The positive evidence, as in MLE, is taken to be the observed data.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"As originally proposed, CE allowed a redefinition of the implicit negative evidence from “all other sentences” (as in MLE) to “sentences like xi, but perturbed.” Allowing segmentation of the training sentences redefines the positive and negative evidence.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Rather than moving probability mass only to full analyses of the training example xi, we also allow probability mass to go to partial analyses of xi.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"By injecting a bias (S 7� 0 or Q > −oc) among tree hypotheses, however, we have gone beyond the CE framework.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"We have added features to the tree model (dependency length-sum, number of breaks), whose weights we extrinsically manipulate over time to impose locality bias CN and improve search on CN.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Another idea, not explored here, is to change the contents of the neighborhood N over time.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Experiment: Locality Bias within CE.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"We combined CE with a fixed-S locality bias for neighborhoods that were successful in the earlier CE experiment, namely DELETEORTRANSPOSE1 for German, English, Turkish, and Portuguese.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Our results, shown in the seventh column of Table 3, show that, in all cases except Turkish, the combination improves over either technique on its own.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
We leave exploration of structural annealing with CE to future work.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Experiment: Segmentation Bias within CE.,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"For (language, N) pairs where CE was effective, we trained models using CE with a fixedQ segmentation model.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Across conditions (Q E [−1,1]), these models performed very badly, hypothesizing extremely local parse trees: typically over 90% of dependencies were length 1 and pointed in the same direction, compared with the 60–70% length-1 rate seen in gold standards.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"To understand why, consider that the CE goal is to maximize the score of a sentence and all its segmentations while minimizing the scores of neighborhood sentences and their segmentations.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"An ngram model can accomplish this, since the same n-grams are present in all segmentations of x, and (some) different n-grams appear in N(x) (for LENGTH and DELETEORTRANSPOSE1).","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"A bigram-like model that favors monotone branching, then, is not a bad choice for a CE learner that must account for segmentations of x and N(x).","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
Why doesn’t CE without segmentation resort to n-gram-like models?,"{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"Inspection of models trained using the standard CE method (no segmentation) with transposition-based neighborhoods TRANSPOSE1 and DELETEORTRANSPOSE1 did have high rates of length-1 dependencies, while the poorly-performing DELETE1 models found low length-1 rates.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"This suggests that a bias toward locality (“n-gram-ness”) is built into the former neighborhoods, and may partly explain why CE works when it does.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"We achieved a similar locality bias in the likelihood framework when we broadened the hypothesis space, but doing so under CE over-focuses the model on local structures.","{'title': '6 Comparison and Combination with Contrastive Estimation', 'number': '6'}"
"We compared errors made by the selected EM condition with the best overall condition, for each language.","{'title': '7 Error Analysis', 'number': '7'}"
We found that the number of corrected attachments always outnumbered the number of new errors by a factor of two or more.,"{'title': '7 Error Analysis', 'number': '7'}"
"Further, the new models are not getting better by merely reversing the direction of links made by EM; undirected accuracy also improved significantly under a sign test (p < 10−6), across all six languages.","{'title': '7 Error Analysis', 'number': '7'}"
"While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover.","{'title': '7 Error Analysis', 'number': '7'}"
"One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization.","{'title': '8 Future Work', 'number': '8'}"
This is quite observable in the results we have presented.,"{'title': '8 Future Work', 'number': '8'}"
"An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection.","{'title': '8 Future Work', 'number': '8'}"
"For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005).","{'title': '8 Future Work', 'number': '8'}"
"Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figures of merit on successful learning trajectories, or plausibility criteria (Eisner and Karakos, 2005).","{'title': '8 Future Work', 'number': '8'}"
Grammar induction serves as a tidy example for structural annealing.,"{'title': '8 Future Work', 'number': '8'}"
"In future work, we envision that other kinds of structural bias and annealing will be useful in other difficult learning problems where hidden structure is required, including machine translation, where the structure can consist of word correspondences or phrasal or recursive syntax with correspondences.","{'title': '8 Future Work', 'number': '8'}"
"The technique bears some similarity to the estimation methods described by Brown et al. (1993), which started by estimating simple models, using each model to seed the next.","{'title': '8 Future Work', 'number': '8'}"
"We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.","{'title': '9 Conclusion', 'number': '9'}"
"We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.","{'title': '9 Conclusion', 'number': '9'}"
