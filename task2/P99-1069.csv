col1,col2
Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.,{}
"We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical",{}
Probabilistic methods have revolutionized computational linguistics.,"{'title': '1 Introduction', 'number': '1'}"
They can provide a systematic treatment of preferences in parsing.,"{'title': '1 Introduction', 'number': '1'}"
"Given a suitable estimation procedure, stochastic models can be &quot;tuned&quot; to reflect the properties of a corpus.","{'title': '1 Introduction', 'number': '1'}"
"On the other hand, &quot;Unification-Based&quot; Grammars (UBGs) can express a variety of linguistically-important syntactic and semantic constraints.","{'title': '1 Introduction', 'number': '1'}"
"However, developing Stochastic &quot;Unification-based&quot; Grammars (SUBGs) has not proved as straightforward as might be hoped.","{'title': '1 Introduction', 'number': '1'}"
"The simple &quot;relative frequency&quot; estimator for PCFGs yields the maximum likelihood parameter estimate, which is to say that it minimizes the Kulback-Liebler divergence between the training and estimated distributions.","{'title': '1 Introduction', 'number': '1'}"
"On the other hand, as Abney (1997) points out, the context-sensitive dependencies that &quot;unification-based&quot; constraints introduce render the relative frequency estimator suboptimal: in general it does not maximize the likelihood and it is inconsistent.","{'title': '1 Introduction', 'number': '1'}"
"Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.","{'title': '1 Introduction', 'number': '1'}"
"However, the Monte-Carlo parameter estimation procedure that Abney proposes seems to be computationally impractical for reasonable-sized grammars.","{'title': '1 Introduction', 'number': '1'}"
Sections 3 and 4 describe two new estimation procedures which are computationally tractable.,"{'title': '1 Introduction', 'number': '1'}"
Section 5 describes an experiment with a small LFG corpus provided to us by Xerox PARC.,"{'title': '1 Introduction', 'number': '1'}"
"The log linear framework and the estimation procedures are extremely general, and they apply directly to stochastic versions of HPSG and other theories of grammar.","{'title': '1 Introduction', 'number': '1'}"
We follow the statistical literature in using the term feature to refer to the properties that parameters are associated with (we use the word &quot;attribute&quot; to refer to the attributes or features of a UBG's feature structure).,"{'title': '2 Features in SUBGs', 'number': '2'}"
Let SZ be the set of all possible grammatical or well-formed analyses.,"{'title': '2 Features in SUBGs', 'number': '2'}"
Each feature f maps a syntactic analysis to E S/ to a real value f (w).,"{'title': '2 Features in SUBGs', 'number': '2'}"
The form of a syntactic analysis depends on the underlying linguistic theory.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"For example, for a PCFG co would be parse tree, for a LFG co would be a tuple consisting of (at least) a c-structure, an fstructure and a mapping from c-structure nodes to f-structure elements, and for a Chomskyian transformational grammar co would be a derivation.","{'title': '2 Features in SUBGs', 'number': '2'}"
Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).,"{'title': '2 Features in SUBGs', 'number': '2'}"
"PCFGs, Gibbs distributions, Maximum-Entropy distributions and Markov Random Fields are all examples of log-linear models.","{'title': '2 Features in SUBGs', 'number': '2'}"
A log-linear model associates each feature f3 with a real-valued parameter 03.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"A log-linear model with m features is one in which the likelihood P(w) of an analysis w is: While the estimators described below make no assumptions about the range of the f, in the models considered here the value of each feature f2 (w) is the number of times a particular structural arrangement or configuration occurs in the analysis (..v, so fi(co) ranges over the natural numbers.","{'title': '2 Features in SUBGs', 'number': '2'}"
"For example, the features of a PCFG are indexed by productions, i.e., the value f(w) of feature f, is the number of times the ith production is used in the derivation w. This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998).","{'title': '2 Features in SUBGs', 'number': '2'}"
This tree structure has the important consequence that simple &quot;relative-frequencies&quot; yield maximumlikelihood estimates for the 02.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"Extending a PCFG model by adding additional features not associated with productions will in general add additional dependencies, destroy the tree structure, and substantially complicate maximum likelihood estimation.","{'title': '2 Features in SUBGs', 'number': '2'}"
"This is the situation for a SUBG, even if the features are production occurences.","{'title': '2 Features in SUBGs', 'number': '2'}"
The unification constraints create non-local dependencies among the productions and the dependency graph of a SUBG is usually not a tree.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"Consequently, maximum likelihood estimation is no longer a simple matter of computing relative frequencies.","{'title': '2 Features in SUBGs', 'number': '2'}"
"But the resulting estimation procedures (discussed in detail, shortly), albeit more complicated, have the virtue of applying to essentially arbitrary features—of the production or non-production type.","{'title': '2 Features in SUBGs', 'number': '2'}"
"That is, since estimators capable of finding maximum-likelihood parameter estimates for production features in a SUBG will also find maximum-likelihood estimates for non-production features, there is no motivation for restricting features to be of the production type.","{'title': '2 Features in SUBGs', 'number': '2'}"
Linguistically there is no particular reason for assuming that productions are the best features to use in a stochastic language model.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"For example, the adjunct attachment ambiguity in (1) results in alternative syntactic structures which use the same productions the same number of times in each derivation, so a model with only production features would necessarily assign them the same likelihood.","{'title': '2 Features in SUBGs', 'number': '2'}"
"Thus models that use production features alone predict that there should not be a systematic preference for one of these analyses over the other, contrary to standard psycholinguistic results.","{'title': '2 Features in SUBGs', 'number': '2'}"
"There are many different ways of choosing features for a SUBG, and each of these choices makes an empirical claim about possible distributions of sentences.","{'title': '2 Features in SUBGs', 'number': '2'}"
Specifying the features of a SUBG is as much an empirical matter as specifying the grammar itself.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"For any given UBG there are a large (usually infinite) number of SUBGs that can be constructed from it, differing only in the features that each SUBG uses.","{'title': '2 Features in SUBGs', 'number': '2'}"
"In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995).","{'title': '2 Features in SUBGs', 'number': '2'}"
"Adjunct and argument features indicate adjunct and argument attachment respectively, and permit the model to capture a general argument attachment preference.","{'title': '2 Features in SUBGs', 'number': '2'}"
"In addition, there are specialized adjunct and argument features corresponding to each grammatical function used in LFG (e.g., SUBJ, OBJ, COMP, XCOMP, ADJUNCT, etc.).","{'title': '2 Features in SUBGs', 'number': '2'}"
There are features indicating both high and low attachment (determined by the complexity of the phrase being attached to).,"{'title': '2 Features in SUBGs', 'number': '2'}"
Another feature indicates nonright-branching nonterminal nodes.,"{'title': '2 Features in SUBGs', 'number': '2'}"
There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).,"{'title': '2 Features in SUBGs', 'number': '2'}"
Each f-structure attributeatomic value pair which appears in any feature structure is also used as a feature.,"{'title': '2 Features in SUBGs', 'number': '2'}"
"We also use a number of features identifying syntactic structures that seem particularly important in these corpora, such as a feature identifying NPs that are dates (it seems that date interpretations of NPs are preferred).","{'title': '2 Features in SUBGs', 'number': '2'}"
"We would have liked to have included features concerning specific lexical items (to capture head-to-head dependencies), but we felt that our corpora were so small that the associated parameters could not be accurately estimated.","{'title': '2 Features in SUBGs', 'number': '2'}"
"Suppose = wi, , wr, is a training corpus of n syntactic analyses.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
Letting f3(c75) = f3 POI the log likelihood of the corpus CZ) and its derivatives are: where E0 (f') is the expected value of h under the distribution determined by the parameters 0.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The maximum-likelihood estimates are the 0 which maximize log 14 (iv).,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"The chief difficulty in finding the maximum-likelihood estimates is calculating E9 (f), which involves summing over the space of well-formed syntactic structures a There seems to be no analytic or efficient numerical way of doing this for a realistic SUBG.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Abney (1997) proposes a gradient ascent, based upon a Monte Carlo, procedure for estimating Eo(f3).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"The idea is to generate random samples of feature structures from the distribution PO(w), where 0 is the current parameter estimate, and to use these to estimate Eo(f3), and hence the gradient of the likelihood.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Samples are generated as follows: Given a SUBG, Abney constructs a covering PCFG based upon the SUBG and 0, the current estimate of 0.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The derivation trees of the PCFG can be mapped onto a set containing all of the SUBG's syntactic analyses.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Monte Carlo samples from the PCFG are comparatively easy to generate, and sample syntactic analyses that do not map to well-formed SUBG syntactic structures are then simply discarded.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"This generates a stream of syntactic structures, but not distributed according to P(w) (distributed instead according to the restriction of the PCFG to the SUBG).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Abney proposes using a Metropolis acceptancerejection method to adjust the distribution of this stream of feature structures to achieve detailed balance, which then produces a stream of feature structures distributed according to Pei (w).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"While this scheme is theoretically sound, it would appear to be computationally impractical for realistic SUBGs.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
Every step of the proposed procedure (corresponding to a single step of gradient ascent) requires a very large number of PCFG samples: samples must be found that correspond to well-formed SUBGs; many such samples are required to bring the Metropolis algorithm to (near) equilibrium; many samples are needed at equilibrium to properly estimate E(f3).,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The idea of a gradient ascent of the likelihood (2) is appealing—a simple calculation reveals that the likelihood is concave and therefore free of local maxima.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"But the gradient (in particular, E9(f3)) is intractable.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"This motivates an alternative strategy involving a data-based estimate of E9(f3): where y(w) is the yield belonging to the syntactic analysis w, and yi = y(wi) is the yield belonging to the i'th sample in the training corpus.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The point is that Eo(f3(w)ly(w) yi) is generally computable.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In fact, if SZ(y) is the set of well-formed syntactic structures that have yield y (i.e., the set of possible parses of the string y), then a l expectations only involves summing over the possible syntactic analyses or parses 12(yi) of the strings in the training corpus.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"While it is possible to construct UBGs for which the number of possible parses is unmanageably high, for many grammars it is quite manageable to enumerate the set of possible parses and thereby directly evaluate Eo (f3(w)iy (w ) = yi ) .","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Therefore, we propose replacing the gradient, (3), by and performing a gradient ascent.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Of course (6) is no longer the gradient of the likelihood function, but fortunately it is (exactly) the gradient of (the log of) another criterion: Instead of maximizing the likelihood of the syntactic analyses over the training corpus, we maximize the conditional likelihood of these analyses given the observed yields.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In our experiments, we have used a conjugate-gradient optimization program adapted from the one presented in Press et al. (1992).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The problem of estimating parameters for log-linear models is not new.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"It is especially difficult in cases, such as ours, where a large sample space makes the direct computation of expectations infeasible.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Many applications in spatial statistics, involving Markov random fields (MRF), are of this nature as well.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In his seminal development of the MRF approach to spatial statistics, Besag introduced a &quot;pseudolikelihood&quot; estimator to address these difficulties (Besag, 1974; Besag, 1975), and in fact our proposal here is an instance of his method.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In general, the likelihood function is replaced by a more manageable product of conditional likelihoods (a pseudo-likelihood—hence the designation PLO, which is then optimized over the parameter vector, instead of the likelihood itself.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In many cases, as in our case here, this substitution side steps much of the computational burden without sacrificing consistency (more on this shortly).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
What are the asymptotics of optimizing a pseudo-likelihood function?,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
Look first at the likelihood itself.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
For large n: where 00 is the true (and unknown) parameter vector.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Up to a constant, (8) is the negative of the Kullback-Leibler divergence between the true and estimated distributions of syntactic analyses.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"As sample size grows, maximizing likelihood amounts to minimizing divergence.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
As for pseudo-likelihood: So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Maximum likelihood estimation is consistent: under broad conditions the sequence of distributions Po , associated with the maximum likelihood estimator for 00 given the samples con, converges to Poo.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Pseudo-likelihood is also consistent, but in the present implementation it is consistent for the conditional distributions Poo (wly(w)) and not necessarily for the full distribution Poo (see Chi (1998)).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
It is not hard to see that pseudo-likelihood will not always correctly estimate Poo.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
Suppose there is a feature L which depends only on yields: f2(w) = fz(y(w)).,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
(Later we will refer to such features as pseudo-constant.),"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In this case, the derivative of PLo (1.0 with respect to 0i is zero; PL0(65) contains no information about 0.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In fact, in this case any value of 0i gives the same conditional distribution Po (w ) y(w)); 0i is irrelevant to the problem of choosing good parses.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Despite the assurance of consistency, pseudolikelihood estimation is prone to over fitting when a large number of features is matched against a modest-sized training corpus.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"One particularly troublesome manifestation of over fitting results from the existence of features which, relative to the training set, we might term &quot;pseudo-maximal&quot;: Let us say that a feature f is pseudo-maximal for a yield y if Vwf E 1/(y) f(w) > f (co') where w is any correct parse of y, i.e., the feature's value on every correct parse (.4) of y is greater than or equal to its value on any other parse of y. Pseudominimal features are defined similarly.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
It is easy to see that if h is pseudo-maximal on each sentence of the training corpus then the parameter assignment 03 = oo maximizes the corpus pseudo-likelihood.,"{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"(Similarly, the assignment 03 = —oo maximizes pseudo-likelihood if fy is pseudo-minimal over the training corpus).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Such infinite parameter values indicate that the model treats pseudo-maximal features categorically; i.e., any parse with a non-maximal feature value is assigned a zero conditional probability.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Of course, a feature which is pseudo-maximal over the training corpus is not necessarily pseudo-maximal for all yields.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"This is an instance of over fitting, and it can be addressed, as is customary, by adding a regularization term that promotes small values of 0 to the objective function.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"A common choice is to add a quadratic to the log-likelihood, which corresponds to multiplying the likelihood itself by a normal distribution.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"In our experiments, we multiplied the pseudo-likelihood by a zero-mean normal in 01, 0,„ with diagonal covariance, and with standard deviation ay for 03 equal to 7 times the maximum value of fi found in any parse in the training corpus.","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"(We experimented with other values for cry, but the choice seems to have little effect).","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
"Thus instead of maximizing the log pseudo-likelihood, we choose 0 to maxi","{'title': '3 A pseudo-likelihood estimator for log linear models', 'number': '3'}"
The pseudo-likelihood estimator described in the last section finds parameter values which maximize the conditional probabilities of the observed parses (syntactic analyses) given the observed sentences (yields) in the training corpus.,"{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.,"{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
"This suggests another possible objective function: choose 0 to maximize the number Co (c3) of times the maximum likelihood parse (under 0) is in fact the correct parse, in the training corpus.","{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
"C9(c73) is a highly discontinuous function of 0, and most conventional optimization algorithms perform poorly on it.","{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).,"{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
This procedure is much more computationally intensive than the gradient-based pseudo-likelihood procedure.,"{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
Its computational difficulty grows (and the quality of solutions degrade) rapidly with the number of features.,"{'title': '4 A maximum correct estimator for log linear models', 'number': '4'}"
Ron Kaplan and Hadar Shemtov at Xerox PARC provided us with two LFG parsed corpora.,"{'title': '5 Empirical evaluation', 'number': '5'}"
"The Verbmobil corpus contains appointment planning dialogs, while the Homecentre corpus is drawn from Xerox printer documentation.","{'title': '5 Empirical evaluation', 'number': '5'}"
Table 1 summarizes the basic properties of these corpora.,"{'title': '5 Empirical evaluation', 'number': '5'}"
"These corpora contain packed c/fstructure representations (Maxwell III and Kaplan, 1995) of the grammatical parses of each sentence with respect to Lexical-Functional grammars.","{'title': '5 Empirical evaluation', 'number': '5'}"
The corpora also indicate which of these parses is in fact the correct parse (this information was manually entered).,"{'title': '5 Empirical evaluation', 'number': '5'}"
"Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.","{'title': '5 Empirical evaluation', 'number': '5'}"
Table 2 describes the properties of the features used for each corpus.,"{'title': '5 Empirical evaluation', 'number': '5'}"
In addition to the two estimators described above we also present results from a baseline estimator in which all parses are treated as equally likely (this corresponds to setting all the parameters 01 to zero).,"{'title': '5 Empirical evaluation', 'number': '5'}"
We evaluated our estimators using held-out test corpus (7)test .,"{'title': '5 Empirical evaluation', 'number': '5'}"
We used two evaluation measures.,"{'title': '5 Empirical evaluation', 'number': '5'}"
"In an actual parsing application a SUBG might be used to identify the correct parse from the set of grammatical parses, so ouorfitrestevaluation measure counts the number c(.5st) of sentences in the test corpus CA-hest whose maximum likelihood parse under the estimated model 0 is actually the correct parse.","{'title': '5 Empirical evaluation', 'number': '5'}"
"If a sentence has 1 most likely parses (i.e., all 1 parses have the same conditional probability) and one of these parses is the correct parse, then we score 1// for this sentence.","{'title': '5 Empirical evaluation', 'number': '5'}"
"The second evaluation measure is the pseudolikelihood of the test corpus is the likelihood of the correct parses given their yields, so pseudolikelihood measures how much of the probability mass the model puts onto the correct analyses.","{'title': '5 Empirical evaluation', 'number': '5'}"
"This metric seems more relevant to applications where the system needs to estimate how likely it is that the correct analysis lies in a certain set of possible parses; e.g., ambiguitypreserving translation and human-assisted disambiguation.","{'title': '5 Empirical evaluation', 'number': '5'}"
"To make the numbers more manageable, we actually present the negative logarithm of the pseudo-likelihood rather than the pseudo-likelihood itself—so smaller is better.","{'title': '5 Empirical evaluation', 'number': '5'}"
Because of the small size of our corpora we evaluated our estimators using a 10-way crossvalidation paradigm.,"{'title': '5 Empirical evaluation', 'number': '5'}"
"We randomly assigned sentences of each corpus into 10 approximately equal-sized subcorpora, each of which was used in turn as the test corpus.","{'title': '5 Empirical evaluation', 'number': '5'}"
We evaluated on each subcorpus the parameters that were estimated from the 9 remaining subcorpora that served as the training corpus for this run.,"{'title': '5 Empirical evaluation', 'number': '5'}"
The evaluation scores from each subcorpus were summed in order to provide the scores presented here.,"{'title': '5 Empirical evaluation', 'number': '5'}"
Table 3 presents the results of the empirical evaluation.,"{'title': '5 Empirical evaluation', 'number': '5'}"
The superior performance of both estimators on the Verbmobil corpus probably reflects the fact that the non-rule features were designed to match both the grammar and content of that corpus.,"{'title': '5 Empirical evaluation', 'number': '5'}"
The pseudolikelihood estimator performed better than the correct-parses estimator on both corpora under both evaluation metrics.,"{'title': '5 Empirical evaluation', 'number': '5'}"
There seems to be substantial over learning in all these models; we routinely improved performance by discarding features.,"{'title': '5 Empirical evaluation', 'number': '5'}"
"With a small number of features the correct-parses estimator typically scores better than the pseudo-likelihood estimator on the correct-parses evaluation metric, but the pseudo-likelihood estimator always scores better on the pseudo-likelihood evaluation metric.","{'title': '5 Empirical evaluation', 'number': '5'}"
This paper described a log-linear model for SUBGs and evaluated two estimators for such models.,"{'title': '6 Conclusion', 'number': '6'}"
"Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.","{'title': '6 Conclusion', 'number': '6'}"
"Indeed, the number and choice of features strongly influences the performance of the model.","{'title': '6 Conclusion', 'number': '6'}"
The estimated models are able to identify the correct parse from the set of all possible parses approximately 50% of the time.,"{'title': '6 Conclusion', 'number': '6'}"
We would have liked to introduce features corresponding to dependencies between lexical items.,"{'title': '6 Conclusion', 'number': '6'}"
"Log-linear models are well-suited for lexical dependencies, but because of the large number of such dependencies substantially larger corpora will probably be needed to estimate such mo dels .1 'Alternatively, it may be possible to use a simpler non-SUBG model of lexical dependencies estimated from a much larger corpus as the reference distribution with parses of the test corpus that were the correct parses, and — log PL(E5test) is the negative logarithm of the pseudo-likelihood of the test corpus.","{'title': '6 Conclusion', 'number': '6'}"
"However, there may be applications which can benefit from a model that performs even at this level.","{'title': '6 Conclusion', 'number': '6'}"
"For example, in a machine-assisted translation system a model like ours could be used to order possible translations so that more likely alternatives are presented before less likely ones.","{'title': '6 Conclusion', 'number': '6'}"
"In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.","{'title': '6 Conclusion', 'number': '6'}"
