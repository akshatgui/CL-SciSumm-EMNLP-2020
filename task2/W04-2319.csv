col1,col2
"2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.",Introduction
"We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.",Introduction
"We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.",Experiment/Discussion
"We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.",Experiment/Discussion
Natural meetings offer rich opportunities for studying a variety of complex discourse phenomena.,Experiment/Discussion
"Meetings contain regions of high speaker overlap, affective variation, complicated interaction structures, abandoned or interrupted utterances, and other interesting turn-taking and discourse-level phenomena.",Experiment/Discussion
"In addition, meetings that occur naturally involve real topics, debates, issues, and social dynamics that should generalize more readily to other real meetings than might data collected using artificial scenarios.",Experiment/Discussion
"Thus meetings pose interesting challenges to descriptive and theoretical models of discourse, as well as to researchers in the speech recognition community [4,7,9,13,14,15].",Experiment/Discussion
We describe a new corpus of hand-annotated dialog acts and adjacency pairs for roughly 72 hours of naturally occurring multi-party meetings.,Experiment/Discussion
The meetings were recorded at the International Computer Science Institute (ICSI) as part of the ICSI Meeting Recorder Project [9].,Experiment/Discussion
Word transcripts and audio files from that corpus are available through the Linguistic Data Consortium (LDC).,Experiment/Discussion
"In this paper, we provide a first description of the meeting recorder dialog act (MRDA) corpus, a companion set of annotations that augment the word transcriptions with discourse-level segmentations, dialog act (DA) information, and adjacency pair information.",Experiment/Discussion
"The corpus is currently available online for research purposes [16], and we plan a future release through the LDC.",Experiment/Discussion
The ICSI Meeting Corpus data is described in detail in [9].,Experiment/Discussion
"It consists of 75 meetings, each roughly an hour in length.",Experiment/Discussion
"There are 53 unique speakers in the corpus, and an average of about 6 speakers per meeting.",Experiment/Discussion
"Reflecting the makeup of the Institute, there are more male than female speakers (40 and 13, respectively).",Experiment/Discussion
"There are a28 native English speakers, although many of the nonnative English speakers are quite fluent.",Experiment/Discussion
"Of the 75 meetings, 29 are meetings of the ICSI meeting recorder project itself, 23 are meetings of a research group focused on robustness in automatic speech recognition, 15 involve a group discussing natural language processing and neural theories of language, and 8 are miscellaneous meeting types.",Experiment/Discussion
The last set includes 2 very interesting meetings involving the corpus transcribers as participants (example included in [16]).,Experiment/Discussion
"Annotation involved three types of information: marking of DA segment boundaries, marking of DAs themselves, and marking of correspondences between DAs (adjacency pairs, [12]).",Experiment/Discussion
Each type of annotation is described in detail in [7].,Experiment/Discussion
"Segmentation methods were developed based on separating out speech regions having different discourse functions, but also paying attention to pauses and intonational grouping.",Experiment/Discussion
"To distinguish utterances that are prosodically one unit but which contain multiple DAs, we use a pipe bar (  |) in the annotations.",Experiment/Discussion
"This allows the researcher to either split or not split at the bar, depending on the research goals.",Experiment/Discussion
"We examined existing annotation systems, including [1,2,5,6,8,10,11], for similarity to the style of interaction in the ICSI meetings.",Experiment/Discussion
"We found that SWBD-DAMSL [11], a system adapted from DAMSL [6], provided a fairly good fit.",Experiment/Discussion
"Although our meetings were natural, and thus had real agenda items, the dialog was less like human-human or human-machine task-oriented dialog added in MRDA.",Experiment/Discussion
Tags in italics are based on the SWBD-DAMSL version but have had meanings modified for MRDA.,Experiment/Discussion
"The ordering of tags in the table is explained as follows: In the mapping of DAMSL tags to SWBD-DAMSL tags in the SWBDDAMSL manual, tags were ordered in categories such as “Communication Status”, “Information Requests”, and so on.",Experiment/Discussion
"In the mapping of MRDA tags to SWBD-DAMSL tags here, we have retained the same overall ordering of tags within the table, but we do not explicitly mark the higher-level SWBD-DAMSL categories in order to avoid confusion, since categorical structure differs in the two systems (see [7]).",Experiment/Discussion
"(e.g., [1,2,10]) and more like human-human casual conversation ([5,6,8,11]).",Experiment/Discussion
"Since we were working with English rather than Spanish, and did not view a large tag set as a problem, we preferred [6,11] over [5,8] for this work.",Experiment/Discussion
"We modified the system in [11] a number of ways, as indicated in Figure 1 and as explained further in [7].",Experiment/Discussion
"The MRDA system requires one “general tag” per DA, and attaches a variable number of following “specific tags”.",Experiment/Discussion
"Excluding nonlabelable cases, there are 11 general tags and 39 specific tags.",Experiment/Discussion
"There are two disruption forms (%-, %--), two types of indecipherable utterances (x, %) and a non-DA tag to denote rising tone (rt).",Experiment/Discussion
"An interface allowed annotators to play regions of speech, modify transcripts, and enter DA and adjacency pair information, as well as other comments.",Experiment/Discussion
"Meetings were divided into 10 minute chunks; labeling time averaged about 3 hours per chunk, although this varied considerably depending on the complexity of the dialog.",Experiment/Discussion
An example from one of the meetings is shown in Figure 2 as an illustration of some of the types of interactions we observe in the corpus.,Experiment/Discussion
Audio files and additional sample excerpts are available from [16].,Experiment/Discussion
In addition to the obvious high degree of overlap—roughly one third of all words are overlapped—note the explicit struggle for the floor indicated by the two failed floor grabbers (fg) by speakers c5 and c6.,Experiment/Discussion
"Furthermore, 6 of the 19 total utterances express some form of agreement or disagreement (arp, aa, and nd) with previous utterances.",Experiment/Discussion
"Also, of the 19 utterances within the excerpt, 9 are incomplete due to interruption by another talker, as is typical of many regions in the corpus showing high speaker overlap.",Experiment/Discussion
"We find in related work that regions of high overlap correlate with high speaker involvement, or “hot spots” [15].",Experiment/Discussion
The example also provides a taste of the frequency and complexity of adjacency pair information.,Experiment/Discussion
"For example, within only half a minute, speaker c5 has interacted with speakers c3 and c6, and speaker c6 has interacted with speakers c2 and c5.",Experiment/Discussion
"We computed interlabeler reliability among the three labelers for both segmentation (into DA units) and DA labeling, using randomly selected excerpts from the 75 labeled meetings.",Experiment/Discussion
"Since agreement on DA segmentation does not appear to have standard associated metrics in the literature, we developed our own approach.",Experiment/Discussion
"The philosophy is that any difference in words at the beginning and/or end of a DA could result in a different label for that DA, and the more words that are mismatched, the more likely the difference in label.",Experiment/Discussion
"As a very strict measure of reliability, we used the following approach: (1) Take one labeler’s transcript as a reference.",Experiment/Discussion
(2) Look at each other labeler’s words.,Experiment/Discussion
"For each word, look at the utterance it comes from and see if the reference has the exact same utterance.",Experiment/Discussion
"(3) If it does, there is a match.",Experiment/Discussion
"Match every word in the utterance, and then mark the matched utterance in the reference so it cannot be matched again (this prevents felicitous matches due to identical repeated words).",Experiment/Discussion
"(4) Repeat this process for each word in each reference-labeler pair, and rotate to the next labeler as the reference.",Experiment/Discussion
Note that this metric requires perfect matching of the full utterance a word is in for that word to be matched.,Experiment/Discussion
"For example in the following case, labelers agree on 3 segmentation locations, but the agreement on our metric is only 0.14, since only 1 of 7 words is matched: Overall segmentation results on this metric are provided by labeler pair in Table 1.",Experiment/Discussion
"We examined agreement on DA labels using the Kappa statistic [3], which adjusts for chance agreement.",Experiment/Discussion
"Because of the large number of unique full label combinations, we report Kappa values in Table 2 using various class mappings distributed with the corpus.",Experiment/Discussion
Values are shown by labeler pair.,Experiment/Discussion
"The overall value of Kappa for our basic, six-way classmap (Map1) is 0.80, representing good agreement for this type of task.",Experiment/Discussion
We provide basic statistics based on the dialog act labels for the 75 meetings.,Experiment/Discussion
"If we ignore the tag marking rising intonation (rt), since this is not a DA tag, we find 180,218 total tags.",Experiment/Discussion
Table 3 shows the distribution of the tags in more detail.,Experiment/Discussion
"If instead we look at only the 11 obligatory general tags, for which there is one per DA, and if we split labels at the pipe bar, the total is 113,560 (excluding tags that only include a disruption label).",Experiment/Discussion
The distribution of general tags is shown in Table 4.,Experiment/Discussion
We include other useful information with the corpus.,Results/Conclusion
"Word-level time information is available, based on alignments from an automatic speech recognizer.",Results/Conclusion
Annotator comments are also provided.,Results/Conclusion
"We suggest various ways to group the large set of labels into a smaller set of classes, depending on the research focus.",Results/Conclusion
"Finally, the corpus contains information that may be useful in for developing automatic modeling of prosody, such as hand-marked annotation of rising intonation.",Results/Conclusion
"We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.",Results/Conclusion
"This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).",Results/Conclusion
The views are those of the authors and do not represent the views of the funding agencies.,Results/Conclusion
