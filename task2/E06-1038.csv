col1,col2
We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.,{}
The parsers are trained out-of-domain and contain a significant amount of noise.,{}
We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.,{}
"This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.",{}
The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.,"{'title': '1 Introduction', 'number': '1'}"
Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.,"{'title': '1 Introduction', 'number': '1'}"
"Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.","{'title': '1 Introduction', 'number': '1'}"
"We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005).","{'title': '1 Introduction', 'number': '1'}"
"In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning.","{'title': '1 Introduction', 'number': '1'}"
"We will work in a supervised learning setting and assume as input a training set T=(xt,yt)|?","{'title': '1 Introduction', 'number': '1'}"
| tï¿½1 of original sentences xt and their compressions yt.,"{'title': '1 Introduction', 'number': '1'}"
"We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs.","{'title': '1 Introduction', 'number': '1'}"
"Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development.","{'title': '1 Introduction', 'number': '1'}"
A handful of sentences occur twice but with different compressions.,"{'title': '1 Introduction', 'number': '1'}"
We randomly select a single compression for each unique sentence in order to create an unambiguous training set.,"{'title': '1 Introduction', 'number': '1'}"
Examples from this data set are given in Figure 1.,"{'title': '1 Introduction', 'number': '1'}"
"Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}.","{'title': '1 Introduction', 'number': '1'}"
"We define the function I(yi) E {1, ... , n} that maps word yi in the compression to the index of the word in the original sentence.","{'title': '1 Introduction', 'number': '1'}"
"Finally we include the constraint I(yi) < I(yi+1), which forces each word in x to occur at most once in the compression y. Compressions are evaluated on three criteria, Typically grammaticality and importance are traded off with compression rate.","{'title': '1 Introduction', 'number': '1'}"
"The longer our compressions, the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning.","{'title': '1 Introduction', 'number': '1'}"
The paper is organized as follows: Section 2 discusses previous approaches to sentence compression.,"{'title': '1 Introduction', 'number': '1'}"
"In particular, we discuss the advantages and disadvantages of the models of Knight and Marcu (2000).","{'title': '1 Introduction', 'number': '1'}"
"In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.","{'title': '1 Introduction', 'number': '1'}"
"We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence, dropped words and phrases from the original sentence, and features over noisy dependency and phrase-structure trees for the original sentence.","{'title': '1 Introduction', 'number': '1'}"
We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the compression.,"{'title': '1 Introduction', 'number': '1'}"
Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu (2000) and finally Section 5 discusses some areas of future work.,"{'title': '1 Introduction', 'number': '1'}"
Knight and Marcu (2000) first tackled this problem by presenting a generative noisy-channel model and a discriminative tree-to-tree decision tree model.,"{'title': '2 Previous Work', 'number': '2'}"
"The noisy-channel model defines the problem as finding the compressed sentence with maximum conditional probability P(y) is the source model, which is a PCFG plus bigram language model.","{'title': '2 Previous Work', 'number': '2'}"
"P(x|y) is the channel model, the probability that the long sentence is an expansion of the compressed sentence.","{'title': '2 Previous Work', 'number': '2'}"
"To calculate the channel model, both the original and compressed versions of every sentence in the training set are assigned a phrase-structure tree.","{'title': '2 Previous Work', 'number': '2'}"
"Given a tree for a long sentence x and compressed sentence y, the channel probability is the product of the probability for each transformation required if the tree for y is to expand to the tree for x.","{'title': '2 Previous Work', 'number': '2'}"
The tree-to-tree decision tree model looks to rewrite the tree for x into a tree for y.,"{'title': '2 Previous Work', 'number': '2'}"
The model uses a shift-reduce-drop parsing algorithm that starts with the sequence of words in x and the corresponding tree.,"{'title': '2 Previous Work', 'number': '2'}"
"The algorithm then either shifts (considers new words and subtrees for x), reduces (combines subtrees from x into possibly new tree constructions) or drops (drops words and subtrees from x) on each step of the algorithm.","{'title': '2 Previous Work', 'number': '2'}"
A decision tree model is trained on a set of indicative features for each type of action in the parser.,"{'title': '2 Previous Work', 'number': '2'}"
These models are then combined in a greedy global search algorithm to find a single compression.,"{'title': '2 Previous Work', 'number': '2'}"
"Though both models of Knight and Marcu perform quite well, they do have their shortcomings.","{'title': '2 Previous Work', 'number': '2'}"
"The noisy-channel model uses a source model that is trained on uncompressed sentences, even though the source model is meant to represent the probability of compressed sentences.","{'title': '2 Previous Work', 'number': '2'}"
The channel model requires aligned parse trees for both compressed and uncompressed sentences in the training set in order to calculate probability estimates.,"{'title': '2 Previous Work', 'number': '2'}"
"These parses are provided from a parsing model trained on out-of-domain data (the WSJ), which can result in parse trees with many mistakes for both the original and compressed versions.","{'title': '2 Previous Work', 'number': '2'}"
This makes alignment difficult and the channel probability estimates unreliable as a result.,"{'title': '2 Previous Work', 'number': '2'}"
"On the other hand, the decision tree model does not rely on the trees to align and instead simply learns a tree-totree transformation model to compress sentences.","{'title': '2 Previous Work', 'number': '2'}"
The primary problem with this model is that most of the model features encode properties related to including or dropping constituents from the tree with no encoding of bigram or trigram surface features to promote grammaticality.,"{'title': '2 Previous Work', 'number': '2'}"
"As a result, the model will sometimes return very short and ungrammatical compressions.","{'title': '2 Previous Work', 'number': '2'}"
Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression.,"{'title': '2 Previous Work', 'number': '2'}"
"We argue in the next section that ideally, parse trees should be treated solely as a source of evidence when making compression decisions to be balanced with other evidence such as that provided by the words themselves.","{'title': '2 Previous Work', 'number': '2'}"
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model.,"{'title': '2 Previous Work', 'number': '2'}"
"The resulting systems typically return informative and grammatical sentences, however, they do so at the cost of compression rate.","{'title': '2 Previous Work', 'number': '2'}"
Riezler et al. (2003) present a discriminative sentence compressor over the output of an LFG parser that is a packed representation of possible compressions.,"{'title': '2 Previous Work', 'number': '2'}"
"Though this model is highly likely to return grammatical compressions, it required the training data be human annotated with syntactic trees.","{'title': '2 Previous Work', 'number': '2'}"
"For the rest of the paper we use x = x1 ... xn to indicate an uncompressed sentence and y = y1 ... ym a compressed version of x, i.e., each yj indicates the position in x of the jth word in the compression.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"We always pad the sentence with dummy start and end words, x1 = -START- and xn = -END-, which are always included in the compressed version (i.e. y1 = x1 and ym = xn).","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
We will show in Section 3.2 how this factorization also allows us to include features on dropped phrases and subtrees from both a dependency and a phrase-structure parse of the original sentence.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"However, here they are merely treated as evidence for the discriminative learner, which will set the weight of each feature relative to the other (possibly overlapping) features to optimize the models accuracy on the observed data.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
We define a recurrence as follows It is easy to show that C[n] represents the score of the best compression for sentence x (whose length is n) under the first-order score factorization we made.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
We can show this by induction.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"If we assume that C[j] is the highest scoring compression that ends at word xj, for all j < i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
This table can be filled in O(n2).,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a).","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
This decoding algorithm is dynamic with respect to compression rate.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"That is, the algorithm will return the highest scoring compression regardless of length.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words since this is rarely seen in the training data.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"In fact, it turns out that our learned compressions have a compression rate very similar to the gold standard.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"That said, there are some instances when a static compression rate is preferred.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
A user may specifically want a 25% compression rate for all sentences.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
This is not a problem for our decoding algorithm.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"We simply augment the dynamic programming table and calculate C[i][r], which is the score of the best compression of length r that ends at word xi.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"This table can be filled in as follows Thus, if we require a specific compression rate, we simple determine the number of words r that satisfy this rate and calculate C[n][r].","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
The new complexity is O(n2r).,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
So far we have defined the score of a compression as well as a decoding algorithm that searches the entire space of compressions to find the one with highest score.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"This all relies on a score factorization over adjacent words in the compression, s(x, I(yjâ1), I(yj)) = w Â· f(x, I(yjâ1), I(yj)).","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
"In Section 3.3 we describe an online large-margin method for learning w. Here we present the feature representation f(x, I(yjâ1), I(yj)) for a pair of adjacent words in the compression.","{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
These features were tuned on a development data set.,"{'title': '3 Discriminative Sentence Compression', 'number': '3'}"
The first set of features are over adjacent words yjâ1 and yj in the compression.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"These include the part-of-speech (POS) bigrams for the pair, the POS of each word individually, and the POS context (bigram and trigram) of the most recent word being added to the compression, yj.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"These features are meant to indicate likely words to include in the compression as well as some level of grammaticality, e.g., the adjacent POS features âJJ&VBâ would get a low weight since we rarely see an adjective followed by a verb.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We also add a feature indicating if yjâ1 and yj were actually adjacent in the original sentence or not and we conjoin this feature with the above POS features.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Note that we have not included any lexical features.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We found during experiments on the development data that lexical information was too sparse and led to overfitting, so we rarely include such features.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Instead we rely on the accuracy of POS tags to provide enough evidence.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Next we added features over every dropped word in the original sentence between yjâ1 and yj, if there were any.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"These include the POS of each dropped word, the POS of the dropped words conjoined with the POS of yjâ1 and yj.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"If the dropped word is a verb, we add a feature indicating the actual verb (this is for common verbs like âisâ, which are typically in compressions).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Finally we add the POS context (bigram and trigram) of each dropped word.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
These features represent common characteristics of words that can or should be dropped from the original sentence in the compressed version (e.g. adjectives and adverbs).,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We also add a feature indicating whether the dropped word is a negation (e.g., not, never, etc.).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We also have a set of features to represent brackets in the text, which are common in the data set.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The first measures if all the dropped words between yjâ1 and yj have a mismatched or inconsistent bracketing.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The second measures if the left and right-most dropped words are themselves both brackets.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"These features come in handy for examples like, The Associated Press ( AP ) reported the story, where the compressed version is The Associated Press reported the story.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Information within brackets is often redundant.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The previous set of features are meant to encode common POS contexts that are commonly retained or dropped from the original sentence during compression.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, they do so without a larger picture of the function of each word in the sentence.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, we are merely going to use them as an additional source of features.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for or against particular compressions.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The learning algorithm will set the feature weight accordingly depending on each features discriminative power.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, we stress this aspect of our model due to the history of compression systems using syntax to provide hard structural constraints on the output.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Lets consider the sentence x = Mary saw Ralph on Tuesday after lunch, with corresponding parses given in Figure 2.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"In particular, lets consider the feature representation f(x,3,6).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"That is, the feature representation of making Ralph and after adjacent in the compression and dropping the prepositional phrase on Tuesday.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The first set of features we consider are over dependency trees.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
For every dropped word we add a feature indicating the POS of the words parent in the tree.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"For example, if the dropped words parent is root, then it typically means it is the main verb of the sentence and unlikely to be dropped.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We also add a conjunction feature of the POS tag of the word being dropped and the POS of its parent as well as a feature indicating for each word being dropped whether it is a leaf node in the tree.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We also add the same features for the two adjacent words, but indicating that they are part of the compression.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
For the phrase-structure features we find every node in the tree that subsumes a piece of dropped text and is not a child of a similar node.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
In this case the PP governing on Tuesday.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We then add features indicating the context from which this node was dropped.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
For example we add a feature specifying that a PP was dropped which was the child of a VP.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We also add a feature indicating that a PP was dropped which was the left sibling of another PP, etc.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Ideally, for each production in the tree we would like to add a feature indicating every node that was dropped, e.g.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
âVPâVBD NP PP PP â VPâVBD NP PPâ.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, we cannot necessarily calculate this feature since the extent of the production might be well beyond the local context of first-order feature factorization.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Furthermore, since the training set is so small, these features are likely to be observed very few times.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"In this section we have described a rich feature set over adjacent words in the compressed sentence, dropped words and phrases from the original sentence, and properties of deep syntactic trees of the original sentence.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Note that these features in many ways mimic the information already present in the noisy-channel and decision-tree models of Knight and Marcu (2000).,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Our bigram features encode properties that indicate both good and bad words to be adjacent in the compressed sentence.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
This is similar in purpose to the source model from the noisy-channel system.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, in that system, the source model is trained on uncompressed sentences and thus is not as representative of likely bigram features for compressed sentences, which is really what we desire.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Our feature set also encodes dropped words and phrases through the properties of the words themselves and through properties of their syntactic relation to the rest of the sentence in a parse tree.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
These features represent likely phrases to be dropped in the compression and are thus similar in nature to the channel model in the noisy-channel system as well as the features in the tree-to-tree decision tree system.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, we use these syntactic constraints as soft evidence in our model.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"That is, they represent just another layer of evidence to be considered during training when setting parameters.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"This differs from the models of Knight and Marcu (2000), which treat the noisy parses as gold-standard when calculating probability estimates.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"An important distinction we should make is the notion of supported versus unsupported features (Sha and Pereira, 2003).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Supported features are those that are on for the gold standard compressions in the training.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"For instance, the bigram feature âNN&VBâ will be supported since there is most likely a compression that contains a adjacent noun and verb.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"However, the feature âJJ&VBâ will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Our model includes all features, including those that are unsupported.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The advantage of this is that the model can learn negative weights for features that are indicative of bad compressions.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"This is not difficult to do since most features are POS based and the feature set size even with all these features is only 78,923.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Having defined a feature encoding and decoding algorithm, the last step is to learn the feature weights w. We do this using the Margin Infused Relaxed Algorithm (MIRA), which is a discriminative large-margin online learning technique shown in Figure 3 (Crammer and Singer, 2003).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"On each iteration, MIRA considers a single instance from the training set (xt, yt) and updates the weights so that the score of the correct compression, yt, is greater than the score of all other compressions by a margin proportional to their loss.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Many weight vectors will satisfy these constraints so we pick the one with minimum change from the previous setting.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We define the loss to be the number of words falsely retained or dropped in the incorrect compression relative to the correct one.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"For instance, if the correct compression of the sentence in Figure 2 is Mary saw Ralph, then the compression Mary saw after lunch would have a loss of 3 since it incorrectly left out one word and included two others.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Of course, for a sentence there are exponentially many possible compressions, which means that this optimization will have exponentially many constraints.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"We follow the method of McDonald et al. (2005b) and create constraints only on the k compressions that currently have the highest score, bestk(x; w).","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
This can easily be calculated by extending the decoding algorithm with standard Viterbi k-best techniques.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"On the development data, we found that k = 10 provided the best performance, though varying k did not have a major impact overall.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
Furthermore we found that after only 3-5 training epochs performance on the development data was maximized.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
The final weight vector is the average of all weight vectors throughout training.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
"Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training.","{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We found it to be particularly important for this data set.,"{'title': '3.2.1 Word/POS Features', 'number': '4'}"
We use the same experimental methodology as Knight and Marcu (2000).,"{'title': '4 Experiments', 'number': '5'}"
We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5.,"{'title': '4 Experiments', 'number': '5'}"
"For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system.","{'title': '4 Experiments', 'number': '5'}"
The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.,"{'title': '4 Experiments', 'number': '5'}"
"We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree model is nearer to our system (around 57-58%).","{'title': '4 Experiments', 'number': '5'}"
The noisy-channel model typically returned longer compressions.,"{'title': '4 Experiments', 'number': '5'}"
Results are shown in Table 1.,"{'title': '4 Experiments', 'number': '5'}"
We present the average score over all judges as well as the standard deviation.,"{'title': '4 Experiments', 'number': '5'}"
The evaluation for the decision tree system of Knight and Marcu is strikingly similar to the original evaluation in their work.,"{'title': '4 Experiments', 'number': '5'}"
This provides strong evidence that the evaluation criteria in both cases were very similar.,"{'title': '4 Experiments', 'number': '5'}"
"Table 1 shows that all models had similar compressions rates, with humans preferring to compress a little more aggressively.","{'title': '4 Experiments', 'number': '5'}"
"Not surprisingly, the human compressions are practically all grammatical.","{'title': '4 Experiments', 'number': '5'}"
A quick scan of the evaluations shows that the few ungrammatical human compressions were for sentences that were not really grammatical in the first place.,"{'title': '4 Experiments', 'number': '5'}"
Of greater interest is that the compressions of our system are typically more grammatical than the decision tree model of Knight and Marcu.,"{'title': '4 Experiments', 'number': '5'}"
"When looking at importance, we see that our system actually does the best â even better than humans.","{'title': '4 Experiments', 'number': '5'}"
The most likely reason for this is that our model returns longer sentences and is thus less likely to prune away important information.,"{'title': '4 Experiments', 'number': '5'}"
"For example, consider the sentence The chemical etching process used for glare protection is effective and will help if your office has the fluorescent-light overkill thatâs typical in offices The human compression was Glare protection is effective, whereas our model compressed the sentence to The chemical etching process used for glare protection is effective.","{'title': '4 Experiments', 'number': '5'}"
"A primary reason that our model does better than the decision tree model of Knight and Marcu is that on a handful of sentences, the decision tree compressions were a single word or noun-phrase.","{'title': '4 Experiments', 'number': '5'}"
For such sentences the evaluators typically rated the compression a 1 for both grammaticality and importance.,"{'title': '4 Experiments', 'number': '5'}"
"In contrast, our model never failed in such drastic ways and always output something reasonable.","{'title': '4 Experiments', 'number': '5'}"
This is quantified in the standard deviation of the two systems.,"{'title': '4 Experiments', 'number': '5'}"
"Though these results are promising, more large scale experiments are required to really ascertain the significance of the performance increase.","{'title': '4 Experiments', 'number': '5'}"
Ideally we could sample multiple training/testing splits and use all sentences in the data set to evaluate the systems.,"{'title': '4 Experiments', 'number': '5'}"
"However, since these systems require human evaluation we did not have the time or the resources to conduct these experiments.","{'title': '4 Experiments', 'number': '5'}"
Here we aim to give the reader a flavor of some common outputs from the different models.,"{'title': '4 Experiments', 'number': '5'}"
Three examples are given in Table 4.1.,"{'title': '4 Experiments', 'number': '5'}"
The first shows two properties.,"{'title': '4 Experiments', 'number': '5'}"
"First of all, the decision tree model completely breaks and just returns a single noun-phrase.","{'title': '4 Experiments', 'number': '5'}"
"Our system performs well, however it leaves out the complementizer of the relative clause.","{'title': '4 Experiments', 'number': '5'}"
This actually occurred in a few examples and appears to be the most common problem of our model.,"{'title': '4 Experiments', 'number': '5'}"
A post-processing rule should eliminate this.,"{'title': '4 Experiments', 'number': '5'}"
"The second example displays a case in which our system and the human system are grammatical, but the removal of a prepositional phrase hurts the resulting meaning of the sentence.","{'title': '4 Experiments', 'number': '5'}"
"In fact, without the knowledge that the sentence is referring to broadband, the compressions are meaningless.","{'title': '4 Experiments', 'number': '5'}"
This appears to be a harder problem â determining which prepositional phrases can be dropped and which cannot.,"{'title': '4 Experiments', 'number': '5'}"
"The final, and more interesting, example presents two very different compressions by the human and our automatic system.","{'title': '4 Experiments', 'number': '5'}"
"Here, the human kept the relative clause relating what languages the source code is available in, but dropped the main verb phrase of the sentence.","{'title': '4 Experiments', 'number': '5'}"
Our model preferred to retain the main verb phrase and drop the relative clause.,"{'title': '4 Experiments', 'number': '5'}"
This is most likely due to the fact that dropping the main verb phrase of a sentence is much less likely in the training data than dropping a relative clause.,"{'title': '4 Experiments', 'number': '5'}"
Two out of four evaluators preferred the compression returned by our system and the other two rated them equal.,"{'title': '4 Experiments', 'number': '5'}"
In this paper we have described a new system for sentence compression.,"{'title': '5 Discussion', 'number': '6'}"
This system uses discriminative large-margin learning techniques coupled with a decoding algorithm that searches the space of all compressions.,"{'title': '5 Discussion', 'number': '6'}"
In addition we defined a rich feature set of bigrams in the compression and dropped words and phrases from the original sentence.,"{'title': '5 Discussion', 'number': '6'}"
The model also incorporates soft syntactic evidence in the form of features over dependency and phrase-structure trees for each sentence.,"{'title': '5 Discussion', 'number': '6'}"
This system has many advantages over previous approaches.,"{'title': '5 Discussion', 'number': '6'}"
"First of all its discriminative nature allows us to use a rich dependent feature set and to optimize a function directly related to compresThe fi rst new product, ATF Protype , is a line of digital postscript typefaces that will be sold in packages of up to six fonts.","{'title': '5 Discussion', 'number': '6'}"
ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .,"{'title': '5 Discussion', 'number': '6'}"
The fi rst new product .,"{'title': '5 Discussion', 'number': '6'}"
ATF Protype is a line of digital postscript typefaces will be sold in packages of up to six fonts .,"{'title': '5 Discussion', 'number': '6'}"
"Finally, another advantage of broadband is distance.","{'title': '5 Discussion', 'number': '6'}"
Another advantage is distance.,"{'title': '5 Discussion', 'number': '6'}"
Another advantage of broadband is distance.,"{'title': '5 Discussion', 'number': '6'}"
Another advantage is distance.,"{'title': '5 Discussion', 'number': '6'}"
"The source code , which is available for C , Fortran , ADA and VHDL , can be compiled and executed on the same system or ported to other target platforms .","{'title': '5 Discussion', 'number': '6'}"
"The source code is available for C , Fortran , ADA and VHDL .","{'title': '5 Discussion', 'number': '6'}"
The source code is available for C .,"{'title': '5 Discussion', 'number': '6'}"
"The source code can be compiled and executed on the same system or ported to other target platforms . sion accuracy during training, both of which have been shown to be beneficial for other problems.","{'title': '5 Discussion', 'number': '6'}"
"Furthermore, the system does not rely on the syntactic parses of the sentences to calculate probability estimates.","{'title': '5 Discussion', 'number': '6'}"
"Instead, this information is incorporated as just another form of evidence to be considered during training.","{'title': '5 Discussion', 'number': '6'}"
This is advantageous because these parses are trained on out-of-domain data and often contain a significant amount of noise.,"{'title': '5 Discussion', 'number': '6'}"
A fundamental flaw with all sentence compression systems is that model parameters are set with the assumption that there is a single correct answer for each sentence.,"{'title': '5 Discussion', 'number': '6'}"
"Of course, like most compression and translation tasks, this is not true, consider, TapeWare , which supports DOS and NetWare 286 , is a value-added process that lets you directly connect the QA150-EXAT to a file server and issue a command from any workstation to back up the server The human annotated compression is, TapeWare supports DOS and NetWare 286.","{'title': '5 Discussion', 'number': '6'}"
"However, another completely valid compression might be, TapeWare lets you connect the QA150-EXAT to a fi le server.","{'title': '5 Discussion', 'number': '6'}"
These two compressions overlap by a single word.,"{'title': '5 Discussion', 'number': '6'}"
Our learning algorithm may unnecessarily lower the score of some perfectly valid compressions just because they were not the exact compression chosen by the human annotator.,"{'title': '5 Discussion', 'number': '6'}"
"A possible direction of research is to investigate multilabel learning techniques for structured data (McDonald et al., 2005a) that learn a scoring function separating a set of valid answers from all invalid answers.","{'title': '5 Discussion', 'number': '6'}"
Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem.,"{'title': '5 Discussion', 'number': '6'}"
The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knightâs systems.,"{'title': 'Acknowledgments', 'number': '7'}"
Thanks also to Hal DaumÂ´e and Fernando Pereira for useful discussions.,"{'title': 'Acknowledgments', 'number': '7'}"
"Finally, the author thanks the four reviewers for evaluating the compressed sentences.","{'title': 'Acknowledgments', 'number': '7'}"
This work was supported by NSF ITR grants 0205448 and 0428193.,"{'title': 'Acknowledgments', 'number': '7'}"
