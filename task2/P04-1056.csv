col1,col2
Most information extraction (IE) systems treat separate potential extractions as independent.,{}
"However, in many cases, considering influences potential extractions could improve overall accuracy.",{}
"Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems.",{}
"We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.",{}
This allows for &quot;collective information extraction&quot; that exploits the mutual influence between possible extractions.,{}
Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.,{}
"Information extraction (IE), locating references to specific types of items in natural-language documents, is an important task with many practical applications.","{'title': '1 Introduction', 'number': '1'}"
"Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999).","{'title': '1 Introduction', 'number': '1'}"
"One of the current best empirical approaches to IE is conditional random fields (CRF's) (Lafferty et al., 2001).","{'title': '1 Introduction', 'number': '1'}"
"CRF's are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003).","{'title': '1 Introduction', 'number': '1'}"
"In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods (including HMM, SVM, MaxEnt, and rule-based methods) on the task of tagging references to human proteins in Medline abstracts (Bunescu et al., 2004), CRF's were found to significantly out-perform competing techniques.","{'title': '1 Introduction', 'number': '1'}"
"As typically applied, CRF's, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation.","{'title': '1 Introduction', 'number': '1'}"
"However, in many cases, considering influences between extractions can be very useful.","{'title': '1 Introduction', 'number': '1'}"
"For example, in our protein-tagging task, repeated references to the same protein are common.","{'title': '1 Introduction', 'number': '1'}"
"If the context surrounding one occurrence of a phrase is very indicative of it being a protein, then this should also influence the tagging of another occurrence of the same phrase in a different context which is not indicative of protein references.","{'title': '1 Introduction', 'number': '1'}"
"Relational Markov Networks (RMN's) (Taskar et al., 2002) are a generalization of CRF's that allow for collective classification of a set of related entities by integrating information from features of individual entities as well as the relations between them.","{'title': '1 Introduction', 'number': '1'}"
"Results on classifying connected sets of web pages have verified the advantage of this approach (Taskar et al., 2002).","{'title': '1 Introduction', 'number': '1'}"
"In this paper, we present an approach to collective information extraction using RMN's that simultaneously extracts all of the information from a document by exploiting the textual content and context of each relevant substring as well as the document relationships between them.","{'title': '1 Introduction', 'number': '1'}"
Experiments on human protein tagging demonstrate the advantages of collective extraction on several annotated corpora of Medline abstracts.,"{'title': '1 Introduction', 'number': '1'}"
"Given a collection of documents D, we associate with each document dE Da set of candidate entities d.E, in our case a restricted set of token sequences from the document.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
Each entity e E d.E is characterized by a predefined set of boolean features e.F.,"{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"This set of features is the same for all candidate entities, and it can be assimilated with the relational database definition of a table.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"One particular feature is e.label which is set to 1 if e is considered a valid extraction, and 0 otherwise.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"In this document model, labels are the only hidden features, and the inference procedure will try to find a most probable assignment of values to labels, given the current model parameters.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"Each document is associated with an undirected graphical model, with nodes corresponding directly to entity features, one node for each feature of each candidate entity in the document.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
The set of edges is created by matching clique templates against the entire set of entities d.E.,"{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"A clique template is a procedure that finds all subsets of entities satisfying a given constraint, after which, for each entity subset, it connects a selected set of feature nodes so that they form a clique.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"Formally, there is a set of clique templates C, with each template c E C specified by: Given a set, E, of nodes, Mc(E) C 2E consists of subsets of entities whose feature nodes S, are to be connected in a clique.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"In previous applications of RMNs, the selected subsets of entities for a given template have the same size; however, our clique templates may match a variable number of entities.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"The set 5, may contain the same feature from different entities.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"Usually, for each entity in the matching set, its label is included in Sc.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are described in detail.,"{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"Depending on the number of hidden labels in we define two categories of clique templates: After the graph model for a document d has been completed with cliques from all templates, the probability distribution over the random field of hidden entity labels d.Y given the observed features d.X is computed as: The above distribution presents the RMN as a Markov random field (MRF) with the clique templates as a method for tying potential values across different cliques in the graphical model.","{'title': '2 The RMN Framework for Entity Recognition', 'number': '2'}"
"Like most entity names, almost all proteins in our data are base noun phrases or parts of them.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"Therefore, such substrings are used to determine candidate entities.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"To avoid missing options, we adopt a very broad definition of base noun phrase.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"Definition 1: A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from { &quot;JJ&quot;, &quot;VBN&quot;, &quot;VBG&quot;, &quot;POS&quot;, &quot;NN&quot;, &quot;NNS&quot;, &quot;NNP&quot;, &quot;NNPS&quot;, &quot;CD&quot;, &quot;-&quot;}, and whose last word (the head) is tagged either as a noun, or a number.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"Candidate extractions consist of base NPs, augmented with all their contiguous subsequences headed by a noun or number.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"The set of features associated with each candidate is based on the feature templates introduced in (Collins, 2002), used there for training a ranking algorithm on the extractions returned by a maximum-entropy tagger.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"Many of these features use the concept of word type, which allows a different form of token generalization than POS tags.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"The short type of a word is created by replacing any maximal contiguous sequences of capital letters with 'A', of lowercase letters with 'a', and of digits with '0'.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"For example, the word TGF-1 would be mapped to type A-0.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"Consequently, each token position i in a candidate extraction provides three types of information: the word itself wi, its POS tag t, and its short type si.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"The full set of features types is listed in Table 1, where we consider a generic elabel φHD=enzyme elabel φPF=A0_a ... φSF=A0_a ... φSF=a Note that the factor graph above has an equivalent RMN graph consisting of a one-node clique only, on which it is hard to visualize the various potentials involved.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
"There are cases where different factor graphs may yield the same underlying RMN graph, which makes the factor graph representation preferable.","{'title': '3 Candidate Entities and Entity Features', 'number': '3'}"
Global clique templates enable us to model hypothesized influences between entities from the same document.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"They connect the label nodes of two or more entities, which, in the factor graph, translates into potential nodes connected to at least two label nodes.","{'title': '5 Global Clique Templates', 'number': '4'}"
"In our experiments we use three global templates: Overlap Template (OT): No two entity names overlap in the text i.e if the span of one entity is [Si, el] and the span of another entity is [82, e2], and Si < 82, then el < 82.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Repeat Template (RT): If multiple entities in the same document are repetitions of the same name, their labels tend to have the same value (i.e. most of them are protein names, or most of them are not protein names).","{'title': '5 Global Clique Templates', 'number': '4'}"
"Later we discuss situations in which repetitions of the same protein name are not tagged as proteins, and design an approach to handle this.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Acronym Template (AT): It is common convention that a protein is first introduced by its long name, immediately followed by its short-form (acronym) in parentheses.","{'title': '5 Global Clique Templates', 'number': '4'}"
The definition of a candidate extraction from Section 3 leads to many overlapping entities.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"For example, 'glutathione S - transferase' is a base NP, and it generates five candidate extractions: 'glutathione', 'glutathione S', 'glutathione S - transferase', 'S - transferase', and 'transferase'.","{'title': '5 Global Clique Templates', 'number': '4'}"
"If 'gintathione S - transferase' has label-value 1, because the other four entities overlap with it, they should all have label-value 0.","{'title': '5 Global Clique Templates', 'number': '4'}"
"This type of constraint is enforced by the overlap template whose M operator matches any two overlapping candidate entities, and which connects their label nodes (specified in S) through a potential node with a potential function cb that allows at most one of them to have label-value 1, as illustrated in Table 2.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Continuing with the previous example, because 'gintathione S' and 'S - transferase' are two overlapping entities, the factor graph model will contain an overlap potential node connected to the label nodes of these two entities.","{'title': '5 Global Clique Templates', 'number': '4'}"
"An alternative solution for the overlap template is to create a potential node for each token position that is covered by at least two candidate entities in the document, and connect it to their label nodes.","{'title': '5 Global Clique Templates', 'number': '4'}"
The difference in this case is that the potential node will be connected to a variable number of entity label nodes.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"However this second approach has the advantage of creating fewer potential nodes in the document factor graph, which results in faster inference.","{'title': '5 Global Clique Templates', 'number': '4'}"
"We could specify the potential for the repeat template in a similar 2-by-2 table, this time leaving the table entries to be learned, given that it is not a hard constraint.","{'title': '5 Global Clique Templates', 'number': '4'}"
However we can do better by noting that the vast majority of cases where a repeated protein name is not also tagged as a protein happens when it is part of a larger phrase that is tagged.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"For example, 'HDAC1 enzyme' is a protein name, therefore 'HDAC1' is not tagged in this phrase, even though it may have been tagged previously in the abstract where it was not followed by 'enzyme'.","{'title': '5 Global Clique Templates', 'number': '4'}"
We need a potential that allows two entities with the same text to have different labels if the entity with label-value 0 is inside another entity with label-value 1.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"But a candidate entity may be inside more than one &quot;including&quot; entity, and the number of including entities may vary from one candidate extraction to another.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Using the example from Section 5.1, the candidate entity 'glutathione' is included in two other entities: 'glutathione S' and 'glutathione S - transferase'.","{'title': '5 Global Clique Templates', 'number': '4'}"
"In order to instantiate potentials over variable number of label nodes, we introduce a logical OR clique template that matches a variable number of entities.","{'title': '5 Global Clique Templates', 'number': '4'}"
"When this template matches a subset of entities el, e2, ... , en, it will create an auxiliary OR entity e,, with a single feature e„ .1 abel .","{'title': '5 Global Clique Templates', 'number': '4'}"
"The potential function is set so that it assigns a non-zero potential only when e, .1 abel = el .1 abel V e2.1abel V ...V en .1 abel .","{'title': '5 Global Clique Templates', 'number': '4'}"
"The cliques are only created as needed, e.g. when the auxiliary OR variable is required by repeat and acronym clique templates.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Figure 3 shows the factor graph for a samverges, it gives a good approximation to the correct marginals.","{'title': '5 Global Clique Templates', 'number': '4'}"
"The algorithm works by altering the belief at each label node by repeatedly passing messages between the node and all potential nodes connected to it (Kschischang et al., 2001).","{'title': '5 Global Clique Templates', 'number': '4'}"
"As many of the label nodes are indirectly connected through potential nodes instantiated by global templates, their belief values will propagate in the graph and mutually influence each other, leading in the end to a collective labeling decision.","{'title': '5 Global Clique Templates', 'number': '4'}"
The time complexity of computing messages from a potential node to a label node is exponential in the number of label nodes attached to the potential.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"Since this &quot;fan-in&quot; can be large for OR potential nodes, this step required optimization.","{'title': '5 Global Clique Templates', 'number': '4'}"
"Fortunately, due to the special form of the OR potential, and the normalization before each message-passing step, we were able to develop a linear-time algorithm for this special case.","{'title': '5 Global Clique Templates', 'number': '4'}"
Details are omitted due to limited space.,"{'title': '5 Global Clique Templates', 'number': '4'}"
"Following a maximum likelihood estimation, we shall use the log-linear representation of potentials: where A is a vector of binary features, one for each configuration of values for X, and K. Let w be the concatenated vector of all potential parameters wc.","{'title': '7 Learning Potentials in Factor Graphs', 'number': '5'}"
"One approach to finding the maximum-likelihood solution for w is to use a gradient-based method, which requires computing the gradient of the log-likelihood with respect to potential parameters wc.","{'title': '7 Learning Potentials in Factor Graphs', 'number': '5'}"
"It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w. This expectation is expensive to compute, since it requires summing over all possible configurations of candidate entity labels from a given document.","{'title': '7 Learning Potentials in Factor Graphs', 'number': '5'}"
"To circumvent this complexity, we use Collins' voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01.","{'title': '7 Learning Potentials in Factor Graphs', 'number': '5'}"
We have tested the RMN approach on two datasets that have been hand-tagged for human protein names.,"{'title': '8 Experimental Results', 'number': '6'}"
The first dataset is Yapexl which consists of 200 Medline abstracts.,"{'title': '8 Experimental Results', 'number': '6'}"
"Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999).","{'title': '8 Experimental Results', 'number': '6'}"
It contains a total of 3713 protein references.,"{'title': '8 Experimental Results', 'number': '6'}"
"The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).","{'title': '8 Experimental Results', 'number': '6'}"
"It consists of 225 Medline abstracts, of which 200 are known to describe interactions between human proteins, while the other 25 do not refer to any interaction.","{'title': '8 Experimental Results', 'number': '6'}"
There are 4084 protein references in this dataset.,"{'title': '8 Experimental Results', 'number': '6'}"
"We compared the performance of three systems: LT-RMN is the RMN approach using local templates and the overlap template, GLT-RMN is the full RMN approach, using both local and global templates, and CRF, which uses a CRF for labeling token sequences.","{'title': '8 Experimental Results', 'number': '6'}"
"We used the CRF implementation from (McCallum, 2002) with the set of tags and features used by the MaximumEntropy tagger described in (Bunescu et al., 2004).","{'title': '8 Experimental Results', 'number': '6'}"
"All Medline abstracts were tokenized and then POS tagged using Brill's tagger (Brill, 1995).","{'title': '8 Experimental Results', 'number': '6'}"
"Each extracted protein name in the test data was compared to the human-tagged data, with the positions taken into account.","{'title': '8 Experimental Results', 'number': '6'}"
Two extractions are considered a match if they consist of the same character sequence in the same position in the text.,"{'title': '8 Experimental Results', 'number': '6'}"
"Results are shown in Tables 3 and 4 which give average precision, recall, and F-measure using 10-fold cross validation.","{'title': '8 Experimental Results', 'number': '6'}"
"These tables show that, in terms of Fmeasure, the use of global templates for modto improve a Maximum-Entropy tagger; however, these features do not fully capture the mutual influence between the labels of acronyms and their long forms, or between entity repetitions.","{'title': '8 Experimental Results', 'number': '6'}"
"In particular, they only allow earlier extractions in a document to influence later ones and not vice-versa.","{'title': '8 Experimental Results', 'number': '6'}"
"The RMN approach handles these and potentially other mutual influences between entities in a more complete, probabilistically sound manner.","{'title': '8 Experimental Results', 'number': '6'}"
We have presented an approach to collective information extraction that uses Relational Markov Networks to reason about the mutual influences between multiple extractions.,"{'title': '10 Conclusions and Future Work', 'number': '7'}"
"A new type of clique template — the logical OR template — was introduced, allowing a variable number of relevant entities to be used by other clique templates.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
"Soft correlations between repetitions and acronyms and their long form in the same document have been captured by global clique templates, allowing for local extraction decisions to propagate and mutually influence each other.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
"Regarding future work, a richer set of features for the local templates would likely improve performance.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
"Currently, LT-RMN's accuracy is still significantly less than CRF's, which limits the performance of the full system.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
Another limitation is the approximate inference used by both RMN methods.,"{'title': '10 Conclusions and Future Work', 'number': '7'}"
"The number of factor graphs for which the sum-product algorithm did not converge was non-negligible, and our approach stopped after a fix number of iterations.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
"Besides exploring improvements to loopy belief propagation that increase computational cost (Yedidia et al., 2000), we intend to examine alternative approximate-inference methods.","{'title': '10 Conclusions and Future Work', 'number': '7'}"
This work was partially supported by grants IIS-0117308 and IIS-0325116 from the NSF.,"{'title': '11 Acknowledgements', 'number': '8'}"
