col1,col2
algebraic path problem (shortest paths; matrix inver- 34(3):191–219.,{}
Richard Sproat and Michael Riley.,{}
1996.,{}
Compilation of weighted finite-state transducers from decision trees. of the 34th Annual Meeting of the Andreas Stolcke and Stephen M. Omohundro.,{}
1994.,{}
Best-first model merging for hidden Markov model induction.,{}
Tech.,{}
"Report ICSI TR-94-003, Berkeley, CA.",{}
Robert Endre Tarjan.,{}
1981a.,{}
"A unified approach to path of the 28(3):577–593, July.",{}
Robert Endre Tarjan.,{}
1981b.,{}
"Fast algorithms for solving problems. of the 28(3):594–614, July.",{}
G. van Noord and D. Gerdemann.,{}
2001.,{}
An extendible regular expression compiler for finite-state approaches natural language processing.,{}
In Impleno.,{}
22 in Springer Lecture Notes in CS.,{}
"Rational relations on strings have become widespread in language and speech engineering (Roche and Schabes, 1997).","{'title': '1 Background and Motivation', 'number': '1'}"
"Despite bounded memory they are well-suited to describe many linguistic and textual processes, either exactly or approximately.","{'title': '1 Background and Motivation', 'number': '1'}"
"A relation is a set of (input, output) pairs.","{'title': '1 Background and Motivation', 'number': '1'}"
Relations are more general than functions because they may pair a given input string with more or fewer than one output string.,"{'title': '1 Background and Motivation', 'number': '1'}"
The class of so-called rational relations admits a nice declarative programming paradigm.,"{'title': '1 Background and Motivation', 'number': '1'}"
Source code describing the relation (a regular expression) is compiled into efficient object code (in the form of a 2-tape automaton called a finite-state transducer).,"{'title': '1 Background and Motivation', 'number': '1'}"
The object code can even be optimized for runtime and code size (via algorithms such as determinization and minimization of transducers).,"{'title': '1 Background and Motivation', 'number': '1'}"
"This programming paradigm supports efficient nondeterminism, including parallel processing over infinite sets of input strings, and even allows “reverse” computation from output to input.","{'title': '1 Background and Motivation', 'number': '1'}"
Its unusual flexibility for the practiced programmer stems from the many operations under which rational relations are closed.,"{'title': '1 Background and Motivation', 'number': '1'}"
"It is common to define further useful operations (as macros), which modify existing relations not by editing their source code but simply by operating on them “from outside.” ∗A brief version of this work, with some additional material, first appeared as (Eisner, 2001a).","{'title': '1 Background and Motivation', 'number': '1'}"
A leisurely journal-length version with more details has been prepared and is available.,"{'title': '1 Background and Motivation', 'number': '1'}"
"The entire paradigm has been generalized to weighted relations, which assign a weight to each (input, output) pair rather than simply including or excluding it.","{'title': '1 Background and Motivation', 'number': '1'}"
"If these weights represent probabilities P(input, output) or P(output  |input), the weighted relation is called a joint or conditional (probabilistic) relation and constitutes a statistical model.","{'title': '1 Background and Motivation', 'number': '1'}"
"Such models can be efficiently restricted, manipulated or combined using rational operations as before.","{'title': '1 Background and Motivation', 'number': '1'}"
An artificial example will appear in §2.,"{'title': '1 Background and Motivation', 'number': '1'}"
"The availability of toolkits for this weighted case (Mohri et al., 1998; van Noord and Gerdemann, 2001) promises to unify much of statistical NLP.","{'title': '1 Background and Motivation', 'number': '1'}"
"Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding,' including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).","{'title': '1 Background and Motivation', 'number': '1'}"
"Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.","{'title': '1 Background and Motivation', 'number': '1'}"
"Unfortunately, there is a stumbling block: Where do the weights come from?","{'title': '1 Background and Motivation', 'number': '1'}"
"After all, statistical models require supervised or unsupervised training.","{'title': '1 Background and Motivation', 'number': '1'}"
"Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.","{'title': '1 Background and Motivation', 'number': '1'}"
"Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.","{'title': '1 Background and Motivation', 'number': '1'}"
"For example, the forward-backward algorithm (Baum, 1972) trains only Hidden Markov Models, while (Ristad and Yianilos, 1996) trains only stochastic edit distance.","{'title': '1 Background and Motivation', 'number': '1'}"
"In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run.","{'title': '1 Background and Motivation', 'number': '1'}"
"'Given output, find input to maximize P(input, output).","{'title': '1 Background and Motivation', 'number': '1'}"
"This paper aims to provide a remedy through a new paradigm, which we call parameterized finitestate machines.","{'title': '1 Background and Motivation', 'number': '1'}"
It lays out a fully general approach for training the weights of weighted rational relations.,"{'title': '1 Background and Motivation', 'number': '1'}"
"First §2 considers how to parameterize such models, so that weights are defined in terms of underlying parameters to be learned.","{'title': '1 Background and Motivation', 'number': '1'}"
§3 asks what it means to learn these parameters from training data (what is to be optimized?,"{'title': '1 Background and Motivation', 'number': '1'}"
"), and notes the apparently formidable bookkeeping involved.","{'title': '1 Background and Motivation', 'number': '1'}"
§4 cuts through the difficulty with a surprisingly simple trick.,"{'title': '1 Background and Motivation', 'number': '1'}"
"Finally, §5 removes inefficiencies from the basic algorithm, making it suitable for inclusion in an actual toolkit.","{'title': '1 Background and Motivation', 'number': '1'}"
Such a toolkit could greatly shorten the development cycle in natural language engineering.,"{'title': '1 Background and Motivation', 'number': '1'}"
"Finite-state machines, including finite-state automata (FSAs) and transducers (FSTs), are a kind of labeled directed multigraph.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"For ease and brevity, we explain them by example.","{'title': '2 Transducers and Parameters', 'number': '2'}"
Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"1a shows a probabilistic FST with input alphabet E = {a, b}, output alphabet A = {x, z}, and all states final.","{'title': '2 Transducers and Parameters', 'number': '2'}"
It may be regarded as a device for generating a string pair in E* x A* by a random walk from Q.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"Two paths exist that generate both input aabb and output xz: Each of the paths has probability .0002646, so the probability of somehow generating the pair (aabb, xz) is .0002646 + .0002646 = .0005292.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Abstracting away from the idea of random walks, arc weights need not be probabilities.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Still, define a path’s weight as the product of its arc weights and the stopping weight of its final state.","{'title': '2 Transducers and Parameters', 'number': '2'}"
Thus Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"1a defines a weighted relation f where f(aabb, xz) = .0005292.","{'title': '2 Transducers and Parameters', 'number': '2'}"
This particular relation does happen to be probabilistic (see §1).,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"It represents a joint distribution (since Ex,y f(x, y) = 1).","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Meanwhile, Fig.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"1c defines a conditional one (bx Ey f(x, y) = 1).","{'title': '2 Transducers and Parameters', 'number': '2'}"
This paper explains how to adjust probability distributions like that of Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
1a so as to model training data better.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
The algorithm improves an FST’s numeric weights while leaving its topology fixed.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
How many parameters are there to adjust in Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
1a?,"{'title': '2 Transducers and Parameters', 'number': '2'}"
That is up to the user who built it!,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"An FST model with few parameters is more constrained, making optimization easier.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Some possibilities: generate E if heads, F if tails.” E*λ = (AE)∗(1−A) means “repeatedly flip an A-weighted coin and keep repeating E as long as it comes up heads.” These 4 parameters have global effects on Fig.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"1a, thanks to complex parameter tying: arcs ® b:p −) @, ® b:q −) ® in Fig.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"1b get respective probabilities (1 − A)µν and (1 − µ)ν, which covary with ν and vary oppositely with µ.","{'title': '2 Transducers and Parameters', 'number': '2'}"
Each of these probabilities in turn affects multiple arcs in the composed FST of Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
1a.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"We offer a theorem that highlights the broad applicability of these modeling techniques.4 If f(input, output) is a weighted regular relation, then the following statements are equivalent: (1) f is a joint probabilistic relation; (2) f can be computed by a Markovian FST that halts with probability 1; (3) f can be expressed as a probabilistic regexp, i.e., a regexp built up from atomic expressions a : b (for a E E U {E}, b E A U {E}) using concatenation, probabilistic union +p, and probabilistic closure *p. For defining conditional relations, a good regexp language is unknown to us, but they can be defined in several other ways: (1) via FSTs as in Fig.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"1c, (2) by compilation of weighted rewrite rules (Mohri and Sproat, 1996), (3) by compilation of decision trees (Sproat and Riley, 1996), (4) as a relation that performs contextual left-to-right replacement of input substrings by a smaller conditional relation (Gerdemann and van Noord, 1999),5 (5) by conditionalization of a joint relation as discussed below.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"A central technique is to define a joint relation as a noisy-channel model, by composing a joint relation with a cascade of one or more conditional relations as in Fig.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"1 (Pereira and Riley, 1997; Knight and Graehl, 1998).","{'title': '2 Transducers and Parameters', 'number': '2'}"
"The general form is illustrated by 3Conceptually, the parameters represent the probabilities of reading another a (A); reading another b (ν); transducing b to p rather than q (µ); starting to transduce p to a rather than x (p).","{'title': '2 Transducers and Parameters', 'number': '2'}"
"P(v, z) def = Ew,x,y P(v|w)P(w, x)P(y|x)P(z|y), implemented by composing 4 machines.6,7 There are also procedures for defining weighted FSTs that are not probabilistic (Berstel and Reutenauer, 1988).","{'title': '2 Transducers and Parameters', 'number': '2'}"
Arbitrary weights such as 2.7 may be assigned to arcs or sprinkled through a regexp (to be compiled into E:E/2.7 −)arcs).,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"A more subtle example is weighted FSAs that approximate PCFGs (Nederhof, 2000; Mohri and Nederhof, 2001), or to extend the idea, weighted FSTs that approximate joint or conditional synchronous PCFGs built for translation.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"These are parameterized by the PCFG’s parameters, but add or remove strings of the PCFG to leave an improper probability distribution.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Fortunately for those techniques, an FST with positive arc weights can be normalized to make it jointly or conditionally probabilistic: ization, which simply divides each f(x, y) by Ex,,y, f(x', y') (joint case) or by Ey, f(x, y') (conditional case).","{'title': '2 Transducers and Parameters', 'number': '2'}"
"To implement the joint case, just divide stopping weights by the total weight of all paths (which §4 shows how to find), provided this is finite.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"In the conditional case, let g be a copy of f with the output labels removed, so that g(x) finds the desired divisor; determinize g if possible (but this fails for some weighted FSAs), replace all weights with their reciprocals, and compose the result with f.9 6P(w, x) defines the source model, and is often an “identity FST” that requires w = x, really just an FSA.","{'title': '2 Transducers and Parameters', 'number': '2'}"
7We propose also using n-tape automata to generalize to “branching noisy channels” (a case of dendroid distributions).,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"In Ew,x P(v|w)P(v,|w)P(w, x)P(y|x), the true transcription w can be triply constrained by observing speech y and two errorful transcriptions v, v', which independently depend on w. 8A corresponding problem exists in the joint case, but may be easily avoided there by first pruning non-coaccessible states.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"9It suffices to make g unambiguous (one accepting path per string), a weaker condition than determinism.","{'title': '2 Transducers and Parameters', 'number': '2'}"
When this is not possible (as in the inverse of Fig.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"1b, whose conditionalizaNormalization is particularly important because it enables the use of log-linear (maximum-entropy) parameterizations.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Here one defines each arc weight, coin weight, or regexp weight in terms of meaningful features associated by hand with that arc, coin, etc.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Each feature has a strength E R>0, and a weight is computed as the product of the strengths of its features.10 It is now the strengths that are the learnable parameters.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"This allows meaningful parameter tying: if certain arcs such asu:i �—*, �—*, and a:ae o:e �—* share a contextual “vowel-fronting” feature, then their weights rise and fall together with the strength of that feature.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Such approaches have been tried recently in restricted cases (McCallum et al., 2000; Eisner, 2001b; Lafferty et al., 2001).","{'title': '2 Transducers and Parameters', 'number': '2'}"
"Normalization may be postponed and applied instead to the result of combining the FST with other FSTs by composition, union, concatenation, etc.","{'title': '2 Transducers and Parameters', 'number': '2'}"
"A simple example is a probabilistic FSA defined by normalizing the intersection of other probabilistic FSAs f1, f2,.","{'title': '2 Transducers and Parameters', 'number': '2'}"
.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
.. (This is in fact a log-linear model in which the component FSAs define the features: string x has log fi(x) occurrences of feature i.),"{'title': '2 Transducers and Parameters', 'number': '2'}"
"In short, weighted finite-state operators provide a language for specifying a wide variety of parameterized statistical models.","{'title': '2 Transducers and Parameters', 'number': '2'}"
Let us turn to their training.,"{'title': '2 Transducers and Parameters', 'number': '2'}"
"We are primarily concerned with the following training paradigm, novel in its generality.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
Let fθ : E* xA* —* R>0 be a joint probabilistic relation that is computed by a weighted FST.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
The FST was built by some recipe that used the parameter vector 0.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Changing 0 may require us to rebuild the FST to get updated weights; this can involve composition, regexp compilation, multiplication of feature strengths, etc.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"(Lazy algorithms that compute arcs and states of tion cannot be realized by any weighted FST), one can sometimes succeed by first intersecting g with a smaller regular set in which the input being considered is known to fall.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"In the extreme, if each input string is fully observed (not the case if the input is bound by composition to the output of a one-to-many FST), one can succeed by restricting g to each input string in turn; this amounts to manually dividing f(x, y) by g(x). fθ on demand (Mohri et al., 1998) can pay off here, since only part of fθ may be needed subsequently.)","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"As training data we are given a set of observed (input, output) pairs, (xi, yi).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"These are assumed to be independent random samples from a joint distribution of the form fe(x, y); the goal is to recover the true ˆ0.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Samples need not be fully observed (partly supervised training): thus xi C E*, yi C A* may be given as regular sets in which input and output were observed to fall.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"For example, in ordinary HMM training, xi = E* and represents a completely hidden state sequence (cf.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Ristad (1998), who allows any regular set), while yi is a single string representing a completely observed emission sequence.11 What to optimize?","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Maximum-likelihood estimation guesses 0ˆ to be the 0 maximizing Hi fθ(xi, yi).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Maximum-posterior estimation tries to maximize P(0)·Hi fθ(xi, yi) where P(0) is a prior probability.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"In a log-linear parameterization, for example, a prior that penalizes feature strengths far from 1 can be used to do feature selection and avoid overfitting (Chen and Rosenfeld, 1999).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"The EM algorithm (Dempster et al., 1977) can maximize these functions.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Roughly, the E step guesses hidden information: if (xi, yi) was generated from the current fθ, which FST paths stand a chance of having been the path used?","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
(Guessing the path also guesses the exact input and output.),"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
The M step updates 0 to make those paths more likely.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
EM alternates these steps and converges to a local optimum.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
The M step’s form depends on the parameterization and the E step serves the M step’s needs.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
Let fθ be Fig.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"1a and suppose (xi, yi) = (a(a + b)*, xxz).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"During the E step, we restrict to paths compatible with this observation by computing xi o fθ o yi, shown in Fig.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
2.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"To find each path’s posterior probability given the observation (xi, yi), just conditionalize: divide its raw probability by the total probability (Pz� 0.1003) of all paths in Fig.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
2.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"11To implement an HMM by an FST, compose a probabilistic FSA that generates a state sequence of the HMM with a conditional FST that transduces HMM states to emitted symbols.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
But that is not the full E step.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
The M step uses not individual path probabilities (Fig.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
2 has infinitely many) but expected counts derived from the paths.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Crucially, §4 will show how the E step can accumulate these counts effortlessly.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"We first explain their use by the M step, repeating the presentation of §2: in Fig.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"2 is “really” to traverse Q a:x Rosenfeld, 1999).12 For globally normalized, joint models, the predicted vector is ecf(E*, A*).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"If the log-linear probabilities are conditioned on the state and/or the input, the predicted vector is harder to describe (though usually much easier to compute).13 12IIS is itself iterative; to avoid nested loops, run only one iteration at each M step, giving a GEM algorithm (Riezler,1999).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Alternatively, discard EM and use gradient-based optimization.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"13For per-state conditional normalization, let Dj,a be the set of arcs from state j with input symbol a E E; their weights are normalized to sum to 1.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Besides computing c, the E step must count the expected number dj,a of traversals of arcs in each Dj,a.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Then the predicted vector given θ is Ej,a dj,a ·(expected feature counts on a randomly chosen arc in Dj,a).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Per-state joint normalization (Eisner, 2001b, §8.2) is similar but drops the dependence on a.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
The difficult case is global conditional normalization.,"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"It arises, for example, when training a joint model of the form fθ = · · · (gθ o hθ) · · ·, where hθ is a conditional It is also possible to use this EM approach for discriminative training, where we wish to maximize Hi P(yi  |xi) and fθ(x, y) is a conditional FST that defines P(y  |x).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"The trick is to instead train a joint model g o fθ, where g(xi) defines P(xi), thereby maximizing Hi P(xi) · P(yi  |xi).","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"(Of course, the method of this paper can train such compositions.)","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"If x1,... xn are fully observed, just define each g(xi) = 1/n.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"But by choosing a more general model of g, we can also handle incompletely observed xi: training g o fθ then forces g and fθ to cooperatively reconstruct a distribution over the possible inputs and do discriminative training of fθ given those inputs.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
(Any parameters of g may be either frozen before training or optimized along with the parameters of fθ.),"{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"A final possibility is that each xi is defined by a probabilistic FSA that already supplies a distribution over the inputs; then we consider xi o fθ o yi directly, just as in the joint model.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"Finally, note that EM is not all-purpose.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"It only maximizes probabilistic objective functions, and even there it is not necessarily as fast as (say) conjugate gradient.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"For this reason, we will also show below how to compute the gradient of fθ(xi, yi) with respect to 0, for an arbitrary parameterized FST fθ.","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"We remark without elaboration that this can help optimize task-related objective functions, such as E Ey(P(xi, y)α/ Ey' P(xi, y�)α) · error(y, yi). i","{'title': '3 Estimation in Parameterized FSTs', 'number': '3'}"
"It remains to devise appropriate E steps, which looks rather daunting.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
Each path in Fig.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"2 weaves together parameters from other machines, which we must untangle and tally.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"In the 4-coin parameterization, path observed heads and tails of the 4 coins.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"This nontrivially works out to (4, 1, 0,1,1,1,1, 2).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"For other parameterizations, the path must instead yield a vector of arc traversal counts or feature counts.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Computing a count vector for one path is hard enough, but it is the E step’s job to find the expected value of this vector—an average over the infinitely log-linear model of P(v  |u) for u E E'*, v E 0'*.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Then the predicted count vector contributed by h is Ei EuEΣ,∗ P(u xi, yi) · ech(u, 0'*).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The term Ei P(u  |xi, yi) computes the expected count of each u E E'*.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
It may be found by a variant of §4 in which path values are regular expressions over E'*. many paths π through Fig.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"2 in proportion to their posterior probabilities P(π  |xi, yi).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The results for all (xi, yi) are summed and passed to the M step.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Abstractly, let us say that each path π has not only a probability P(π) E [0, 1] but also a value val(π) in a vector space V , which counts the arcs, features, or coin flips encountered along path π.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
The value of a path is the sum of the values assigned to its arcs.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The E step must return the expected value of the unknown path that generated (xi, yi).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"For example, if every arc had value 1, then expected value would be expected path length.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
Letting H denote the set of paths in xi o fe o yi (Fig.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"2), the expected value is14 The denominator of equation (1) is the total probability of all accepting paths in xi o f o yi.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"But while computing this, we will also compute the numerator.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The idea is to augment the weight data structure with expectation information, so each weight records a probability and a vector counting the parameters that contributed to that probability.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"We will enforce an invariant: the weight of any pathset H must be (&EΠ P(π), &EΠ P(π) val(π)) E R>0 x V , from which (1) is trivial to compute.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
Berstel and Reutenauer (1988) give a sufficiently general finite-state framework to allow this: weights may fall in any set K (instead of R).,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
Multiplication and addition are replaced by binary operations ® and ® on K. Thus ® is used to combine arc weights into a path weight and ® is used to combine the weights of alternative paths.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"To sum over infinite sets of cyclic paths we also need a closure operation *, interpreted as k* = (D'0 ki.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The usual finite-state algorithms work if (K, ®, ®, *) has the structure of a closed semiring.15 Ordinary probabilities fall in the semiring (R>0, +, x, *).16 Our novel weights fall in a novel If an arc has probability p and value v, we give it the weight (p, pv), so that our invariant (see above) holds if H consists of a single length-0 or length-1 path.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
The above definitions are designed to preserve our invariant as we build up larger paths and pathsets.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"® lets us concatenate (e.g.) simple paths π1, π2 to get a longer path π with P(π) = P(π1)P(π2) and val(π) = val(π1) + val(π2).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"The definition of ® guarantees that path π’s weight will be (P(π), P(π) · val(π)).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"® lets us take the union of two disjoint pathsets, and * computes infinite unions.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"To compute (1) now, we only need the total weight ti of accepting paths in xi o f o yi (Fig.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
2).,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"This can be computed with finite-state methods: the machine (exxi)of o(yixc) is aversion that replaces all input:output labels with c: c, so it maps (E, 6) to the same total weight ti.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
Minimizing it yields a onestate FST from which ti can be read directly!,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
The other “magical” property of the expectation semiring is that it automatically keeps track of the tangled parameter counts.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"For instance, recall that traversing Q a:x −) Q should have the same effect as traversing both the underlying arcs ® a:p −) ® and © p:x −) ©.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"And indeed, if the underlying arcs have values v1 and v2, then the composed arc @ a:x −) @ gets weight �,„1,p1v1) ® p ( g �N2,p2v2) = (p1p2, p1p2(v1 + v2)), just as if it had value v1 + v2.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Some concrete examples of values may be useful: Really we are manipulating weighted relations, not FSTs.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"We may combine FSTs, or determinize or minimize them, with any variant of the semiringweighted algorithms.17 As long as the resulting FST computes the right weighted relation, the arrangement of its states, arcs, and labels is unimportant.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
The same semiring may be used to compute gradients.,"{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"We would like to find fθ(xi, yi) and its gradient with respect to θ, where fθ is real-valued but need not be probabilistic.","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Whatever procedures are used to evaluate fθ(xi, yi) exactly or approximately—for example, FST operations to compile fθ followed by minimization of (c x xi) o fθ o (yi x c)—can simply be applied over the expectation semiring, replacing each weight p by (p, Vp) and replacing the usual arithmetic operations with ⊕, ⊗, etc.18 (2)–(4) preserve the gradient ((2) is the derivative product rule), so this computation yields (fθ(xi, yi), Vfθ(xi, yi)).","{'title': '4 The E Step: Expectation Semirings', 'number': '4'}"
"Now for some important remarks on efficiency: • Computing ti is an instance of the well-known algebraic path problem (Lehmann, 1977; Tar an, 1981a).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
Let Ti = xiofoyi.,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
Then ti is the total semiring weight w0n of paths in Ti from initial state 0 to final state n (assumed WLOG to be unique and unweighted).,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
"It is wasteful to compute ti as suggested earlier, by minimizing (cxxi)of o(yixE), since then the real work is done by an c-closure step (Mohri, 2002) that implements the all-pairs version of algebraic path, whereas all we need is the single-source version.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"If n and m are the number of states and edges,19 then both problems are O(n3) in the worst case, but the single-source version can be solved in essentially O(m) time for acyclic graphs and other reducible flow graphs (Tar an, 1981b).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"For a general graph Ti, Tar an (1981b) shows how to partition into “hard” subgraphs that localize the cyclicity or irreducibility, then run the O(n3) algorithm on each subgraph (thereby reducing n to as little as 1), and recombine the results.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
The overhead of partitioning and recombining is essentially only O(m).,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
"• For speeding up the O(n3) problem on subgraphs, one can use an approximate relaxation technique (Mohri, 2002).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"Efficient hardware implementation is also possible via chip-level parallelism (Rote, 1985).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"• In many cases of interest, Ti is an acyclic graph.20 Then Tar an’s method computes w0j for each j in topologically sorted order, thereby finding ti in a linear number of ⊕ and ⊗ operations.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"For HMMs (footnote 11), Ti is the familiar trellis, and we would like this computation of ti to reduce to the forwardbackward algorithm (Baum, 1972).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
But notice that it has no backward pass.,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
"In place of pushing cumulative probabilities backward to the arcs, it pushes cumulative arcs (more generally, values in V ) forward to the probabilities.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"This is slower because our ⊕ and ⊗ are vector operations, and the vectors rapidly lose sparsity as they are added together.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
We therefore reintroduce a backward pass that lets us avoid ⊕ and ⊗ when computing ti (so they are needed only to construct Ti).,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
This speedup also works for cyclic graphs and for any V .,"{'title': '5 Removing Inefficiencies', 'number': '5'}"
"Write wjk as (pjk, vjk), and let w1jk = (p1jk, v1 jk) denote the weight of the edge from j to k.19 Then it can be shown that w0n = (p0n, Ej,k p0jv1jkpkn).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"The forward and backward probabilities, p0j and pkn, can be computed using single-source algebraic path for the simpler semiring (R, +, x, ∗)—or equivalently, by solving a sparse linear system of equations over R, a much-studied problem at O(n) space, O(nm) time, and faster approximations (Greenbaum, 1997).","{'title': '5 Removing Inefficiencies', 'number': '5'}"
"Here, the forward and backward probabilities can be computed in time only O(m + n log n) (Fredman and Tar an, 1987). k-best variants are also possible.","{'title': '5 Removing Inefficiencies', 'number': '5'}"
We have exhibited a training algorithm for parameterized finite-state machines.,"{'title': '6 Discussion', 'number': '6'}"
"Some specific consequences that we believe to be novel are (1) an EM algorithm for FSTs with cycles and epsilons; (2) training algorithms for HMMs and weighted contextual edit distance that work on incomplete data; (3) endto-end training of noisy channel cascades, so that it is not necessary to have separate training data for each machine in the cascade (cf.","{'title': '6 Discussion', 'number': '6'}"
"Knight and Graehl, 20If xi and yi are acyclic (e.g., fully observed strings), and f (or rather its FST) has no a : a cycles, then composition will “unroll” f into an acyclic machine.","{'title': '6 Discussion', 'number': '6'}"
"If only xi is acyclic, then the composition is still acyclic if domain(f) has no a cycles.","{'title': '6 Discussion', 'number': '6'}"
"1998), although such data could also be used; (4) training of branching noisy channels (footnote 7); (5) discriminative training with incomplete data; (6) training of conditional MEMMs (McCallum et al., 2000) and conditional random fields (Lafferty et al., 2001) on unbounded sequences.","{'title': '6 Discussion', 'number': '6'}"
We are particularly interested in the potential for quickly building statistical models that incorporate linguistic and engineering insights.,"{'title': '6 Discussion', 'number': '6'}"
"Many models of interest can be constructed in our paradigm, without having to write new code.","{'title': '6 Discussion', 'number': '6'}"
"Bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective functions, and finite-state algorithms to all of them.","{'title': '6 Discussion', 'number': '6'}"
"To avoid local maxima, one might try deterministic annealing (Rao and Rose, 2001), or randomized methods, or place a prior on θ.","{'title': '6 Discussion', 'number': '6'}"
"Another extension is to adjust the machine topology, say by model merging (Stolcke and Omohundro, 1994).","{'title': '6 Discussion', 'number': '6'}"
Such techniques build on our parameter estimation method.,"{'title': '6 Discussion', 'number': '6'}"
The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.,"{'title': '6 Discussion', 'number': '6'}"
"For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.","{'title': '6 Discussion', 'number': '6'}"
