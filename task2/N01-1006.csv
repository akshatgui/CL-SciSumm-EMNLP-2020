col1,col2
Transformation-based learning has been successfully employed to solve many natural language processing problems.,Introduction
It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.,Introduction
"However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.",Introduction
"In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance.",Introduction
"The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).",Introduction
The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner.,Introduction
This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.,Introduction
Much research in natural language processing has gone into the development of rule-based machine learning algorithms.,Experiment/Discussion
These algorithms are attractive because they often capture the linguistic features of a corpus in a small and concise set of rules.,Experiment/Discussion
"Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.",Experiment/Discussion
"It is a flexible method which is easily extended to various tasks and domains, and it has been applied to a wide variety of NLP tasks, including part of speech tagging (Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996), phrase chunking (Florian et al., 2000), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997).",Experiment/Discussion
"Furthermore, transformationbased learning achieves state-of-the-art performance on several tasks, and is fairly resistant to overtraining (Ramshaw and Marcus, 1994).",Experiment/Discussion
"Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks.",Experiment/Discussion
"For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.",Experiment/Discussion
"This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.",Experiment/Discussion
"In this paper, we present a novel method which enables a transformation-based learner to reduce its training time dramatically while still retaining all of its learning power.",Experiment/Discussion
"In addition, we will show that our method scales better with training data size.",Experiment/Discussion
The central idea of transformation-based learning (TBL) is to learn an ordered list of rules which progressively improve upon the current state of the training set.,Experiment/Discussion
"An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.",Experiment/Discussion
"The following definitions and notations will be used throughout the paper: where Since we are not interested in rules that have a negative objective function value, only the rules that have a positive good (r) need be examined.",Experiment/Discussion
"This leads to the following approach: The system thus learns a list of rules in a greedy fashion, according to the objective function.",Experiment/Discussion
"When no rule that improves the current state of the training set beyond a pre-set threshold can be found, the training phase ends.",Experiment/Discussion
"During the application phase, the evaluation set is initialized with the initial class assignment.",Experiment/Discussion
The rules are then applied sequentially to the evaluation set in the order they were learned.,Experiment/Discussion
The final classification is the one attained when all rules have been applied.,Experiment/Discussion
"As was described in the introductory section, the long training time of TBL poses a serious problem.",Experiment/Discussion
"Various methods have been investigated towards ameliorating this problem, and the following subsections detail two of the approaches.",Experiment/Discussion
One of the most time-consuming steps in transformation-based learning is the updating step.,Experiment/Discussion
"The iterative nature of the algorithm requires that each newly selected rule be applied to the corpus, and the current state of the corpus updated before the next rule is learned.",Experiment/Discussion
Ramshaw & Marcus (1994) attempted to reduce the training time of the algorithm by making the update process more efficient.,Experiment/Discussion
"Their method requires each rule to store a list of pointers to samples that it applies to, and for each sample to keep a list of pointers to rules that apply to it.",Experiment/Discussion
"Given these two sets of lists, the system can then easily: These two processes are performed multiple times during the update process, and the modification results in a significant reduction in running time.",Experiment/Discussion
The disadvantage of this method consists in the system having an unrealistically high memory requirement.,Experiment/Discussion
"For example, a transformation-based text chunker training upon a modestly-sized corpus of 200,000 words has approximately 2 million rules active at each iteration.",Experiment/Discussion
"The additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l The ICA system (Hepple, 2000) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance.",Experiment/Discussion
"To achieve the speedup, the ICA system disallows any interaction between the learned rules, by enforcing the following two assumptions: 'We need to note that the 200k-word corpus used in this experiment is considered small by NLP standards.",Experiment/Discussion
Many of the available corpora contain over 1 million words.,Experiment/Discussion
"As the size of the corpus increases, so does the number of rules and the additional memory space required. state change per sample.",Experiment/Discussion
"In other words, at most one rule is allowed to apply to each sample.",Experiment/Discussion
"This mode of application is similar to that of a decision list (Rivest, 1987), where an sample is modified by the first rule that applies to it, and not modified again thereafter.",Experiment/Discussion
"In general, this assumption will hold for problems which have high initial accuracy and where state changes are infrequent.",Experiment/Discussion
"The ICA system was designed and tested on the task of part-of-speech tagging, achieving an impressive reduction in training time while suffering only a small decrease in accuracy.",Experiment/Discussion
The experiments presented in Section 4 include ICA in the training time and performance comparisons�.,Experiment/Discussion
"Samuel (1998) proposed a Monte Carlo approach to transformation-based learning, in which only a fraction of the possible rules are randomly selected for estimation at each iteration.",Experiment/Discussion
The µ-TBL system described in Lager (1999) attempts to cut down on training time with a more efficient Prolog implementation and an implementation of &quot;lazy&quot; learning.,Experiment/Discussion
"The application of a transformation-based learning can be considerably sped-up if the rules are compiled in a finite-state transducer, as described in Roche and Schabes (1995).",Experiment/Discussion
"The approach presented here builds on the same foundation as the one in (Ramshaw and Marcus, 1994): instead of regenerating the rules each time, they are stored into memory, together with the two values good (r) and bad (r).",Experiment/Discussion
"The following notations will be used throughout this section: t, and t, = T[s]g the samples on which the rule applies and changes them to the correct classification; therefore, good(r) = jG(r)j. t, and C[s] = T [s]g the samples on which the rule applies and changes the classification from correct to incorrect; similarly, bad(r) = jB(r)j.",Experiment/Discussion
"Given a newly learned rule b that is to be applied to S, the goal is to identify the rules r for which at least one of the sets G (r) , B (r) is modified by the application of rule b.",Experiment/Discussion
"Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged.",Experiment/Discussion
"2The algorithm was implemented by the the authors, following the description in Hepple (2000).",Experiment/Discussion
"The presentation is complicated by the fact that, in many NLP tasks, the samples are not independent.",Experiment/Discussion
"For instance, in POS tagging, a sample is dependent on the classification of the preceding and succeeding 2 samples (this assumes that there exists a natural ordering of the samples in S).",Experiment/Discussion
"Let V (s) denote the &quot;vicinity&quot; of a sample the set of samples on whose classification the sample s might depend on (for consistency, s 2 V (s)); if samples are independent, then V (s) = fsg.",Experiment/Discussion
Let s be a sample on which the best rule b applies (i.e.,Experiment/Discussion
[b (s)] =6 C [s]).,Experiment/Discussion
We need to identify the rules r that are influenced by the change s ! b (s).,Experiment/Discussion
Let r be such a rule. f (r) needs to be updated if and only if there exists at least one sample s' such that Each of the above conditions corresponds to a specific update of the good (r) or bad (r) counts.,Experiment/Discussion
"We will discuss how rules which should get their good or bad counts decremented (subcases (1) and (2)) can be generated, the other two being derived in a very similar fashion.",Experiment/Discussion
"The key observation behind the proposed algorithm is: when investigating the effects of applying the rule b to sample s, only samples s' in the set V (s) need to be checked.",Experiment/Discussion
Any sample s' that is not in the set {sIb changes s} can be ignored since s' = b(s').,Experiment/Discussion
Let s' 2 V (s) be a sample in the vicinity of s. There are 2 cases to be examined one in which b applies to s' and one in which b does not: Case I: c (s') = c (b (s')) (b does not modify the classification of sample s').,Experiment/Discussion
"We note that the condition and the formula s' 2 B (r) and b (s') 2� B (r) is equivalent to (for the full details of the derivation, inferred from the definition of G (r) and B (r), please refer to Florian and Ngai (2001)).",Experiment/Discussion
"These formulae offer us a method of generating the rules r which are influenced by the modification s' —� b (s'): (a) If p (b (s')) = false then decrease good (r), where r is the rule created with predicate p s.t. target T [s'];",Experiment/Discussion
"(a) If p (b (s')) = false then for all the rules r whose predicate is p3 and tr =� C [s'] decrease bad (r); The algorithm for generating the rules r that need their good counts (formula (3)) or bad counts (formula (4)) increased can be obtained from the formulae (1) (respectively (2)), by switching the states s' and b (s'), and making sure to add all the new possible rules that might be generated (only for (3)).",Experiment/Discussion
Case II: C [s'] =� C [b (s')] (b does change the classification of sample s').,Experiment/Discussion
"In this case, the formula (5) is transformed into: (again, the full derivation is presented in Florian and Ngai (2001)).",Experiment/Discussion
"The case of (2), however, is much simpler.",Experiment/Discussion
"It is easy to notice that C [s'] =� C [b (s')] and s' E B (r) implies that b (s') E� B (r); indeed, a necessary condition for a sample s' to be in a set C [b (s)] = tr in formula (1) and removing the test altogether for case of (2).",Experiment/Discussion
The formulae used to generate rules r that might have their counts increased (equations (3) and (4)) are obtained in the same fashion as in Case I.,Experiment/Discussion
"At every point in the algorithm, we assumed that all the rules that have at least some positive outcome (good (r) > 0) are stored, and their score computed.",Experiment/Discussion
"Therefore, at the beginning of the algorithm, all the rules that correct at least one wrong classification need to be generated.",Experiment/Discussion
"The bad counts for these rules are then computed by generation as well: in every position that has the correct classification, the rules that change the classification are generated, as in Case 4, and their bad counts are incremented.",Experiment/Discussion
The entire FastTBL algorithm is presented in Figure 1.,Experiment/Discussion
"Note that, when the bad counts are computed, only rules that already have positive good counts are selected for evaluation.",Experiment/Discussion
This prevents the generation of useless rules and saves computational time.,Experiment/Discussion
The number of examined rules is kept close to the minimum.,Experiment/Discussion
"Because of the way the rules are generated, most of them need to modify either one of their counts.",Experiment/Discussion
"Some additional space (besides the one needed to represent the rules) is necessary for representing the rules in a predicate hash in order to For all samples s that satisfy C [s] =� T [s], generate all rules r that correct the classification of s; increase good (r).",Experiment/Discussion
For all samples s that satisfy C [s] = T [s] generate all predicates p s.t. p (s) = true; for each rule r s.t. pr = p and tr =� C [s] increase bad (r).,Experiment/Discussion
1: Find the rule b = argmaxrER f (r). have a straightforward access to all rules that have a given predicate; this amount is considerably smaller than the one used to represent the rules.,Experiment/Discussion
"For example, in the case of text chunking task described in section 4, only approximately 30Mb additional memory is required, while the approach of Ramshaw and Marcus (1994) would require approximately 450Mb.",Experiment/Discussion
"As mentioned before, the original algorithm has a number of deficiencies that cause it to run slowly.",Experiment/Discussion
Among them is the drastic slowdown in rule learning as the scores of the rules decrease.,Experiment/Discussion
"When the best rule has a high score, which places it outside the tail of the score distribution, the rules in the tail will be skipped when the bad counts are calculated, since their good counts are small enough to cause them to be discarded.",Experiment/Discussion
"However, when the best rule is in the tail, many other rules with similar scores can no longer be discarded and their bad counts need to be computed, leading to a progressively longer running time per iteration.",Experiment/Discussion
"Our algorithm does not suffer from the same problem, because the counts are updated (rather than recomputed) at each iteration, and only for the samples that were affected by the application of the latest rule learned.",Experiment/Discussion
"Since the number of affected samples decreases as learning progresses, our algorithm actually speeds up considerably towards the end of the training phase.",Experiment/Discussion
"Considering that the number of low-score rules is a considerably higher than the number of high-score rules, this leads to a dramatic reduction in the overall running time.",Experiment/Discussion
This has repercussions on the scalability of the algorithm relative to training data size.,Experiment/Discussion
"Since enlarging the training data size results in a longer score distribution tail, our algorithm is expected to achieve an even more substantial relative running time improvement over the original algorithm.",Experiment/Discussion
Section 4 presents experimental results that validate the superior scalability of the FastTBL algorithm.,Experiment/Discussion
"Since the goal of this paper is to compare and contrast system training time and performance, extra measures were taken to ensure fairness in the comparisons.",Experiment/Discussion
"To minimize implementation differences, all the code was written in C++ and classes were shared among the systems whenever possible.",Experiment/Discussion
"For each task, the same training set was provided to each system, and the set of possible rule templates was kept the same.",Experiment/Discussion
"Furthermore, extra care was taken to run all comparable experiments on the same machine and under the same memory and processor load conditions.",Experiment/Discussion
"To provide a broad comparison between the systems, three NLP tasks with different properties were chosen as the experimental domains.",Experiment/Discussion
"The first task, part-of-speech tagging, is one where the commitment assumption seems intuitively valid and the samples are not independent.",Experiment/Discussion
"The second task, prepositional phrase attachment, has examples which are independent from each other.",Experiment/Discussion
"The last task is text chunking, where both independence and commitment assumptions do not seem to be valid.",Experiment/Discussion
"A more detailed description of each task, data and the system parameters are presented in the following subsections.",Experiment/Discussion
Four algorithms are compared during the following experiments: The goal of this task is to assign to each word in the given sentence a tag corresponding to its part of speech.,Experiment/Discussion
"A multitude of approaches have been proposed to solve this problem, including transformation-based learning, Maximum Entropy models, Hidden Markov models and memory-based approaches.",Experiment/Discussion
"The data used in the experiment was selected from the Penn Treebank Wall Street Journal, and is the same used by Brill and Wu (1998).",Experiment/Discussion
The training set contained approximately 1M words and the test set approximately 200k words.,Experiment/Discussion
Table 1 presents the results of the experiment4.,Experiment/Discussion
All the algorithms were trained until a rule with a score of 2 was reached.,Experiment/Discussion
"The FastTBL algorithm performs very similarly to the regular TBL, while running in an order of magnitude faster.",Experiment/Discussion
"The two assumptions made by the ICA algorithm result in considerably less training time, but the performance is also degraded (the difference in performance is statistically significant, as determined by a signed test, at a significance level of 0.001).",Experiment/Discussion
Also present in Table 1 are the results of training Brill's tagger on the same data.,Experiment/Discussion
The results of this tagger are presented to provide a performance comparison with a widely used tagger.,Experiment/Discussion
"Also worth mentioning is that the tagger achieved an accuracy of 96.76% when trained on the entire data5; a Maximum Entropy tagger (Ratnaparkhi, 1996) achieves 96.83% accuracy with the same training data/test data.",Experiment/Discussion
Prepositional phrase attachment is the task of deciding the point of attachment for a given prepositional phrase (PP).,Experiment/Discussion
"As an example, consider the following two sentences: In Sentence 1, the PP &quot;with soap and water&quot; describes the act of washing the shirt.",Experiment/Discussion
"In Sentence 2, however, the PP &quot;with pockets&quot; is a description for the shirt that was washed.",Experiment/Discussion
Most previous work has concentrated on situations which are of the form VP NP1 P NP2.,Experiment/Discussion
"The problem is cast as a classification task, and the sentence is reduced to a 4-tuple containing the preposition and the non-inflected base forms of the head words of the verb phrase VP and the two noun phrases NP1 and NP2.",Experiment/Discussion
"For example, the tuple corresponding to the two above sentences would be: Many approaches to solving this this problem have been proposed, most of them using standard machine learning techniques, including transformationbased learning, decision trees, maximum entropy and backoff estimation.",Experiment/Discussion
The transformation-based learning system was originally developed by Brill and Resnik (1994).,Experiment/Discussion
"The data used in the experiment consists of approximately 13,000 quadruples (VP NP1 P NP2) extracted from Penn Treebank parses.",Experiment/Discussion
"The set is split into a test set of 500 samples and a training set of 12,500 samples.",Experiment/Discussion
The templates used to generate rules are similar to the ones used by Brill and Resnik (1994) and some include WordNet features.,Experiment/Discussion
All the systems were trained until no more rules could be learned.,Experiment/Discussion
Table 2 shows the results of the experiments.,Experiment/Discussion
"Again, the ICA algorithm learns the rules very fast, but has a slightly lower performance than the other two TBL systems.",Experiment/Discussion
"Since the samples are inherently independent, there is no performance loss because of the independence assumption; therefore the performance penalty has to come from the commitment assumption.",Experiment/Discussion
"The Fast TBL algorithm runs, again, in a order of magnitude faster than the original TBL while preserving the performance; the time ratio is only 13 in this case due to the small training size (only 13000 samples).",Experiment/Discussion
"Text chunking is a subproblem of syntactic parsing, or sentence diagramming.",Experiment/Discussion
Syntactic parsing attempts to construct a parse tree from a sentence by identifying all phrasal constituents and their attachment points.,Experiment/Discussion
"Text chunking simplifies the task by dividing the sentence into non-overlapping phrases, where each word belongs to the lowest phrasal constituent that dominates it.",Experiment/Discussion
The following example shows a sentence with text chunks and part-ofspeech tags: The problem can be transformed into a classification task.,Experiment/Discussion
"Following Ramshaw & Marcus' (1999) work in base noun phrase chunking, each word is assigned a chunk tag corresponding to the phrase to which it belongs .",Experiment/Discussion
"The following table shows the above sentence with the assigned chunk tags: and the part-of-speech tags were generated by Brill's tagger (Brill, 1995).",Experiment/Discussion
All the systems are trained to completion (until all the rules are learned).,Experiment/Discussion
Table 3 shows the results of the text chunking experiments.,Experiment/Discussion
"The performance of the FastTBL algorithm is the same as of regular TBL's, and runs in an order of magnitude faster.",Experiment/Discussion
"The ICA algorithm again runs considerably faster, but at a cost of a significant performance hit.",Experiment/Discussion
There are at least 2 reasons that contribute to this behavior: 1.,Experiment/Discussion
The initial state has a lower performance than the one in tagging; therefore the independence assumption might not hold.,Experiment/Discussion
"25% of the samples are changed by at least one rule, as opposed to POS tagging, where only 2.5% of the samples are changed by a rule.",Experiment/Discussion
2.,Experiment/Discussion
The commitment assumption might also not hold.,Experiment/Discussion
"For this task, 20% of the samples that were modified by a rule are also changed again by another one.",Experiment/Discussion
A question usually asked about a machine learning algorithm is how well it adapts to larger amounts of training data.,Experiment/Discussion
"Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.",Experiment/Discussion
The experiment was performed with the part-ofspeech data set.,Experiment/Discussion
The four algorithms were trained on training sets of different sizes; training times were recorded and averaged over 4 trials.,Experiment/Discussion
The results are presented in Figure 2(a).,Experiment/Discussion
"It is obvious that the Fast TBL algorithm is much more scalable than the regular TBL displaying a linear dependency on the amount of training data, while the regular TBL has an almost quadratic dependency.",Experiment/Discussion
The explanation for this behavior has been given in Section 3.3.,Experiment/Discussion
"Figure 2(b) shows the time spent at each iteration versus the iteration number, for the original TBL and fast TBL systems.",Experiment/Discussion
"It can be observed that the time taken per iteration increases dramatically with the iteration number for the regular TBL, while for the FastTBL, the situation is reversed.",Experiment/Discussion
"The consequence is that, once a certain threshold has been reached, the incremental time needed to train the FastTBL system to completion is negligible.",Experiment/Discussion
We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.,Results/Conclusion
"This method allows a transformation-based algorithm to train an observed 13 to 139 times faster than the original one, while preserving the final performance of the algorithm.",Results/Conclusion
"The method was tested in three different domains, each one having different characteristics: part-of-speech tagging, prepositional phrase attachment and text chunking.",Results/Conclusion
"The results obtained indicate that the algorithmic improvement generated by our method is not linked to a particular task, but extends to any classification task where transformation-based learning can be applied.",Results/Conclusion
"Furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks.",Results/Conclusion
"The increased speed of the Fast TBL algorithm also enables its usage in higher level machine learning algorithms, such as adaptive boosting, model combination and active learning.",Results/Conclusion
"Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.",Results/Conclusion
"The stability, resistance to overtraining, the existence of probability estimates and, now, reasonable speed make TBL an excellent candidate for solving classification tasks in general.",Results/Conclusion
"The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.",Results/Conclusion
"The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.",Results/Conclusion
