col1,col2
"This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus.",{}
NomBank is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus.,{}
"The University of Pennsylvania’s PropBank, NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text.",{}
This paper describes the NomBank project in detail including its specifications and the process involved in creating the resource.,{}
This paper introduces the NomBank project.,"{'title': '1 Introduction', 'number': '1'}"
"When complete, NomBank will provide argument structure for instances of about 5000 common nouns in the Penn Treebank II corpus.","{'title': '1 Introduction', 'number': '1'}"
NomBank is part of a larger effort to add layers of annotation to the Penn Treebank II corpus.,"{'title': '1 Introduction', 'number': '1'}"
"PropBank (Kingsbury et al., 2002; Kingsbury and Palmer, 2002; University of Pennsylvania, 2002), NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text.","{'title': '1 Introduction', 'number': '1'}"
These annotation projects may be viewed as part of what we think of as an a la carte strategy for corpus-based natural language processing.,"{'title': '1 Introduction', 'number': '1'}"
"The fragile and inaccurate multistage parsers of a few decades were replaced by treebank-based parsers, which had better performance, but typically provided more shallow analyses.1 As the same set of data is annotated with more and more levels of annotation, a new type of multistage processing becomes possible that could reintroduce this information, 1A treebank-based parser output is defined by the treebank on which it is based.","{'title': '1 Introduction', 'number': '1'}"
"As these treebanks tend to be of a fairly shallow syntactic nature, the resulting parsers tend to be so also. but in a more robust fashion.","{'title': '1 Introduction', 'number': '1'}"
Each stage of processing is defined by a body of annotated data which provides a symbolic framework for that level of representation.,"{'title': '1 Introduction', 'number': '1'}"
"Researchers are free to create and use programs that map between any two levels of representation, or which map from bare sentences to any level of representation.2 Furthermore, users are free to shop around among the available programs to map from one stage to another.","{'title': '1 Introduction', 'number': '1'}"
"The hope is that the standardization imposed by the annotated data will insure that many researchers will be working within the same set of frameworks, so that one researcher’s success will have a greater chance of benefiting the whole community.","{'title': '1 Introduction', 'number': '1'}"
"Whether or not one adapts an a la carte approach, NomBank and PropBank projects provide users with data to recognize regularizations of lexically and syntactically related sentence structures.","{'title': '1 Introduction', 'number': '1'}"
"For example, suppose one has an Information Extraction System tuned to a hiring/firing scenario (MUC, 1995).","{'title': '1 Introduction', 'number': '1'}"
One could use NomBank and PropBank to generalize patterns so that one pattern would do the work of several.,"{'title': '1 Introduction', 'number': '1'}"
"Given a pattern stating that the object (ARG1) of appoint is John and the subject (ARG0) is IBM, a PropBank/NomBank enlightened system could detect that IBM hired John from the following strings: IBM appointed John, John was appointed by IBM, IBM’s appointment of John, the appointment of John by IBM and John is the current IBM appointee.","{'title': '1 Introduction', 'number': '1'}"
Systems that do not regularize across predicates would require separate patterns for each of these environments.,"{'title': '1 Introduction', 'number': '1'}"
The NomBank project went through several stages before annotation could begin.,"{'title': '1 Introduction', 'number': '1'}"
We had to create specifications and various lexical resources to delineate the task.,"{'title': '1 Introduction', 'number': '1'}"
"Once the task was set, we identified classes of words.","{'title': '1 Introduction', 'number': '1'}"
"We used these classes to approximate lexical entries, make time estimates and create automatic procedures to aid in 2Here, we use the term “level of representation” quite loosely to include individual components of what might conventionally be considered a single level.","{'title': '1 Introduction', 'number': '1'}"
"REL = growth, ARG1 = in dividends, ARG2-EXT = 12%, ARGM-TMP = next year 8. a possible U.S. troop reduction in South Korea[NOM W/ARGMs] REL = reduction, ARG1 = U.S. troop, ARGM-LOC = in South Korea, ARGM-ADV = possible annotation.","{'title': '1 Introduction', 'number': '1'}"
"For the first nine months of the project, the NomBank staff consisted of one supervisor and one annotator.","{'title': '1 Introduction', 'number': '1'}"
"Once the specifications were nailed down, we hired additional annotators to complete the project.","{'title': '1 Introduction', 'number': '1'}"
This paper provides an overview of the project including an abbreviated version of the specifications (the full version is obtainable upon request) and a chronicle of our progress.,"{'title': '1 Introduction', 'number': '1'}"
"Figure 1 lists some sample NomBank propositions along with the class of the noun predicate (NOM stands for nominalization, DEFREL is a type of relational noun).","{'title': '2 The Specifications', 'number': '2'}"
"For each “markable” instance of a common noun in the Penn Treebank, annotators create a “proposition”, a subset of the features REL, SUPPORT, ARG0, ARG1, ARG2, ARG3, ARG4, ARGM paired with pointers to phrases in Penn Treebank II trees.","{'title': '2 The Specifications', 'number': '2'}"
"A noun instance is markable if it is accompanied by one of its arguments (ARG0, ARG1, ARG2, ARG3, ARG4) or if it is a nominalization (or similar word) and it is accompanied by one of the allowable types of adjuncts (ARGM-TMP, ARGMLOC, ARGM-ADV, ARGM-EXT, etc.)","{'title': '2 The Specifications', 'number': '2'}"
"– the same set of adjuncts used in PropBank.3 The basic idea is that each triple REL, SENSE, ARGNUM uniquely defines an argument, given a particular sense of a particular REL (or predicate), where ARGNUM is one of the numbered arguments (ARG0, ARG1, ARG2, ARG3, ARG4) and SENSE is one of the senses of that REL.","{'title': '2 The Specifications', 'number': '2'}"
"The arguments are essentially the same as the initial relations of Relational Grammar (Perlmutter and Postal, 1984; Rosen, 1984).","{'title': '2 The Specifications', 'number': '2'}"
"For example, agents tend to be classified as ARG0 (RG’s initial subject), patients and themes tend to be classified as ARG1 (RG’s initial object) and indirect objects of all kinds tend to be classified as ARG2.","{'title': '2 The Specifications', 'number': '2'}"
"The lexical entry or frame for each noun provides one inventory of argument labels for each sense of that word.4 Each proposition (cf. figure 1) consists of an instance of an argument-taking noun (REL) plus arguments (ARG0, ARG1, ARG2, ), SUPPORT items and/or adjuncts (ARGM).","{'title': '2 The Specifications', 'number': '2'}"
"SUPPORT items are words that link arguments that occur outside an NP to the nominal predicate that heads that NP, e.g., “made” SUPPORTS “We” as the ARG0 of decision in We made a decision.","{'title': '2 The Specifications', 'number': '2'}"
ARGMs are adjuncts of the noun.,"{'title': '2 The Specifications', 'number': '2'}"
"However, we only mark the sort of adjuncts that also occur in sentences: locations (ARGM-LOC), temporal (ARGM-TMP), sentence adverbial (ARGM-ADV) and various others.","{'title': '2 The Specifications', 'number': '2'}"
"Before we could begin annotation, we needed to classify all the common nouns in the corpus.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
We needed to know which nouns were markable and make initial approximations of the inventories of senses and arguments for each noun.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"Toward this end, we pooled a number of resources: COMLEX Syntax (Macleod et al., 1998a), NOMLEX (Macleod et al., 1998b) and the verb classes from (Levin, 1993).","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
We also used string matching techniques and hand classification in combination with programs that automatically merge crucial features of these resources.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"The result was NOMLEX-PLUS, a NOMLEX-style dictionary, which includes the original 1000 entries in NOMLEX plus 6000 additional entries (Meyers et al., 2004).","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"The resulting noun classes include verbal nominalizations (e.g., destruction, knowledge, believer, recipient), adjectival nominalizations (ability, bitterness), and 16 other classes such as relational (father, president) and partitive nouns (set, variety).","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"NOMLEX-PLUS helped us break down the nouns into classes, which in turn helped us gain an understanding of the difficulty of the task and the manpower needed to complete the task.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
We used a combination of NOMLEX-PLUS and PropBank’s lexical entries (or frames) to produce automatic approximations of noun frames for NomBank.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
These entries specify the inventory of argument roles for the annotators.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"For nominalizations of verbs that were covered in PropBank, we used straightforward procedures to convert existing PropBank lexical entries to nominal ones.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"However, other entries needed to be created by automatic means, by hand or by a combination of the two.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
Figure 2 compares the PropBank lexical entry for the verb claim with the NomBank entry for the noun claim.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"The noun claim and the verb claim share both the ASSERT sense and the SEIZE sense, permitting the same set of argument roles for those senses.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"However, only the ASSERT sense is actually attested in the sample PropBank corpus that was available when we began working on NomBank.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
Thus we added the SEIZE sense to both the noun and verb entries.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
The noun claim also has a LAWSUIT sense which bears an entry similar to the verb sue.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
Thus our initial entry for the noun claim was a copy of the verb entry at that time.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
An annotator edited the frames to reflect noun usage – she added the second and third senses to the noun frame and updated the verb frame to include the second sense.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"In NOMLEX-PLUS, we marked anniversary and advantage as “cousins” of nominalizations indicating that their lexical entries should be modeled respectively on the verbs commemorate and exploit, although both entries needed to be modified in some respect.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"We use the term “cousins” of nominalizations to refer to those nouns which take argument structure similar to some verb (or adjective), but which are not morphologically related to that word.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
Examples are provided in Figure 3 and 4.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"For adjective nominalizations, we began with simple procedures which created frames based on NOMLEX-PLUS entries (which include whether the subject is +/-sentient).","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
The entry for “accuracy” (the nominalization of the adjective accurate) plus a simple example is provided in figure 5 – the ATTRIBUTE-LIKE frame is one of the most common frames for adjective nominalizations.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"To cover the remaining nouns in the corpus, we created classes of lexical items and manually constructed one frame for each class.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
Each member of a class was was given the corresponding frame.,"{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"Figure 6 provides a sample of these classes, along with descriptions of their frames.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"As with the nominalization cousins, annotators sometimes had to adjust these frames for particular words.","{'title': '3 Lexical Entries and Noun Classes', 'number': '3'}"
"Beginning with the PropBank and NomBank propositions in Figure 7, it is straight-forward to derive the Roles: ARG0 = agent, ARG1 = thing remembered, ARG2 = times celebrated Noun Example: Investors celebrated the second anniversary of Black Monday.","{'title': '4 A Merged Representation', 'number': '4'}"
"Roles: ARG0 = exploiter, ARG1 = entity exploited Noun Example: Investors took advantage of Tuesday ’s stock rally.","{'title': '1.', 'number': '5'}"
"REL = advantage, SUPPORT = took, ARG0 = Investors, ARG1 = of Tuesday’s stock rally 1.","{'title': '1.', 'number': '5'}"
"ATTRIBUTE-LIKE Roles: ARG1 =theme Noun Example: the accuracy of seasonal adjustments built into the employment data REL = accuracy, ARG1 = of seasonal adjustments built into PropBank: REL = gave, ARG0 = they, ARG1 = a standing ovation, ARG2 = the chefs NomBank: REL = ovation, ARG0 = they, ARG1 = the chefs, SUPPORT = gave combined PropBank/NomBank graphical representation in Figure 8 in which each role corresponds to an arc label.","{'title': '1.', 'number': '5'}"
"For this example, think of the argument structure of the noun ovation as analogous to the verb applaud.","{'title': '1.', 'number': '5'}"
"According to our analysis, they are both the givers and the applauders and the chefs are both the recipients of something given and the ones who are applauded.","{'title': '1.', 'number': '5'}"
Gave and ovation have two distinct directional relations: a standing ovation is something that is given and gave serves as a link between ovation and its two arguments.,"{'title': '1.', 'number': '5'}"
This diagram demonstrates how NomBank is being designed for easy integration with PropBank.,"{'title': '1.', 'number': '5'}"
We believe that this is the sort of predicate argument representation that will be needed to easily merge this work with other annotation efforts.,"{'title': '1.', 'number': '5'}"
As of this writing we have created the various lexicons associated with NomBank.,"{'title': '5 Analysis of the Task', 'number': '6'}"
"This has allowed us to break down the task as follows: There are approximately 240,000 instances of common nouns in the PTB (approximately one out of every 5 words).","{'title': '5 Analysis of the Task', 'number': '6'}"
"At least 36,000 of these are nouns that cannot take arguments and therefore need not be looked at by an There are approximately 99,000 instances of verbal nominalizations or related items (e.g., cousins) There are approximately 34,000 partitives (including 6,000 instances of the percent sign), 18,000 subject nominalizations, 14,000 environmental nouns, 14,000 relational nouns and fewer instances of the various other classes.","{'title': '5 Analysis of the Task', 'number': '6'}"
"Approximately 1/6 of the cases are instances of nouns which occur in multiple classes.5 The difficulty of the annotation runs the gamut from nominalization instances which include the most arguments, the most adjuncts and the most instances of support to the partitives, which have the simplest and most predictable structure.","{'title': '5 Analysis of the Task', 'number': '6'}"
We have conducted some preliminary consistency tests for about 500 instances of verbal nominalizations during the training phases of NomBank.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
These tests yielded inter-annotator agreement rates of about 85% for argument roles and lower for adjunct roles.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"We are currently engaging in an effort to improve these results.6 We have identified certain main areas of disagreement including: disagreements concerning SUPPORT verbs and the shared arguments that go with them; disagreements about role assignment to prenominals; and differences between annotators caused by errors (typos, slips of the mouse, ill-formed output, etc.)","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In addition to improving our specifications and annotator help texts, we are beginning to employ some automatic means for error detection.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For inconsistencies with SUPPORT, our main line of attack has been to outline problems and solutions in our specifications.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"We do not have any automatic system in effect yet, although we may in the near future.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"SUPPORT verbs (Gross, 1981; Gross, 1982; Mel’ˆcuk, 1988; Mel’ˆcuk, 1996; Fontenelle, 1997) are verbs which 5When a noun fits into multiple categories, those categories may predict multiple senses, but not necessarily.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, drive has a nominalization sense (He went for a drive) and an attribute sense (She has a lot of drive).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Thus the lexical entry for drive includes both senses.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In constrast, teacher in the math teacher has the same analysis regardless of whether one thinks of it as the nominalization of teach or as a relational (ACTREL) noun.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
6Consistency is the average precision and recall against a gold standard.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The preliminary tests were conducted during training, and only on verbal nominalizations. connect nouns to one (or more) of their arguments via argument sharing.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, in John took a walk, the verb took “shares” its subject with the noun walk.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
SUPPORT verbs can be problematic for a number of reasons.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
First of all the concept of argument sharing is not black and white.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"To illustrate these shades of gray, compare the relation of Mary to attack in: Mary’s attack against the alligator, Mary launched an attack against the alligator, Mary participated in an attack against the alligator, Mary planned an attack against the alligator and Mary considered an attack against the alligator.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In each subsequent example, Mary’s “level of agency” decreases with respect to the noun attack.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"However, in each case Mary may still be viewed as some sort of potential attacker.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
It turned out that the most consistent position for us to take was to assume all degrees of argument-hood (in this case subject-hood) were valid.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"So, we would mark Mary as the ARG0 of attack in all these instances.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"This is consistent with the way control and raising structures are marked for verbs, e.g., John is the subject of leave and do in John did not seem to leave and John helped do the project under most accounts of verbal argument structure that take argument sharing (control, raising, etc.) into account.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Of course a liberal view of SUPPORT has the danger of overgeneration.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Consider for example, Market conditions led to the cancellation of the planned exchange.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
The unwary annotator might assume that market conditions is the ARG0 (or subject) of cancellation.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In fact, the combination lead to and cancellation do not have any of the typical features of SUPPORT described in figure 9.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"However, the final piece of evidence is that market conditions violate the selection restrictions of cancellation.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Thus the following paraphrase is ill-formed *Market conditions canceled the planned exchange.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
This suggests that market conditions is the subject of lead and not the subject of cancellation.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Therefore, this is not an instance of support in spite of the apparent similarity.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We require that the SUPPORT relation be lexical.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In other words, there must be something special about a SUPPORT verb or the combination of the SUPPORT verb and the noun to license the argument sharing relation.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In addition to SUPPORT, we have cataloged several argument sharing phenomena which are markable.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, consider the sentence, President Bush arrived for a celebration.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Clearly, President Bush is the ARG0 of celebration (one of the people celebrating).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"However, arrive is not a SUPPORT verb.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The phrase for a celebration is a subject-oriented adverbial, similar to adverbs like willingly, which takes the subject of the sentence as an argument.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Thus President Bush could also be the subject of celebration in President Bush waddled into town for the celebration and many similar sentences that contain this PP.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Finally, there are cases where argument sharing may Support verb/noun pairs can be idiosyncratically connected to the point that some researchers would call them idioms or phrasal verbs, e.g., take a walk, keep tabs on.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The verb can be essentially “empty”, e.g., make an attack, have a visit.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The “verb/noun” combination may take a different set of arguments than either does alone, e.g., take advantage of.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Some support verbs share the subject of almost any nominalization in a particular argument slot.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example attempt shares its subject with most following nominalizations, e.g., He attempted an attack.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
These are the a lot like raising/control predicates.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In some cases, the support verb and noun are from similar semantic classes, making argument sharing very likely, e.g., fight a battle. be implied by discourse processes, but which we do not mark (as we are only handling sentence-level phenomena).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, the words proponent and rival strongly imply that certain arguments appear in the discourse, but not necessarily in the same sentence.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example in They didn’t want the company to fall into the hands of a rival, there is an implication that the company is an ARG1 of rival, i.e., a rival should be interpreted as a rival of the company.7 The connection between a rival and the company is called a “bridging” relation (a process akin to coreference, cf.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"(Poesio and Vieira, 1998)) In other words, fall into the hands of does not link “rival” with the company by means of SUPPORT.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The fact that a discourse relation is responsible for this connection becomes evident when you see that the link between rival and company can cross sentence boundaries, e.g., The company was losing money.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
This was because a rival had come up with a really clever marketing strategy.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"ARGM is the annotation tag used for nonarguments, also known as adjuncts.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For nouns, it was decided to only tag such types of adjuncts as are also found with verbs, e.g., temporal, locative, manner, etc.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
The rationale for this included: (1) only the argument-taking common nouns are being annotated and other sorts of adjuncts occur with common nouns in general; (2) narrowing the list of potential labels helped keep the labeling consistent; and (3) this was the minimum set of adjuncts that would keep the noun annotation consistent with the verb annotation.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Unfortunately, it was not always clear whether a prenominal modifier (particularly an adjective) fell into one of our classes or not.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"If an annotator felt that a modifier was somehow “important”, there was a temptation to push it into one of the modifier classes even if it was not a perfect fit.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Furthermore, some annotators had a broader view than others as to the sorts of semantic relationships that fell within particular classes of adjuncts, particularly locative (LOC), manner (MNR) and extent (EXT).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Unlike the SUPPORT verbs, which are often idiosyncratic to particular nominal predicates, adjunct prenominal modifiers usually behave the same way regardless of the noun with which they occur.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"In order to identify these lexical properties of prenominals, we created a list of all time nouns from COMLEX Syntax (ntime1 and ntime2) and we created a specialized dictionary of adjectives with adverbial properties which we call ADJADV.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The list of adjective/adverb pairs in ADJADV came from two sources: (1) a list of adjectives that are morphologically linked to -ly adverbs created using some string matching techniques; and (2) adjective/adverb pairs from CATVAR (Habash and Dorr, 2003).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We pruned this list to only include adjectives found in the Penn Treebank and then edited out inappropriate word pairs.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We completed the dictionary by transferring portions of the COMLEX Syntax adverb entries to the corresponding adjectives.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We now use ADJADV and our list of temporal nouns to evaluate NOMBANK annotation of modifiers.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Each annotated left modifier is compared against our dictionaries.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"If a modifier is a temporal noun, it can bear the ARGM-TMP role (temporal adjunct role), e.g., the temporal noun morning can fill the ARGM-TMP slot in the morning broadcast.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Most other common nouns are compatible with argument role slots (ARG0, ARG1, etc.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"), e.g., the noun news can fill the ARG1 slot in the news broadcast.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Finally, roles associated with adjectives depend on their ADJADV entry, e.g., possible can be an ARGM-ADV in possible broadcasts due to the epistemic feature encoded in the lexical entry for possible (derived from the corresponding adjverb possibly).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Discrepancies between these procedures and the annotator are resolved on a case by case basis.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"If the dictionary is wrong, the dictionary should be changed, e.g., root, as in root cause was added to the dictionary as a potential MNR adjective with a meaning like the adverb basically.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"However, if the annotator is wrong, the annotation should be changed, e.g., if an annotator marked “slow” as a ARGM-TMP, the program would let them know that it should be a ARGMMNR.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
This process both helps with annotation accuracy and enriches our lexical database.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We used other procedures to detect errors including: Nom-type Argument nominalizations are nominalizations that play the role of one of the arguments in the ROLESET.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Thus the word acquirer should be assigned the ARG0 role in the following example because acquirer is a subject nominalization: REL = acquirer, ARG0 = acquirer, ARG1 = of Manville, ARGM-ADV = possible A procedure can compare the NOMLEX-PLUS entry for each noun to each annotated instance of that noun to check for incompatibilities.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Illformedness Impossible instances are ruled out.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
Checks are made to make sure obligatory labels (REL) are present and illegal labels are not.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Similarly, procedures make sure that infinitive arguments are marked with the -PRD function tag (a PropBank convention).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Probable Illformedness Certain configurations of role labels are possible, but very unlikely.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, the same argument role should not appear more than once (the stratal uniqueness condition in Relational Grammar or the theta criterion in Principles and parameters, etc.).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Furthermore, it is unlikely for the first word of a sentence to be an argument unless the main predicate is nearby (within three words) or unless there is a nearby support verb.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Finally, it is unlikely that there is an empty category that is an argument of a predicate noun unless the empty category is linked to some real NP.B WRONG-POS We use procedures that are part of our systems for generating GLARF, a predicate argument framework discussed in (Meyers et al., 2001a; Meyers et al., 2001b), to detect incorrect parts of speech in the Penn Treebank.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"If an instance is predicted to be a part of speech other than a common noun, but it is still tagged, that instance is flagged.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"For example, if a word tagged as a singular common noun is the first word in a VP, it is probably tagged with the wrong part of speech.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"The processes described in the previous subsections are used to create a list of annotation instances to check along with short standardized descriptions of what was wrong, e.g., wrong-pos, non-functional (if there were two identical argument roles), etc.","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
"Annotators do a second pass EEmpty categories mark “invisible” constituents in the Treebank, e.g., the subject of want in John wanted e to leave. on just these instances (currently about 5 to 10% of the total).","{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We will conduct a formal evaluation of this procedure over the next month.,"{'title': '6 Error Analysis and Error Detection', 'number': '7'}"
We are just starting a new phase in this project: the creation of an automatic annotator.,"{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"Using techniques similar to those described in (Meyers et al., 1998) in combination with our work on GLARF (Meyers et al., 2001a; Meyers et al., 2001b), we expect to build a hand-coded PROPBANKER a program designed to produce a PropBank/NomBank style analysis from Penn Treebank style input.","{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"Although the PropBanker should work with input in the form of either treebank annotation or treebankbased parser output, this project only requires application to the Penn Treebank itself.","{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources.","{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"Depending on its accuracy, automatically produced annotation should be useful as either a preprocessor or as an error detector.","{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"We expect high precision for very simple frames, e.g., nouns like lot as in figure 10.","{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
Annotators will have the opportunity to judge whether particular automatic annotation is “good enough” to serve as a preprocessor.,"{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
We hypothesize that a comparison of automatic annotation that fails this level of accuracy against the hand annotation will still be useful for detecting errors.,"{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
Comparisons between the hand annotated data and the automatically annotated data will yield a set of instances that warrant further checking along the same lines as our previously described error checking mechanisms.,"{'title': '7 Future Research: Automatic Annotation', 'number': '8'}"
"This paper outlines our current efforts to produce NomBank, annotation of the argument structure for most common nouns in the Penn Treebank II corpus.","{'title': '8 Summary', 'number': '9'}"
This is part of a larger effort to produce more detailed annotation of the Penn Treebank.,"{'title': '8 Summary', 'number': '9'}"
Annotation for NomBank is progressing quickly.,"{'title': '8 Summary', 'number': '9'}"
We began with a single annotator while we worked on setting the task and have ramped up to four annotators.,"{'title': '8 Summary', 'number': '9'}"
We continue to work on various quality control procedures which we outline above.,"{'title': '8 Summary', 'number': '9'}"
"In the near future, we intend to create an automatic annotation program to be used both as a preprocessor for manual annotation and as a supplement to error detection.","{'title': '8 Summary', 'number': '9'}"
"The argument structure of NPs has been less studied both in theoretical and computational linguistics, than the argument structure of verbs.","{'title': '8 Summary', 'number': '9'}"
"As with our work on NOMLEX, we are hoping that NomBank will substantially contribute to improving the NLP community’s ability to understand and process noun argument structure.","{'title': '8 Summary', 'number': '9'}"
tion or the policy of the U.S. Government.,"{'title': 'Acknowledgments', 'number': '10'}"
"We would also like to acknowledge the people at the University of Pennsylvania who helped make NomBank possible, including, Martha Palmer, Scott Cotton, Paul Kingsbury and Olga Babko-Malaya.","{'title': 'Acknowledgments', 'number': '10'}"
"In particular, the use of PropBank’s annotation tool and frame files proved invaluable to our effort.","{'title': 'Acknowledgments', 'number': '10'}"
