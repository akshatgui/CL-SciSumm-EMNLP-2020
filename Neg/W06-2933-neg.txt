The pseudo-projective approach (Nivre and Nilsson, 2005) $$$$$ The SVM parameters fall into the following ranges: γ: 0.12–0.20; r: 0.0–0.6; C: 0.1–0.7; c: 0.01–1.0.
The pseudo-projective approach (Nivre and Nilsson, 2005) $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.
The pseudo-projective approach (Nivre and Nilsson, 2005) $$$$$ Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.

Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). $$$$$ The evaluation shows that labeled pseudo-projective dependency parsing, using a deterministic parsing algorithm and SVM classifiers, gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1.26 in the treebank, this means that they are normally adjacent to the head word. shared task, although the level of accuracy varies considerably between languages.
Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). $$$$$ A very similar pattern is found for Spanish (Civit Torruella and MartiAntonin, 2002), although this cannot be explained by a high proportion of non-projective structures.
Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). $$$$$ We are grateful for the support from T ¨UB˙ITAK (The Scientific and Technical Research Council of Turkey) and the Swedish Research Council.
Table 5 shows the official results for submitted parser outputs. The two participant groups with the highest total score are McDonald et al (2006) and Nivre et al (2006). $$$$$ Finally, it is worth noting that coordination, which is often problematic in parsing, has high accuracy.

Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. $$$$$ )5 A dry run at the end of the development phase gave a labeled attachment score of 80.46 over the twelve required languages.
Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. $$$$$ We are grateful for the support from T ¨UB˙ITAK (The Scientific and Technical Research Council of Turkey) and the Swedish Research Council.
Even though McDonald et al (2006) and Nivre et al (2006) obtained very similar overall scores, a more detailed look at their performance shows clear differences. $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.

Firstto Third-Order Features The feature templates of first to third-order features are mainly drawn from previous work on graph based parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al, 2006) and dual decomposition-based parsing (Martins et al, 2011). $$$$$ (For the remaining languages, the training data has not been split at all.
Firstto Third-Order Features The feature templates of first to third-order features are mainly drawn from previous work on graph based parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al, 2006) and dual decomposition-based parsing (Martins et al, 2011). $$$$$ )5 A dry run at the end of the development phase gave a labeled attachment score of 80.46 over the twelve required languages.
Firstto Third-Order Features The feature templates of first to third-order features are mainly drawn from previous work on graph based parsing (McDonald and Pereira, 2006), transition-based parsing (Nivre et al, 2006) and dual decomposition-based parsing (Martins et al, 2011). $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.

includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). $$$$$ Our methodology for performing this task is based on four essential components: All experiments have been performed using MaltParser (Nivre et al., 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1
includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). $$$$$ The evaluation shows that labeled pseudo-projective dependency parsing, using a deterministic parsing algorithm and SVM classifiers, gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1.26 in the treebank, this means that they are normally adjacent to the head word. shared task, although the level of accuracy varies considerably between languages.
includes the most accurate parsers among Nivre et al (2006), McDonald et al (2006), Martins et al (2010), Martins et al (2011), Martins et al (2013), Koo et al (2010), Rush and Petrov (2012), Zhang and McDonald (2012) and Zhang et al (2013). $$$$$ The first consists of determiners and particles, which have an unlabeled attachment score over 80% and which are found within a distance of 1–1.4 IGs from their head.7 The second group mainly contains subjects, objects and different kinds of adjuncts, with a score in the range 60–80% and a distance of 1.8–5.2 IGs to their head.

Both methods are sometimes combined (Nivre et al, 2006b). $$$$$ We are grateful for the support from T ¨UB˙ITAK (The Scientific and Technical Research Council of Turkey) and the Swedish Research Council.
Both methods are sometimes combined (Nivre et al, 2006b). $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
Both methods are sometimes combined (Nivre et al, 2006b). $$$$$ The unlabeled score is less competitive, with only Turkish having the highest reported score, which indirectly indicates that the integration of labels into the parsing process primarily benefits labeled accuracy.
Both methods are sometimes combined (Nivre et al, 2006b). $$$$$ Since this variant of the algorithm increases the overall nondeterminism, it has only been used for the data sets that include informative root labels (Arabic, Czech, Portuguese, Slovene).

We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a). $$$$$ Features of the type DEPREL have a special status in that they are extracted during parsing from the partially built dependency graph and may therefore contain errors, whereas all the other features have gold standard values during both training and parsing.2 Based on previous research, we defined a base model to be used as a starting point for languagespecific feature selection.
We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a). $$$$$ We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
We represented features with a parameter format partly inspired by MaltParser (Nivre et al, 2006a). $$$$$ The latter result must be related both to the relatively fine-grained inventory of dependency labels for adverbials and to attachment ambiguities that involve prepositional phrases.

These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. $$$$$ Since this variant of the algorithm increases the overall nondeterminism, it has only been used for the data sets that include informative root labels (Arabic, Czech, Portuguese, Slovene).
These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. $$$$$ Finally, it is worth noting that coordination, which is often problematic in parsing, has high accuracy.
These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. $$$$$ The SVM parameters fall into the following ranges: γ: 0.12–0.20; r: 0.0–0.6; C: 0.1–0.7; c: 0.01–1.0.
These parameters are identical to Nivre et al (2006b) to enable a comparison of the scores. We evaluated the feature candidates on a development set using the labeled and unlabeled attachment scores (LAS and UAS) that we computed with the eval.pl script from CoNLL-X. $$$$$ The evaluation shows that labeled pseudo-projective dependency parsing, using a deterministic parsing algorithm and SVM classifiers, gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1.26 in the treebank, this means that they are normally adjacent to the head word. shared task, although the level of accuracy varies considerably between languages.

Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. $$$$$ Moreover, when we break down the results according to whether the head of a dependency is part of a multiple-IG word or a complete (single-IG) word, we observe a highly significant difference in accuracy, with only 53.2% unlabeled attachment score for multiple-IG heads versus 83.7% for single-IG heads.
Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. $$$$$ The number of features in the optimized models varies from 16 (Turkish) to 30 (Spanish), but the models use all fields available for a given language, except that FORM is not used for Turkish (only LEMMA).
Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
Following (Nivre et al, 2006), the encoding scheme called Head in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.

We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
We used the base feature model defined in (Nivre et al, 2006) for all the languages but Arabic, Chinese, Czech, and Turkish. $$$$$ To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser.

For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature $$$$$ Our methodology for performing this task is based on four essential components: All experiments have been performed using MaltParser (Nivre et al., 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1
For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature $$$$$ The results for Arabic (Hajiˇc et al., 2004; Smrˇz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).
For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
For Arabic, Chinese, and Czech, we used the same feature models used in the CoNLL-X 948 shared task by (Nivre et al, 2006), and for Turkish we used again the base feature model but extended it with a single feature $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.

Talbanken05 is a Swedish tree bank converted to dependency format, containing both written and spoken language (Nivre et al, 2006a). For each token, Talbanken05 contains information on word form, part of speech, head and dependency relation, as well as various morphosyntactic and/orlexical semantic features. $$$$$ To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser.

As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best performing parser for Swedish. $$$$$ The main optimization criterion has been labeled attachment score on held-out data, using ten-fold cross-validation for all data sets with 100k tokens or less, and an 80-20 split into training and devtest sets for larger datasets.
As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best performing parser for Swedish. $$$$$ Our methodology for performing this task is based on four essential components: All experiments have been performed using MaltParser (Nivre et al., 2006), version 0.4, which is made available together with the suite of programs used for pre- and post-processing.1
As our baseline, we use the settings optimized for Swedish in the CoNLL-X shared task (Nivre et al,2006b), where this parser was the best performing parser for Swedish. $$$$$ The CoNLL-X shared task consists in parsing texts in multiple languages using a single dependency parser that has the capacity to learn from treebank data.

MaltParser (Nivre et al, 2006) is a language independent system for data-driven dependency parsing which is freely available. It is based on a deterministic parsing strategy in combination with tree bank-induced classifiers for predicting parsing actions. $$$$$ One possible explanation in this case may be the fact that dependency graphs in the Spanish data are sparsely labeled, which may cause problem for a parser that relies on dependency labels as features.
MaltParser (Nivre et al, 2006) is a language independent system for data-driven dependency parsing which is freely available. It is based on a deterministic parsing strategy in combination with tree bank-induced classifiers for predicting parsing actions. $$$$$ We use SVM classifiers to predict the next action of a deterministic parser that builds labeled projective dependency graphs in an incremental fashion.
MaltParser (Nivre et al, 2006) is a language independent system for data-driven dependency parsing which is freely available. It is based on a deterministic parsing strategy in combination with tree bank-induced classifiers for predicting parsing actions. $$$$$ The labeled attachment score varies from 91.65 to 65.68 but is above average for all languages.

For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006). $$$$$ The unlabeled score is less competitive, with only Turkish having the highest reported score, which indirectly indicates that the integration of labels into the parsing process primarily benefits labeled accuracy.
For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006). $$$$$ In this group, information about case and possessive features of nominals is important, which is found in the FEATS field in the data representation.
For the training of the Malt parser model that we use in the stacking experiments, we use learner and parser settings identical to the ones optimized for German in the CoNLL-X shared task (Nivre et al, 2006). $$$$$ A very similar pattern is found for Spanish (Civit Torruella and MartiAntonin, 2002), although this cannot be explained by a high proportion of non-projective structures.

We use Nivre et al, (2006)'s dependency parser. $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
We use Nivre et al, (2006)'s dependency parser. $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.

Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). $$$$$ Japanese (Kawata and Bartels, 2000), despite a very high accuracy, is different in that attachment score drops from 98% to 85%, as we go from length 1 to 2, which may have something to do with the data consisting of transcribed speech with very short utterances.
Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.
Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). $$$$$ Non-projective dependencies are captured indirectly by projectivizing the training data for the classifiers and applying an inverse transformation to the output of the parser.
Unigram label information has been used in MaltParser (Nivre et al, 2006a; Nivre, 2006). $$$$$ The importance of this kind of ambiguity is reflected also in the drastic difference in accuracy between noun pre-modifiers (AT) (F > 97%) and noun post-modifiers (ET) (F Pz� 75%).

In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. $$$$$ To analyze in depth the factors determining this variation, and to improve our parsing methods accordingly to meet the challenges posed by the linguistic diversity, will be an important research goal for years to come.
In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. $$$$$ The evaluation shows that labeled pseudo-projective dependency parsing, using a deterministic parsing algorithm and SVM classifiers, gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1.26 in the treebank, this means that they are normally adjacent to the head word. shared task, although the level of accuracy varies considerably between languages.
In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. $$$$$ The results for Arabic (Hajiˇc et al., 2004; Smrˇz et al., 2002) are characterized by low root accuracy as well as a rapid degradation of attachment score with arc length (from about 93% for length 1 to 67% for length 2).
In principle, it is restricted to projective dependency forests, but it can be used in conjunction with the pseudo-projective transformation (Nivre et al2006) in order to capture a restricted subset of non projective forests. $$$$$ We are grateful for the support from T ¨UB˙ITAK (The Scientific and Technical Research Council of Turkey) and the Swedish Research Council.

Table 4 $$$$$ The evaluation shows that labeled pseudo-projective dependency parsing, using a deterministic parsing algorithm and SVM classifiers, gives competitive parsing accuracy for all languages involved in the 7Given that the average IG count of a word is 1.26 in the treebank, this means that they are normally adjacent to the head word. shared task, although the level of accuracy varies considerably between languages.
Table 4 $$$$$ We are grateful for the support from T ¨UB˙ITAK (The Scientific and Technical Research Council of Turkey) and the Swedish Research Council.
Table 4 $$$$$ Moreover, when we break down the results according to whether the head of a dependency is part of a multiple-IG word or a complete (single-IG) word, we observe a highly significant difference in accuracy, with only 53.2% unlabeled attachment score for multiple-IG heads versus 83.7% for single-IG heads.

To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event. $$$$$ Features of the type DEPREL have a special status in that they are extracted during parsing from the partially built dependency graph and may therefore contain errors, whereas all the other features have gold standard values during both training and parsing.2 Based on previous research, we defined a base model to be used as a starting point for languagespecific feature selection.
To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event. $$$$$ We also want to thank Atanas Chanev for assistance with Slovene, the organizers of the shared task for all their hard work, and the creators of the treebanks for making the data available.
To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event. $$$$$ The main optimization criterion has been labeled attachment score on held-out data, using ten-fold cross-validation for all data sets with 100k tokens or less, and an 80-20 split into training and devtest sets for larger datasets.
To further put the obtained results into context, Table 4 compares the performance of the arc-eager parser with the projective buffer transition most suitable for each dataset with the results obtained by the parser with the pseudo-projective transformation by Nivre et al2006) in the CoNLL-X shared task, one of the top two performing systems in that event. $$$$$ Another limitation of the parsing algorithm is that it does not assign dependency labels to roots, i.e., to tokens having HEAD=0.
