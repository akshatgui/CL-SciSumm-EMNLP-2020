We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. $$$$$ As with most complex discrete models, the bulk of the work is in computing expected counts under p(r, f, c  |w, s; 0).
We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. $$$$$ It is interesting to note that skyCover=75-100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text. separate multinomial distribution over words from which w is drawn.
We use the model of Liang et al (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. $$$$$ The second domain is weather forecasts, for which we created a new dataset.

We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009). $$$$$ The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities).
We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009). $$$$$ The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities).
We achieved a BLEU score of 51.5 on the combined task of content selection and generation, which is more than a two-fold improvement over a model similar to that of Liang et al (2009). $$$$$ Formally, our model is a hierarchical hidden semi-Markov model conditioned on s. Inference in the E-step can be done using a dynamic program similar to the inside-outside algorithm.

We used the dataset created by Liang et al (2009). $$$$$ The record choice model specifies a distribution over an ordered sequence of records r = (r1, ... , r|r|), where each record ri E s. This model is intended to capture two types of regularities in the discourse structure of language.
We used the dataset created by Liang et al (2009). $$$$$ To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
We used the dataset created by Liang et al (2009). $$$$$ To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
We used the dataset created by Liang et al (2009). $$$$$ Integer Fields (t = INT) For integer fields, we want to capture the intuition that a numeric quantity v is rendered in the text as a word which is possibly some other numerical value w due to stylistic factors.

By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one. $$$$$ In this less restricted data setting, we must resolve multiple ambiguities: (1) the segmentation of the text into utterances; (2) the identification of relevant facts, i.e., the choice of records and aspects of those records; and (3) the alignment of utterances to facts (facts are the meaning representations of the utterances).
By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one. $$$$$ We assume that we are given a world state represented by a set of records along with a text, an unsegmented sequence of words.
By defining features on the entire field set, we can capture any correlation structure over the fields; in contrast, Liang et al (2009) generates a sequence of fields in which a field can only depend on the previous one. $$$$$ .

In particular, a word is chosen from the parameters learned in the model of Liang et al (2009). $$$$$ Their best approach (KRISPER) obtains 67% F1; our method achieves 76.5%.
In particular, a word is chosen from the parameters learned in the model of Liang et al (2009). $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.

We use the model of Liang et al (2009) to impute the decisions. $$$$$ By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.
We use the model of Liang et al (2009) to impute the decisions. $$$$$ Other times, it might be customary to round v (e.g., wind speeds are typically rounded to a multiple of 5).
We use the model of Liang et al (2009) to impute the decisions. $$$$$ This domain is simplified in that the segmentation is known.

We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors $$$$$ Each line is annotated with a (possibly empty) set of records.
We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors $$$$$ Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain.
We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors $$$$$ We used the dataset created by Chen and Mooney (2008), which contains 1919 scenarios from the 2001–2004 Robocup finals.
We found that using the unsupervised model of Liang et al (2009) to automatically produce aligned training scenarios (Section 4.3.1) was less effective than it was in the other two domains due to two factors $$$$$ In the M-step, we optimize the parameters 0 by normalizing the expected counts computed in the E-step.

Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. $$$$$ To cope with these challenges, we propose a probabilistic generative model that treats text segmentation, fact identification, and alignment in a single unified framework.
Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.
Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. $$$$$ To cope with these challenges, we propose a probabilistic generative model that treats text segmentation, fact identification, and alignment in a single unified framework.
Liang et al (2009) introduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. $$$$$ In our experiments, we initialized EM with a uniform distribution for each multinomial and applied add-0.1 smoothing to each multinomial in the M-step.

On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. $$$$$ Integer Fields (t = INT) For integer fields, we want to capture the intuition that a numeric quantity v is rendered in the text as a word which is possibly some other numerical value w due to stylistic factors.
On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. $$$$$ These lines correspond to clauses in the weather domain and sentences in the Robocup and NFL domains.
On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. $$$$$ 3.
On the other hand, Liang et al (2009) introduced a probabilistic generative model for learning semantic correspondences in ambiguous training data consisting of sentences paired with observed world states. $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.

On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. $$$$$ We assume that we are given a world state represented by a set of records along with a text, an unsegmented sequence of words.
On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. $$$$$ Formally, our probabilistic model places a distribution over (r, f, c, w) and factorizes according to the three stages as follows: p(r, f, c, w  |s) = p(r  |s)p(f  |r)p(c, w  |r, f, s) The following three sections describe each of these stages in more detail.
On the other hand, Liang et al (2009) proposed a probabilistic generative approach to produce a Viterbi alignment between NL and MRs. They use a hierarchical semi-Markov generative model that first determines which facts to discuss and then generates words from the predicates and arguments of the chosen facts. $$$$$ Their best approach (KRISPER) obtains 67% F1; our method achieves 76.5%.

Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. $$$$$ Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain.
Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. $$$$$ However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems.
Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.
Chen et al (2010) recently reported results on utilizing the improved alignment produced by Liang et al (2009)'s model to initialize their own iterative retraining method. $$$$$ Formally, our model is a hierarchical hidden semi-Markov model conditioned on s. Inference in the E-step can be done using a dynamic program similar to the inside-outside algorithm.

Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics. $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.
Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics. $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.
Motivated by this prior research, our approach combines the generative alignment model of Liang et al (2009) with the generative semantic parsing model of Lu et al (2008) in order to fully exploit the NL syntax and its relationship to the MR semantics. $$$$$ We believe that further progress is possible with a richer model.

 $$$$$ Furthermore, our model provides a more detailed analysis of the correspondence between the world state and text, rather than just producing a single alignment decision.
 $$$$$ We therefore created a variant, Model 2’, where we constrained each field to generate exactly one word.
 $$$$$ A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.
 $$$$$ Though we still trail supervised techniques, which attain numbers in the 68–80 range, we have made substantial progress over our baseline using an unsupervised method.

Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w $$$$$ We have presented a generative model of correspondences between a world state and an unsegmented stream of text.
Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w $$$$$ The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities).
Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w $$$$$ The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007).
Like Liang et al (2009)'s generative alignment model, our model is designed to estimate P (w $$$$$ By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.

The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). $$$$$ All of the fields in this domain are categorical, which means there is no a priori association between the field value pink1 and the word pink].
The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). $$$$$ To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). $$$$$ We also fixed the backoff probability to 0.1 instead of learning it and enforced zero numerical deviation on integer field values.
The natural language generation model covers the roles of both the field choice model and word choice models of Liang et al (2009). $$$$$ To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.

We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it. $$$$$ Nonetheless, we were able to obtain reasonable results on this task.
We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it. $$$$$ We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
We also tried using a Markov model to order arguments like Liang et al (2009), but preliminary experimental results showed that this additional component actually decreased performance rather than improving it. $$$$$ We use the EM algorithm to maximize (3), which alternates between the E-step and the M-step.

 $$$$$ We use the EM algorithm to maximize (3), which alternates between the E-step and the M-step.
 $$$$$ In the E-step, we compute expected counts according to the posterior p(r, f, c  |w, s; 0).
 $$$$$ Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain.

In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. $$$$$ We produce a predicted set of line-record pairs A' by aligning a line to a record ri if the span of (the utterance corresponding to) ri overlaps the line.
In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. $$$$$ For example, the ballstopped record occurs frequently but is never mentioned in the text.
In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. $$$$$ By having a joint model of salience, coherence, and segmentation, as well as a detailed rendering of the values in the world state into words in the text, we are able to cope with the increased ambiguity that arises in this new data setting, successfully pushing the limits of unsupervision.
In particular, our proposed model outperforms the generative alignment model of Liang et al (2009), indicating that the extra linguistic information and MR grammatical structure used by Lu et al (2008)'s generative language model make our overall model more effective than a simple Markov+ bag-of-words model for language generation. $$$$$ To capture these two phenomena, we define a Markov model on the record types (and given the record type, a record is chosen uniformly from the set of records with that type): where s(t) def = 1r E s : r.t = t} and r0.t is a dedicated START record type.2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last.

Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation. $$$$$ We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation. $$$$$ We produce a predicted set of line-record pairs A' by aligning a line to a record ri if the span of (the utterance corresponding to) ri overlaps the line.
Compared to Liang et al (2009), our more accurate (i.e. higher F-measure) matchings provide a clear improvement in both semantic parsing and tactical generation. $$$$$ However, we did not tune any hyperparameters, but rather used generic values which worked well enough across all three domains.

In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption. $$$$$ In the M-step, we optimize the parameters 0 by normalizing the expected counts computed in the E-step.
In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption. $$$$$ Sometimes the exact value v is used (e.g., in reporting football statistics).
In particular, we showed that our alignments provide a better foundation for learning accurate semantic parsers and tactical generators compared to those of Liang et al (2009), whose generative model is limited by a simple bag-of-words assumption. $$$$$ The input is a set of scenarios D, each of which is a text w paired with a world state s. We maximize the marginal likelihood of our data, summing out the latent variables (r, f, c): where 0 are the parameters of the model (all the multinomial probabilities).
