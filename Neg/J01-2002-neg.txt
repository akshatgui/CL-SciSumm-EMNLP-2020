This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ Mostly because of the less detailed tagset, the average ambiguity of the tags is lower than LOB's, at 2.34 tags per token in the corpus.
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ We start our detailed examination with the words that are most often mistagged.
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ The results are examined in more detail in Section 5, where we discuss such aspects as accuracy on specific words or tags, the influence of inconsistent training data, training set size, the contribution of individual component taggers, and tagset granularity.
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ Here it is much harder to provide numerical examples, as the problematic situation must first be recognized.

For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ For both MXP and HMM, the Wotan tagger indeed yields a better WotanLite tagging than the WotanLite tagger itself, thus supporting the hypothesis.
For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ As for base taggers, the first two differences are easily explained: Unigram has to deal with unknown words, while LexProb does not, and TnT is a more advanced trigram system.
For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ For the combination, a method called credibility profiles worked best.
For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ Even though they have relatively low frequencies, they are ranked high on the error lists.

Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ We can also see from these results that WSJ, although it is about the same size as LOB, and has a smaller tagset, has a higher difficulty level than LOB.
Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ WPDV shows a relatively constant significant improvement over all data sets.
Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ All combination taggers outperform their best component.

Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ In all experiments, simple voting was used to combine component tagger decisions.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ After comparison, their outputs are combined using several voting strategies and second-stage classifiers.

Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ In Abney, Schapire, and Singer (1999), ADABOOST variants are used for tagging WSJ material.
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ The written part, which we use in our experiments, consists of about 750K words, in samples ranging from 53 to 451 words.
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ For each level we list the accuracy (%) and the percentage of errors made by the best individual tagger that can be corrected by combination (Err).
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ The reasons for this choice of task are several.

This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ The curves for WPDV and Maccent appear to be leveling out towards the right end of the graph.
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ In Section 6, we discuss the results in the light of related work, after which we conclude (Section 7) with a summary of the most important observations and interesting directions for future research.
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ The cases where the tagger is found to be wrong provide interesting information as well.

These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ The curves for WPDV and Maccent appear to be leveling out towards the right end of the graph.
These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ All combination taggers outperform their best component.
These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ The most straightforward method to combine the results of multiple taggers is to do an n-way vote.
These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ All combination taggers outperform their best component.

In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ All combination taggers outperform their best component.
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system.
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ Each type of learning method brings its own &quot;inductive bias&quot; to the task and will produce a classifier with slightly different characteristics, so that different methods will tend to produce different errors.
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ Resulting accuracy is comparable to, but not better than, that of the maximum entropy tagger.

The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ The hypothesis is that both types of approaches can yield a more accurate model from the same training data than the most accurate component of the combination, and that given enough training data the arbiter type of method will be able to outperform the gang type.'
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ There are separate templates for known words (mainly based on local word and tag context), and for unknown words (based on suffix, prefix, and other lexical information).
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ The difference between single classifiers learning to combine information sources, i.e., their input features (see Roth [1998] for a general framework), and the combination of ensembles of classifiers trained on subsets of those features is not always very clear anyway.
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ This research was done while the second and third authors were at Tilburg University.

This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ MBT is always better than TBL, but is outperformed by both MXP and HMM.
This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ This recent research shows that the combination approach is potentially useful for many NLP tasks apart from tagging.
This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ All combination taggers outperform their best component.

We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ The first data set we use for our experiments consists of the tagged Lancaster-Oslo/Bergen corpus (LOB [Johansson 1986]).
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ This is not the case for all the following (arbiter type) combination methods, a fact which we have recently exploited in bootstrapping a word class tagger for a new corpus from existing taggers with completely different tagsets (Zavrel and Daelemans 2000).
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ In other words, it uses less specific rather than more specific information.
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ Component classifiers here are based on different information sources (subsets of features), e.g., capitalization of current word, and the triple &quot;string, capitalization, and tag&quot; of the word to the left of the current word are the basis for the training of some of their component classifiers.
