This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ WPDV and Maccent make the best use of the extra information, with WPDV having an edge for less consistent data (WSJ) and Maccent for material with a high error rate (Wotan).
This simple idea has been applied to a variety of classification problems ranging from optical character recognition to medical diagnosis, part-of-speech tagging (see Dietterich 1997 and van Halteren et al 2001 for overviews), and notably supervised WSD (Florian et al, 2002). $$$$$ This does not require any tuning of the voting mechanism on training data.

For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ The model is trained by iteratively adding binary features with the largest gain in the probability of the training data, and estimating the weights using a numerical optimization method called improved iterative scaling.
For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ When we compare it with the other taggers used in this paper, we see that a trigram HMM tagger uses a very limited set of features (Table 1).
For example, the state of the art POS tagger is an ensemble of individual taggers (van Halterenet al, 2001), each of which must process the text separately. $$$$$ Table 5 lists both the accuracies of several ideal combiners (`)/0) and the error reduction in relation to the best base tagger for the data set in question (A Err).22 For example, on LOB, &quot;All ties correct&quot; produces 1,941 errors (corresponding to an accuracy of 98.31%), which is 31.3% less than HMM's 2,824 errors.

Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ This research was done while the second and third authors were at Tilburg University.
Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
Illustrating the negative impact of annotation errors on computational uses of annotated corpora, van Halteren et al (2001) compare taggers trained and tested on the Wall Street Journal (WSJ, Marcus et al, 1993) and the Lancaster-Oslo-Bergen (LOB, Johansson, 1986) corpora and find that the results for the WSJ perform significantly worse. $$$$$ The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.

Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ First, we make use of the gang effect.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ Again we list both the accuracies of the combiners (&quot;Yo) and the error reduction in relation to the best base tagger (AEâ€ž).
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ A high number of errors for a word is due to a combination of tagging difficulty and frequency.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods.

Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ First, there is reason to believe that better results can be obtained by using the probability distributions generated by the component systems, rather than just their best guesses (see, for example, Ting and Witten [19974.
Given that the latest literature on POS tagging using Penn Treebank reports an accuracy of around 97% with in-domain training data (van Halteren et al, 2001), we achieve a very reasonable performance, considering these errors. $$$$$ We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.

This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ As for base taggers, the first two differences are easily explained: Unigram has to deal with unknown words, while LexProb does not, and TnT is a more advanced trigram system.
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ For the other two versions (Tags+Word and Tags+Context), a value of k = 3 is used, and each overlapping feature is weighted by its Information Gain (Daelemans, Van den Bosch, and Weijters 1997).
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ In Tufi* (1999), a single tagger generator is trained on different corpora representing different language registers.
This data set was also used in (van Halteren et al, 2001), therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch. $$$$$ The method that was eventually developed we have called Weighted Probability Distribution Voting (henceforth WPDV).

These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ The amount of improvement varies from 11,3% error reduction for WSJ to 24.3% for LOB.
These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ For the combiners, the situation is rather inconclusive.
These sentences received no contextual labels and thus not all of the training data used in (van Halteren et al, 2001) could be used in the Wotan experiment. $$$$$ The authors would like to thank the creators of the tagger generators and classification systems used here for making their systems available, and Thorsten Brants, Guy De Pauw, Erik Tjong Kim Sang, Inge de Monnink, the other members of the CNTS, ILK, and TOSCA research groups, and the anonymous reviewers for comments and discussion.

In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ Now none of these is especially bothersome at run-time (most of the computational difficulties being experienced during training), but when combining N systems, the time needed to process the text can be expected to be at least a factor N+1 more than when using a single system.
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ Since then the method has also been applied to other NLP tasks with good results (see Section 6).
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ However, most of the time, the choice between combining or not combining will have to be based on evidence from carefully designed pilot experiments, for which this paper can only hope to provide suggestions and encouragement.
In the Wotan experiment, 36K sentences (628K words) are used for training (compared to 640K words in (van Halteren et al, 2001)), and 4176 sentences (72K words) are used for testing. $$$$$ However, we have already seen that MBT has severe problems in dealing with the complex Wotan data.

The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ Here we use a slight adaptation of the tagset.
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ After comparison, their outputs are combined using several voting strategies and second-stage classifiers.
The Wotan data is annotated with a tag set consisting of 345 tags (although a number of 341 is reported in (van Halteren et al, 2001)). $$$$$ The data set that is used appears to be the primary factor in the variation, especially the data set's consistency.

This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ The most important rule templates are of the form if context = x change tag, to tag) where context is some condition on the tags of the neighbouring words.
This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ All combination taggers outperform their best component.
This result is very similar to the 92.06% reported by Van Halteren, Zavrel and Daelemans in (van Halteren et al, 2001) who used the TnT trigram tagger (Brants, 2000) on the same training and testing data. $$$$$ In general, it has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than any of the individual systems.

We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ Partly as a result of this, but also very much because of word compounding in Dutch, we see a much higher percentage of new tokens-6.24% tokens unseen in the training set.
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ However, most of the time, the choice between combining or not combining will have to be based on evidence from carefully designed pilot experiments, for which this paper can only hope to provide suggestions and encouragement.
We have experimented with various classifier combination methods, such as those described in (Brill and Wu, 1998) or (van Halteren et al., 2001), and got improved results, as expected. $$$$$ Second, in the present paper we have used disagreement between a fixed set of component classifiers.
