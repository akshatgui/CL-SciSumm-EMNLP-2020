In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed $$$$$ (1) Observations are typically words and their lexicalfeatures in the task of POS tagging.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed $$$$$ The algorithm can enumerate all possible decomposition structures and find the highest prob ability sequence together with the corresponding decomposition structure in polynomial time.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed $$$$$ Verbs are usually tagged in later stages because their tags are likely to be ambiguous.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed $$$$$ This characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitivelyperform experiments.

SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ In addition, we made the development set from section 21 3.
SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ We applied the best model on the development set in each chunk representation type to the test data.
SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ Networks (Toutanova et al, 2003) 97.24 Perceptron (Collins, 2002) 97.11 SVM (Gimenez and Marquez, 2003) 97.05 HMM (Brants, 2000) 96.48 Easiest-first 97.10 Full Bidirectional 97.15Table 3: POS tagging accuracy on the test set (Sec tions 22-24 of the WSJ, 5462 sentences).provided by (Toutanova et al, 2003) except for com plex features such as crude company-name detectionfeatures because they are specific to the Penn Tree bank and we could not find the exact implementation details.
SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ In theirmodeling, the local classifiers can always use the in formation about future tags, but that could cause a double-counting effect of tag information.

Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ Regularization is important in maximum entropy modeling to avoid overfitting to the training data.
Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ This result indicates thatbidirectional inference with maximum entropy mod eling can achieve comparable performance to other state-of-the-art POS tagging methods.
Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ Also, vot ing methods generally need many tagging processes to be run on a sentence, which makes it difficult to build a fast tagger.Our algorithm can be seen as an ensemble classi fier by which we choose the highest probability oneamong the different taggers with all possible decom position structures.
Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ Tag featuresare not necessarily used in all the models.

Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tag ging, named entity recognition and text chunking.
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ Although sequential classification approachescould suffer from label bias problems, they have sev eral advantages over CRFs.
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ features cannot be used in left-to right models.
