In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). $$$$$ Kudo et al (2001) attained performance improvement in chunking by conducting weighted voting of multiple SVMs trained with distinct chunk representations.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). $$$$$ Shen et al (2003) re ported a 4.9% error reduction of supertagging by 472 Representation Method Order Recall Precision F-score Speed (tokens/sec) IOB2 Left-to-right 1 93.17 93.05 93.11 1,775 2 93.13 92.90 93.01 989 Right-to-left 1 92.92 92.82 92.87 1,635 2 92.92 92.74 92.87 927 Dependency Networks 1 92.71 92.91 92.81 2,534 2 92.61 92.95 92.78 1,893 Easiest-first 1 93.17 93.04 93.11 2,441 2 93.35 93.32 93.33 1,248 Full Bidirectional 1 93.29 93.14 93.21 712 2 93.26 93.12 93.19 48 Start/End Left-to-right 1 92.98 92.69 92.83 861 2 92.96 92.67 92.81 439 Right-to-left 1 92.92 92.83 92.87 887 2 92.89 92.74 92.82 451 Dependency Networks 1 87.10 89.56 88.32 1,894 2 87.16 89.44 88.28 331 Easiest-first 1 93.33 92.95 93.14 1,950 2 93.31 92.95 93.13 1,016 Full Bidirectional 1 93.52 93.26 93.39 392 2 93.44 93.20 93.32 4 Table 5: Chunking F-scores on the development set.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). $$$$$ The biggest difference between ourapproach and such voting methods is that the lo cal classifier in our bidirectional inference methodscan have rich information for decision.
In order to produce POS-based surprisal estimates, versions of both the training and experimental texts with their words replaced by POS were developed: The BNC sentences were parsed by the Stanford Parser, version 1.6.7 (Klein and Manning, 2003), whilst the experimental texts were tagged by an automatic tagger (Tsuruoka and Tsujii, 2005), with posterior review and correction by hand following the Penn Treebank Project Guidelines (Santorini, 1991). $$$$$ Table 1 lists the feature templates used in our experiments.

SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ Indeed, even the Viterbi decoding of second-order markov models for POS tagging is not practical unless some pruning methodis involved.
SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tag ging, named entity recognition and text chunking.
SSP performs POS tagging using an off-the-shelf tagger (Tsuruoka and Tsujii, 2005). $$$$$ ef fects which make the model lock onto conditionally consistent but jointly unlikely sequences.

Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ Exper imental results of part-of-speech tagging and text chunking show that the proposedbidirectional inference methods consis tently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achievedby state-of-the-art learning algorithms in cluding kernel support vector machines.
Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ A perceptron algorithm gives 97.11% (Collins, 2002).
Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al 2003) and (Tsuruoka and Tsujii, 2005). $$$$$ Although SVMs do not output probabilities, theeasiest-first method would be easily applied by considering the margins output by SVMs as the confi dence of local classification.

Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ Bidirectional inference methods clearly out performed unidirectional methods.
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ Support vector machines with appropri ate kernels is a good candidate because they havegood generalization performance as a single classi fier.
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ The model gives equally good per formance as the maximum entropy modeling with Gaussian priors (Chen and Rosenfeld, 1999), and the size of the resulting model is much smaller thanthat of Gaussian priors because most of the param eters become zero.
Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. $$$$$ Although SVMs do not output probabilities, theeasiest-first method would be easily applied by considering the margins output by SVMs as the confi dence of local classification.
