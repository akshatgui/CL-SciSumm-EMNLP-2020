In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class.
In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ We also removed speech segments containing the term “amendment”, since we found during initial inspection that these speeches generally reflect a speaker’s opinion on an amendment, and this opinion may differ from the speaker’s opinion on the underlying bill under discussion.
In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually.
In (Thomas et al, 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. $$$$$ 11Unfortunately, this policy leads to inferior test-set agreeAn important observation is that precision may be more important than accuracy in deciding which agreement links to add: false positives with respect to agreement can cause speech segments to be incorrectly assigned the same label, whereas false negatives mean only that agreement-based information about other speech segments is not employed.

votes on the bill under discussion (Thomas et al, 2006). $$$$$ But we stress that the most important observation we can make from Table 5 is that once again, the addition of agreement information leads to substantial improvements in accuracy.
votes on the bill under discussion (Thomas et al, 2006). $$$$$ GovTrack (http://govtrack.us) is an independent website run by Joshua Tauberer that collects publicly available data on the legislative and fundraising activities of U.S. congresspeople.
votes on the bill under discussion (Thomas et al, 2006). $$$$$ We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.

(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ Each debate consists of a series of speech segments, where each segment is a sequence of uninterrupted utterances by a single speaker.
(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ Hence, in the general case we wish to be able to express “soft” preferences for all of an author’s statements to receive the same label, where the strengths of such constraints could, for instance, vary according to the time elapsed between the statements.
(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ It would be interesting to investigate the application of such methods to our problem.
(Thomas et al, 2006), or personal preferences for topics (Grimmer, 2009) would enrich the model and better illuminate the interaction of influence and topic. $$$$$ We also removed speech segments containing the term “amendment”, since we found during initial inspection that these speeches generally reflect a speaker’s opinion on an amendment, and this opinion may differ from the speaker’s opinion on the underlying bill under discussion.

Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. $$$$$ We thus view our results as demonstrating the potentially large benefits of exploiting sentiment-related discourse-segment relationships in sentiment-analysis tasks.
Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. $$$$$ Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations.
Thomas et al (2006) achieved accuracies of 71.3% by using speaker agreement information in the graph-based MinCut/Maxflow algorithm, as compared to accuracies around 70% via an an SVM classifier operating on content alone. $$$$$ We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.

The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). $$$$$ Or, we could even attempt to model relationships between topics or concepts, in a kind of extension of collaborative filtering.
The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). $$$$$ The enhanced accuracies are obtained via a fairly primitive automatically-acquired “agreement detector” and a conceptually simple method for integrating isolated-document and agreement-based information.
The same applies to the task of subgroup detection (as done by (AbuJbara et al., 2012), (Anand et al, 2011) or (Thomas et al, 2006)) . In order to produce a finer-grained model of positions, we want to develop a model that places positions stated in text along a one-dimensional scale, as done by (Slapin and Proksch, 2008) with their system called Wordfish, (Gabel and Huber, 2000), (Laver and Garry, 2000), (Laver et al, 2003) or (Sim et al, 2013). $$$$$ To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.

Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class.
Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ Relationships between the unlabeled items Carvalho and Cohen (2005) consider sequential relations between different types of emails (e.g., between requests and satisfactions thereof) to classify messages, and thus also explicitly exploit the structure of conversations.
Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.
Instead of selecting sentences from the manifesto that cover a topic, the position could be extracted from the manifesto using topic models, as shown in (Thomas et al, 2006) and (Gerrish and Blei, 2011). $$$$$ We now turn to the more interesting problem of representing the preferences that speech segments may have for being assigned to the same class.

Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). $$$$$ To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.
Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al (2006). $$$$$ IIS-0329064.

However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. $$$$$ A minimum-cost assignment thus represents an optimum way to classify the speech segments so that each one tends not to be put into the class that the individual-document classifier disprefers, but at the same time, highly associated speech segments tend not to be put in different classes.
However, like other cascaded approaches (e.g., Thomas et al (2006), Mao and Lebanon (2006)), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. $$$$$ To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.

We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). $$$$$ These labels are then used to train an SVM classifier, the output of which is subsequently used to create weights on agreement links in the test set as follows.
We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). $$$$$ An exception is Grefenstette et al. (2004), who experimented with determining the political orientation of websites essentially by classifying the concatenation of all the documents found on that site.
We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). $$$$$ We concentrated on debates regarding “controversial” bills (ones in which the losing side generated at least 20% of the speeches) because these debates should presumably exhibit more interesting discourse structure.
We also use the U.S. Congressional floor debates transcripts from Thomas et al (2006). $$$$$ Due to its extensive cross-referencing and collating of information, it was nominated for a 2006 “Webby” award.

For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http $$$$$ The problem of agreement identification can be decomposed into two sub-problems: identifying references and their targets, and deciding whether each reference represents an instance of agreement.
For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http $$$$$ Given the current state of the art in sentiment analysis, it is doubtful that one could determine the (probably topic-specific) relationship between presented evidence and speaker opinion.
For our experiments, we evaluate our methods using the speaker based speech-segment classification setting as described in Thomas et al (2006). Datasets in the required format for SVMsle are available at http $$$$$ In our view, the contribution of our work is the examination of new types of relationships, not the method by which such relationships are incorporated into the classification decision.

In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ We concentrated on debates regarding “controversial” bills (ones in which the losing side generated at least 20% of the speeches) because these debates should presumably exhibit more interesting discourse structure.
In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ Recently, several alternative, often quite sophisticated approaches to collective classification have been proposed (Neville and Jensen, 2000; Lafferty et al., 2001; Getoor et al., 2002; Taskar et al., 2002; Taskar et al., 2003; Taskar et al., 2004; McCallum and Wellner, 2004).
In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ A wide range of relationships between text segments can be modeled as positive-strength links.
In the other setting described in Thomas et al (2006) (segment-based speech-segment classification), around 39% of 1051 Table 1 $$$$$ However, this comes at the cost of greatly reducing agreement accuracy (development: 64.38%; test: 66.18%) due to lowered recall levels.

 $$$$$ To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.
 $$$$$ The speech segments remained grouped by debate, with 38 debates assigned to the training set, 10 to the test set, and 5 to the development set; we require that the speech segments from an individual debate all appear in the same set because our goal is to examine classification of speech segments in the context of the surrounding discussion.
 $$$$$ Here we discuss two types of constraints that are considered in this work.
 $$$$$ It would be interesting to investigate the application of such methods to our problem.

Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. $$$$$ While the development-set results would induce us to utilize the standard threshold value of 0, which is sub-optimal on the test set, the Bagr = 0 agreement-link policy still achieves noticeable improvement over not using agreement links (test set: 70.81% vs. 67.21%).
Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. $$$$$ We randomly split the data into training, test, and development (parameter-tuning) sets representing roughly 70%, 20%, and 10% of our data, respectively (see Table 1).
Thomas et al (2006) presented a method based on support vector machines to determine whether the speeches made by participants represent support or opposition to proposed legislation, using transcripts of U.S. congressional floor debates. $$$$$ To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another.

Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). $$$$$ Obviously, incorporating agreement information provides additional benefit only when the input documents are relatively difficult to classify individually.
Our aggregation technique does, however, presuppose consistency of opinions, in a similar way to Thomas et al (2006). $$$$$ It would be interesting to investigate the application of such methods to our problem.

Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. $$$$$ This section outlines the main steps of the process by which we created our corpus (download site: www.cs.cornell.edu/home/llee/data/convote.html).
Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. $$$$$ Indeed, in other settings (e.g., a movie-discussion listserv) one may not be able to determine the participants’ political leanings, and such information may not lead to significantly improved results even if it were available. tween two speakers, such as explicit assertions (“I second that!”) or quotation of messages in emails or postings (see Mullen and Malouf (2006) but cf.
Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. $$$$$ For example, perhaps we could infer that two speakers sharing a common opinion on evolutionary biologist Richard Dawkins (a.k.a.
Thomas et al 2006 address the same problem of determining support and opposition as applied to congressional floor-debates. $$$$$ This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics.

The first baseline is based on the work of (Thomas et al 2006). $$$$$ Agrawal et al. (2003)).
The first baseline is based on the work of (Thomas et al 2006). $$$$$ We automatically discarded those speech segments belonging to a class of formulaic, generally one-sentence utterances focused on the yielding of time on the house floor (for example, “Madam Speaker, I am pleased to yield 5 minutes to the gentleman from Massachusetts”), as such speech segments are clearly off-topic.
The first baseline is based on the work of (Thomas et al 2006). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of any sponsoring institutions, the U.S. government, or any other entity.

We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. $$$$$ Although one can also implement this assumption via concatenation of same-speaker speech segments (see Section 4.3), we view the fact that our graph-based framework incorporates both hard and soft constraints in a principled fashion as an advantage of our approach.
We used the speaker agreement component presented in (Thomas et al 2006) as a baseline. $$$$$ We showed that the integration of even very limited information regarding inter-document relationships can significantly increase the accuracy of support/opposition classification.

Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). $$$$$ While such functionality is well beyond the scope of our current study, we are optimistic that we can develop methods to exploit additional types of relationships in future work.
Other work that has considered different discourse functions in sentiment analysis, have experimented on detecting arguments (Somasundaran et al, 2007) and the stance of political debates (Thomas et al, 2006). $$$$$ This includes both official documents, such as the full text of laws and the proceedings of legislative bodies, and unofficial documents, such as postings on weblogs (blogs) devoted to politics.

Stances in online debates $$$$$ It would be interesting to investigate the application of such methods to our problem.
Stances in online debates $$$$$ Same-speaker constraints: In Congressional debates and in general social-discourse contexts, a single speaker may make a number of comments regarding a topic.
Stances in online debates $$$$$ We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation.
Stances in online debates $$$$$ Then, any class assignment c = c(s1), c(s2), ... , c(sn) can be assigned a cost where c(s) is the “opposite” class from c(s).

Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ For example, we may find textual4 evidence of a high likelihood of agreement be4Because we are most interested in techniques applicable across domains, we restrict consideration to NLP aspects of the problem, ignoring external problem-specific information.
Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ Whether or not better speech-segment classification is ultimately achieved is discussed in the next sections.
Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ Each debate consists of a series of speech segments, where each segment is a sequence of uninterrupted utterances by a single speaker.
Burfoot et al, (2011) builds on the work of (Thomas et al, 2006) and proposes collective classification using speaker contextual features (e.g., speaker intentions based on vote labels). $$$$$ Let s1, s2, ... , sn be the sequence of speech segments within a given debate, and let Y and N stand for the “yea” and “nay” class, respectively.
