Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG).

Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ In our experiments, we implement two baseline methods for comparison: Baseline-1: Baseline-1 follows the method proposed in (Quirk et al., 2004), which generates paraphrases using typical SMT tools.
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ Paraphrase generation aims to generate a paraphrase for a source sentence in a certain application.
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ Second, we will extend the method to other applications, We hope it can serve as a universal framework for most if not all applications.

Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ We will try to address sentence restructuring in our future work.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ If the applications are not considered, all units of an input sentence that can be paraphrased using the PTs will be extracted as source units.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.

Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ (1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.
Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ Language Model: We use a tri-gram language model in this work.

The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ The algorithm currently used is simple but greedy, which may miss some useful paraphrase units.
The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ This is because in sentence similarity computation, only the target units appearing in the reference sentences are kept in paraphrase planning.
The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access.
The results show that: (1) although the candidate paraphrases acquired by MT engines are noisy, they provide good raw materials for further paraphrase generation; (2) the selection-based technique is effective, which results in the best performance; (3) the decoding based technique is promising, which can generate paraphrases that are different from the candidates; (4) both the selection-based and decoding-based techniques outperform a state-of-the-art approach SPG (Zhao et al, 2009). $$$$$ This paper proposes a method for statistical paraphrase generation.

Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Similar to typical SMT, a large parallel corpus is needed as training data in the SMT-based PG.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ All source and target units are listed below the input sentence, in which the first two source units are phrases, while the third and fourth are a pattern and a collocation, respectively.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Paraphrase generation (PG) is important in plenty of NLP applications.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ In our experiments, we use the proposed method to generate paraphrases for three different applications.

We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ They paraphrase a sentence s by replacing its words with WordNet synonyms, so that s can be more similar in wording to another sentence s'.
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ We therefore introduce a new optimization objective function in this paper.
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ Paraphrases are alternative ways that convey the same meaning.

The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Paraphrase Model: Paraphrase generation is a decoding process.
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Results show that the percentages of test sentences that can be paraphrased are 97.2%, 95.4%, and 56.8% for the applications of sentence compression, simplification, and similarity computation, respectively.
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.

In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ There have been researchers trying to propose uniform PG methods for multiple applications.
In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ This paper proposes a method for statistical paraphrase generation.

The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ However, it cannot generate other types of paraphrases but only synonym substitution.
The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access.
The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ In the second phase, it selects an optimal substitute for each given word from the synonyms according to the context in s. This kind of method is simple, since the thesaurus synonyms are easy to access.

More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ Thesaurus-based methods: The thesaurus-based methods generate a paraphrase t for a source sentence s by substituting some words in s with their synonyms (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006).
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ Despite the similarity between PG and MT, the statistical model used in SMT cannot be directly The SPG method proposed in this work contains three components, i.e., sentence preprocessing, paraphrase planning, and paraphrase generation (Figure 1).
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ This paper proposes a method for statistical paraphrase generation.
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ Researchers employ the existing SMT models for PG (Quirk et al., 2004).

To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ The basic assumption is that a paraphrase should contain as many correct unit replacements as possible.
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ We consider three paraphrase applications in our experiments, including sentence compression, sentence simplification, and sentence similarity computation.
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ We compute the kappa statistic between the raters.

Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003).
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ Thus any sentence can be used in development and test.
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG).
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).

 $$$$$ SMT-based methods: SMT-based methods viewed PG as monolingual MT, i.e., translating s into t that are in the same language.
 $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
 $$$$$ The language model is trained using a 9 GB English corpus.
 $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.

Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ As can be seen, the first and fourth source units are filtered in paraphrase planning, since none of their paraphrases achieve the application (i.e., shorter in bytes than the source).
Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ The parameters that result in the highest rf on the development set are finally selected.
Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.

Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ The contributions are as follows.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ The algorithm currently used is simple but greedy, which may miss some useful paraphrase units.

Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ We combine the three sub-models based on a log-linear framework and get the SPG model: We use five PTs in this work (except the selfparaphrase table), in which each pair of paraphrase units has a score assigned by the score function of the corresponding method.
Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation.
Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ PT-4 contains 1,018,371 pairs of paraphrase patterns.

As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ We call this process paraphrase planning, which is formally defined as in Figure 2.
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ The generated paraphrases are then manually scored based on adequacy, fluency, and usability.
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ (1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.

Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ Some researchers then tried to automatically extract paraphrase rules (Lin and Pantel, 2001; Barzilay and Lee, 2003; Zhao et al., 2008b), which facilitates the rule-based PG methods.

More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ (1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ In addition, it exploits multiple resources, including paraphrase phrases, patterns, and collocations, to resolve the data shortage problem and generate more varied paraphrases.
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ The details of the corpora, methods, and score functions are presented in (Zhao et al., 2008a).
