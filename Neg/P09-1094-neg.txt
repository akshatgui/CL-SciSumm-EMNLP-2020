Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ This is because in sentence similarity computation, only the target units appearing in the reference sentences are kept in paraphrase planning.
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ However, for the sentence similarity computation purpose in our experiments, we want to evaluate if the method can enhance the stringlevel similarity between two paraphrase sentences.
Researchers have made great efforts to improve paraphrasing from different perspectives, such as paraphrase extraction (Zhao et al, 2007), paraphrase generation (Quirk et al, 2004), model optimization (Zhao et al., 2009) and etc. $$$$$ In our experiments, we use the proposed method to generate paraphrases for three different applications.

Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ However, the research of PG is far from enough.
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ However, the research of PG is far from enough.
Zhao et al (2009) proposes an unified paraphrasing framework that can be adapted to different applications using different usability models. $$$$$ In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance.

Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ In our experiments, the development set contains 200 sentences and the test set contains 500 sentences, both of which are randomly selected from the human translations of 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ Since paraphrase applications are not considered by the baselines, each baseline method outputs a single best paraphrase for each test sentence.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ In this case, we only count the PT that provides the largest paraphrase score, i.e., kˆ = arg maxk{φk(¯si, ¯ti)λk}.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ This paper proposes a method for statistical paraphrase generation.

Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ This kind of method usually involves two phases, i.e., candidate extraction and paraphrase validation.
Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ Here is a brief description of the different scales for the criteria:
Zhao et al (2008) apply SMT-style decoding for paraphrasing, using several log linear weighted resources while Zhao et al (2009) filter out paraphrase candidates and weight paraphrase features according to the desired NLP task. $$$$$ This paper proposes a method for statistical paraphrase generation.

The results show that $$$$$ Further results show that the average number of unit replacements in each sentence is 5.36, 4.47, and 1.87 for sentence compression, simplification, and similarity computation.
The results show that $$$$$ One can refer to (Zhao et al., 2008b) for the details.
The results show that $$$$$ Actually, it is defined similarly to the translation model in SMT (Koehn et al., 2003).

Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ Our future work will be carried out along two directions.
Zhao et al (2009) further improved the method by introducing a usability sub-model into the paraphrase model so as to generate varied paraphrases for different applications. $$$$$ We define rr as: rr = wdev(r)/wdev(s), where wdev(r) is the total number of words in the replaced units on the development set, and wdev(s) is the number of words of all sentences on the development set.

We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ Usability Model: The usability model prefers paraphrase units that can better achieve the application.
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ Baseline-2: Baseline-2 extends Baseline-1 by combining multiple resources.
We compare our method with a state-of-the art approach SPG (Zhao et al, 2009), which is a statistical approach specially designed for PG. $$$$$ However, the research of PG is far from enough.

The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ The parameters that result in the highest rf on the development set are finally selected.
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Second, we will extend the method to other applications, We hope it can serve as a universal framework for most if not all applications.
The general idea is not anymore to produce a text from data, but to transform a text so as to ensure that it has desirable properties appropriate for some intended application (Zhao et al, 2009). $$$$$ Here is a brief description of the different scales for the criteria:

In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
In machine translation, n-gram-based evaluation measures like BLEU have been criticized exactly because they can not cope sufficiently well with paraphrases (Callison-Burch et al, 2006), which play a central role in abstractive sentence compression (Zhao et al, 2009a). $$$$$ Moreover, developing a NLG system is also not trivial.

The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ Based on the above consideration, we acquire experiment data from the human references of the MT evaluation, which provide several human translations for each foreign sentence.
The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.
The candidate compressions were generated by first using GA-EXTR and then applying existing paraphrasing rules (Zhao et al, 2009b) to the best extractive compressions of GA-EXTR. $$$$$ The labeling results are shown in the upper part of Table 1.

More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ The proposed method generates paraphrases for the input sentences in each application.
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ Paraphrase generation (PG) is important in plenty of NLP applications.
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ Therefore, data shortage becomes the major limitation of the method.
More recently, Zhao et al (2009a) presented a sentence paraphrasing method that can be configured for different tasks, including a form of sentence compression. $$$$$ In this case, we only count the PT that provides the largest paraphrase score, i.e., kˆ = arg maxk{φk(¯si, ¯ti)λk}.

To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ PT-2 and PT-3 contain 92,358, and 17,668 pairs of paraphrase phrases, respectively.
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ Our SPG model contains three sub-models: a paraphrase model, a language model, and a usability model, which control the adequacy, fluency, and usability of the paraphrases, respectively1.
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ The algorithm currently used is simple but greedy, which may miss some useful paraphrase units.
To obtain candidate compressions, we first applied GA-EXTR to the 346 source sentences, and we then applied the paraphrasing rules of Zhao et al (2009b) to the resulting extractive compressions; we provide more information about GA-EXTR and the paraphrasing rules below. $$$$$ Our future work will be carried out along two directions.

Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ There are two main threads in the research of paraphrasing, i.e., paraphrase recognition and paraphrase generation (PG).
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ The generated paraphrases are then manually scored based on adequacy, fluency, and usability.
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ Paraphrases are alternative ways that convey the same meaning.
Zhao et al (2009b) associate each paraphrasing rule with a score, intended to indicate its quality. $$$$$ The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.

 $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
 $$$$$ SMT-based methods: SMT-based methods viewed PG as monolingual MT, i.e., translating s into t that are in the same language.
 $$$$$ To address this problem, we have tried combining multiple resources to improve the SMT-based PG model (Zhao et al., 2008a).
 $$$$$ We therefore introduce a new optimization objective function in this paper.

Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ In addition, it exploits multiple resources, including paraphrase phrases, patterns, and collocations, to resolve the data shortage problem and generate more varied paraphrases.
Also, some studies have been done in paraphrase generation in NLG (Zhao et al, 2009), (Chevelu et al, 2009). $$$$$ Their method contains two main parts, namely the lexical simplifier and syntactic simplifier.

Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ The language model is trained using a 9 GB English corpus.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Similar to typical SMT, a large parallel corpus is needed as training data in the SMT-based PG.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ (2) It generates paraphrases for different applications with a uniform model, rather than presenting distinct methods for each application.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ Thus here we just conduct an informal comparison with these methods.

Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ We assign the score “1” for any pair of paraphrase collocations.
Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ As can be seen, the first and fourth source units are filtered in paraphrase planning, since none of their paraphrases achieve the application (i.e., shorter in bytes than the source).
Application-driven paraphrase generation provides indirect means of evaluating paraphrase generation (Zhao et al., 2009). $$$$$ The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).

As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ These methods can be roughly viewed as special cases of ours, which only focus on the sentence similarity computation application and only use one kind of paraphrase resource.
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ (2) It generates paraphrases for different applications with a uniform model, rather than presenting distinct methods for each application.
As an illustration of the need to combine grammatical paraphrasing with data-driven paraphrasing, consider the example that Zhao et al (2009) use to illustrate the application of their paraphrasing method to similarity detection, shown in Table 1. $$$$$ (3) It uses multiple resources, including paraphrase phrases, patterns, and collocations, to relieve data shortage and generate more varied and interesting paraphrases.

Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ (3) It uses multiple resources, including paraphrase phrases, patterns, and collocations, to relieve data shortage and generate more varied and interesting paraphrases.
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ (1) It is the first statistical model specially designed for paraphrase generation, which is based on the analysis of the differences between paraphrase generation and other researches, especially machine translation.
Zhao et al (2008a) enrich this approach by adding multiple resources (e.g., thesaurus) and further extend the method by generating different paraphrase in different applications (Zhao et al, 2009). $$$$$ In detail, we use the first translation of a foreign sentence as the source s and the second translation as the reference s' for similarity computation.

More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ They paraphrase a sentence s by replacing its words with WordNet synonyms, so that s can be more similar in wording to another sentence s'.
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ The contributions are as follows.
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ Special thanks to Wanxiang Che, Ruifang He, Yanyan Zhao, Yuhang Guo and the anonymous reviewers for insightful comments and suggestions.
More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al, 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). $$$$$ The research was supported by NSFC (60803093, 60675034) and 863 Program (2008AA01Z144).
