 $$$$$ For all experiments, the training BLEU score remains significantly lower than the maximum obtainable BLEU score shown in line and line .
 $$$$$ There are phrase-based and word-based features: The feature is a ’unigram’ phrase-based feature capturing the identity of a block.
 $$$$$ We use the block set described in (Al-Onaizan et al., 2004), the use of a different block set may effect translation results.

In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ 3.
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ Table 3 presents experimental results in terms of uncased BLEU 6.
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ In this local re-ordering model (Tillmann and Zhang, 2005; Kumar and Byrne, 2005) a block with orientation is generated relative to its predecessor block .
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ It cannot be enumerated for practical purposes.

It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The initial block sequence training data contains only a single alternative.

Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The procedure is outlined in Table 1.
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ This work was partially supported by the GALE project under the DARPA contract No.
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The effect of this training procedure can be seen in Figure 2: each decoding step on the training data adds a high-scoring block sequence to the discriminative training and decoding performance on the training data is improved after each iteration (along with the test data decoding performance).
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ Although at this stage the system performance is not yet better than previous approaches, good translation results are achieved on a standard translation task.

Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ For the most complex model, the number of features is about million (ignoring all features that occur only once).
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993).
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ Additional phrase-based features include block orientation, target and source phrase bigram features.
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems such as part-of-speech tagging or shallow parsing, both in modeling and parameter training.

The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Again, convergence results can be proved for this procedure.
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Although at this stage the system performance is not yet better than previous approaches, good translation results are achieved on a standard translation task.
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Line and line of Table 3 show the training data averaged BLEU score obtained by searching for the highest BLEU scoring block sequence for each training sentence pair as described in Section 2.

Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993).
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ Although in order to achieve fast convergence with a theoretical guarantee, we should use Eq.
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ This work was partially supported by the GALE project under the DARPA contract No.

Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ While being similar to (Tillmann and Zhang, 2005), the current procedure is more automated with comparable performance.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ This work was partially supported by the GALE project under the DARPA contract No.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ Again, convergence results can be proved for this procedure.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ Starting point for the block-based translation model is a block set, e.g. about million Arabic-English phrase pairs for the experiments in this paper.

 $$$$$ Another important direction for performance improvement is to design methods that better approximate Eq.

 $$$$$ The loss function in Eq.
 $$$$$ Essentially the lemma says that if the decoder works well on these difficult alternatives (relevant points), then it works well on the whole space.
 $$$$$ In other words, if maximizes the scoring function , then also maximizes the BLEU metric.

Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ Rather than predicting local block neighbors as in (Tillmann and Zhang, 2005) , here the model parameters are trained in a global setting.
Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.

Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ This property is critical as it shows that as long as Eq.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ No translation, language, or distortion model probabilities are used as in earlier work on SMT.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ This step does not require the use of a decoder.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

 $$$$$ While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems such as part-of-speech tagging or shallow parsing, both in modeling and parameter training.
 $$$$$ Such probability features include language model, translation or distortion probabilities, which are commonly used in current SMT approaches 1.
 $$$$$ 1, but the model is trained in a global setting as shown below.
 $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.

For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.
For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ The current approach does not use specialized probability features as in (Och, 2003) in any stage during decoder parameter training.

This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper.
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ The decoding process is decomposed into local decision steps based on Eq.
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ This paper views phrase-based SMT as a block sequence generation process.

This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ Each block sequence corresponds to a candidate translation.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ No translation, language, or distortion model probabilities are used as in earlier work on SMT.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ Therefore in Table 2, we adopt an approximation, where the relevant set is updated by adding the decoder output at each stage.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ This step does not require the use of a decoder.

Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ Blocks are phrase pairs consisting of target and source phrases and local phrase re-ordering is handled by including so-called block orientation.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ The choice of our formulation is convex, which ensures that we are able to find the global optimum even for large scale problems.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ HR001106-2-0001.

This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ A perceptronlike algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004).
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ A monotone block sequence is generated except for the possibility to handle some local phrase re-ordering.
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ This work was partially supported by the GALE project under the DARPA contract No.
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.
