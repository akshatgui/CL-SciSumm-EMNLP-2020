 $$$$$ 6.
 $$$$$ In the training data where target translations are given, a BLEU score can be calculated for each against the target translations.
 $$$$$ Finally, some discussion and future work is presented in Section 5.
 $$$$$ To formulate this idea, we attempt to find a decoding parameter such that for each sentence in the training data, sequences in with the highest BLEU scores should get scores higher than those with low BLEU scores.

In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ Another important direction for performance improvement is to design methods that better approximate Eq.
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ Here, searching for the highest BLEU scoring block sequence is restricted to local re-ordering as is the model-based decoding (as shown in Fig.
In (Tillmann and Zhang, 2006) the model is optimized to produce a block orientation and the target sentence is used only for computing a sentence level BLEU. $$$$$ The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.

It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ Moreover, it gives a bound on the size of an approximate relevant set with a certain accuracy.5 The approximate solution of Eq.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ The novel algorithm differs computationally from earlier work in discriminative training algorithms for SMT (Och, 2003) as follows: No additional development data set is necessary as the weight vector is trained on bilingual training data only.
It might be the case that a larger k-best, or revisiting previous strategies for y+ and y− selection, such as bold updating, local updating (Liang et al, 2006b), or maxBLEU updating (Tillmann and Zhang, 2006) might have a greater impact. $$$$$ Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.

Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ 1.
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The advantage of this approach is that it can easily handle tens of millions of features, e.g. up to million features for the experiments in this paper.
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ .
Exceptions where discriminative SMT has been used on large training data are Liang et al. (2006a) who trained 1.5 million features on 67,000 sentences, Blunsom et al. (2008) who trained 7.8 million rules on 100,000 sentences, or Tillmann and Zhang (2006) who used 230,000 sentences for training. $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.

Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ The decoding process is decomposed into local decision steps based on Eq.
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ Allowing local block swapping in this search procedure yields a much improved BLEU score of .
Tillmann and Zhang (2006) use a BLEU oracle decoder for discriminative training of a local reordering model. $$$$$ The BLEU score is computed with respect to the single reference translation provided by the parallel training data.

The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ While the global training approach presented in this paper is simple, after iterations or so, the alternatives that are being added to the relevant set differ very little from each other, slowing down the training considerably such that the set of possible block translations might not be fully explored.
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ To formulate this idea, we attempt to find a decoding parameter such that for each sentence in the training data, sequences in with the highest BLEU scores should get scores higher than those with low BLEU scores.
The translation probability can also be discriminatively trained such as in Tillmann and Zhang (2006). $$$$$ Another important direction for performance improvement is to design methods that better approximate Eq.

Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ The paper is structured as follows: Section 2 presents the baseline block sequence model and the feature representation.
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ Note that due to the truth and alternative set up, we always have .
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ For the results with word-based features, the decoder still generates phrase-to-phrase translations, but all the scoring is done on the word level.
Tillmann and Zhang (2006) describe a perceptron style algorithm for training millions of features. $$$$$ Therefore our method, which employs less domain specific knowledge, is both simpler and more extensible than previous approaches.

Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ The key component is a new procedure to directly optimize the global scoring function used by a SMT decoder.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ We refer to this formulation as ’costMargin’ (cost-sensitive margin) method: for each training sentence the ’costMargin’ between the ’true’ block sequence set and the ’alternative’ block sequence set is maximized.
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ A perceptronlike algorithm that handles global features in the context of re-ranking is also presented in (Shen et al., 2004).
Both Liang, et al (2006), and Tillmann and Zhang (2006) report on effective machine translation (MT) models involving large numbers of features with discriminatively trained weights. $$$$$ This work was partially supported by the GALE project under the DARPA contract No.

 $$$$$ 1.
 $$$$$ The possibility of this reduction is based on the following theoretical result.
 $$$$$ This work was partially supported by the GALE project under the DARPA contract No.

 $$$$$ The block orientation se❱quence is generated under the restriction that the concatenated source phrases of the blocks yield the input sentence.
 $$$$$ HR001106-2-0001.
 $$$$$ Section 4 presents results on a standard Arabic-English translation task.

Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ HR001106-2-0001.
Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ The choice of our formulation is convex, which ensures that we are able to find the global optimum even for large scale problems.
Tillmann and Zhang (2006) trained their feature set using an on line discriminative algorithm. $$$$$ 1.

Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ These features correspond to the use of a language model, but the weights for theses features are trained on the parallel training data only.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ HR001106-2-0001.
Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance. $$$$$ The work in this paper substantially differs from previous work in SMT based on the noisy channel approach presented in (Brown et al., 1993).

 $$$$$ No translation, language, or distortion model probabilities are used as in earlier work on SMT.
 $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.

For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ The word-based models perform surprisingly well, i.e. the model in line uses only three feature types: model features like in Section 2, distortion features, and target language m-gram features up to .
For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.
For instance, some max-margin methods restrict their computations to a set of examples from a feasible set, where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). $$$$$ This is important since the language model can be trained on a much larger monolingual corpus.

This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ We intentionally leave the implementation details of the (*) step and (**) step open.
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ The authors would like to thank the anonymous reviewers for their detailed criticism on this paper.
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006). $$$$$ No translation, language, or distortion model probabilities are used as in earlier work on SMT.

This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ Another important direction for performance improvement is to design methods that better approximate Eq.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ This paper presents a view of phrase-based SMT as a sequential process that generates block orientation sequences.
This is the main motivation of (Tillmann and Zhang,2006), where the authors compute high BLEU hypotheses by running a conventional decoder so as to maximize a per-sentence approximation of BLEU-4, under a simple (local) reordering models. $$$$$ The training algorithm is evaluated on a standard Arabic-English translation task.

Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ In the training data where target translations are given, a BLEU score can be calculated for each against the target translations.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ 4 may not be optimal, and using different choices may lead to future improvements.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ This work was partially supported by the GALE project under the DARPA contract No.
Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrase based decoder on the accuracy of the translations. $$$$$ It also has the property that if for the BLEU scores holds, then the loss value is small (proportional to ).

This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ Moreover, the training procedure treats the decoder as a black-box, and thus can be used to optimize any decoding scheme.
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ After each training iteration the test data is decoded as well.
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ 3.
This is referred to in past work as maxBLEU (Tillmann and Zhang, 2006) (MB). $$$$$ During decoding, we maximize the score of a block orientation sequence where is a block, is its predecessor block, and eft ight eutral is a threevalued orientation component linked to the block : a block is generated to the left or the right of its predecessor block , where the orientation of the predecessor block is ignored.
