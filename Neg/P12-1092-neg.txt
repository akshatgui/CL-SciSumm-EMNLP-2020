The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ 1
The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ However, they have not been directly eval uated on word similarity tasks, which are importantfor tasks such as information retrieval and summarization.
The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Ournew multi-prototype neural language model outperforms previous neural models and competitive base lines on this new dataset.
The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pado and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ 4.2 WordSim-353.

We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ Acknowledgments The authors gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ 1
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ In order to learn multiple prototypes, we firstgather the fixed-sized context windows of all occur rences of a word (we use 5 words before and after the word occurrence).
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ The final score is the sum of the two scores: score = scorel + scoreg (7) The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.

We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. $$$$$ These learned word embeddings can be used to represent word contexts as low-dimensional weighted average vectors, which are then clustered to form different meaning groups and used to learn multi-prototype vectors.
We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. $$$$$ This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.
We also experiment with publicly released word embeddings (Huang et al, 2012), which were trained using both local and global context. $$$$$ Ournew multi-prototype neural language model outperforms previous neural models and competitive base lines on this new dataset.

In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ 73.4 ESA Wiki.
In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ In order to learn multiple prototypes, we firstgather the fixed-sized context windows of all occur rences of a word (we use 5 words before and after the word occurrence).
In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ Reisinger and Mooney (2010b) introduced a multi-prototype VSM whereword sense discrimination is first applied by clus tering contexts, and then prototypes are built using the contexts of the sense-labeled words.
In particular, many such representations are designed to capture lexical semantic properties and are quite effective features in semantic processing, including named entity recognition (Turian et al, 2009), word sense disambiguation (Huang et al., 2012), and lexical entailment (Baroni et al., 2012). $$$$$ They used up to 3 previ ous head words and showed increased performance on language modeling.

Source embeddings $$$$$ However, most of these models arebuilt with only local context and one represen tation per word.
Source embeddings $$$$$ Moreover, using all contexts of a homonymous or polysemous word to build a single prototype could hurt the representation, which cannot represent any one of the meanings well as it is influenced by all meanings of the word.
Source embeddings $$$$$ Most of these models used relative local contexts of between 2 to 10 words.
Source embeddings $$$$$ Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.

 $$$$$ This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.
 $$$$$ ... sum score river water shore global semantic vector ? play weighted average Figure 1: An overview of our neural language model.
 $$$$$ The nearest neighbors of ?market?

Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ Note that our model achieves higher corre lation (64.2) than either using local context alone (C&W: 55.3) or using global context alone (Our Model-g: 22.8).
Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ i=1 k?
Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ We show that our multi-prototype model improves upon thesingle-prototype version and outperforms other neu ral language models and baselines on this dataset.
Huang et al (2012) compare, in passing, one count model and several predict DSMs on the standard WordSim353 benchmark (Table 3 of their paper). $$$$$ We compute scores g(s, d) and g(sw, d) where sw is swith the last word replaced by wordw, and g(?, ?) is the scoring function that represents the neural networks used.

The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ We introduced a new dataset with human judgments on similarity between pairs of words in context, so as to evaluate model?s abilities to capture homonymy and polysemy of words in context.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Since word interpretation in context is important especially for homonymous and polysemous words, we introduce a new dataset with human judgments on similarity between pairs of words in sentential context.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ To find the 2We used the MaxEnt Treebank POS tagger in the python nltk library.
