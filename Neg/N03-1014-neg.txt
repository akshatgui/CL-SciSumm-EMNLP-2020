we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ Most previous work on statistical parsing has used a history-based probability model with a hand-crafted set of features to represent the derivation history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000).
we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins’ previous work on re-ranking using a finite set of features (Collins, 2000).
we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank.
we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). $$$$$ We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.

Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). $$$$$ Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). $$$$$ A log-linear model (also known as a maximum entropy model, and as the normalized exponential output function) is used to estimate the probability distribution over the four types of decisions, shifting, projecting, attaching, and modifying.
Of the previous work on using neural net works for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson,2003). $$$$$ We found that a branching factor of 10 was large enough that it had virtually no effect on the validation accuracy.

This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ Training a Simple Synchrony Network (SSN) is similar to training a log-linear model.
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ Ratnaparkhi (1999) defines a very general set of features for the histories of a shift-reduce parsing model, but the results are not as good as models which use a more linguistically informed set of features for a top-down parsing model (Collins, 1999; Charniak, 2000).
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ This machine learning method is specifically designed for processing unbounded structures.
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003). $$$$$ When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank.

The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ The most important pruning occurs after each word has been shifted onto the stack.
The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ We present a neural network method for inducing representations of parse histories and using these history representations to estimate the probabilities needed by a statistical left-corner parser.
The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ We then tested the best models for each vocabulary size on the testing set.4 Standard measures of performance are shown in table 1.5 The top panel of table 1 lists the results for the nonlexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (Costa et al., 2001), an earlier statistical left-corner parser (Manning and Carpenter, 1997), and a PCFG (Charniak, 1997).
The input features for these log linear models are the real-valued vectors computed by h (d1, ... ,di), as explained in more detail in (Henderson, 2003). $$$$$ This input imposes an appropriate bias because the induced history features which are relevant to previous derivation decisions involving top are likely to be relevant to the decision at step as well.

In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins’ previous work on re-ranking using a finite set of features (Collins, 2000).
In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ A separate log-linear model is used to estimate the probability distribution over node labels given that projecting ✒project project , which is multiplied is chosen by the probability estimate for projecting to the probability ✒get estimates for that set of decisions project .
In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ The probabilities are then assumed to be independent of all the information about the history which is not captured by the chosen features.
In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. $$$$$ Loglinear models, as with most probability estimation methods, require that there be a finite set of features on which the probability is conditioned.

Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ This sequence is the parse for the phrase structure tree.
Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ Ratnaparkhi (1999) defines a very general set of features for the histories of a shift-reduce parsing model, but the results are not as good as models which use a more linguistically informed set of features for a top-down parsing model (Collins, 1999; Charniak, 2000).
Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ But it also allows us to make use of our a priori knowledge by imposing structurally specified and linguistically appropriate biases on the search for a good history representation.
Instead we use a pruning strategy similar to that described in (Henderson, 2003), where it was applied to a considerably harder search problem: constituent parsing with a left-corner parsing order. $$$$$ The neural network estimates the parameters in two stages, first computing a representation of the derivation history and then computing a probability distribution over the possible decisions given that history.

We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. $$$$$ However, some of this information is more likely to be included than other of this information, which is the source of the model’s soft biases.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. $$$$$ When we introduce independence assumptions by cutting off access to information from more distant parts of the structure, performance degrades dramatically.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output rich information comprising both a parse tree and semantic role labels robustly, that is without any significant degradation of the parser's accuracy on the original parsing task. $$$$$ The mapping from the derivation history to the history representation is computed with the recursive application of a function .

The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ The probabilities of parser actions are conditioned on this induced history representation, rather than being conditioned on a set of hand-crafted history features chosen a priori.
The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ The standard way to handle this problem is to hand-craft a finite set of features which provides a sufficient summary of the history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000).
The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ The left-corner ordering for derivations allows very severe pruning without significant loss in accuracy, which is crucial to the success of our parser due to the relatively high computational cost of computing probability estimates with a neural network rather than with the simpler methods typically employed in NLP.
The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. $$$$$ Most previous work on statistical parsing has used a history-based probability model with a hand-crafted set of features to represent the derivation history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000).

Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ Only a fixed beam of the best 100 derivations are allowed to continue to the next word.
Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy.
Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ To sion on the testing set. test the effects of varying vocabulary sizes on performance and tractability, we trained three different models.
Henderson's parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours. $$$$$ The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance.

Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ A beam search is used to search for the most probable parse given the neural network’s probability estimates.
Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ The degradation appears to be caused by increased problems with overtraining, due to the large number of additional weights.
Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ Interestingly, a smaller, although still substantial, degradation occurs when the previous history representation for the same node is replaced with its node label.
Briefly, our method consists in augmenting a state-of-the-art statistical parser (Henderson, 2003), whose architecture and properties make it particularly adaptive to new tasks. $$$$$ In the work presented here, we automatically induce a finite set of real valued features to represent the parse history.

Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ We can then define a probabilistic model of phrase structure trees by defining a probabilistic model of each parser action in its parse context, and apply machine learning techniques to learn this model of parser actions.
Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ The parsing strategy starts with a stack that contains a node labeled ROOT (step 0) and must end in the same configuration (step 9).
Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ We believe that their poor performance is due to a network design which does not take into consideration the recency bias discussed in section 4.
Our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art result sin function labelling, by suitably extending a Simple Synchrony Network (SSN) parser (Henderson, 2003) into a single integrated system. $$$$$ We used the Penn Treebank (Marcus et al., 1993) to perform empirical experiments on this parsing model.

We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. $$$$$ A log-linear model (also known as a maximum entropy model, and as the normalized exponential output function) is used to estimate the probability distribution over the four types of decisions, shifting, projecting, attaching, and modifying.
We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. $$$$$ In the second two models this replacement simply removes the bias towards paying attention to more structurally local information, without imposing any independence assumptions.
We use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which crucially do not make any explicit independence assumptions, and learn to smooth across rare feature combinations. $$$$$ Inputting the last decision is sufficient to provide the SSN with a complete specification of the derivation history.

SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ In the work presented here, we automatically induce a finite set of real valued features to represent the parse history.
SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.
SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ Then the subtrees for the node’s remaining children are parsed in their left-to-right order.
SSN parsers, on the other hand, do not state any explicit independence assumptions: they induce a finite history representation of an unbounded sequence of moves, so that the representation of a move i − 1 is included in the inputs to the represention of the next move i, as explained in more detail in (Henderson, 2003). $$$$$ In addition, because the inputs to include previous history representations, the mapping is defined recursively.

H03 indicates the model illustrated in (Henderson, 2003). $$$$$ The bottom panel of table 1 lists the results for the two lexicalized models (SSN-Freq 200 and SSN-Freq 20) and five recent statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000; Collins, 2000; Bod, 2001).
H03 indicates the model illustrated in (Henderson, 2003). $$$$$ The degradation appears to be caused by increased problems with overtraining, due to the large number of additional weights.
H03 indicates the model illustrated in (Henderson, 2003). $$$$$ The resulting statistical parser achieves performance (89.1% F-measure) on the Penn Treebank which is only 0.6% below the best current parser for this task, despite using a smaller vocabulary size and less prior linguistic knowledge.

(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ To precisely specify this ordering, it is sufficient to characterize the state of the parser as a stack of nodes which are in the process of being parsed, as illustrated on the right in figure 1.
(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ The parsing strategy starts with a stack that contains a node labeled ROOT (step 0) and must end in the same configuration (step 9).
(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ This resulted in a vocabulary size of 512 tag-word pairs.
(Henderson, 2003) tested the effect of larger input vocabulary on SSN performance by changing the frequency cut-off that selects the input tag-word pairs. $$$$$ ASSN divides the processing of a structure into a set of sub-processes, with one sub-process for each node of the structure.

In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ Crucial to this success is the use of structurally determined soft biases in inducing the representation of the parse history, and no use of hard independence assumptions.
In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ A log-linear model (also known as a maximum entropy model, and as the normalized exponential output function) is used to estimate the probability distribution over the four types of decisions, shifting, projecting, attaching, and modifying.
In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ Previous work on applying SSNs to natural language parsing (Henderson, 2000) has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work.
In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. $$$$$ The unbounded nature of phrase structure trees does not pose a problem for this approach, because increasing the number of nodes only increases the number of times the SSN network needs to perform a computation, and not the number of parameters in the computation which need to be trained.

Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ Previous work on applying SSNs to natural language parsing (Henderson, 2000) has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work.
Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ This means that only estimates for the tag-word pairs with the correct tag need to be computed.
Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ In each modified model there is a reduction in performance, as compared to the case where all these history representations are used (SSN-Freq 200).
Unlike (Titov and Henderson, 2007b), in the shared task we used only the simplest feed-forward approximation, which replicates the computation of a neural network of the type proposed in (Henderson, 2003). $$$$$ The unbounded nature of phrase structure trees does not pose a problem for this approach, because increasing the number of nodes only increases the number of times the SSN network needs to perform a computation, and not the number of parameters in the computation which need to be trained.

We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ Previous work on applying SSNs to natural language parsing (Henderson, 2000) has not been general enough to be applied to the Penn Treebank, so it is not possible to compare results directly to this work.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ A neural network is trained simultaneously to estimate the probabilities of parser actions and to induce a finite representation of the unbounded parse history.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ When a relatively small vocabulary of words is used, performance is only marginally below the best current parser accuracy.
We present work to test the hypothesis that a current statistical parser (Henderson, 2003) can output richer information robustly, that is without any significant degradation of the parser's accuracy on the original parsing task, by explicitly modelling semantic role labels as the interface between syntax and semantics. $$$$$ We believe that their poor performance is due to a network design which does not take into consideration the recency bias discussed in section 4.

To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ Because the function is nonlinear, the induction of this history representation allows the training process to explore a much more general set of estimators than would be possible with a log-linear model alone (i.e.
To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ The probabilities of parser actions are conditioned on this induced history representation, rather than being conditioned on a set of hand-crafted history features chosen a priori.
To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs (Hornik et al., 1989), whereas a log-linear model alone can only estimate probabilities where the category-conditioned probability distributions of the pre-defined inputs are in a restricted form of the exponential family (Bishop, 1995).
To achieve the complex task of assigning semantic role labels while parsing, we use a family of statistical parsers, the Simple Synchrony Network (SSN) parsers (Henderson, 2003), which do not make any explicit independence assumptions, an dare therefore likely to adapt without much modification to the current problem. $$$$$ The lack of a large improvement suggests that the SSN-Freq 200 model already learns the significance of lexical heads, but perhaps a different method for incorporating the bias towards con7If a node’s head child is a word, then that word is the node’s lexical head.

(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. $$$$$ When we introduce independence assumptions by cutting off access to information from more distant parts of the structure, performance degrades dramatically.
(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. $$$$$ When trained on just part-of-speech tags, the resulting parser achieves the best current performance of a non-lexicalized parser on the Penn Treebank.
(Henderson, 2003) exploits this bias by directly inputting information which is considered relevant at a given step to the history representation of the constituent on the top of the stack before that step. $$$$$ As will be discussed in the next section, maps previous history representations plus pre-defined features of the derivation history to a real-valued vector .
