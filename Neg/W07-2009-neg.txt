Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). $$$$$ In this paper we describe the English Lexical Substitution task for SemEval.
Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). $$$$$ We tookthe word with the largest similarity (or smallest dis tance for ?SD and l1) for best and the top 10 for oot.
Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). $$$$$ We then mixed the substitutes from the human annotators with those of the systems.
Our systems consistently perform better when a mode exists, which makes sense because those are instances in which the annotators are in agreement (McCarthy and Navigli, 2007). $$$$$ 49 then Hi would be glad glad glad merry merry cheerful jovial.

Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. $$$$$ Pairwise inter-annotator agreement was 27.75%.
Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. $$$$$ Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.
Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. $$$$$ It is possible for those interested in disam biguation to focus on this, rather than the choice of substitutes, by using the union of responses from the annotators in future experiments.
Our evaluation framework is inspired by the lexical substitution task (McCarthy and Navigli, 2007), where a system attempts to generate a word (or a set of words) to replace a target word, such that the meaning of the sentence is preserved. $$$$$ Whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation, the jury is still out on this point.

(OOT) evaluation metrics defined by McCarthy and Navigli (2007). $$$$$ UNT is best at find ing the mode, particularly on oot, though it is the most complicated system exploiting a great many knowledge sources and components.
(OOT) evaluation metrics defined by McCarthy and Navigli (2007). $$$$$ Finding alternative words that canoccur in given contexts would potentially be useful to many applications such as question answer ing, summarisation, paraphrase acquisition (Daganet al, 2006), text simplification and lexical acquisi tion (McCarthy, 2002).
(OOT) evaluation metrics defined by McCarthy and Navigli (2007). $$$$$ Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.
(OOT) evaluation metrics defined by McCarthy and Navigli (2007). $$$$$ Finding alternative words that canoccur in given contexts would potentially be useful to many applications such as question answer ing, summarisation, paraphrase acquisition (Daganet al, 2006), text simplification and lexical acquisi tion (McCarthy, 2002).

The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. $$$$$ 5 We retain the same ordering for the further analysis tables when we look at subsets of the data.
The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. $$$$$ One problem is that WSD systems have been tested on fine-grained inventories, rendering the task harder than it need be for many applications (Ide and Wilks, 2006).
The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. $$$$$ Indeed, since the systems in SemEval did not know the candidate substitutes for a word before hand, the lexical resource is evaluatedas much as the context based disambiguation com ponent.
The first one, the LEXSUB-PARA dataset, is a small subset of the Lexical Substitution corpus (McCarthy and Navigli, 2007) that was specifically created for this task. $$$$$ Pairwise inter-annotator agreement was 27.75%.

While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. $$$$$ We also acknowledge support to the second author from INTEROP NoE (508011, 6th EU FP).
While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. $$$$$ Multiword detection pairwise agreement was 92.30% and agreement on the identification of the exact form of the actual multiword was 44.13%.
While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. $$$$$ Choosing a lexical substitute for a given word is not clear cut and there is inherently variation in thetask.
While Dinu and Lapata (Dinu and Lapata, 2010b) did show improvement over context insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions. $$$$$ Both annotators and sys tems select one or more substitutes for the target word in the context of a sentence.

Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. $$$$$ In this paper we describe the English Lexical Substitution task for SemEval.
Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. $$$$$ The subjects are also asked to identify 48 if they feel the target word is an integral part of a phrase, and what that phrase was.
Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. $$$$$ SWAG1, SWAG2, USYD, UNT) to obtain counts fordisambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) infor mation from the BNC.
Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models. $$$$$ In this paper we describe the English Lexical Substitution task for SemEval.

Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. $$$$$ We thank Serge Sharoff for the use of his Internet corpus, Julie Weeds for the software we used for producing the distributional similarity baselines and Suzanne Stevenson for suggesting the oot task .
Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. $$$$$ Multiword detection pairwise agreement was 92.30% and agreement on the identification of the exact form of the actual multiword was 44.13%.
Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. $$$$$ In this paper we describe the English Lexical Substitution task for SemEval.
Recently, McCarthy and Navigli (2007) proposed the English Lexical Substitution task (hereafter referred to as LEXSUB) under the auspices of SemEval-2007. $$$$$ 2.1 Data Selection.

For more information on LEXSUB, see McCarthy and Navigli (2007). $$$$$ IRST2 does well at finding the mode in best.
For more information on LEXSUB, see McCarthy and Navigli (2007). $$$$$ There is no ordering of the guesses and the Mode scores give credit where the mode was found in one of the system?s 10 guesses.

Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. $$$$$ Annotators can provide up to three substitutes but all should be equally as good.
Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. $$$$$ UNT is best at find ing the mode, particularly on oot, though it is the most complicated system exploiting a great many knowledge sources and components.
Within the scope of our paper, rule application is handled similarly to Lexical Substitution (McCarthy and Navigli, 2007), considering the contextual relationship between the text and the rule. $$$$$ Fellowship to the first author.

SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). $$$$$ Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for ? for this measure.
SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). $$$$$ 5.1 Post Hoc Analysis.
SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). $$$$$ The lexical substitution task follows on fromsome previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by re searchers on a task which has potential for NLP applications.
SWAT-E is the best system for out often, as several of the items that were emphasized through duplication were also correct. The results are much higher than for LEXSUB (McCarthy and Navigli, 2007). $$$$$ Indeed, since the systems in SemEval did not know the candidate substitutes for a word before hand, the lexical resource is evaluatedas much as the context based disambiguation com ponent.

The past work which is most similar to ours is derived from the lexical substitution track of SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ In the task, annotators and systems find an alternative substitute word or phrase for a target word in context.
The past work which is most similar to ours is derived from the lexical substitution track of SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ 9 teams registered and 8 participated, and two ofthese teams (SWAG and IRST) each entered two sys tems, we distinguish the first and second systems with a 1 and 2 suffix respectively.The systems all used 1 or more predefined inven tories.
The past work which is most similar to ours is derived from the lexical substitution track of SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ We think this task is an interesting one in which to evaluate automatic approaches of capturing lexical meaning.

In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ It is possible for those interested in disam biguation to focus on this, rather than the choice of substitutes, by using the union of responses from the annotators in future experiments.
In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ Since a pre-defined inventory is not used, the task allows usto compare lexical resources as well as disambiguation techniques without a bias to any predefined inventory.
In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for ? for this measure.
In particular, this technique ranks a given list of synonyms according to a similarity metric based on the occurrences in the Web 1T 5-gram corpus, which specify n-grams frequencies in a large Web sample. This technique achieved the state-of-the-art performance on the English Lexical Substitution task at SemEval 2007 (McCarthy and Navigli, 2007). $$$$$ P = ? ai:i?A ? res?ai freqres |Hi| |A| (5) R = ? ai:i?T ? res?ai freqres |Hi| |T | (6) Mode P = ? ai:i?AM 1 if any guess ? ai = mi |AM | (7) Mode R = ? ai:i?TM 1 if any guess ? ai = mi |TM | (8) mw measures For this measure, a system must identify items where the target is part of a multiword and what the multiword is. The annotators do not all have linguistics background, they are simply asked if the target is an integral part of a phrase, and if so what the phrase is. Sometimes this option is usedby the subjects for paraphrasing a phrase of the sentence, but typically it is used when there is a mul tiword.

To evaluate the system's ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). $$$$$ Whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation, the jury is still out on this point.
To evaluate the system's ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). $$$$$ 9 teams registered and 8 participated, and two ofthese teams (SWAG and IRST) each entered two sys tems, we distinguish the first and second systems with a 1 and 2 suffix respectively.The systems all used 1 or more predefined inven tories.
To evaluate the system's ability to come up with suitable substitutes from scratch, we use the measures designed to evaluate systems that took part in the original English lexical substitution task (McCarthy and Navigli, 2007). $$$$$ We released 300 for the trial data and kept the remaining 1710 for the test release.

We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&P's state of the art model. $$$$$ 9 teams registered and 8 participated, and two ofthese teams (SWAG and IRST) each entered two sys tems, we distinguish the first and second systems with a 1 and 2 suffix respectively.The systems all used 1 or more predefined inven tories.
We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&P's state of the art model. $$$$$ The lexical substitution task follows on fromsome previous ideas (McCarthy, 2002) to examine the capabilities of WSD systems built by re searchers on a task which has potential for NLP applications.
We evaluate our model on a paraphrase ranking task on a subset of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) data, and compare it to a random baseline and E&P's state of the art model. $$$$$ Fellowship to the first author.

Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. $$$$$ response if the target is part of a proper name and ?NIL?
Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. $$$$$ We thank Serge Sharoff for the use of his Internet corpus, Julie Weeds for the software we used for producing the distributional similarity baselines and Suzanne Stevenson for suggesting the oot task .
Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. $$$$$ Indeed, since the systems in SemEval did not know the candidate substitutes for a word before hand, the lexical resource is evaluatedas much as the context based disambiguation com ponent.
Instead, we automatically extract confusable candidates from a parallel corpus. Synonym extraction (Wu and Zhou, 2003), lexical substitution (McCarthy and Navigli, 2007) and paraphrasing (Madnani and Dorr, 2010) are related to collocation correction in the sense that they try to find semantically equivalent words or phrases. $$$$$ Let MW be the subset of T for which there is such a multiword response from a majority of at least 2 annotators.

The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). $$$$$ 4.
The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). $$$$$ Whilstresearchers believe that it will ultimately prove useful for applications which need some degree of se mantic interpretation, the jury is still out on this point.
The results are reported in McCarthy and Navigli (2007) and in more detail in McCarthy and Navigli (in press). $$$$$ The task involves both finding the synonyms and disambiguating the context.Participating systems are free to use any lexical resource.

Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). $$$$$ For WordNet oot we found up to 10 synonyms using criteria 1 to 4 in order until 10 were found: 1.
Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). $$$$$ We thank Serge Sharoff for the use of his Internet corpus, Julie Weeds for the software we used for producing the distributional similarity baselines and Suzanne Stevenson for suggesting the oot task .
Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). $$$$$ Most used web queries (HIT, MELB, UNT) or web data (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for ? for this measure.
Our first experiment is carried out on the SemEval 2007 lexical substitution task dataset (McCarthy and Navigli, 2007). $$$$$ We thank Serge Sharoff for the use of his Internet corpus, Julie Weeds for the software we used for producing the distributional similarity baselines and Suzanne Stevenson for suggesting the oot task .

To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. $$$$$ Fellowship to the first author.
To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. $$$$$ There is a subtask which re quires identifying cases where the word isfunctioning as part of a multiword in the sen tence and detecting what that multiword is.
To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. $$$$$ We also acknowledge support to the second author from INTEROP NoE (508011, 6th EU FP).
To evaluate the performance of our model, we use various subsets of the SemEval 2007 lexical substitution task (McCarthy and Navigli, 2007) dataset. $$$$$ We thank Serge Sharoff for the use of his Internet corpus, Julie Weeds for the software we used for producing the distributional similarity baselines and Suzanne Stevenson for suggesting the oot task .

We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). $$$$$ Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.
We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). $$$$$ We thank the annotators for their hard work.
We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). $$$$$ Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.
We use it because we want to compare our model with E&P. P10 measures the percentage of gold-standard paraphrases in the top-ten list of paraphrases as ranked by the system, and can be defined as follows (McCarthy and Navigli, 2007). $$$$$ Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.

We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources. $$$$$ On the other hand, we believe the task taps into human understanding of word meaning and we hope that computers that perform well on this task will have potential in NLP applications.
We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources. $$$$$ SWAG1, SWAG2, USYD, UNT) to obtain counts fordisambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) infor mation from the BNC.
We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources. $$$$$ Unfortunately we do not have space to show the analysis for the MAN and RAND subsets here.
We explore this suggestion, implementing a lexical substitution (McCarthy and Navigli,2007) approach to dialogue generation with sentiment, using the Valentino approach and associated resources. $$$$$ Let H be the set of annotators, T be the set of test items with 2 or more responses (non NIL or NAME) and hi be the set of responses for an item i ? T for annotator h ? H . For each i ? T we calculate the mode (mi) i.e. the most frequent response provided that there is a response more frequent than the others.
