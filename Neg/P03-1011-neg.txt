Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ Careful parameterization of the probability model allows it to be estimated at no additional cost in computational complexity.
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ Thus, the alignment algorithm is constrained by the bracketing on the target side, but does not generate the entire target tree structure.
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ As one example, our tree representation is unlexicalized, but we expect conditioning the model on more lexical information to improve results, whether this is done by percolating lexical heads through the existing trees or by switching to a strict dependency representation.

Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ For scoring the viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words:1 where A is the set of word pairs aligned by the automatic system, and G the set aligned in the gold standard.
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ The probability P(Tb|Ta) of transforming the source tree Ta into target tree Tb is modeled in a sequence of steps proceeding from the root of the target tree down.
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ We evaluate our translation models both in terms agreement with human-annotated word-level alignments between the sentence pairs.
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ The vocabulary size (number of unique types) was 4700 words in English, and 3279 in Korean — before splitting multi-morphemic words, the Korean vocabulary size was 10059.

 $$$$$ The tree-to-tree model has better theoretical complexity than the tree-to-string model, being quadratic rather than quartic in sentence length, and we found this to be a significant advantage in practice.
 $$$$$ The second step is the choice of the inserted word Pt(f|NULL), which is predicted without any conditioning information.

However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ A further consequence of allowing elementary trees of size one or two is that some reorderings not allowed when reordering the children of each individual node separately are now possible.
However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ Korean is an agglutinative language, and words often contain sequences of meaning-bearing suffixes.
However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ As seen for other tasks (Carroll and Charniak, 1992; Merialdo, 1994), the likelihood criterion used in EM training may not be optimal when evaluating a system against human labeling.

Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ While we did not see a benefit in alignment error from using syntactic trees in both languages, there is a significant practical benefit in computational efficiency.
Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ For reasons of computation speed, trees with more than 5 children were excluded from the experiments described below.
Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ The maximum number of children of a node in the Korean trees was 23 (this corresponds to a comma-separated list of items).
Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ In this paper, we introduce “loosely” tree-based alignment techniques to address this problem.

Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ Another important question we plan to pursue is the degree to which these results will be borne out with larger corpora, and how the models may be refined as more training data is available.
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.

Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ The model’s efficiency, however, comes at a cost.
Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ We evaluate our translation models both in terms agreement with human-annotated word-level alignments between the sentence pairs.
Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ For both the tree-tostring and tree-to-tree models, the cloning operation improved results, indicating that adding the flexibility to handle structural divergence is important when using syntax-based models.

Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ Results in terms of alignment error rate indicate that the clone operation results in better alignments in both cases.
Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ Results in terms of alignment error rate indicate that the clone operation results in better alignments in both cases.
Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ Given that α specifies a new cloned node as a child of Ej, the choice of which node to clone is made as in the tree-to-string model: Because a node from the source tree is cloned with equal probability regardless of whether it has already been “used” or not, the probability of a clone operation can be computed under the same dynamic programming assumptions as the basic tree-to-tree model.

Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ Careful parameterization of the probability model allows it to be estimated at no additional cost in computational complexity.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ As one example, our tree representation is unlexicalized, but we expect conditioning the model on more lexical information to improve results, whether this is done by percolating lexical heads through the existing trees or by switching to a strict dependency representation.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ “Tree-to-String” is the model of Yamada and Knight (2001), and “Tree-to-String, Clone” allows the node cloning operation of Section 2.1.

Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ This gave an average of 21 tokens for the Korean sentences.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ On our Korean-English corpus, we found roughly equivalent performance for the unstructured IBM models, and the both the tree-to-string and tree-totree models when using cloning.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ The corpus contains 5083 sentences, of which we used 4982 as training data, holding out 101 sentences for evaluation.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ In part to deal with this problem, Yamada and Knight (2001) flatten the trees in a pre-processing step by collapsing nodes with the same lexical head-word.

Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ The error rates shown in Table 2 represent the minimum over training iterations; training was stopped for each model when error began to increase.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ The average English sentence length was 16.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ We begin by summarizing the model of Yamada and Knight (2001), which can be thought of as representing translation as an Alexander Calder mobile.

For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ Given that α specifies a new cloned node as a child of Ej, the choice of which node to clone is made as in the tree-to-string model: Because a node from the source tree is cloned with equal probability regardless of whether it has already been “used” or not, the probability of a clone operation can be computed under the same dynamic programming assumptions as the basic tree-to-tree model.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ Another important question we plan to pursue is the degree to which these results will be borne out with larger corpora, and how the models may be refined as more training data is available.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ Thus, while the algorithm is exponential in m, the fan-out of the grammar, it is polynomial in the size of the input string.

We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ The model can be thought of as a synchronous tree substitution grammar, with probabilities parameterized to generate the target tree conditioned on the structure of the source tree.
We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ The probability of adding a clone of original node εi as a child of node εj is calculated in two steps: first, the choice of whether to insert a clone under εj, with probability Pins(clone|εj), and the choice of which original node to copy, with probability where Pmakeclone is the probability of an original node producing a copy.
We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ This allows, for example, an English subject-verb-object (SVO) structure, which is analyzed as having a VP node spanning the verb and object, to be re-ordered as VSO in a language such as Arabic.
We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ As the second step, French words can be inserted at each node of the parse tree.

(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ Insertions are modeled in two steps, the first predicting whether an insertion to the left, an insertion to the right, or no insertion takes place with probability Pins, conditioned on the syntactic category of the node and that of its parent.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ We present analogous extensions for both tree-to-string and tree-to-tree models that allow alignments not obeying the constraints of the original syntactic tree (or tree pair), although such alignments are dispreferred because they incur a cost in probability.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ We found better agreement with the human alignments when fixing Pins(left) in the Tree-to-String model to a constant rather than letting it be determined through the EM training.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ The maximum number of children of a node in the Korean trees was 23 (this corresponds to a comma-separated list of items).

(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ The corpus contains 5083 sentences, of which we used 4982 as training data, holding out 101 sentences for evaluation.
(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ After the clone operation takes place, the transformation of source into target tree takes place using the tree decomposition and subtree alignment operations as before.
(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ Given that α specifies a new cloned node as a child of Ej, the choice of which node to clone is made as in the tree-to-string model: Because a node from the source tree is cloned with equal probability regardless of whether it has already been “used” or not, the probability of a clone operation can be computed under the same dynamic programming assumptions as the basic tree-to-tree model.

(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ Given the original tree, the re-ordering, insertion, and translation probabilities at each node are independent of the choices at any other node.
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ We first present the tree-to-string model, followed by the tree-to-tree model, before moving on to alignment results for a parallel syntactically annotated Korean-English corpus, measured in terms of alignment perplexities on held-out test data, and agreement with human-annotated word-level alignments.
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ Computing the alignment between the 2m children of the elementary tree on either side requires choosing which subset of source nodes to delete, O(22m), which subset of target nodes to insert (or clone), O(22m), and how to reorder the remaining nodes from source to target tree, O((2m)!).
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ Our loosely tree-based alignment techniques allow statistical models of machine translation to make use of syntactic information while retaining the flexibility to handle cases of non-isomorphic source and target trees.

However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ Not only are many independence assumptions made, but many alignments between source and target sentences simply cannot be represented.
However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ We present analogous extensions for both tree-to-string and tree-to-tree models that allow alignments not obeying the constraints of the original syntactic tree (or tree pair), although such alignments are dispreferred because they incur a cost in probability.

Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ While we did not see a benefit in alignment error from using syntactic trees in both languages, there is a significant practical benefit in computational efficiency.
Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ Our loosely tree-based alignment techniques allow statistical models of machine translation to make use of syntactic information while retaining the flexibility to handle cases of non-isomorphic source and target trees.
Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ The maximum number of children of a node in the Korean trees was 23 (this corresponds to a comma-separated list of items).

Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ This leads us to think it may be helpful to allow departures from Tree-to-String Tree-to-Tree the constraints of the parallel bracketing, if it can be done in without dramatically increasing computational complexity.
Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ For reference, the parameterization of all four models is summarized in Table 1.
Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ For reference, the parameterization of all four models is summarized in Table 1.

The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ The maximum number of children of a node in the Korean trees was 23 (this corresponds to a comma-separated list of items).
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ Results are shown in Table 2.
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ These independence relations are analogous to those of a stochastic context-free grammar, and allow for efficient parameter estimation by an inside-outside Expectation Maximization (EM) algorithm.
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ Given the original tree, the re-ordering, insertion, and translation probabilities at each node are independent of the choices at any other node.
