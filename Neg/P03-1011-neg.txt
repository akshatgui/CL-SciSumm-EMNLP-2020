Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ The lexical translation probabilities Pt(f|e) for each of our tree-based models are initialized from Model 1, and the node re-ordering probabilities are initialized uniformly.
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ The approach of optimizing a small number of metaparameters has been applied to machine translation by Och and Ney (2002).
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ For reasons of computation speed, trees with more than 5 children were excluded from the experiments described below.
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.

Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ This gave an average of 21 tokens for the Korean sentences.
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ Without such an assumption, the parameter estimation becomes a problem of parsing with crossing dependencies, which is exponential in the length of the input string (Barton, 1985).
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ The average English sentence length was 16.
Initially work focused on word-based alignment, but more recent research also addresses alignment at the higher levels (substrings, syntactic phrases or trees), e.g., [Gildea, 2003]. $$$$$ Korean is an agglutinative language, and words often contain sequences of meaning-bearing suffixes.

 $$$$$ Figure 1 shows the viterbi alignment produced by the “Tree-to-String, Clone” system on one sentence from our test set.
 $$$$$ While the model learned by EM tends to overestimate the total number of aligned word pairs, fixing a higher probability for insertions results in fewer total aligned pairs and therefore a better trade-off between precision and recall.
 $$$$$ This gave an average of 21 tokens for the Korean sentences.

However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ These independence relations are analogous to those of a stochastic context-free grammar, and allow for efficient parameter estimation by an inside-outside Expectation Maximization (EM) algorithm.
However, even after extending this model by allowing cloning operations on subtrees, Gildea (2003) found that parallel trees over-constrained the alignment problem, and achieved better results with a tree-to-string model than with a tree-to-tree model using two trees. $$$$$ The average Korean sentence length was 13 words.

Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ Results in terms of alignment error rate indicate that the clone operation results in better alignments in both cases.
Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ Results are shown in Table 2.
Additional linguistic knowledge sources such as dependency trees or parse trees were used in (Cherry and Lin, 2003) and (Gildea, 2003). $$$$$ Results in terms of alignment error rate indicate that the clone operation results in better alignments in both cases.

Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ We remain hopeful that two trees can provide more information than one, and feel that extensions to the “loosely” tree-based approach are likely to demonstrate this using larger corpora.
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ Only recently have hybrid approaches begun to emerge, which apply probabilistic models to a structured representation of the source text.
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ Tree-to-tree alignment techniques such as probabilistic tree substitution grammars (Hajiˇc et al., 2002) can be trained on parse trees from parallel treebanks.
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-to-string or tree-to-tree alignment models. $$$$$ On our Korean-English corpus, we found roughly equivalent performance for the unstructured IBM models, and the both the tree-to-string and tree-totree models when using cloning.

Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ ), quadratic in the size of the input sentences, but exponential in the fan-out of the grammar. is capable of generating the alignment WYXZ.
Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
Gildea (2003) trained a system on parallel constituent trees from the Korean-English Treebank, evaluating agreement with hand-annotated word alignments. $$$$$ However, the transformed tree must not only match the surface string of the target language, but also the tree structure assigned to the string by the treebank annotators.

Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ The lexical translation probabilities Pt(f|e) for each of our tree-based models are initialized from Model 1, and the node re-ordering probabilities are initialized uniformly.
Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ Syntactic trees have been annotated by hand for both the Korean and English sentences; in this paper we will be using only the Korean trees, modeling their transformation into the English text.
Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ For the purposes of our model, we represented the syntax trees using a fairly aggressive tokenization, breaking multimorphemic words into separate leaves of the tree.
Our model of alignment is that of Gildea (2003), reviewed in Section 2 and extended to dependency trees in Section 3. $$$$$ The French word is predicted conditioned only on the English word, and each English word can generate at most one French word, or can generate a NULL symbol, representing deletion.

Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ As the first step in the translation process, the children of each node in the tree can be re-ordered.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ While we did not see a benefit in alignment error from using syntactic trees in both languages, there is a significant practical benefit in computational efficiency.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ “Tree-to-String” is the model of Yamada and Knight (2001), and “Tree-to-String, Clone” allows the node cloning operation of Section 2.1.
Both our constituent and dependency models make use of the "clone" operation introduced by Gildea (2003), which allows words to be aligned even in cases of radically mismatched trees, at a cost in the probability of the alignment. $$$$$ Thus, while the algorithm is exponential in m, the fan-out of the grammar, it is polynomial in the size of the input string.

Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ We found that when training on sentences of up 60 words, the tree-to-tree alignment was 20 times faster than tree-to-string alignment.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ The average Korean sentence length was 13 words.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models. $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.

Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ 77% of the Korean trees had no more than four children at any node, 92% had no more than five children, and 96% no more than six children.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ As a minimal example, take the tree: Of the six possible re-orderings of the three terminals, the two which would involve crossing the bracketing of the original tree (XZY and YZX) are not allowed.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.

For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ The improvement was particularly significant for the tree-to-tree model, because using syntactic trees on both sides of the translation pair, while desirable as an additional source of information, severely constrains possible alignments unless the cloning operation is allowed.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ We augment a model of translation based on re-ordering nodes in syntactic trees in order to allow alignments not conforming to the original tree structure, while keeping computational complexity polynomial in the sentence length.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ On our Korean-English corpus, we found roughly equivalent performance for the unstructured IBM models, and the both the tree-to-string and tree-totree models when using cloning.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ For reasons of computation speed, trees with more than 5 children were excluded from the experiments described below.

We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ On our Korean-English corpus, we found roughly equivalent performance for the unstructured IBM models, and the both the tree-to-string and tree-totree models when using cloning.
We began with the tree-to-tree alignment model presented by Gildea (2003). $$$$$ It is important to note that Pmakeclone is not dependent on whether a clone of the node in question has already been made, and thus a node may be “reused” any number of times.

(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ Another important question we plan to pursue is the degree to which these results will be borne out with larger corpora, and how the models may be refined as more training data is available.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ For any node with m children, m! re-orderings are possible, each of which is assigned a probability Porder conditioned on the syntactic categories of the parent node and its children.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ We remain hopeful that two trees can provide more information than one, and feel that extensions to the “loosely” tree-based approach are likely to demonstrate this using larger corpora.
(Gildea, 2003) and (Galley et al, 2004) discuss different ways of generalizing the tree-level cross linguistic correspondence relation, so it is not confined to single tree nodes, thereby avoiding a continuity assumption. $$$$$ For the purposes of our model, we represented the syntax trees using a fairly aggressive tokenization, breaking multimorphemic words into separate leaves of the tree.

(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ Because we restrict our elementary trees to include at most one child of the root node on either side, choosing elementary trees for a node pair is O(m2), where m refers to the maximum number of children of a node.
(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ For example, with our simple tree if nodes A and B are considered as one elementary tree, with probability Pelem(ta|A ⇒ BZ), their collective children will be reordered with probability giving the desired word ordering XZY.
(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ For example, with our maximum of two nodes, no transformation of the tree the generative probability model should be thought of as only generating single nodes on the target side.
(Gildea, 2003) performs tree-to-tree alignment, but treats it as part of a generative statistical translation model, rather than a seperate task. $$$$$ However, real bitexts generally do not exhibit parse-tree isomorphism, whether because of systematic differences between how languages express a concept syntactically (Dorr, 1994), or simply because of relatively free translations in the training material.

(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ Our loosely tree-based alignment techniques allow statistical models of machine translation to make use of syntactic information while retaining the flexibility to handle cases of non-isomorphic source and target trees.
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ For reasons of computation speed, trees with more than 5 children were excluded from the experiments described below.
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ Korean is an agglutinative language, and words often contain sequences of meaning-bearing suffixes.
(Gildea, 2003) outlines an algorithm for use in syntax-based statistical models of MT, applying a statistical TSG with probabilities parameterized to generate the target tree conditioned on the structure of the source tree. $$$$$ For example, in the case of the input tree This operation, combined with the deletion of the original node Z, produces the alignment (XZY) that was disallowed by the original tree reordering model.

However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ The probability P(Tb|Ta) of transforming the source tree Ta into target tree Tb is modeled in a sequence of steps proceeding from the root of the target tree down.
However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ In this paper, we introduce “loosely” tree-based alignment techniques to address this problem.
However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ We found that when training on sentences of up 60 words, the tree-to-tree alignment was 20 times faster than tree-to-string alignment.
However, unlike (Gildea, 2003), we treat the problem of alignment as a seperate task rather than as part of a generative translation model. $$$$$ For reasons of computation speed, trees with more than 5 children were excluded from the experiments described below.

Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ The vocabulary size (number of unique types) was 4700 words in English, and 3279 in Korean — before splitting multi-morphemic words, the Korean vocabulary size was 10059.
Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ We first present the tree-to-string model, followed by the tree-to-tree model, before moving on to alignment results for a parallel syntactically annotated Korean-English corpus, measured in terms of alignment perplexities on held-out test data, and agreement with human-annotated word-level alignments.
Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ This is done by adding a new subtree cloning operation to either tree-to-string or tree-to-tree alignment algorithms.
Initially work focused on word-based alignment, but more and more work is also addressing alignment at the higher levels (substrings, syntactic phrases or trees), e.g., (Meyers et al, 1996), (Gildea, 2003). $$$$$ Syntactic trees have been annotated by hand for both the Korean and English sentences; in this paper we will be using only the Korean trees, modeling their transformation into the English text.

Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ Because we restrict our elementary trees to include at most one child of the root node on either side, choosing elementary trees for a node pair is O(m2), where m refers to the maximum number of children of a node.
Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ Figure 1 shows an example from our Korean-English corpus where the clone operation allows the model to handle a case of wh-movement in the English sentence that could not be realized by any reordering of subtrees of the Korean parse.
Yamada and Knight (2001) introduced tree-to-string alignment on Japanese data, and Gildea (2003) performed tree-to-tree alignment on the Korean Treebank, allowing for non-isomorphic structures; he applied this to word-to-word alignment. $$$$$ IBM Models 1, 2, and 3 refer to Brown et al. (1993).

The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ While this constraint gives us a way of using syntactic information in translation, it may in many cases be too rigid.
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ “Tree-to-String” is the model of Yamada and Knight (2001), and “Tree-to-String, Clone” allows the node cloning operation of Section 2.1.
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003). $$$$$ As with the tree-to-string cloning operation, this independence assumption is essential to keep the complexity polynomial in the size of the input sentences.
