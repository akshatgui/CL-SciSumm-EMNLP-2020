 $$$$$ Furthermore, in accordance to standard practices we only evaluate unlabeled attachment score (UAS) due to the fact that each treebank uses a different – possibly non-overlapping – label set.
 $$$$$ For all other data sets, we train a part-of-speech tagger on the training data in order to tag the development and evaluation data.
 $$$$$ More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997; K¨ubler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, * The majority of this work was performed while the author was an intern at Google, New York, NY.
 $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.

 $$$$$ When compared to the scores on the original data set (48.9%, 50.3% and 50.7%, respectively) we can see that not only is the baseline system doing much better, but that the improvements from cross-lingual clustering are much more pronounced.
 $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.
 $$$$$ The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER).
 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).

 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
 $$$$$ The simple PROJECTED CLUSTERS work well, but the X-LINGUAL CLUSTERS provide even larger improvements.
 $$$$$ Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.
 $$$$$ Provided a clustering CS, we assign the target word t ∈ VT to the cluster with which it is most often aligned: where [·� is the indicator function.

Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ First, that high-level tasks, such as syntactic parsing, can be learned reliably using coarse-grained statistics, such as part-of-speech tags, in place of fine-grained statistics such as lexical word identities.
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ This is the first study with such a broad view on this subject, in terms of language diversity.
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ For EN we use sections 02-21, 22, and 23 of the Penn WSJ Treebank (Marcus et al., 1993) for training, development and evaluation.

Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction.
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011).
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.

Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising.
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ With the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = (wi)mi=1, with wi E V U {S}, where S is a designated start-ofsegment symbol, factors as Compare this to the model of Brown et al. (1992): By incorporating cross-lingual cluster features in a m linguistic transfer system, we are for the first time L'(w; C) = P(wi|C(wi))P(C(wi)|C(wi−1)) . combining SSL and cross-lingual transfer. i=1
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ (1992) for syntactic chunking and NER.

Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ Indeed, here we see the difference in performance become clearer, with the cross-lingual cluster model reducing errors by 14% relative to the non-cross-lingual model and upwards of 22% relative for IT.
Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.
Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.

Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006; Nivre et al., 2007).
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ For example, Turian et al. (2010) assessed the effectiveness of the word embedding techniques of Collobert and Weston (2008) and Mnih and Hinton (2007) along with the word clustering technique of Brown et al.
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ All results of the previous section rely on the availability of large quantities of language specific annotations for each task.

This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ In the parsing experiments, we use the following data sets.
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003).
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ The simple PROJECTED CLUSTERS work well, but the X-LINGUAL CLUSTERS provide even larger improvements.
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ We evaluate both the PROJECTED CLUSTERS and the X-LINGUAL CLUSTERS.

 $$$$$ For the CoNLL data sets we use the part-of-speech tags provided with the data.
 $$$$$ The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER).

A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant.
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ (1992) for syntactic chunking and NER.
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ For languages in which our unlabeled data did not have at least 1 million types, we considered all types.

 $$$$$ When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
 $$$$$ However, X-LINGUAL CLUSTERS provides roughly the same performance as PROJECTED CLUSTERS suggesting that even simple methods of cross-lingual clustering are sufficient for direct transfer dependency parsing.
 $$$$$ In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003).
 $$$$$ This is because the target language annotations that we use for evaluation differ from the Stanford dependency annotation.

 $$$$$ The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.
 $$$$$ This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine.
 $$$$$ Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.

 $$$$$ The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant.
 $$$$$ We approximately optimize (3) with the alternating procedure in Algorithm 1, in which we iteratively maximize LS and LT , keeping the other factors fixed.
 $$$$$ First, we had linguists, who were also fluent speakers of German, re-annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies.
 $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.

 $$$$$ We evaluate both the PROJECTED CLUSTERS and the X-LINGUAL CLUSTERS.
 $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.
 $$$$$ FR was converted using the procedure defined in Candito et al. (2010).

 $$$$$ We measure the percentage of modifiers in subject and object dependencies that modify the correct word.
 $$$$$ Assuming that we have an algorithm for generating cross-lingual word clusters (see Section 4.2), we can augment the delexicalized parsing algorithm to use these word cluster features at training and testing time.
 $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.
 $$$$$ As for the transfer parser, when training the source NER model, we regularize the model more heavily by setting Q = 0.1.

Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations.
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.

 $$$$$ For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006; Nivre et al., 2007).
 $$$$$ We measure the percentage of modifiers in subject and object dependencies that modify the correct word.
 $$$$$ Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available.
 $$$$$ Table 5 lists the results of the transfer experiments for dependency parsing.

 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
 $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.
 $$$$$ This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced.

 $$$$$ Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap.
 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
