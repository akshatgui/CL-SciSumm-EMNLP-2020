 $$$$$ More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997; K¨ubler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, * The majority of this work was performed while the author was an intern at Google, New York, NY.
 $$$$$ Unlabeled data for training the monolingual cluster models was extracted from one year of newswire articles from multiple sources from a news aggregation website.
 $$$$$ However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008).
 $$$$$ This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011).

 $$$$$ We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al., 2008).
 $$$$$ This consists of 0.8 billion (DA) to 121.6 billion (EN) tokens per language.
 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
 $$$$$ Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.

 $$$$$ We show that by augmenting the delexicalized direct transfer system of McDonald et al. (2011) with cross-lingual cluster features, we are able to reduce its error by up to 13% relative.
 $$$$$ First, we show that these results hold true for a number of languages across families.
 $$$$$ Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance.

Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006; Nivre et al., 2007).
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ As for the transfer parser, when training the source NER model, we regularize the model more heavily by setting Q = 0.1.
Recently, Tackstrom et al (2012) tested the incorporation of cluster features from unlabeled corpora in a multilingual setting, giving an algorithm for inducing cross-lingual clusters. $$$$$ For these experiments we train the perceptron for only five epochs in order to prevent over-fitting, which is an acute problem due to the divergence between the training and testing data sets in this setting.

Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ One likely culprit is a divergence between the tokenization schemes used in the treebank and in our unlabeled data, which for Indo-European languages is closely related to the Penn Treebank tokenization.
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages.
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ Note that we did not tune hyper-parameters of the supervised learning methods and of the clustering method, such as the number of clusters (Turian et al., 2010; Faruqui and Pad´o, 2010), and that we did not apply any heuristic for data cleaning such as that used by Turian et al. (2010).
Specifically, we extend the method recently proposed by Tackstrom et al (2012), which is based on cross-lingual word cluster features. $$$$$ While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large data sets we can get reliable statistics directly on the wordto-class transitions (Uszkoreit and Brants, 2008).

Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age.
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ Appendix A contains the details of the training, testing, unlabeled and parallel/aligned data sets.
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ Both EN and ZH were converted to dependencies using v1.6.8 of the Stanford Converter (De Marneffe et al., 2006).
Specifically, we extend the direct transfer method proposed by Tackstrom et al (2012) in two ways. $$$$$ FR was converted using the procedure defined in Candito et al. (2010).

Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011).
Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages.
Recently, Tackstrom et al (2012) developed an algorithm for inducing cross-lingual word clusters and proposed to use these clusters to enrich the feature space of direct transfer systems. $$$$$ Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.

Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction.
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering.
Tackstrom et al (2012) showed that this is, at least to some degree, achievable by coupling monolingual class-based language models, via word alignments. $$$$$ Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.

This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011).
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages.
This is due to limitations in the sequence labeling software used and gives slightly lower results, across the board, than those reported by Tackstrom et al (2012). $$$$$ For EN we use sections 02-21, 22, and 23 of the Penn WSJ Treebank (Marcus et al., 1993) for training, development and evaluation.

 $$$$$ Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.
 $$$$$ Appendix A contains the details of the training, testing, unlabeled and parallel/aligned data sets.
 $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.
 $$$$$ It is perhaps not surprising that RU sees a large gain as it is a highly inflected language, making observations of lexical features far more sparse.

A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ We use the same feature models as in the monolingual case, with the exception that we use universal part-of-speech tags for all languages and we remove the capitalization feature when transferring from EN to DE.
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ Broken down per language, reductions on the test set vary from substantial for NL (30%) and EN (26%), down to more modest for DE (17%) and ES (12%).
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ Third, that these coarse-grained statistics are robustly available across languages.
A later extension of Tackstrom et al (2012) enriches this representation with cross-lingual word clusters, considerably improving the performance. $$$$$ In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: guages for dependency parsing and 4 languages for named-entity recognition (NER).

 $$$$$ The baselines are all comparable to the state-of-theart.
 $$$$$ For these experiments we train the perceptron for only five epochs in order to prevent over-fitting, which is an acute problem due to the divergence between the training and testing data sets in this setting.
 $$$$$ The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing (NLP).
 $$$$$ Provided a clustering CS, we assign the target word t ∈ VT to the cluster with which it is most often aligned: where [·� is the indicator function.

 $$$$$ The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).
 $$$$$ Using this data, NO CLUSTERS obtains 60.0% UAS, PROJECTED CLUSTERS 63.6% and X-LINGUAL CLUSTERS 64.4%.
 $$$$$ Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap.
 $$$$$ In our second set of experiments, we evaluate direct transfer of a NER system trained on EN to DE, ES and NL.

 $$$$$ We use the part-of-speech tags supplied with the data, except for ES where we instead use universal part-of-speech tags (Petrov et al., 2011).
 $$$$$ For FR we used the French Treebank (Abeill´e and Barrier, 2004) with splits defined in Candito et al. (2010).
 $$$$$ The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations.
 $$$$$ When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.

 $$$$$ The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant.
 $$$$$ This shows the utility of cross-lingual cluster features for syntactic transfer.
 $$$$$ We use the part-of-speech tags supplied with the data, except for ES where we instead use universal part-of-speech tags (Petrov et al., 2011).
 $$$$$ Word cluster features have been shown to be useful in various tasks in natural language processing, including syntactic dependency parsing (Koo et al., 2008; Haffari et al., 2011; Tratz and Hovy, 2011), syntactic chunking (Turian et al., 2010), and NER (Freitag, 2004; Miller et al., 2004; Turian et al., 2010; Faruqui and Pad´o, 2010).

 $$$$$ While previous work has focused primarily on English, we extend these results to other languages along two dimensions.
 $$$$$ This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced.
 $$$$$ We exclude KO and ZH as initial experiments proved direct transfer a poor technique when transferring parsers between such diverse languages.

Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues.
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%.
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ For the NER experiments we use the training, development and evaluation data sets from the CoNLL 2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for all four languages (DE, EN, ES and NL).
Unfortunately, the models presented in the previous work, such as Zeman and Resnik (2008), McDonald et al (2011) and Tackstrom et al (2012), were not made available, so we reproduced the direct transfer algorithm of McDonald et al (2011), using Malt parser (Nivre, 2008) and the same set of features. $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.

 $$$$$ We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper.
 $$$$$ Denote these likelihood functions respectively by LS(wS; CS) and LT (wT; CT), where we have overloaded notation so that the word sequences denoted by wS and wT include much more plentiful non-aligned data when taken as an argument of the monolingual likelihood functions.
 $$$$$ This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure.

 $$$$$ Recently, Dhillon et al. (2011) proposed a word embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER.
 $$$$$ (1992) for syntactic chunking and NER.
 $$$$$ It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure.
 $$$$$ The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER).

 $$$$$ First, we had linguists, who were also fluent speakers of German, re-annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies.
 $$$$$ The feature model used for the NER tagger is shown in Table 2.
 $$$$$ This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models.
 $$$$$ (1992) for syntactic chunking and NER.
