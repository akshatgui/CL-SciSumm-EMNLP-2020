MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ For each hypothesis output Xi, we construct k bootstrap replicates by drawing ` segments uniformly, with replacement, from Xi, together with its corresponding reference.
MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
MERT was done only for the baseline system; these same weights were used for all experiments to control for the effect of MERT instability. In the future, we plan to experiment with approach specific optimization and to use recent published suggestions on controlling for optimizer instability (Clark et al, 2011). $$$$$ In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets.

Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ We thank Michael Denkowski, Kevin Gimpel, Kenneth Heafield, Michael Heilman, and Brendan O’Connor for insightful feedback.
Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.
Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003).
Following Clark et al (2011), we report average scores over five random tuning replications to account for optimizer instability. $$$$$ In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets.

We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ In this paper, we present a series of experiments demonstrating that optimizer instability can account for substantial amount of variation in translation quality,2 which, if not controlled for, could lead to incorrect conclusions.
We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on hypothesis testing.
We measure statistical significance using MultEval (Clark et al, 2011), which implements a stratified approximate randomization test to account for multiple tuning replications. $$$$$ Since this is not always practical (more test data may not be availabile), the widely-used bootstrap resampling method (§3) also controls for test set effects by resampling multiple “virtual” test sets from a single set, making it possible to infer distributional parameters such as the standard deviation of the translation metric over (very similar) test sets.6 Furthermore, this can be done for each of our optimizer samples.

Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). $$$$$ In this paper, we consider how to make such experiments more statistically reliable.
Tuning of models used minimum error rate training (Och, 2003), repeated 3 times and averaged (Clark et al., 2011). $$$$$ The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003).

In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ Using the n optimizer samples, with mi as the translation quality measurement of 5METEOR version 1.2 with English ranking parameters and all modules. the development set for the ith optimization run, and m is the average of all mis, we report the standard deviation over the tuning set as sdev: (mi − m)2 n − 1 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance.
In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.
In order to alleviate the impact of MERT (Och, 2003) instability, we followed the suggestion of Clark et al (2011) to run MERT three times and report average BLEU/NIST scores over the three runs for all our experiments. $$$$$ No experiment can completely control for all possible confounding variables.

Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ Nor are metric scores (even if they are statistically reliable) a substitute for thorough human analysis.
Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ In this paper, we present a series of experiments demonstrating that optimizer instability can account for substantial amount of variation in translation quality,2 which, if not controlled for, could lead to incorrect conclusions.
Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ No experiment can completely control for all possible confounding variables.
Following the recommendation of Clark et al (2011), we ran the optimization three times and repeated evaluation with each set of feature weights. $$$$$ This is a nonparametric test that approximates a paired permutation test by sampling permutations (Noreen, 1989).

Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ However, it is not generally feasible to perform as many replications as we did, so here we turn to the question of how to compare two systems, accounting for optimizer noise, but without running 300 replications.
Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric.
Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.
Since both MERT and PRO tuning toolkits involve randomness in their implementations, all BLEU scores reported in the experiments are the average of five tuning runs, as suggested by Clark et al (2011) for fairer comparisons. $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.

Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.
Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ In this paper, we consider how to make such experiments more statistically reliable.
Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ We now turn to the question of the cdec decoder (Koehn et al., 2007; Chiang, 2007; how to reduce the probability falling into this trap.
Aware of the low stability of MERT (Clark et al, 2011), we run MERT three times and report the average BLEU score including the standard deviation. $$$$$ In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.

We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.
We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets.
We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.
We tuned with minimum error rate training (Och, 2003) using Z-MERT (Zaidan, 2009) and present the mean BLEU score on test data over three separate runs (Clark et al, 2011). $$$$$ To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data.

We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.
We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003).
We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ We thank Michael Denkowski, Kevin Gimpel, Kenneth Heafield, Michael Heilman, and Brendan O’Connor for insightful feedback.
We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al, 2011). $$$$$ We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.

The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). $$$$$ AR estimates the probability (p-value) that a measured difference in metric scores arose by chance by randomly exchanging sentences between the two systems.
The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). $$$$$ We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately.
The BLEU scores reported in this paper are the average of 5 independent runs of independent batch-MIRA weight training, as suggested by (Clark et al, 2011). $$$$$ The cdec ing (Cahill et al., 2008).

In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ The p-values reported are the pvalues at the edges of the 95% confidence interval (CI) according to AR seen in the 250 simulated comparison scenarios.
In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on hypothesis testing.
In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.
In order to make the results more reliable, it is necessary to repeat the experiment several times (Clark et al, 2011). $$$$$ We therefore suggest: set evaluation be performed at least three times; more replications may be necessary for experimental manipulations with more subtle effects; • Use of the median system according to a trusted metric when manually analyzing system output; preferably, the median should be determined based on one test set and a second test set should be manually analyzed.

All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003).
All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data.
All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ We thank Michael Denkowski, Kevin Gimpel, Kenneth Heafield, Michael Heilman, and Brendan O’Connor for insightful feedback.
All boldfaced results were found to be significantly better than the baseline at the 95% confidence level using method described in (Clark et al, 2011) with 3 separate MERT tuning runs for each system. $$$$$ This research was supported in part by the National Science Foundation through TeraGrid resources provided by Pittsburgh Supercomputing Center under TG-DBS110003; the National Science Foundation under IIS-0713402, IIS-0844507, IIS-0915187, and IIS0915327; the DARPA GALE program, the U. S. Army Research Laboratory, and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533.

While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ In this paper, we present a series of experiments demonstrating that optimizer instability can account for substantial amount of variation in translation quality,2 which, if not controlled for, could lead to incorrect conclusions.
While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system.
While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ However, we have also shown that it is possible to control for it with very few replications (Table 2).
While is generally not useful to test experimental manipulations based on a single tuning run (Clark et al., 2011) and with different monolingual language modelling data, we note these figures simply to situate our results within the state of the art. $$$$$ The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003).

We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data.
We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ Nor are metric scores (even if they are statistically reliable) a substitute for thorough human analysis.
We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ Foster and Kuhn (2009) mea- sampling each element of the vector from (−1, 1) sured the instability of held-out BLEU scores across and then renormalizing so it has length 1.
We collected the BLEU, TER, and Meteor scores using MultEval (Clark et al, 2011), and the ROUGE-SU4 scores using the RELEASE-1.5.5.pl script. $$$$$ We then show that it is possible to control for this variable with a high degree of confidence with only a few replications of the experiment and conclude by suggesting new best practices for significance testing for machine translation.
