Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. $$$$$ Lee et al. (2003) use a corpus of manually segmented words, which appears to be a subset of the first release of the ATB (110,000 words), and thus comparable to our training corpus.
Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. $$$$$ In our approach, we determine all morphological properties of a word at once, so we can use this information to determine tokenization.
Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. $$$$$ This is due to Arabic’s rich system of affixation and clitics and the omission of disambiguating short vowels and other orthographic diacritics in standard orthography (“undiacritized orthography”).
Both Diab et al (2004) and Habash and Rambow (2005) use support-vector machines with local features; the former for tokenization, POS tagging, and base phrase chunking; the latter for full morphological disambiguation. $$$$$ We have shown that the use of a morphological analyzer is beneficial in POS tagging, and we believe our results are the best published to date for tokenization of naturally occurring input (in undiacritized orthography) and POS tagging.

Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ We have shown that the use of a morphological analyzer is beneficial in POS tagging, and we believe our results are the best published to date for tokenization of naturally occurring input (in undiacritized orthography) and POS tagging.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ There is not a single possible or obvious tokenization scheme: a tokenization scheme is an analytical tool devised by the researcher.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ We take the comparison between our results on POS tagging and those of Diab et al. (2004) to indicate that the use of a morphological analyzer is beneficial for Arabic as well.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ Like the English Penn Treebank, the corpus is a collection of news texts.

Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al, 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ The first version is fully consistent with neither ATB1 nor ATB2.
Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al, 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ We discuss our performance on the POS feature in Section 8.
Most available Arabic NLP tools and resources model morphology using surface inflectional features and do not mark rationality; this includes the PATB (Maamouri et al, 2004), the Buckwalter morphological analyzer (BAMA) (Buckwalter, 2004) and tools using them such as the Morphological Analysis and Disambiguation for Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ Thus, the fact that the stem is composed of a root, a pattern, and an infix vocalism is not relevant except as it affects broken plurals and verb aspect. morphological tag, cannot be done successfully using methods developed for English because of data sparseness.

MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005). $$$$$ The performance of our classifiers is good on TE1 (third column), and only slightly worse on TE2 (fifth column).
MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005). $$$$$ We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).
MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA (Habash and Rambow, 2005). $$$$$ He concludes that for highly inflectional languages “the use of an independent morphological dictionary is the preferred choice [over] more annotated data”.

In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ First, the agreement is the number of classifiers agreeing with the analysis.
In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ For example, nHw ‘towards’ is tagged as AV, N, or V (in the same syntactic contexts).
In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ In training, only the correct analysis is good.
In a previous publication, we described the Morphological Analysis and Disambiguation of Arabic (MADA) system (Habash and Rambow, 2005). $$$$$ The difference in performance between TE1 and TE2 shows the difference between the ATB1 and ATB2 (different source of news, and also small differences in annotation).

The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. $$$$$ We evaluate in this section how well our morphological disambiguation curacy measures for each input word whether it gets tokenized correctly, independently of the number of resulting tokens; the token-based measures refer to the four token fields into which the ATB splits each word determines the ATB tokenization.
The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. $$$$$ Our training data consists of a set of all possible morphological analyses for each word, with the unique correct analysis marked.
The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. $$$$$ Finally, we can determine the POS tag, for any morphologically motivated POS tagset.
The algorithm we proposed in (Habash and Rambow, 2005) for choosing the best BAMA analysis simply counts the number of predicted values for the set of linguistic features in each candidate analysis. $$$$$ We intend to explore to what extent we can transfer models trained on Standard Arabic to dialectal morphological disambiguation.

To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). $$$$$ We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.
To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). $$$$$ Sometimes this can simplify the analysis: for example, a p (ta marbuta) must be word-final in Arabic orthography, and thus a word-medial p in a recreated input word reliably signals a token boundary.
To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). $$$$$ 8The ATB generates normalized forms of certain clitics and of the word stem, so that the resulting tokens are not simply the result of splitting the original words.
To create these schemes, we use MADA, an off-the-shelf resource for Arabic morphological disambiguation (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Habash and Sadat, 2006). $$$$$ We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.

We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). $$$$$ We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).
We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). $$$$$ In TR2, we have 737 words without analysis (0.61% of the entire corpus, giving us a coverage of about 99.4% on domainsimilar text for the Buckwalter lexicon).
We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). $$$$$ We discuss the training and decoding of these classifiers in Section 5.
We tokenize using the MADA morphological disambiguation system (Habash and Rambow, 2005), and TOKAN, a general Arabic tokenizer (Sadat and Habash, 2006). $$$$$ However, there is a fair amount of descriptive work on dialectal morphology, so that dialectal morphological analyzers may be easier to come by than dialect corpora.

MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). $$$$$ We map our best solutions as chosen by the Maj model in Section 6 to the English tagset, and we furthermore assume (as do Diab et al. (2004)) the gold standard tokenization.
MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). $$$$$ We now describe how we train classifiers for the morphological features in Figure 2.
MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). $$$$$ 2In this paper, we only discuss inflectional morphology.
MADA (Habash and Rambow, 2005) is used to pre-process the Arabic text for the translation model and 5-gram language model (LM). $$$$$ Since for these cases, there is no meaningful solution in the data, we have removed them from the evaluation (but not from training).

Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). $$$$$ Analyses 1 and 4 are both nouns.
Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). $$$$$ He concludes that for highly inflectional languages “the use of an independent morphological dictionary is the preferred choice [over] more annotated data”.
Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). $$$$$ We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.
Our data is gold tokenized; however, all of the features we use are predicted using MADA (Habash and Rambow, 2005) following the work of Marton et al (2010). $$$$$ In addition, a huge unannotated corpus (155 million words) is used to iteratively learn additional stems.

For predicting morphological features, we use the MADA system (Habash and Rambow, 2005). $$$$$ In addition, we use the word form.
For predicting morphological features, we use the MADA system (Habash and Rambow, 2005). $$$$$ The work reported in this paper was supported by NSF Award 0329163.
For predicting morphological features, we use the MADA system (Habash and Rambow, 2005). $$$$$ In TR2 (168,296 words), the number of NO FUNC labels has been reduced to 853 (0.5%).

We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). $$$$$ We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).
We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). $$$$$ While there have been many publications on computational morphological analysis for Arabic (see (Al-Sughaiyer and Al-Kharashi, 2004) for an excellent overview), to our knowledge only Diab et al. (2004) perform a large-scale corpus-based evaluation of their approach.
We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). $$$$$ We obtain a score for TE1 of 97.6% on all tokens.
We compare our results with the form-based features from the state-of-the-art morphological analyzer MADA (Habash and Rambow, 2005). $$$$$ In addition, we use the untokenized word form and a binary feature stating whether there is an analysis or not.

They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. $$$$$ Agreement in number occurs only when the nominal subject precedes the verb.
They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. $$$$$ We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).
They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. $$$$$ In contrast, English morphological tagsets usually have about 50 tags, which cover all morphological variation.
They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora. $$$$$ We intend to explore to what extent we can transfer models trained on Standard Arabic to dialectal morphological disambiguation.

We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. $$$$$ We obtain accuracy rates on all tasks in the high nineties.
We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. $$$$$ We intend to apply our approach to Arabic dialects, for which currently no annotated corpora exist, and for which very few written corpora of any kind exist (making the dialects bad candidates even for unsupervised learning).
We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. $$$$$ We obtain accuracy rates on all tasks in the high nineties.
We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source. $$$$$ The performance is generally below the baseline BL.

The Arabic side is segmented according to the Arabic Treebank tokenization scheme (Maamouri et al, 2004) using the MADA + TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). $$$$$ We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
The Arabic side is segmented according to the Arabic Treebank tokenization scheme (Maamouri et al, 2004) using the MADA + TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). $$$$$ Thus, the fact that the stem is composed of a root, a pattern, and an infix vocalism is not relevant except as it affects broken plurals and verb aspect. morphological tag, cannot be done successfully using methods developed for English because of data sparseness.
The Arabic side is segmented according to the Arabic Treebank tokenization scheme (Maamouri et al, 2004) using the MADA + TOKAN morphological analyzer and tokenizer (Habash and Rambow, 2005). $$$$$ The agreement, but not the weighted agreement, uses Yamcha’s Viterbi decoding. sifier agrees with the analysis, and with what confidence level.

Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). $$$$$ However, there is a fair amount of descriptive work on dialectal morphology, so that dialectal morphological analyzers may be easier to come by than dialect corpora.
Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). $$$$$ This gives us a total of 71 machine learning features per word.

The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). $$$$$ We conclude that our data representation provides an adequate basis for performing machine learning experiments.
The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). $$$$$ In eight cases, our POS tag differed from that in the ATB file; all but one case were plausible changes among Noun, Adjective, Adverb and Proper Noun resulting from missing entries in the Buckwalter’s lexicon.
The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). $$$$$ For each of the ten classifiers, Yamcha then returns a confidence value for each possible value of the classifier, and in addition it marks the value that is chosen during subsequent Viterbi decoding (which need not be the value with the highest confidence value because of the inclusion of dynamic features).
The Morphological Analysis and Disambiguation of Arabic (MADA) system is described in (Habash and Rambow, 2005). $$$$$ The work reported in this paper was supported by NSF Award 0329163.

Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. $$$$$ In this paper, we show that the use of a morphological analyzer outperforms other tagging methods for Arabic; to our knowledge, we present the best-performing wide-coverage tokenizer on naturally occurring input and the bestperforming morphological tagger for Arabic.
Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. $$$$$ We investigate different ways of making this choice in Section 6.
Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. $$$$$ We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.
Habash and Rambow (2005) use SVM-classifiers for individual morphological features and a simple combining scheme for choosing among competing analyses proposed by the dictionary. $$$$$ While the token-based evaluation is identical to that performed by Diab et al. (2004), the results are not directly comparable as they did not use actual input words, but rather recreated input words from the regenerated tokens in the ATB.

To make our results more comparable to those by Habash and Rambow (2005), we converted the test set with the POS tags from the whole word tagger to their tokenization and to a reduced tag set of 15 tags. $$$$$ Third, we use the SVM-based Yamcha (which uses Viterbi decoding) rather than an exponential model; however, we do not consider this difference crucial and do not contrast our learner with others in this paper.
To make our results more comparable to those by Habash and Rambow (2005), we converted the test set with the POS tags from the whole word tagger to their tokenization and to a reduced tag set of 15 tags. $$$$$ However, there is a fair amount of descriptive work on dialectal morphology, so that dialectal morphological analyzers may be easier to come by than dialect corpora.

Therefore, we repeated the experiments above with POS tags predicted by the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow, 2005). $$$$$ We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.
Therefore, we repeated the experiments above with POS tags predicted by the Morphological Analysis and Disambiguation for Arabic (MADA) toolkit (Habash and Rambow, 2005). $$$$$ For unseen words, the choice is made randomly.
