Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ This method is a heuristic, but previous work (e.g., Komodakis et al. (2007)) has shown that it is effective in practice; we use it in this paper.
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ This paper has considered approaches for MAP inference; for closely related methods that compute approximate marginals, see Wainwright et al. (2005b).
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ We now show that the example algorithms solve their respective LP relaxations given in the previous section.

This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ We similarly use Z to refer to the set of valid tag structures under the extended representation.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ This leaves open the question of how to recover the LP solution (i.e., the pair (x∗1, x∗2) that achieves this maximum); we discuss this point in section 6.2.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ In fact, if we drop the y(i, t) = z(i, t) constraints from the optimization problem, the problem splits into two parts, each of which can be efficiently solved using dynamic programming: solves the harder optimization problem using an existing CFG parser and trigram tagger.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ There are O(|N|3n3) such rule productions.

In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ The algorithm (omitted for brevity) is identical to the algorithm in figure 1, but with Iuni, Y, Z, Bhead, and Ocfg, and Btag replaced with Ifirst, H, D, Bdep respectively.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Subgradient algorithms perform updates that are similar to gradient descent: The previous section described how the method in figure 4 can be used to minimize the dual L(u) of the original linear program.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Given the widespread use of dynamic programming in NLP, there should be many applications for the approach.

It is a slight variation of the proof given by Rush et al (2010). $$$$$ We ran the algorithm with a limit of K = 50 iterations.
It is a slight variation of the proof given by Rush et al (2010). $$$$$ We now show that the example algorithms solve their respective LP relaxations given in the previous section.

Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ The optimal parse tree is y* = arg maxyEY y · 0 where y · 0 = Er yr0r is the inner product between y and 0.
Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ The graphs show that values of K less than 50 produce almost identical performance to K = 50, but with fewer cases giving certificates of optimality (with K = 10, the f-score of the method is 90.69%; with K = 5 it is 90.63%).

It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ We introduce Lagrange multipliers u ∈ Rq that enforce the latter set of constraints, giving the Lagrangian: and the dual problem is to find minu∈R' L(u).
It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ paper introduces decomposition a framework for deriving inference algorithms for NLP problems.

To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ First, we initialized α0 to equal 0.5, a relatively large value.
To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ The linear programs we consider take the form The matrices E ∈ Rq×� and F ∈ Rq×l specify q linear “agreement” constraints between x1 ∈ R' and x2 ∈ Rl.

The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ Given the widespread use of dynamic programming in NLP, there should be many applications for the approach.
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ In this approach, the MRF is decomposed into sub-problems corresponding to treestructured subgraphs that together cover all edges of the original graph.
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ For parsing, this theorem implies that: Similar results apply for the POS tagging case.
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ Subgradient algorithms perform updates that are similar to gradient descent: The previous section described how the method in figure 4 can be used to minimize the dual L(u) of the original linear program.

We follows the formulation by Rush et al (2010). $$$$$ The dual L(u) is convex.
We follows the formulation by Rush et al (2010). $$$$$ We use yr and y(r) interchangeably (similarly for 0r and 0(r)) to refer to the r’th component of the vector y.
We follows the formulation by Rush et al (2010). $$$$$ Figure 5 shows performance of the approach as a function of K, the maximum number of iterations of dual decomposition.
We follows the formulation by Rush et al (2010). $$$$$ We similarly use Z to refer to the set of valid tag structures under the extended representation.

An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ Then Proof: This theorem is a special case of Martin et al. (1990), theorem 2.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ 11.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP.

Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ We use the notation conv(Y) in the remainder of this paper, instead of M. For an arbitrary set Y, the marginal polytope conv(Y) can be complex to describe.6 However, Martin et al. (1990) show that for a very general class of dynamic programming problems, the corresponding marginal polytope can be expressed as where A is a p × m matrix, b is vector in Rp, and the value p is linear in the size of a hypergraph representation of the dynamic program.
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ We assume parameter vectors for the two problems, θcfg ∈ R|I'| and θtag ∈ R|I'tar|.
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ We find three main results: 1) in spite of solving an LP relaxation, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phrase-structure parser to produce the same dependencies as the firstbest output from the dependency parser (the Collins (2003) model has an F1 score of 88.1%; the baseline method has an F1 score of 89.7%; and the dual decomposition method has an F1 score of 90.7%).

Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality.
Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ Note that A and b specify a set of p linear constraints.
Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ The Lagrange multipliers u are a vector in R|Ifirst| enforcing agreement between dependency assignments.
Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ These approaches typically use general-purpose LP or ILP solvers.

AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ We use yr and y(r) interchangeably (similarly for 0r and 0(r)) to refer to the r’th component of the vector y.
AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ Instead, a standard approach is to use a subgradient method.
AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ The problem maxµEconv(Y) µ·0 is a linear programming problem.
AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ However, these methods, although polynomial time, are substantially less efficient than our algorithms, and are considerably more complex to implement.

Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ In section 6.1 we will show that the variables u(i, t) are Lagrange multipliers enforcing agreement constraints, and that the algorithm corresponds to a (sub)gradient method for optimization of a dual function.
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ Each symbol in the grammar takes the form A(h) where A ∈ N is a non-terminal, and h ∈ {1... n} is an index specifying that wh is the head of the constituent.
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ Each such rule implies a dependency (a, b) if a = c, or (a, c) if a = b.

Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ We again find that the method finds an exact solution in almost all cases, with convergence in just a few iterations of decoding.
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ We now give an explicit description of the resulting constraints for CFG parsing:7 similar constraints arise for other dynamic programming algorithms for parsing, for example the algorithms of Eisner (2000).
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ There are two possible parts of speech, A and B, and an additional non-terminal symbol X.
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ From here on we will make exclusive use of extended index sets for CFG parsing and trigram tagging.

We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ Our work is inspired by dual decomposition methods for inference in Markov random fields (MRFs) (Wainwright et al., 2005a; Komodakis et al., 2007; Globerson and Jaakkola, 2007).
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ Then we defined αk = α0 * 2−7k, where 77k is the number of times that L(u(k')) > L(u(k'−1)) for k0 < k. This learning rate drops at a rate of 1/21, where t is the number of times that the dual increases from one iteration to the next.
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005).
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ Rule productions take the form hA(a) → B(b) C(c), i, k, ji where b ∈ {i ... k}, c ∈ {(k + 1) ... j}, and a is equal to b or c, depending on whether A receives its head-word from its left or right child.

However, if it does not converge or we stop early, an approximation must be returned $$$$$ For this objective, the fractional solution has value 0, while all integral points (i.e., all points in 2) have a negative value.
However, if it does not converge or we stop early, an approximation must be returned $$$$$ In many applications of LP relaxations—including the examples discussed in this paper—the relaxation in Eq.
However, if it does not converge or we stop early, an approximation must be returned $$$$$ We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)).
However, if it does not converge or we stop early, an approximation must be returned $$$$$ Then we defined αk = α0 * 2−7k, where 77k is the number of times that L(u(k')) > L(u(k'−1)) for k0 < k. This learning rate drops at a rate of 1/21, where t is the number of times that the dual increases from one iteration to the next.

 $$$$$ First, we initialized α0 to equal 0.5, a relatively large value.
 $$$$$ These guarantees are possible because our algorithms directly solve an LP relaxation.
 $$$$$ There are two possible parts of speech, A and B, and an additional non-terminal symbol X.
 $$$$$ Given the widespread use of dynamic programming in NLP, there should be many applications for the approach.

However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing.
However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ First, define the set Q as follows: Hence Q is the set of all (y, z) pairs that agree on their part-of-speech assignments.
However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008).

The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ Our first example is weighted CFG parsing.
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ Out of 2416 test examples, the algorithm found an exact solution in 98.9% of the cases.
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ 5 specifies that exactly one rule must be used at the top of the tree.
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ As a final point, the following theorem gives an important property of marginal polytopes, which we will use at several points in this paper: Theorem 5.2 (Korte and Vygen (2008), page 66.)
