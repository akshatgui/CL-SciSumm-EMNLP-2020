Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ We assume a context-free grammar, in Chomsky normal form, with a set of non-terminals N. The grammar contains all rules of the form A —* B C and A —* w where A, B, C E N and w E V (it is simple to relax this assumption to give a more constrained grammar).
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ We find three main results: 1) in spite of solving an LP relaxation, empirically the method finds an exact solution on over 99% of the examples; 2) the method converges quickly, typically requiring fewer than 10 iterations of decoding; 3) the method gives gains over a baseline method that forces the phrase-structure parser to produce the same dependencies as the firstbest output from the dependency parser (the Collins (2003) model has an F1 score of 88.1%; the baseline method has an F1 score of 89.7%; and the dual decomposition method has an F1 score of 90.7%).
Finally, in other recent work, Rush et al (2010) describe dual decomposition approaches for other NLP problems. $$$$$ 2 is that it makes explicit the idea of maximizing over all pairs (y, z) under a set of agreement constraints y(i, t) = z(i, t)—this concept will be central to the algorithms in this paper.

This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ For convenience we define an extended index set that makes explicit use of first-order dependeny∈Y z∈Z Dual decomposition exploits this idea; it results in the algorithm given in figure 1.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ First, we define an index set for second-order unlabeled projective dependency parsing.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ The two models were again trained separately.
This method is called dual decomposition (DD) (Rush et al, 2010). $$$$$ Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem.

In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ We then give guarantees of convergence for our algorithms by showing that they are instantiations of Lagrangian relaxation, a general method for solving linear programs of a particular form.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ The integrated parsing problem is then to find where R = {(y, d) : y ∈ H, d ∈ D, y(i, j) = d(i, j) for all (i, j) ∈ Ifirst} This problem has a very similar structure to the problem of integrated parsing and tagging, and we can derive a similar dual decomposition algorithm.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ We compare the accuracy of the dual decomposition approach to two baselines: first, Model 1; and second, a naive integration method that enforces the hard constraint that Model 1 must only consider de11We use a reimplementation that is a slight modification of Collins Model 1, with very similar performance, and which uses the TAG formalism of Carreras et al. (2008). pendencies seen in the first-best output from the dependency parser.

It is a slight variation of the proof given by Rush et al (2010). $$$$$ Our work is also related to methods that incorporate combinatorial solvers within loopy belief propagation (LBP), either for MAP inference (Duchi et al., 2007) or for computing marginals (Smith and Eisner, 2008).
It is a slight variation of the proof given by Rush et al (2010). $$$$$ The algorithm is easy to implement: all that is required is a decoding algorithm for each of the two models, and simple additive updates to the Lagrange multipliers enforcing agreement between the two models.
It is a slight variation of the proof given by Rush et al (2010). $$$$$ In fact, if we drop the y(i, t) = z(i, t) constraints from the optimization problem, the problem splits into two parts, each of which can be efficiently solved using dynamic programming: solves the harder optimization problem using an existing CFG parser and trigram tagger.
It is a slight variation of the proof given by Rush et al (2010). $$$$$ We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)).

Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ Let v be the convex combination of the following two tag sequences, each with probability 0.5: w1/A w2/A w3/A and w1/A w2/B w3/B.
Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ See Koo et al. (2010) for a similar, but less aggressive step size used to solve a different task.
Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ paper introduces decomposition a framework for deriving inference algorithms for NLP problems.
Our approach is conceptually similar to that of Rush et al (2010), which combined separately trained models by enforcing agreement using global inference and solving its linear programming relaxation. $$$$$ We now turn to the problem of recovering a primal solution of the LP.

It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ The structure of this paper is as follows.
It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP.
It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ 4; see figure 3.
It is becoming popular in the NLP community and has been shown to work effectively on several NLP tasks (Rush et al 2010). $$$$$ We will see that dynamic programming algorithms such as CKY can be considered to be very efficient solvers for particular LPs.

To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ We will use similar notation for other problems.
To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ The algorithm is easy to implement: all that is required is a decoding algorithm for each of the two models, and simple additive updates to the Lagrange multipliers enforcing agreement between the two models.
To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ There are several possible extensions of the method we have described.
To find the minimum value, we can use a subgradient method (Rush et al 2010). $$$$$ The constraints in Eq.

The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ We have focused on cases where two models are being combined; the extension to more than two models is straightforward (e.g., see Komodakis et al. (2007)).
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ Other work has considered LP or integer linear programming (ILP) formulations of inference in NLP (Martins et al., 2009; Riedel and Clarke, 2006; Roth and Yih, 2005).
The answer does not always solve the original problem Eq (2), but previous works (e.g., (Rush et al 2010)) has shown that it is effective in practice. $$$$$ It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.

We follows the formulation by Rush et al (2010). $$$$$ First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al. (2008).
We follows the formulation by Rush et al (2010). $$$$$ The resulting inference algorithms provably solve an LP relaxation of the MRF inference problem, often significantly faster than commercial LP solvers (Yanover et al., 2006).
We follows the formulation by Rush et al (2010). $$$$$ We again find that the method finds an exact solution in almost all cases, with convergence in just a few iterations of decoding.

An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ Our first example is weighted CFG parsing.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ We use the same training/dev/test split as in section 7.1.
An example that oscillates can be constructed along lines similar to the one given by Rush et al (2010). $$$$$ The constraints are given in figure 2.

Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ We ran the algorithm with a limit of K = 50 iterations.
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ We have introduced dual-decomposition algorithms for inference in NLP, given formal properties of the algorithms in terms of LP relaxations, and demonstrated their effectiveness on problems that would traditionally be solved using intersections of dynamic programs (Bar-Hillel et al., 1964).
Parsing using dual-decomposition (Rush et al, 2010) seems especially promising in this area. $$$$$ However, unlike LBP, our algorithms have strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality.

Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ In our experiments we found that in the vast majority of cases, case 1 applies, after a small number of iterations; see the next section for more details.
Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ Our approach similarly makes use of combinatorial algorithms to efficiently solve subproblems of the global inference problem.
Alternatively, one can employ dual decomposition (Rush et al, 2010). $$$$$ 10.

AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ Subgradients are tangent lines that lower bound a function even at points of non-differentiability: formally, a subgradient of a convex function L : Rn → R at a point u is a vector gu such that for all v, L(v) ≥ L(u) + gu · (v − u).
AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ Hence any point in Q is also in Q0.
AD resembles the subgradient based algorithm of Rush et al (2010), but it enjoys a faster convergence rate. $$$$$ First, we define an index set for second-order unlabeled projective dependency parsing.

Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ The algorithm optimizes the combined objective by repeatedly solving the two sub-problems separately—that is, it directly Here (i, j) represents a dependency with head wi and modifier wj (i = 0 corresponds to the root symbol in the parse).
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ In fact, if we drop the y(i, t) = z(i, t) constraints from the optimization problem, the problem splits into two parts, each of which can be efficiently solved using dynamic programming: solves the harder optimization problem using an existing CFG parser and trigram tagger.
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ The approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems, together with a simple method for forcing agreement between the different oracles.
Dual decomposition is a generic method that has been proposed for handling decoding (i.e. optimization) with such models, by decoupling the problem into two alternating steps that can each be handled by dynamic programming or other polynomial-time algorithms (Rush et al 2010), an approach that has been applied to Statistical Ma chine Translation (phrase-based (Chang and Collins, 1133 2011) and hierarchical (Rush and Collins, 2011)) among others. $$$$$ Dual decomposition is a classical method for solving optimization problems that can be decomposed into efficiently solvable sub-problems.

Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ The integrated parsing and trigram tagging problem is then to solve where g : Y → Z is a function that maps a parse tree y to its set of trigrams z = g(y).
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008).
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ We assume a context-free grammar, in Chomsky normal form, with a set of non-terminals N. The grammar contains all rules of the form A —* B C and A —* w where A, B, C E N and w E V (it is simple to relax this assumption to give a more constrained grammar).
Though Mate scored higher overall, Berkeley's parser was better at recovering longer-distance relations, suggesting that a combined approach could perhaps work better still (Rush et al, 2010). $$$$$ The dual decomposition method gives a significant gain in precision and recall over the naive combination method, and boosts the performance of Model 1 to a level that is close to some of the best single-pass parsers on the Penn treebank test set.

We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ It to algorithms that are in that they existing decoding algorithms; in that they avoid exact algorithms for the full and often in that empirically they often recover the correct solution in spite of using an LP relaxation.
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ (Case 2) If case 1 does not arise, then a couple of strategies are possible.
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ 10.
We optimize the values of the u (i, t) variables using the same algorithm as Rush et al (2010) for their tagging and parsing problem (essentially a perceptron update). $$$$$ However, a description of the constraints gives valuable intuition for the structure of the marginal polytope.

However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations. $$$$$ We give experimental results on two problems: 1) the combination of two lexicalized parsing models; and 2) the combination of a lexicalized parsing model and a trigram part-of-speech tagger.
However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations. $$$$$ 10.
However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations. $$$$$ The benefit of the formulation in Eq.
However, if it does not converge or we stop early, an approximation must be returned: following Rush et al (2010) we used the highest scoring output of the parsing submodel over all iterations. $$$$$ We now describe the type of models used throughout the paper.

 $$$$$ The optimal parse tree is y* = arg maxyEY y · 0 where y · 0 = Er yr0r is the inner product between y and 0.
 $$$$$ We ran the dual decomposition algorithm with a limit of K = 50 iterations.
 $$$$$ The approach provably solves a linear programming (LP) relaxation of the global inference problem.

However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ Since µ E conv(Y), µ must be a convex combination of 1 or more members of Y; a similar property holds for v. The example is as follows.
However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ It is natural to apply Lagrangian relaxation in cases where the sub-problems maxx1∈X1 01 · x1 and maxx2∈X2 02 · x2 can be efficiently solved by combinatorial algorithms for any values of 01, 02, but where the constraints Ex1 = Fx2 “complicate” the problem.
However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ Unfortunately, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP.
However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). $$$$$ In section 6.1 we will show that the variables u(i, t) are Lagrange multipliers enforcing agreement constraints, and that the algorithm corresponds to a (sub)gradient method for optimization of a dual function.

The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ 1),11 and the 2nd order discriminative dependency parser of Koo et al. (2008).
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ Given a parameter vector 0 = {0r : r ∈ Itag}, the optimal tag sequence is arg maxzEZ z · 0.
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ We first give two examples as an illustration of the approach: 1) integrated parsing and trigram part-ofspeech (POS) tagging; and 2) combined phrasestructure and dependency parsing.
The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). $$$$$ The problem maxµEconv(Y) µ·0 is a linear programming problem.
