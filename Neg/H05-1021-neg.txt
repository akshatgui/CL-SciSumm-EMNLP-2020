A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ However, to obtain the phrase alignments, we need to construct additional FSTs not described here.
A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ u x ?1(x, u) which is the closest Aqrb 1.0 international trade tjArp EAlmyp 0.8 the foreign ministry wzArp xArjyp 0.6 arab league jAmEp dwl Erbyp 0.4 Table 3: MJ-1 parameters for A-E phrase-pairs.To validate alignment under a PPI, we mea sure performance of the TTM word alignmentson French-English (500 sent-pairs) and Chinese English (124 sent-pairs) (Table 4).
A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ The 1000-best lists are augmented with IBM Model-1 (Brown et al., 1993) scores and then rescored with a second setof MET parameters.
A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ The Alignment Template Model (Och et al, 1999) usesphrases rather than words as the basis for transla tion, and defines movement at the level of phrases.

A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ This allows us to per form ?embedded?
A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ The Translation Template Model relies on an in ventory of target language phrases and their source language translations.
A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ The ML parameter estimates for the MJ-2 modelare given in Table 2, with Cx,u(?, b) defined similarly.

There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. $$$$$ In other related work,Bangalore and Ricardi (2001) have trained WF STs for modeling reordering within translation; their WFST parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.
There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. $$$$$ Applying EM to the MJ-1 reordering model gives the following ML parameter estimates for each phrase-pair (u, x).

 $$$$$ This suggests that themodels allow more words to be aligned and thus im prove the recall; MJ-2 gives a further improvementin AR and AER relative to MJ-1.
 $$$$$ ??1(x, u) = Cx,u(0,+1) Cx,u(0,+1) + Cx,u(0, 0) .
 $$$$$ The models are carefully formulated so that they can be implemented as WFSTs, and we show how the models can be incorporated into the Translation Template Model to perform phrasealignment and translation using standard WFST operations.

Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM) $$$$$ EM-style parameter estimation, in which the parameters of the phrase reordering model are estimated using statistics gathered under the complete model that will actually be used in translation.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM) $$$$$ These lattices are then rescored under parameters obtained using MET (MET-basic), and 1000-best lists are generated.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM) $$$$$ We describe stochastic models of localphrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM) $$$$$ MJ-2 reordering restricts the maximum allowablejump to 2 phrases and also insists that the reorder ing take place within a window of 3 phrases.

Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ We also show that the procedure scales as the bitext size is increased.
Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ Cx,u(?, b) is the expected number of times the phrase-pair x, u isaligned with a jump of b phrases when the jump history is ?.
Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ Our formulation ensures that the overall modelprovides a proper parameterized probability distribution over reordered phrase sequences; we empha size that the resulting distribution is not degenerate.
Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ We also show that the procedure scales as the bitext size is increased.

One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ Unlike this approach, our model formulation does not use a tree representation and also ensures that the output sequences are validpermutations of input phrase sequences; we empha size again that the probability distribution induced over reordered phrase sequences is not degenerate.Our reordering models do resemble those of (Till mann, 2004; Tillmann and Zhang, 2005) in that we 167 treat the reordering as a sequence of jumps relativeto the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.
One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ We also show that the procedure scales as the bitext size is increased.
One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ Phrase reordering is modeled as a first order Markovprocess with a single parameter that controls the de gree of movement.
One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ If a phrase-pair (x, u) is never seen in the Viterbi alignments, we back-off to a flat parameter ?1(x, u) = 0.05.

The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ Model parameters are blockspecific and estimated over word aligned trained bi text using simple heuristics.
The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ u x ?1(x, u) which is the closest Aqrb 1.0 international trade tjArp EAlmyp 0.8 the foreign ministry wzArp xArjyp 0.6 arab league jAmEp dwl Erbyp 0.4 Table 3: MJ-1 parameters for A-E phrase-pairs.To validate alignment under a PPI, we mea sure performance of the TTM word alignmentson French-English (500 sent-pairs) and Chinese English (124 sent-pairs) (Table 4).
The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ Although thismodel of reordering is somewhat limited and can not capture all possible phrase movement, it forms a proper parameterized probability distribution over reorderings of phrase sequences.
