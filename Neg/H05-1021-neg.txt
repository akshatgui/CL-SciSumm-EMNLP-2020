A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translationmodel incorporating reordering.
A reordering model in the framework of weighted finite state transducers is described in (Kumar and Byrne, 2005). $$$$$ Finally, the MJ-2 VT model performs better than the flat MJ-2 model, but onlymarginally better than the MJ-1 VT model.

A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ T . The most-likely translation is obtained as the path with the highest probability in T . Alignment Given a sentence-pair (E,F ), a lattice of phrase alignments is obtained by the finite state composition: B = S ? W ? R ? ?
A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ This confirms the translation performance improvements found over smaller training bitexts.
A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ In other related work,Bangalore and Ricardi (2001) have trained WF STs for modeling reordering within translation; their WFST parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.
A reordering model with reordering constraints, such as ITG constraints (Wu, 1997), IBM constraints (Berger et al1996), and local constraints (Kumar and Byrne, 2005) can account for the syntactic differences. $$$$$ Future work will build on these simple structures to produce more powerful models of word and phrase movement in translation.

There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. $$$$$ Our ex periments show that the reordering modelyields substantial improvements in trans lation performance on Arabic-to-English and Chinese-to-English MT tasks.
There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. $$$$$ Unlike this approach, our model formulation does not use a tree representation and also ensures that the output sequences are validpermutations of input phrase sequences; we empha size again that the probability distribution induced over reordered phrase sequences is not degenerate.Our reordering models do resemble those of (Till mann, 2004; Tillmann and Zhang, 2005) in that we 167 treat the reordering as a sequence of jumps relativeto the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.
There are additional variants, such as the Maximum Jump d strategy (MJd), a polynomial-time strategy described by Kumar and Byrne (2005), and possibly others. $$$$$ We make the following conditional independence assumption: P (yK1 |x K 1 , u K 1 ,K, e I 1) = P (y K 1 |x K 1 , u K 1 ).

 $$$$$ Word and Phrase Reordering is a crucial component of Statistical Machine Translation (SMT) systems.However allowing reordering in translation is computationally expensive and in some cases even prov ably NP-complete (Knight, 1999).
 $$$$$ Phrase reordering is modeled as a first order Markovprocess with a single parameter that controls the de gree of movement.
 $$$$$ We now discuss prior work on word and phrase reordering in translation.
 $$$$$ They are implemented by Weighted Finite State Trans ducers.

Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. $$$$$ Cx,u(?, b) is the expected number of times the phrase-pair x, u isaligned with a jump of b phrases when the jump history is ?.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. $$$$$ They are implemented by Weighted Finite State Trans ducers.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. $$$$$ MJ-1 model parameters are estimated over all bitext on A-E and over the non-UN bitext on C-E.
Kumar and Byrne (2005) define two local reordering models for their Translation Template Model (TTM): In the first one, called MJ-1, only adjacent phrases are allowed to swap, and the movement has to be done within a window of 2. $$$$$ Model parameters are blockspecific and estimated over word aligned trained bi text using simple heuristics.

Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ Cx,u(?, b) is the expected number of times the phrase-pair x, u isaligned with a jump of b phrases when the jump history is ?.
Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ We show that with this model it is possible to perform Maximum APosteriori (MAP) decoding (with pruning) and Ex pectation Maximization (EM) style re-estimation of model parameters over large bitext collections.
Iglesias et al. (2009) show that exact FST decoding is feasible for a phrase-based system with limited reordering (the MJ1 model (Kumar and Byrne, 2005)), and de Gispert et al (2010) show that exact FST decoding is feasible for a specific class of hierarchical grammars (shallow-1 grammars). $$$$$ Other researchers (Vogel, 2003; Zens and Ney, 2003; Zens et al, 2004) have reported performance gains in translation by allowing deviations from monotone word and phrase order.

One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ We note thatour implementation allows phrase reordering beyond simply a 1-phrase window, as was done by Till mann.
One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ EM-style parameter estimation, in which the parameters of the phrase reordering model are estimated using statistics gathered under the complete model that will actually be used in translation.
One might try to address this issue by limiting a priori the amount of re-ordering, in the spirit of (Kumar and Byrne, 2005), which would allow to approximate a phrase-based model by a standard transducer, but this would introduce further issues. $$$$$ These models pro vide properly formulated, non-deficient, probability distributions over reorderedphrase sequences.

The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ T . The most-likely translation is obtained as the path with the highest probability in T . Alignment Given a sentence-pair (E,F ), a lattice of phrase alignments is obtained by the finite state composition: B = S ? W ? R ? ?
The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ The PPI for this example is given in Table 1.
The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005). $$$$$ Table 5 gives the performance of the MJ-1 andMJ-2 reordering models when translation is per formed using a 4-gram LM.
