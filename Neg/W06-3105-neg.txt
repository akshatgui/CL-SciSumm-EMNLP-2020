DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).
DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. $$$$$ The top-performing diag-and extraction heuristic (Zens et al., 2002) serves as the baseline for evaluation.1 Each approach – the generative model and heuristic baseline – produces an estimated conditional distribution of English phrases given French phrases.
DeNero et al (2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model. $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).

DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. $$$$$ While this determinism of φEM may be desirable in some circumstances, we found that the ambiguity in φH is often preferable at decoding time.
DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. $$$$$ Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score.
DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. $$$$$ A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.
DeNero et al (2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model. $$$$$ Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b).

This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).
This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ In the equation above, l is the length of the French phrase and k is a tuning parameter.
This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).
This avoids segmentation problems encountered by DeNero et al (2006). $$$$$ In principle, this change will encourage EM to explain training sentences with shorter sentences.

140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. $$$$$ In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.
140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. $$$$$ We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation.
140k sentences up to a certain length. DeNero et al (2006) have explored estimation using EM of phrase pair probabilities under a conditional translation model based on the original source-channel formulation. $$$$$ The ambiguous process of translation can be modeled either by the latent segmentation variable or the phrase translation probabilities.

These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. $$$$$ Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.
These segmentations into bilingual containers (where segmentations are taken inside the containers) are different from the monolingual segmentations used in earlier comparable conditional models (e.g., (DeNero et al, 2006)) which must generate the alignment on top of the segmentations. $$$$$ While in principle it is still intractable, in practice we can compute most sentence pairs’ contributions in under a second each.

As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ While similar to the joint model in Marcu and Wong (2002), our model takes a conditional form compatible with the statistical assumptions used by the Pharaoh decoder.
As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ A maximum phrase length of three was used for all experiments.
As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.
As it has been found out by (DeNero et al, 2006), it is not easy to come up with a simple, effective prior distribution over segmentations that allows for improved phrase pair estimates. $$$$$ This formulation not only serves to reduce very spiked probabilities in OEM, but also boosts the probability of short phrases to encourage their use.

In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ The system follows the structure proposed in the documentation for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b).
In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability.
In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ This result is surprising in light of the reverse situation for word-based statistical translation.
In contrast with the model of (DeNero et al, 2006), who define the segmentations over the source sentence f alone, our model employs bilingual containers thereby segmenting both source and target sides simultaneously. $$$$$ We define a phrase pair to be compatible with a word-alignment if no word in either phrase is aligned with a word outside the other phrase (Zens et al., 2002).

Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. $$$$$ In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.
Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. $$$$$ However, the combinatorial properties of a phrase-based generative model have unfortunate side effects.
Therefore, unlike (DeNeroet al, 2006), our model does not need to generate the word-alignments explicitly, as these are embedded in the segmentations. $$$$$ However, some spurious word alignments can disallow all segmentations for a sentence pair, rendering it unusable for training.

We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score.
We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ Hence, translating I have a spade with OH could yield an error.
We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ The training process is then constrained such that, when evaluating the above sum, only compatible aligned segmentations are considered.
We did not explore mere EM without any smoothing or ITG prior, as we expect it will directly over fit the training data as reported by (DeNero et al, 2006). $$$$$ In the equation above, l is the length of the French phrase and k is a tuning parameter.

The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.
The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.
The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002).
The most similar efforts to ours, mainly (DeNero et al, 2006), conclude that segmentation variables in the generative translation model lead to overfitting while attaining higher likelihood of the training data than the heuristic estimator. $$$$$ In the primary contribution of the paper, we present a series of experiments designed to elucidate what re-estimation learns in this context.

While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ We demonstrate that the phrase analogue of the Dice coefficient is superior to our generative model (a result also echoing previous work).
While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ To test the relative performance of OEM and OH, we evaluated each using an end-to-end translation system from English to French.
While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ In the equation above, l is the length of the French phrase and k is a tuning parameter.

thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. $$$$$ This constraint has two important effects.
thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. $$$$$ Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).
thus generates a number of potentially overlapping in (DeNero et al, 2006), the ambiguity in word alignment is less prevalent than in phrase segmentation. $$$$$ Using word alignments, we can address both problems.4 In particular, we can determine for any aligned segmentation ( 1I1, eI1, a) whether it is compatible with the word-level alignment for the sentence pair.

For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ We also show that interpolation of the two methods can result in a modest increase in BLEU score.
For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ Thus, the first iteration of EM increases the observed likelihood of the training sentences while simultaneously degrading translation performance on the test set.
For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data.
For the German English, French English and English French language tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm ,similar to the one described in (DeNero et al,2006). $$$$$ We demonstrate that the phrase analogue of the Dice coefficient is superior to our generative model (a result also echoing previous work).

This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). $$$$$ Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.
This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). $$$$$ In cases of true ambiguity in the language pair to be translated, parameter estimates that explain the ambiguity using segmentation variables can in some cases yield higher data likelihoods by determinizing phrase translation estimates.
This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al, 2006), without resort to the heuristic estimator of Koehn et al (2003). $$$$$ In particular, we can use the following smoothed update equation during the training loop, which reserves a portion of probability mass for unseen translations.

Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ Consider the french fragment carte sur la table, which could translate to map on the table or notice on the chart.
Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ The performance gap stems primarily from the addition of a hidden segmentation variable, which increases the capacity for overfitting during maximum likelihood training with EM.
Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ On the other hand, the learned table translates the apostrophe to the with probability very near 1. phe, the most common french phrase.
Phrasal SCFG models are subject to a degenerate maximum likelihood solution in which all probability mass is placed on long, or whole sentence, phrase translations (DeNero et al, 2006). $$$$$ In the equation above, l is the length of the French phrase and k is a tuning parameter.

DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.
DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ Thus, the reduced size of the usable training set accounts for some of the degraded performance of OEM relative to OH.
DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ We investigate why weights from generative models underperform heuristic estimates in phrasebased machine translation.
DeNero et al (2006) instead proposed an exponential-time dynamic program pruned using word alignments. $$$$$ In particular, while word level models benefit greatly from re-estimation, phrase-level models do not: the crucial difference is that distinct word alignments cannot all be correct, while distinct segmentations can.

The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006). $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).
The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006). $$$$$ On the other hand, using the peaked entry OEM(the|') incurs virtually no cost to the score of a translation.
The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al, 2006). $$$$$ We comment on both the beneficial instances of segment competition (idioms) as well as the harmful ones (most everything else).

 $$$$$ However, this behavior in turn leads to errors at decoding time.
 $$$$$ The first effect follows from our observation in section 3.2 that many phrase pairs are unusable due to vanishingly small probabilities.
 $$$$$ Re-estimating phrase translation probabilities using a generative model holds the promise of improving upon heuristic techniques.
 $$$$$ We comment on both the beneficial instances of segment competition (idioms) as well as the harmful ones (most everything else).

If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ A remaining challenge is to design more appropriate statistical models which tie segmentations together unless sufficient evidence of true non-compositionality is present; perhaps such models could properly combine the benefits of both current approaches.
If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ Significant approximation and pruning is required to train a generative phrase model and table – such as φEM – with hidden segmentation and alignment variables using the expectation maximization algorithm (EM).
If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).
If we only use the features as traditional SCFG systems, the bi parsing may end with a derivation consists of some giant rules or rules with rare source/target sides, which is called degenerate solution (DeNero et al, 2006). $$$$$ This formulation not only serves to reduce very spiked probabilities in OEM, but also boosts the probability of short phrases to encourage their use.

This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ Second, the time to compute the e-step is reduced.
This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.
This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ This approach allowed us to directly estimate translation probabilities even for rare phrase pairs, which were estimated heuristically in previous work.
This is illustrated in Table 2, which shows the conditional probabilities for rules, obtained by locally normalising the rule feature weights for a simple grammar extracted from the ambiguous pair of sentences presented in DeNero et al (2006). $$$$$ With k = 2.5, this smoothing approach improves BLEU by .007 using 25k training sentences, nearly equaling the heuristic (table 1).
