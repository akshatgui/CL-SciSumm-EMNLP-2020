Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. $$$$$ .
Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. $$$$$ The algorithm can identify paraphrasing relations only between words which occurred in our corpus, which of course does not cover all English tokens.
Some works, such as Barzilay and McKeown (2001), have acquired paraphrasing knowledge automatically. $$$$$ This will allow us to extract macro-syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.

Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences. $$$$$ The first judge found 439(87.8%) pairs as correct paraphrases, and the second judge — 426(85.2%).
Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences. $$$$$ Training of the contextual classifier Using this initial seed, we record contexts around positive and negative paraphrasing examples.
Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences. $$$$$ This work was partially supported by a Louis Morin scholarship and by DARPA grant N6600100-1-8919 under the TIDES program.
Barzilay and McKeown (2001) applied the distributionality hypothesis on such parallel sentences. $$$$$ This will allow us to extract macro-syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.

 $$$$$ The parameter of the contextual classifier is a context length.
 $$$$$ Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.

Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. $$$$$ We did this evaluation on 60% of the full dataset; this is the portion of the data which is publicly available.
Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. $$$$$ We are grateful to Dan Melamed for providing us with the output of his program.
Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. $$$$$ We thank Noemie Elhadad, Mike Collins, Michael Elhadad and Maria Lapata for useful discussions.
Barzilay and McKeown (2001) incorporated part-of-speech information and other morphosyntactic clues into their co-training algorithm. $$$$$ This work was partially supported by a Louis Morin scholarship and by DARPA grant N6600100-1-8919 under the TIDES program.

Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. $$$$$ This will allow us to extract macro-syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.
Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. $$$$$ We showed that a co-training algorithm based on contextual and lexico-syntactic features of paraphrases achieves high performance on our data.
Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. $$$$$ We are grateful to Dan Melamed for providing us with the output of his program.
Inspired by the use of parallel translations to mine paraphrasing lexicons (Barzilay and McKeown, 2001) and the use of MT engines forword sense disambiguation (Diab, 2000), we leverage existing machine translation systems to generate semantically equivalent, albeit lexically and syntactically distinct, questions. $$$$$ We base our method for paraphrasing extraction on the assumption that phrases in aligned sentences which appear in similar contexts are paraphrases.

Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. $$$$$ This increased precision is a clear advantage of our approach and shows that machine translation techniques cannot be used without modification for this task, particularly for producing multi-word paraphrases.
Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. $$$$$ We also record for each token its derivational root, using the CELEX(Baayen et al., 1993) database.
Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. $$$$$ Our approach yields a set of paraphrasing patterns by extrapolating the syntactic and morphological structure of extracted paraphrases.
Barzilay and McKeown (2001) and Callison Burch et al (2006) extracted paraphrases from monolingual parallel corpus where multiple translations were present for the same source. $$$$$ In addition to evaluating our system output through precision and recall, we also compared our results with two other methods.

Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates. $$$$$ So far, three major approaches of collecting paraphrases have emerged: manual collection, utilization of existing lexical resources and corpus-based extraction of similar words.
Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates. $$$$$ This work was partially supported by a Louis Morin scholarship and by DARPA grant N6600100-1-8919 under the TIDES program.
Jacquemin (1999) and Barzilay and McKeown (2001) identify phrase level paraphrases, while Lin and Pantel (2001) and Shinyama et al (2002) acquire structural paraphrases encoded as templates. $$$$$ We are grateful to Dan Melamed for providing us with the output of his program.

Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. $$$$$ Emma burst into tears and he tried to comfort her, saying things to make her smile.
Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. $$$$$ As in the case of syntactic paraphrase features, tags of identical words are marked.
Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. $$$$$ The question here is what type of WordNet relations can be considered as paraphrases.

We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al, 2007) (SMT method), and that of Murata et al (2004) (Mrt method). $$$$$ This process introduces differences in the translations which are an intrinsic part of the creative process.
We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM method), Moses7 (Koehn et al, 2007) (SMT method), and that of Murata et al (2004) (Mrt method). $$$$$ This work was partially supported by a Louis Morin scholarship and by DARPA grant N6600100-1-8919 under the TIDES program.

In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. $$$$$ We are grateful to Dan Melamed for providing us with the output of his program.
In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. $$$$$ We also record for each token its derivational root, using the CELEX(Baayen et al., 1993) database.
In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. $$$$$ Many classical texts have been translated more than once, and these translations are available on-line.
In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3. $$$$$ We thank Noemie Elhadad, Mike Collins, Michael Elhadad and Maria Lapata for useful discussions.

Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. $$$$$ The first of these was a machine translation technique for deriving bilingual lexicons (Melamed, 2001) including detection of non-compositional compounds 4.
Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. $$$$$ This will allow us to extract macro-syntactic paraphrases in addition to local paraphrases which are currently produced by the algorithm.
Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. $$$$$ We thank Noemie Elhadad, Mike Collins, Michael Elhadad and Maria Lapata for useful discussions.
Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point. $$$$$ This simple method achieves good results for our corpus, because 42% of the words in corresponding sentences are identical words on average.

A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). $$$$$ Traditionally, alternative verbalizations are derived from a manual corpus analysis, and are, therefore, application specific.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). $$$$$ We thank Noemie Elhadad, Mike Collins, Michael Elhadad and Maria Lapata for useful discussions.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). $$$$$ This extension will require using a more selective alignment technique (similar to that of (Hatzivassiloglou et al., 1999)).
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al, 2004). $$$$$ (Wechsler, 1998)) have observed that translations “are never identical”, and each translator creates his own interpretations of the text.

We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). $$$$$ For example, “tried” from the first sentence in Figure 1 does not correspond to any other word in the second sentence but “tried”.
We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). $$$$$ 120(94.5%) alignments were identified as correct alignments.
We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). $$$$$ Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
We compare the paraphrases we collect with paraphrases that are derivable from the same corpus using a co training-based paraphrase extraction algorithm (Barzilay and McKeown, 2001). $$$$$ This characteristic of our corpus is similar to problems with noisy and comparable corpora (Veronis, 2000), and it prevents us from using methods developed in the MT community based on clean parallel corpora, such as (Brown et al., 1993).

Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). $$$$$ While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases.
Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). $$$$$ Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). $$$$$ The same question arises with automatically constructed thesauri (Pereira et al., 1993; Lin, 1998).
Since the co-training-based algorithm of Barzilay and McKeown (2001) takes parallel corpus as input, we created out of the MTC corpus 55993 sentence pairs (Each equivalent translation set of cardinality 11 was mapped into (11 2) equivalent translation pairs.). $$$$$ These phrases become the atomic units of the algorithm.

Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. $$$$$ Analyzing the contexts surrounding “ ?
Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. $$$$$ This simple method achieves good results for our corpus, because 42% of the words in corresponding sentences are identical words on average.
Barzilay and McKeown (2001) used monolingual parallel corpora for identifying paraphrases. $$$$$ We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text.

We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. $$$$$ 120(94.5%) alignments were identified as correct alignments.
We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. $$$$$ This extension will require using a more selective alignment technique (similar to that of (Hatzivassiloglou et al., 1999)).
We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. $$$$$ This corpus provides many instances of paraphrasing, because translations preserve the meaning of the original source, but may use different words to convey the meaning.
We find that most paraphrases extracted using the method of Barzilay and McKeown (2001) are quite short. $$$$$ During the preprocessing stage, we perform sentence alignment.

Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al 2004) and DIRT (Lin and Pantel 2001). $$$$$ From a practical point of view, diversity in expression presents a major challenge for many NLP applications.
Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al 2004) and DIRT (Lin and Pantel 2001). $$$$$ In addition to learning lexical paraphrases, the method also learns syntactic paraphrases, by generalizing syntactic patterns of the extracted paraphrases.
Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al 2004) and DIRT (Lin and Pantel 2001). $$$$$ In generation, paraphrasing is employed to create more varied and fluent text.

To generate dialogue sentences for a corresponding discourse structure we are adapting the approach to paraphrasing of Barzilay and McKeown (2001). $$$$$ Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
To generate dialogue sentences for a corresponding discourse structure we are adapting the approach to paraphrasing of Barzilay and McKeown (2001). $$$$$ The second approach — utilization of existing lexical resources, such as WordNet — overcomes the scalability problem associated with an application specific collection of paraphrases.
To generate dialogue sentences for a corresponding discourse structure we are adapting the approach to paraphrasing of Barzilay and McKeown (2001). $$$$$ For example, consider occurrences of the word “boy” in two translations of “Madame Bovary” — E. Marx-Aveling’s translation and Etext’s translation.

 $$$$$ Alignment produces 44,562 pairs of sentences with 1,798,526 words.
 $$$$$ Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.
 $$$$$ A corpus-based approach can provide insights on this question by revealing paraphrases that people use.
 $$$$$ The strength of positive context is defined as , where is the number of times context surrounds positive examples (paraphrase pairs) and is the frequency of the context .

The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. $$$$$ Many classical texts have been translated more than once, and these translations are available on-line.
The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. $$$$$ During the preprocessing stage, we perform sentence alignment.
The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. $$$$$ Initialization Words which appear in both sentences of an aligned pair are used to create the initial “seed” rules.
The approach in (Barzilay and McKeown, 2001) does not use deep linguistic analysis and therefore is suitable to noisy corpora like ours. $$$$$ An example of parallel translations is shown in Figure 1.
