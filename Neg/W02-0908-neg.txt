Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ For each term we extracted a thesaurus entry with 200 potential synonyms and their similarity scores.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ To alleviate this problem we filter the attributes so that only strongly weighted subject, direct-obj and indirect-obj relations are included in the canonical vectors.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ The POS tagging and chunking took 159 minutes, and the relation extraction took an addiWe describe the functions evaluated in these experiments using an extension of the asterisk notation used by Lin (1998a), where an asterisk indicates a set ranging over all existing values of that variable.

All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ Curran and Moens (2002)), which would increase both number of attributes for each term and the total number of terms above the minimum cutoff, this is not nearly fast enough.
All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.
All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ This results in a significant reduction in runtime with only a marginal performance penalty in our experiments.

Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.
Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ Although the minimum cutoff helps by reducing n to a reasonably small value, it does not constrain m in any way.

We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ Much of the existing work on thesaurus extraction and word clustering is based on the observation that related terms will appear in similar contexts.
We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ Ruge, (1997)).
We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ Further, the canonical vector parameters allow for control of the speed/performance trade-off.
We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.

Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ It is also dependent on the functions used to compare the canonical attribute vectors.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ This is because in general they constrain the terms more and partake in fewer idiomatic collocations with the terms.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ Curran and Moens (2002)), which would increase both number of attributes for each term and the total number of terms above the minimum cutoff, this is not nearly fast enough.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.

Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ The weight functions LIN98A, LIN98B, and GREF94 are taken from existing systems (Lin, 1998a; Lin, 1998b; Grefenstette, 1994).
Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ This results in a significant reduction in runtime with only a marginal performance penalty in our experiments.
Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ Finally, we have generalised some set measures using similar reasoning to Grefenstette (1994).

Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ For the 70 terms we create a gold standard from the union of the synonyms from the three thesauri.
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ For instance, the frequent company appears in 11360 grammatical relations, with a total frequency of 69240 occurrences, whereas the infrequent pants appears in only 401 relations with a total frequency of 655 occurrences.
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ Once these contexts have been defined, these systems then use clustering or nearest neighbour methods to find similar terms.

Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ Figure 1 shows some example attributes for idea.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ The simplest measure functions (prefix SET) use the attribute set model from IR and are taken from Manning and Sch¨utze (1999), pp.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ In these experiments we have proposed new measure and weight functions that, as our evaluation has shown, significantly outperform existing similarity functions.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ There are a total of 23207 synonyms for the 70 terms in the gold standard.

Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ The simplest method of evaluation is direct comparison of the extracted thesaurus with a manuallycreated gold standard (Grefenstette, 1994).
Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ The first component extracts the contexts from raw text and compiles them into a statistical description of the contexts each potential thesaurus term appears in.
Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ Surprisingly, the other collocation discovery functions did not perform as well, even though TTEST is not the most favoured for collocation discovery because of its behaviour at low frequency counts.

Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.
Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.

Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ There are a total of 23207 synonyms for the 70 terms in the gold standard.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ For example, the set of attributes of the term w is: (w, , )  {(r, w)  |(w, r, w)} For convenience, we further extend the notation for weighted attribute vectors.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ Alternatively, some systems are based on the observation that related terms appear together in particular contexts.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ In these experiments we have proposed new measure and weight functions that, as our evaluation has shown, significantly outperform existing similarity functions.

Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Early experiments in thesaurus extraction (Grefenstette, 1994) suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success (Lin, 1998a).
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Thesauri have traditionally been used in information retrieval tasks to expand words in queries with synonymous terms (e.g.
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Since the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from semantic resources is regularly leveraged to solve NLP problems.

Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ However, what is needed is some algorithmic reduction that bounds the number of full O(m) vector comparisons performed.
Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ To avoid this problem, we apply a maximum cutoff on the number of terms the attribute appears with.
Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ Introducing a minimum cutoff that ignores low frequency potential synonyms can eliminate many unnecessary comparisons.

In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ There are a total of 23207 synonyms for the 70 terms in the gold standard.
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ To alleviate this problem we filter the attributes so that only strongly weighted subject, direct-obj and indirect-obj relations are included in the canonical vectors.

Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ Some functions (suffix LOG) have an extra log2(f(w, r, w) + 1) factor to promote the influence of higher frequency attributes.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ Grefenstette (1994) uses bit signatures to test for shared attributes, but because of the high frequency of the most common attributes, this does not skip many comparisons.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ Early experiments in thesaurus extraction (Grefenstette, 1994) suffered from the limited size of available corpora, but more recent experiments have used much larger corpora with greater success (Lin, 1998a).

Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ So as long as we find an approximation function and vector such that p << n, the system will run much faster and be much more scalable in m, the number of attributes.
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ This results in a significant reduction in runtime with only a marginal performance penalty in our experiments.
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ INVR is the sum of the inverse rank of each matching synonym, e.g. matching synonyms at ranks 3, 5 and 28 give an inverse rank score of
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.

A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ The use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ We would also like to expand our evaluation to include direct methods used by others (Lin, 1998a) and using the extracted thesaurus in NLP tasks.

Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ If we want to extract a complete thesaurus for 29,737 terms left after the cutoff has been applied, it would take approximately one full week of processing.
Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.

For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ The relations for each term are collected together and counted, producing a context vector of attributes and (adjective, good) 2005 (adjective, faintest) 89 (direct-obj, have) 1836 (indirect-obj, toy) 74 (adjective, preconceived) 42 (adjective, foggiest) 15 their frequencies in the corpus.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ One final difficulty this example shows is that attributes like (direct-obj, get) are not informative.

Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ However, these attributes, although strongly correlated with pants, are in fact too specific and idiomatic to be a good summary, because there are very few other words with similar canonical attributes.
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ Further, the canonical vector parameters allow for control of the speed/performance trade-off.
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ Alternative generalisations are marked with a dagger.
