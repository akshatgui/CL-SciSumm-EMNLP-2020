Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ For the 70 terms we create a gold standard from the union of the synonyms from the three thesauri.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ Alternatively, some systems are based on the observation that related terms appear together in particular contexts.
Curran and Moens (2002b) evaluate thesaurus extractors based on several different models of context on large corpora. $$$$$ Many systems extract grammatical relations using either a broad coverage parser (Lin, 1998a) or shallow statistical tools (Grefenstette, 1994; Curran and Moens, 2002).

All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ Since extracting only 70 thesaurus terms takes about 43 minutes with a minimum cutoff of 5, the efficiency/performance trade-off is particularly important from the perspective of implementing a practical extraction system.
All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ Given that the size of the training corpus could be much larger (cf.
All the systems use the JACCARD similarity metric and TTEST weighting function that were found to be most effective for thesaurus extraction by Curran and Moens (2002a). $$$$$ Since the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from semantic resources is regularly leveraged to solve NLP problems.

Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ In addition, thesaurus compilers cannot keep up with constantly evolving language use and cannot afford to build new thesauri for the many subdomains that NLP techniques are being applied to.
Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ Given that the size of the training corpus could be much larger (cf.
Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ Since the development of WordNet (Fellbaum, 1998) and large electronic thesauri, information from semantic resources is regularly leveraged to solve NLP problems.
Curran and Moens (2002b) have demonstrated that more complex and constrained contexts can yield superior performance, since the correlation between context and target term is stronger than simple window methods. $$$$$ A subscripted asterisk indicates that the variables are bound together: For weight functions we use similar notation: Table 1 defines the measure functions evaluated in these experiments.

We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ As the extracted thesauri do not distinguish between senses, we transform Roget’s and Macquarie into head ordered format by conflating the sense sets containing each term.
We worked with an implementation of the log likelihood ratio (g-Score) as proposed by Dunning (1993) and two variants of the t-score, one considering all values (t-score) and one where only positive values (t-score+) are kept following the results of Curran and Moens (2002). $$$$$ This is because Zipf’s law applies to relations, and so by small increments of the cutoff we eliminate many terms from the tail of the distribution.

Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ Grefenstette (1994) uses bit signatures to test for shared attributes, but because of the high frequency of the most common attributes, this does not skip many comparisons.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ The second system component performs nearestneighbour or cluster analysis to determine which terms are similar based on their context vectors.
Another venue of research may be to exploit different thesauri, such as the ones automatically derived as in (Curran and Moens, 2002). $$$$$ We define a context relation instance as a tuple (w, r, w') where w is the thesaurus term, which occurs in some grammatical relation r with another word w' in the sentence.

Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ This research is supported by Commonwealth and Sydney University Travelling scholarships.
Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ Our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms.
Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ The problem is that the time complexity of thesaurus extraction is not practically scalable to significantly larger corpora.
Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. $$$$$ This research is supported by Commonwealth and Sydney University Travelling scholarships.

Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ In addition, thesaurus compilers cannot keep up with constantly evolving language use and cannot afford to build new thesauri for the many subdomains that NLP techniques are being applied to.
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ Figure 5 shows system performance and speed, as canonical vector size is increased, with the maximum cutoff at 4000, 8000, and 10,000.
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ These experiments show that large-scale thesaurus extraction is practical, and although results are not yet comparable with manually-constructed thesauri, may now be accurate enough to be useful for some NLP tasks.
Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). $$$$$ The use of semantic resources is comin modern but methods to extract lexical semantics have only recently begun to perform well enough for practical use.

Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ Some functions (suffix LOG) have an extra log2(f(w, r, w) + 1) factor to promote the influence of higher frequency attributes.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ Our system keeps track of the sum of the remaining vector which is a significant optimisation, but comes at the cost of increased representation size.
Curran and Moens (2002) report on a large scale evaluation experiment, where they evaluated the performance of various commonly used methods. $$$$$ We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.

Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.
Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ This approximation algorithm is dramatically faster than simple pairwise comparison, with only a small performance penalty, which means that complete thesaurus extraction on large corpora is now feasible.
Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ However, on small corpora, rare direct matches provide limited information for evaluation, and thesaurus coverage is a problem.
Vander Plas and Bouma (2005) present a similar experiment for Dutch, in which they tested most of the best performing measures according to Curran and Moens (2002). $$$$$ X, Y and other Zs) which link synonyms and hyponyms (Hearst, 1992; Caraballo, 1999).

Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.
Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ Vector-space thesaurus extraction systems can be separated into two components.
Curran and Moens (2002) show that synonymy extraction for lexical semantic resources using distributional similarity produces continuing gains in accuracy as the volume of input data increases. $$$$$ The simplest algorithm for thesaurus extraction is nearest-neighbour comparison, which involves pairwise vector comparison of the target with every extracted term.

Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ Figure 2 presents both the performance of the system using direct match evaluation (left axis) and execution times (right axis) for increasing cutoffs.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ 1 5 + 128, and with at most 200 synonyms, the maximum INVR score is 5.878.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Curran and Moens (2002) introduces a vector of canonical attributes (of bounded length k m), selected from the full vector, to represent the term. $$$$$ This research is supported by Commonwealth and Sydney University Travelling scholarships.

Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Our proposed weight functions are motivated by our intuition that highly predictive attributes are strong collocations with their terms.
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Unfortunately, thesauri are expensive and timeconsuming to create manually, and tend to suffer from problems of bias, inconsistency, and limited coverage.
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ Table 3 lists some example terms with frequency and frequency rank data from the PTB, BNC and REUTERS, as well as the number of senses in WordNet and Macquarie, and their maximum and minimum depth in the WordNet hierarchy.
Comparisons made with these low frequency terms are unreliable (Curran and Moens, 2002). $$$$$ This research is supported by Commonwealth and Sydney University Travelling scholarships.

Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ Alternatively, some systems are based on the observation that related terms appear together in particular contexts.
Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ We can do this by introducing another, much shorter vector of canonical attributes, with a bounded length k. If our approximate comparison returns at most p positive results for each term, then the time complexity becomes O(n2k + npm), which, since k is constant, is O(n2 + npm).
Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ For the purposes of evaluation, we selected 70 single-word noun terms for thesaurus extraction.
Recently, there has been much interest in finding words which are distribution ally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). $$$$$ The list of measure and weight functions we compared against is not complete, and we hope to add other functions to provide a general framework for thesaurus extraction experimentation.

In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ Most systems extract co-occurrence and syntactic information from the words surrounding the target term, which is then converted into a vector-space representation of the contexts that each target term appears in (Pereira et al., 1993; Ruge, 1997; Lin, 1998b).
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ The simplest algorithm for thesaurus extraction is nearest-neighbour comparison, which involves pairwise vector comparison of the target with every extracted term.
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ This is because in general they constrain the terms more and partake in fewer idiomatic collocations with the terms.
In these experiments we have used a variant of Dice, proposed by Curran and Moens (2002). $$$$$ The relations for each term are collected together and counted, producing a context vector of attributes and (adjective, good) 2005 (adjective, faintest) 89 (direct-obj, have) 1836 (indirect-obj, toy) 74 (adjective, preconceived) 42 (adjective, foggiest) 15 their frequencies in the corpus.

Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ If we want to extract a complete thesaurus for 29,737 terms left after the cutoff has been applied, it would take approximately one full week of processing.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ Although the minimum cutoff helps by reducing n to a reasonably small value, it does not constrain m in any way.
Pereira et al (1993), Curran and Moens (2002) and Lin (1998) use syntactic features in the vector definition. $$$$$ Curran and Moens (2002)), which would increase both number of attributes for each term and the total number of terms above the minimum cutoff, this is not nearly fast enough.

Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ This consists of four passes over the sentence, associating each noun with the modifiers and verbs from the syntactic contexts they appear in: The relation tuple is then converted to root form using the Sussex morphological analyser (Minnen et al., 2000) and the POS tags are removed.
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ These systems extract related terms directly by recognising linguistic patterns (e.g.
Also, because it has been shown (Curran and Moens, 2002) that negative PMI values worsen the distributional similarity performance, we bound PMI so that PMI (wi, cj)= 0 if PMI (wi, cj) < 0. $$$$$ Further, the canonical vector parameters allow for control of the speed/performance trade-off.

A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ The weight functions LIN98A, LIN98B, and GREF94 are taken from existing systems (Lin, 1998a; Lin, 1998b; Grefenstette, 1994).
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ Table 5 presents the results of evaluating the weight functions.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ The first feature of note is that as we increase the minimum cutoff to 30, the direct match results improve for TTEST, which is probably a result of the TTEST weakness on low frequency counts.
A first major algorithmic approach is to represent word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Curran and Moens, 2002). $$$$$ Each measure is averaged over the extracted synonym lists for all 70 thesaurus terms.

Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ We would like to thank Stephen Clark, Caroline Sporleder, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ Roget’s and Macquarie are topic ordered and the Moby thesaurus is head ordered.
Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ For experimental analysis we have decomposed this function into measure and weight functions.
Curran and Moens (2002b) have demonstrated that dramatically increasing the quantity of text used to extract contexts significantly improves synonym quality. $$$$$ We propose an approximation based on attributes and coarseand fine-grained matching, that reduces the time complexity and execution time of thesaurus extraction with only a marginal performance penalty.

For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ We would also like to expand our evaluation to include direct methods used by others (Lin, 1998a) and using the extracted thesaurus in NLP tasks.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ As the extracted thesauri do not distinguish between senses, we transform Roget’s and Macquarie into head ordered format by conflating the sense sets containing each term.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ Surprisingly, the other collocation discovery functions did not perform as well, even though TTEST is not the most favoured for collocation discovery because of its behaviour at low frequency counts.
For these experiments we use the JACCARD (1) measure and the TTEST (2) weight, as Curran and Moens (2002a) found them to have the best performance in their comparison of many distance measures. $$$$$ Figure 2 presents both the performance of the system using direct match evaluation (left axis) and execution times (right axis) for increasing cutoffs.

Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ Precision of the top n is the percentage of matching synonyms in the top n extracted synonyms.
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ Finally unbounded weights tended to promote the rankings of synonyms from minority senses because the frequent senses are demoted by negative weights.
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ Table 3 lists some example terms with frequency and frequency rank data from the PTB, BNC and REUTERS, as well as the number of senses in WordNet and Macquarie, and their maximum and minimum depth in the WordNet hierarchy.
Curran and Moens (2002a) propose an initial heuristic comparison to reduce the number of full O(m) vector comparisons. $$$$$ We evaluate existing and new similarity metrics for thesaurus extraction, and experiment with the tradeoff between extraction performance and efficiency.
