Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ This suggests that it is unlikely that exact non-projective dependency parsing is tractable for any model richer than the edge-factored model.
Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ There is no closed form solution in the general case, and one adopts the expectation maximization (EM) method, which is a specialization of the standard fixed-point iteration method for the solution of non-linear systems.
Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ The primary problem in treating each dependency as independent is that it is not a realistic assumption.
Koo et al (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edge factored dependency trees and the edge marginals. $$$$$ In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b).

The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). $$$$$ In that work, as is common with many min-risk decoding schemes, T(Gx) is not the entire space of parse structures.
The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). $$$$$ To further justify the algorithms presented here, we outlined a few novel learning and inference settings in which they are required.
The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald and Satta, 2007). $$$$$ For example, in the work of McDonald et al. (2005b) it is simply a linear classifier that is a function of the words in the dependency, the label of the dependency, and any contextual features of the words in the sentence.

The marginal p (yi=k $$$$$ However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems.
The marginal p (yi=k $$$$$ However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems.
The marginal p (yi=k $$$$$ Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992).
The marginal p (yi=k $$$$$ An interesting line of research is to investigate classes of non-projective structures that can be parsed with chart-parsing algorithms and how these classes relate to the languages parsable by other syntactic formalisms.

Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ We denote this value as Zx, To compute this sum it is possible to use the Matrix Tree Theorem for multi-digraphs, Matrix Tree Theorem (Tutte, 1984): Let G be a multi-digraph with nodes V = 10, 1, ... , n} and edges E. Define (Laplacian) matrix Q as a (n + 1)x(n + 1) matrix indexed from 0 to n. For all i and j, define: If the ith row and column are removed from Q to produce the matrix Qi, then the sum of the weights of all directed spanning trees rooted at node i is equal to |Qi |(the determinant of Qi).
Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Manning (2004).
Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ Many learning paradigms can be defined as inference-based learning.
Because these features consider multiple edges, including them in the CRF model would make exact inference intractable (McDonald and Satta, 2007). $$$$$ The complexity results given here suggest that polynomial chart-parsing algorithms do not exist for the non-projective case.

Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ The primary problem in treating each dependency as independent is that it is not a realistic assumption.
Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ An interesting line of research is to investigate classes of non-projective structures that can be parsed with chart-parsing algorithms and how these classes relate to the languages parsable by other syntactic formalisms.
Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ Thus, the existence of bottom-up chart parsing algorithms for projective dependency parsing provides many advantages.
Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence assumptions, such as "arc factorization" for nonprojective dependency parsing (McDonald and Satta, 2007). $$$$$ By solving the above constrained optimization problem with the usual Lagrange multipliers method one gets where for each xα the expectation ((i, j)k)x« is defined as in Section 3, but with the weight w(T) replaced by the probability distribution p(xα|T, nα).

A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ The definition of wk ij depends on the context in which it is being used.
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a).
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ Let (i, j)k represent the kth edge from i to j. Gx encodes all possible labeled dependencies between the words of x.

In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ Many learning paradigms can be defined as inference-based learning.
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ Let i →+ j be a relation that is true if and only if there is a non-empty directed path from node i to node j in some graph under consideration.
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Manning (2004).
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non-projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard.McDonald and Pereira (2006) adopted an approximation based on O (n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. $$$$$ In this paper we investigate several nonprojective parsing algorithms for dependency parsing, providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others, called here the edge-factored model.

Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006).
Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ Thanks to David Smith, Noah Smith and Michael Collins for making drafts of their EMNLP papers available.
Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ As mentioned above, it permits simple augmentation techniques to incorporate non-local information such as arity constraints and Markovization.
Exact parsing under such model, with arbitrary second-order features, is intractable (McDonald and Satta, 2007). $$$$$ We have shown that several computational problems related to parsing can be solved in polynomial time for the class of non-projective dependency models with the assumption that dependency relations are mutually independent.

A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here.
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ It then considers edges from the node i to the node j.
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a).
A projective dependency parse (top), and a non-projective dependency parse (bottom) for two English sentences; examples from McDonald and Satta (2007). $$$$$ We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results.

While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ Let L = {l1, ... ,l|L|} be a set of permissible syntactic edge labels and x = x0x1 · · · x,,, be a sentence such that x0=root.
While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ We also investigate algorithms for non-projective parsing that account for nonlocal information, and present several hardness results.
While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ We now introduce a construction that will be used to establish several hardness results for the computational problems discussed in this paper.
While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta,1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). $$$$$ As mentioned above, it permits simple augmentation techniques to incorporate non-local information such as arity constraints and Markovization.

While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. $$$$$ Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a).
While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NP hard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods. $$$$$ By its definition, Gx is a multi-digraph, which is a digraph that may have more than one edge between any two nodes.

the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). $$$$$ These results extend the work of McDonald et al. (2005b) and help to further our understanding of when exact non-projective algorithms can be employed.
the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). $$$$$ This requires the addition to the model of parameters px,STOP for each x E E, with the normalization condition px,STOP + Py,k pkx,y = 1.

Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ By its definition, Gx is a multi-digraph, which is a digraph that may have more than one edge between any two nodes.
Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ Let L = {l1, ... ,l|L|} be a set of permissible syntactic edge labels and x = x0x1 · · · x,,, be a sentence such that x0=root.
Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard.
Exhaustive non projective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). $$$$$ Thanks to Ben Taskar for pointing out the work of Meil˘a and Jaakkola (2000).

Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ However, for the non-projective case, moving beyond edge-factored models will almost certainly lead to intractable parsing problems.
Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005).
Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ In this subsection we show that when the risk function is of a specific form, this restriction can be dropped.
Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). $$$$$ For example, in the work of McDonald et al. (2005b) it is simply a linear classifier that is a function of the words in the dependency, the label of the dependency, and any contextual features of the words in the sentence.

Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ Let i →+ j be a relation that is true if and only if there is a non-empty directed path from node i to node j in some graph under consideration.
Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ Matrix multiplication currently has known O(n2.38) implementations and it has been widely conjectured that it can be solved in O(n2) (Robinson, 2005).
Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ Thus every possible dependency graph of x must be a subgraph of Gx.
Unfortunately, the non-projective parsing problem is known to be NP-hard for all but the simplest models (McDonald and Satta, 2007). $$$$$ Throughout the rest of this paper, we will refer to any T ∈ T (Gx) as a valid dependency graph for a sentence x.

McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ Let (fu).α represent the expectation of feature fu for the training instance xα, Thus, we can calculate the feature expectation per training instance using the algorithms for computing Z,, and edge expectations.
McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004).
McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ When this analysis is coupled with the projective parsing algorithms of Eisner (1996) and Paskin (2001) we begin to get a clear picture of the complexity for data-driven dependency parsing within an edge-factored framework.
McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for non projective parsing, showing that parsing for a variety of models is NP-hard. $$$$$ In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here.

Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. $$$$$ From this sentence we construct a complete labeled directed graph (digraph) Gx = (Vx, Ex) such that, Gx is a graph where each word in the sentence is a node, and there is a directed edge between every pair of nodes for every possible label.
Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. $$$$$ In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here.
Second, McDonald and Satta (2007) propose an O (n5) algorithm for computing the marginals, as opposed to the O (n3) matrix-inversion approach used by Smith and Smith (2007) and ourselves. $$$$$ Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992).

For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ As a side note, the k-best argmax problem for digraphs can be solved in O(kn2) (Camerini et al., 1980).
For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here.
For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ In the spirit of our effort to understand the nature of exact non-projective algorithms, we examine dependency models that introduce arity constraints as well as permit edge decisions to be dependent on a limited neighbourhood of other edges in the graph.
For example, both papers propose minimum-risk decoding, and McDonald and Satta (2007) discuss unsupervised learning and language modeling, while Smith and Smith (2007) define hidden variable models based on spanning trees. $$$$$ First, we note that the feature functions factor over edges, i.e., fu(T) = (i,j)k∈ET fu(i,j, k).
