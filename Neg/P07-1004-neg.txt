Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ We propose several algorithms with this aim, and present the strengths and weaknesses of each one.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ Again, a new phrase table was trained on these data.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ Experiments showed that putting a large weight on the model trained on labeled data performs best.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ The probability distribution over the phrase pairs thus gets more focused on the (reliable) parts which are relevant for the test data.

In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ All improvements over the baseline are significant at the 95%-level.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ Note that the single improvements achieved here are slightly below the 95%-significance level.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ State-of-the-art SMT systems are trained on large collections of text which consist of bilingual corpora (to learn the parameters of p(s  |t)), and of monolingual target language corpora (for p(t)).
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ The Select function in Algorithm 1 is used to create the additional training data Ti which will be used in the next iteration i + 1 by Estimate to augment the corpus use sentences original bilingual training data.

A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ This method yields slightly higher translation quality than the baseline system.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ The entries marked as † were not attempted because they are not feasible (e.g. full re-training on the NIST data).
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ The error rates are significantly reduced in all three settings, and BLEU score increases in all cases.

Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Secondly, our algorithm constitutes a way of adapting the SMT system to a new domain or style without requiring bilingual training or development data.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Since re-training the full phrase tables is not feasible here, a (small) additional phrase table, specific to U, was trained and plugged into the SMT system as an additional model.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ So the information extracted from untranslated test sentences is equivalent to having an additional 50K sentence pairs.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Note that the single improvements achieved here are slightly below the 95%-significance level.

Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ The decoder weights thus had to be optimized again to determine the appropriate weight for this new phrase table.
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ By transductive, we mean that we repeatedly translate sentences from the development set or test set and use the generated translations to improve the performance of the SMT system.
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ The selection methods investigated here have been shown to be wellsuited to boost the performance of semi-supervised learning for SMT.
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ The phrase posterior probabilities are determined by summing the sentence probabilities of all translation hypotheses in the N-best list which contain this phrase pair.

In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ Selection step using threshold on confidence scores.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ Another problem is that for many language pairs the amount of available bilingual text is very limited.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ We ran the algorithm for 20 iterations and BLEU score increased from 25.3 to 25.7.

Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ The proposed method adapts the trained models to the style and domain of the new input.
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ There are two main reasons for this improvement: Firstly, the selection step provides important feedback for the system.
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ We present detailed experimental evaluations on the French–English EuroParl data set and on data from the NIST Chinese–English largedata track.

We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ To obtain a score for the whole target sentence, the posterior probabilities of all target phrases are multiplied.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ For an analysis of the self-trained phrase tables, examples of translated sentences, and the phrases used in translation, see (Ueffing, 2006).
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ We show a significant improvement in translation quality on both tasks.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ Every time a new corpus is to be translated, an adapted phrase table is created using transductive learning and used with the weight which has been learned on dev1.

To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ The threshold is optimized on the development beforehand.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ As a result, the probabilities of lowquality phrase pairs, such as noise in the table or overly confident singletons, degrade.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.

The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much noise and performance degraded.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ The initially learned phrase table was merged with the learned phrase table in each iteration with a weight of A = 0.1.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ We show a significant improvement in translation quality on both tasks.

From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text in the target language.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Those phrases in the existing phrase tables which are relevant for translating the new data are reinforced.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Semi-supervised learning has been previously applied to improve word alignments.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ These lists are rescored with the certain precise conditions, as described in (Abney, following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U. ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed definin-grams, and sentence length (Zens and Ney, 2006; tions.
