Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ We show a significant improvement in translation quality on both tasks.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ All improvements over the baseline are significant at the 95%-level.
Transductive learning method (Ueffing et al, 2007) which repeatedly re-trains the generated source target N-best hypotheses with the original training data again showed translation performance improvement and demonstrated that the translation model can be reinforced from N-best hypotheses. $$$$$ The decoder weights thus had to be optimized again to determine the appropriate weight for this new phrase table.

In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ In a third experiment, we applied the mixture model idea as explained in Section 3.2.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ The system generates Thus, this distribution over translations of sentences a 5,000-best list of alternative translations for each from U will have the maximum entropy.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ The selection step discards bad machine translations and reinforces phrases of high quality.
In order to use source-side monolingual data, Ueffing et al (2007), Schwenk (2008), Wu et al (2008) and Bertoldi and Federico (2009) employed the transductive learning to first translate the source-side monolingual data using the best configuration (baseline+in-domain lexicon+in-domain language model) and obtain 1-best translation for each source-side sentence. $$$$$ Self-training for SMT was proposed in (Ueffing, 2006).

A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much noise and performance degraded.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ In this paper we explore the use of transductive semi-supervised methods for the effective use of monolingual data from the source language in order to improve translation quality.
A different form of semi-supervised learning (self-training) has been applied to MT by (Ueffing et al, 2007). $$$$$ The confidence estimation, for example, discards translations with low language model scores or posterior probabilities.

Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ All improvements over the baseline are significant at the 95%-level.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ The probability distribution over the phrase pairs thus gets more focused on the (reliable) parts which are relevant for the test data.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ This was done on the dev1 corpus, using the phrase table specific to dev1.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality .

Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm.
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ The selection step discards bad machine translations and reinforces phrases of high quality.
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ For an analysis of the self-trained phrase tables, examples of translated sentences, and the phrases used in translation, see (Ueffing, 2006).
Such self-translation techniques have been introduced by Ueffing et al (2007). $$$$$ This additional phrase table is small and specific to the development or test set it is trained on.

In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ This method yields slightly higher translation quality than the baseline system.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ Selection step using threshold on confidence scores.
In follow up work, this approach was refined (Ueffing et al, 2007). $$$$$ For an analysis of the self-trained phrase tables, examples of translated sentences, and the phrases used in translation, see (Ueffing, 2006).

Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ We fully re-trained the phrase tables on these data and 8,000 test sentence pairs sampled from 20-best lists in each iteration.
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language.
Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U+, or (2) combining the two sets of data and training from the bitext. $$$$$ We ran the algorithm for three iterations and the BLEU score increased from 25.3 to 25.6.

We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ In total, it increases from 24.1 to 24.4 for the 100K filtered corpus, and from 24.5 to 24.8 for 150K, respectively.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ These lists are rescored with the certain precise conditions, as described in (Abney, following models: (a) the different models used in 2004), we can analyze Algorithm 1 as minimizing the decoder which are described above, (b) two dif- the entropy of the distribution over translations of U. ferent features based on IBM Model 1 (Brown et al., However, this is true only when the functions Esti1993), (c) posterior probabilities for words, phrases, mate, Score and Select have very prescribed definin-grams, and sentence length (Zens and Ney, 2006; tions.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ We show a significant improvement in translation quality on both tasks.
We measure the similarity using weighted n-gram coverage (Ueffing et al, 2007b). $$$$$ Our hypothesis is that adding information from source language text can also provide improvements.

To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ The selection and scoring in Algorithm 1 were performed using confidence estimation with a threshold.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ Semi-supervised learning has been previously applied to improve word alignments.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ The proposed method adapts the trained models to the style and domain of the new input.
To make the confidence score for sentences with different lengths comparable, we normalize using the sentence length (Ueffing et al, 2007b). $$$$$ This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language.

The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ Every time a new corpus is to be translated, an adapted phrase table is created using transductive learning and used with the weight which has been learned on dev1.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ We will show how such corpora can be used to achieve higher translation quality.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ We propose several algorithms with this aim, and present the strengths and weaknesses of each one.
The SMT system we applied in our experiments is PORTAGE (Ueffing et al, 2007a). $$$$$ The number of possible events, i.e. phrase pairs or pairs of subtrees in the two languages, is too big to reliably estimate a probability distribution over such pairs.

From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ The third method uses a threshold-based selection method.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ The decoder weights thus had to be optimized again to determine the appropriate weight for this new phrase table.
From machine learning perspective, both proposed methods can be viewed as certain form of transductive learning applied to the SMT task (Ueffing et al, 2007). $$$$$ We fully re-trained the phrase tables on these data and 8,000 test sentence pairs sampled from 20-best lists in each iteration.
