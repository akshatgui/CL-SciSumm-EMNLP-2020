Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). $$$$$ A user only needs to supply a representative text corpus and a small set of seed words for each target category.
Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). $$$$$ The input to the system is a small set of seed words for a category and a representative text corpus.
Our approach builds upon earlier work on corpus-based methods for generating extraction patterns (Riloff, 1996b) and semantic lexicons (Riloff and Shepherd, 1997). $$$$$ Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.

The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. $$$$$ Fortunately, most of them worked fairly well, but some of them did not.
The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. $$$$$ Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used.
The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. $$$$$ The results from this experiment are shown in Figures 4-8.
The corpus-based algorithm that we used to build the semantic lexicon (Riloff and Shepherd, 1997) requires five seed words as input for each semantic category, and produces a ranked list of words that are statistically associated with each category. $$$$$ Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)).

For more details of this algorithm, see (Riloff and Shepherd, 1997). $$$$$ This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.
For more details of this algorithm, see (Riloff and Shepherd, 1997). $$$$$ A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon.
For more details of this algorithm, see (Riloff and Shepherd, 1997). $$$$$ Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used.
For more details of this algorithm, see (Riloff and Shepherd, 1997). $$$$$ The input to the system is a small set of seed words for a category and a representative text corpus.

In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). $$$$$ We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's.
In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). $$$$$ One could have a person verify that each word belongs to the target category before adding it to the seed word list, but this would require human interaction at each iteration of the feedback cycle.
In addition, we exploit syntactic constructions shown to be useful by other studies - lists and conjunctions (Roark and Charniak, 1998), and adjacent words (Riloff and Shepherd, 1997). $$$$$ The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words.

(Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. $$$$$ If a word refers to a part of something that is a member of the category, then it deserves a 4.
(Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. $$$$$ This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.
(Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. $$$$$ Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).
(Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. $$$$$ More experiments are needed to better understand whether this category is inherently difficult or whether a more carefully chosen set of seed words would improve performance.

Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. $$$$$ But many of the words referred to organizations (e.g., FMLN), groups (e.g., forces), and actions (e.g., attacks).
Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. $$$$$ If a word refers to a part of something that is a member of the category, then it deserves a 4.
Riloff and Shepherd (1997) used a semi automatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. $$$$$ We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically.

For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. $$$$$ For example, some of the words rated as 3's for the Vehicle category include: flight, flights, aviation, pilot, airport, and highways.
For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. $$$$$ Only a few new commercial words were identified, such as hotel and restaurant.
For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. $$$$$ Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish).
For example, Riloff and Shepherd (Riloff and Shepherd, 1997) developed a statistical co-occurrence model for semantic lexicon induction that was designed with these structures in mind. $$$$$ We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically.

Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). $$$$$ In retrospect, we realized that there were probably few words in the MUC-4 corpus that referred to commercial establishments.
Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). $$$$$ If a word refers to a part of something that is a member of the category, then it deserves a 4.
Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). $$$$$ If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.
Many of the successful methods follow the unsupervised iterative bootstrapping framework (Riloff and Shepherd, 1997). $$$$$ The remaining nouns are sorted by category score and ranked so that the nouns most strongly associated with the category appear at the top.

NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ The output is a ranked list of words that are associated with the category.
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ For example, feathers and tails are parts of ANIMALS.
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results.

The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). $$$$$ If a word refers to a part of something that is a member of the category, then it deserves a 4.
The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). $$$$$ In general, we found that additional seed words tend to improve performance, but the results were not substantially different using five seed words or using ten.
The idea here is that nouns in conjunctions or appositives tend to be semantically related, as discussed in Riloff and Shepherd (1997) and Roark and Charniak (1998). $$$$$ Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).

Both Hearst (1992) and Riloff and Shepherd (1997) use unparsed text. $$$$$ There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986).
Both Hearst (1992) and Riloff and Shepherd (1997) use unparsed text. $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
Both Hearst (1992) and Riloff and Shepherd (1997) use unparsed text. $$$$$ There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986).

Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. $$$$$ We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.
Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. $$$$$ Some good category words were found, such as rebel, advisers, criminal, and citizen.
Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. $$$$$ The words selected by the user are added to a permanent semantic lexicon with the appropriate category label.
Riloff and Shepherd (1997) suggested using conjunction and appositive data to cluster nouns; however, they approximated this data by just looking at the nearest NP on each side of a particular NP. $$$$$ For example, feathers and tails are parts of ANIMALS.

The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ For example, suppose a user wanted to build a dictionary of Vehicle words.
The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ For example, feathers and tails are parts of ANIMALS.
The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ If a word refers to a part of something that is a member of the category, then it deserves a 4.

Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ Of course, there is also a law of diminishing returns: using a seed word list containing 60 category words is almost like creating a semantic lexicon for the category by hand!
Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ In most cases, semantic knowledge is encoded manually for each application.
Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ The output is a ranked list of words that are associated with the category.
Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ Some good category words were found, such as rebel, advisers, criminal, and citizen.

Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ The Location category performed very well using seed words such as city, town, and province.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ The word M-16 would be in the context windows for both gun and rifle even though there was just one occurrence of it in the sentence.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ Finally, we graphed the results from the human judges.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ We do not claim to understand exactly what types of categories will work well and which ones will not, but our early experiences did shed some light on the strengths and weaknesses of this approach.

These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. $$$$$ But semantic information is difficult to obtain.
These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. $$$$$ For example, zoos and nests are strongly associated with ANIMALS.
These conditions are at least as stringent as those of previous experiments, particularly those of Riloff and Shepherd (1997) who also give credit for words associated with but not belonging to a particular category. $$$$$ 5: CORE MEMBER OF THE CATEGORY: If a word is clearly a member of the category, then it deserves a 5.

Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus.
Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.
Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ Finally, we graphed the results from the human judges.
Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ The output is a ranked list of words that are associated with the category.

The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ Our experiments suggest that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction.
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ While more work needs to be done to refine this procedure and characterize the types of categories it can handle, we believe that this is a promising approach for corpus-based semantic knowledge acquisition.
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ The output of the system is the ranked list of nouns after the final iteration.
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.

Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. $$$$$ Our initial seed words worked well enough that we did not experiment with them very much.
Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. $$$$$ But the key question is whether the ranked list contains many true category members.
Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. $$$$$ As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus.
Riloff and Shepherd (1997) presented a corpus based method that can be used to build semantic lexicons for specific categories. $$$$$ If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.

Following the work of (Riloff and Shepherd, 1997), we adopted the following evaluation setting. $$$$$ But it is worth noting that this category achieved good results, presumably because location names often cluster together in appositives, conjunctions, and nominal compounds.
Following the work of (Riloff and Shepherd, 1997), we adopted the following evaluation setting. $$$$$ If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.
Following the work of (Riloff and Shepherd, 1997), we adopted the following evaluation setting. $$$$$ If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.
