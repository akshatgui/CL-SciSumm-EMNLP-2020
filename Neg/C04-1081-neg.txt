A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ Conditional random fields are configured as a linear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algorithm to efficiently find the most likely label se quence for a given character sequence.
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ The contribution of this paper is three-fold.
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation.

Peng et al (2004) uses the CRFs to address this issue. $$$$$ Improved seg mentation can then be further used to improve new word detection.
Peng et al (2004) uses the CRFs to address this issue. $$$$$ In the second pass of segmentation, the other two mistakes are corrected.
Peng et al (2004) uses the CRFs to address this issue. $$$$$ We 1http://www.mandarintools.com, ftp://xcin.linux.org.tw/pub/xcin/libtabe, http://www.geocities.com/hao510/wordlist noun (e.g.,?,?)
Peng et al (2004) uses the CRFs to address this issue. $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.

In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ Gao et al(2003) uses class-based language for word segmentation where some word cat egory information can be incorporated.
In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ Tradi tionally, new word detection has been considered as a standalone process.
In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ Improved seg mentation can then be further used to improve new word detection.

The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ Our system achieves top performance in two of the runs, and a state-of-the-art per formance on average.
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ However, ?????
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ Many current approaches suffer from either lackof exact inference over sequences or difficulty in incorporating domain knowledge effectively into seg mentation.


CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ First-order: Here the inputs are examined in.
CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ For Chinese, there has been significant research on find ing word boundaries in unsegmented sequences(see (Sproat and Shih, 2002) for a review).
CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ A commonly used prior is a zero-mean Gaussian.
CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ State-of-the-art perfor mance is obtained.

A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.
A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ Many current approaches suffer from either lackof exact inference over sequences or difficulty in incorporating domain knowledge effectively into seg mentation.

Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ ters corresponding to state transitions.
Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ The up per part of the table contains the closed test results, and the lower part contains the open test results.
Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ Feature function are represented as f(yt?2, yt?1, yt,x).
Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ Domain knowledge is either not used, used in a limited way, or used in a complicated way spread across different components.

CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ We also achieve two best runs (ASo and HKc), with a comparable average of 91.9% over the same 6 runs, and a 92.7% average over all the 8 runs.Second, performance varies significantly across dif ferent datasets, indicating that the four datasets havedifferent characteristics and use very different seg mentation guidelines.
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ maybe last-name (e.g.,?,[) plural character (e.g.,?,?)

We follow the format from Peng et al (2004). $$$$$ that the improvement is monotonically related to the OOV rate (OOV rates are listed in Table 1).
We follow the format from Peng et al (2004). $$$$$ New word detection is normally considered as a separate process from segmentation.However, integrating them would benefit both seg mentation and new word detection.
We follow the format from Peng et al (2004). $$$$$ Given a word segmentation proposed by the CRF, we can compute a confidence in each segment.
We follow the format from Peng et al (2004). $$$$$ Howeverthese approaches have been limited by the impossibility of creating a lexicon that includes all possible Chinese words and by the lack of robust statistical inference in the rules.

represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ Unlike heuristic methods, they are principled probabilistic finite state models onwhich exact inference over sequences can be ef ficiently performed.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ Comparing Chinese word segmentation accuracyacross systems can be difficult because many re search papers use different data sets and different ground-rules.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ State-of-the-art perfor mance is obtained.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ However, ?????

RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ These datasets represent four different segmentation standards.
RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ hap pens four times.
RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ hap pens four times.

Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ This gives some empirical evidenceof the advantages of linear-chain CRFs over sliding window maximum entropy models, however, this comparison still requires further investigation sincethere are many factors that could affect the performance such as different features used in both sys tems.
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ 3.2 Feature conjunctions.
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ A confidence threshold of 0.9 is determined by cross-validation.Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004).

Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ letter (English character) punctuation (e.g., # $) last name (e.g.,K) foreign name (e.g.,?)
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results.
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ However, ?????

3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem.
3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ We also present a probabilistic new word detection method, which further improves performance.
3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ Approaches to Chinese segmentation fall roughly into two categories: heuristic dictionary-based methods and statistical machine learning methods.In dictionary-based methods, a predefined dictio nary is used along with hand-generated rules for segmenting input sequence (Wu, 1999).
3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.

 $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.
 $$$$$ This is due to significant inconsistent segmen tation in training and testing (Sproat and Emerson, 2003).
 $$$$$ The contribution of this paper is three-fold.

Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ = ? i logP?(yi|xi) = ? i ( T?
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results.
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ A confidence threshold of 0.9 is determined by cross-validation.Segment confidence is estimated using constrained forward-backward (Culotta and McCallum, 2004).
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ We experiment an alternate version ofGaussian prior in which the variance is feature dependent.

We also used lexical features consulting a dictionary $$$$$ We cast the segmentation problem as one of se quence tagging: Chinese characters that begin a new word are given the START tag, and characters in the middle and at the end of words are given theNONSTART tag.
We also used lexical features consulting a dictionary $$$$$ (they are segmented differently because Viterbi algorithm decodes based on context.).
We also used lexical features consulting a dictionary $$$$$ State-of-the-art perfor mance is obtained.
We also used lexical features consulting a dictionary $$$$$ A common special-case graph structure is a linear chain, which corresponds to a finite state machine, and is suitablefor sequence labeling.

text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ With a Gaussian prior, log-likelihood is penal ized as follows.
text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ We define four different state transition feature functions corresponding to different Markov orders.Higher-order features capture more long-range de pendencies, but also cause more data sparseness problems and require more memory for training.
text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.
text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.

Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags $$$$$ that the improvement is monotonically related to the OOV rate (OOV rates are listed in Table 1).
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags $$$$$ One advantage of CRFs (as well as traditional maximum entropy models) is its flexibility in using arbitrary features of the input.
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags $$$$$ Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem.
