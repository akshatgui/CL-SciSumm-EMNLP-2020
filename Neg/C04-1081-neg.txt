A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ Table 5 summarizes these results, in which the rows are the training datasets and the columns are thetesting datasets.
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ AcknowledgmentsThis work was supported in part by the Center for Intelligent Information Retrieval, in part by The Cen tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS 0326249, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ One advantage of CRFs (as well as traditional maximum entropy models) is its flexibility in using arbitrary features of the input.
A simple approach would be to let a character be a token (i.e., character-based Begin/Inside tagging) so that boundary ambiguity never occur (Peng et al, 2004). $$$$$ Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem.

Peng et al (2004) uses the CRFs to address this issue. $$$$$ We verify this by another test.
Peng et al (2004) uses the CRFs to address this issue. $$$$$ The closed features are obtained from training data alone, by intersecting the character list obtainedfrom training data with corresponding open lexi cons.
Peng et al (2004) uses the CRFs to address this issue. $$$$$ Our results are in the last row.
Peng et al (2004) uses the CRFs to address this issue. $$$$$ Tradi tionally, new word detection has been considered as a standalone process.

In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation.
In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ Since CTB and PK are provided in the GB encod ing while AS and HK use the Big5 encoding, we convert AS and HK datasets to GB in order to make cross-training-and-testing possible.
In last part of the experiments, the generality of the datasets and the toughness of our system are tested (Peng et al, 2004). $$$$$ The closed tests use only material from the training data for the particular corpus being tested.Open tests allows using other material, such as lexicons from Internet.

The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ Maximum entropy models give tremendousflexibility to incorporate arbitrary features.
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ State-of-the-art perfor mance is obtained.
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ Table 6 shows the effect of new word detection on the closed tests.
The superiority of CRFs on Chinese information processing was also demonstrated in word segmentation (Peng et al 2004). $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.

Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ 3.2 Feature conjunctions.
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ We also present a probabilistic new word detection method, which further improves performance.
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ However, ?????
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ State-of-the-art perfor mance is obtained.

CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ To explore this advantage, as well as the importance of domain knowledge, we use many open features from external re sources.
CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ The open features include a large word list (containing single and multiple-character words), a character list, and additional topic or part-of-speech character lexicons obtained from various sources.
CRFs using this technique have been shown to be very successful at the task of Chinese word segmentation (CWS), starting with the model of Peng et al (2004). $$$$$ letter (English character) punctuation (e.g., # $) last name (e.g.,K) foreign name (e.g.,?)

A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results.
A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results.
A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ The confidence in this segment is then Z ? x Zx , a real number between 0 and 1.In order to increase recall of new words, we consider not only the most likely (Viterbi) segmen tation, but the segmentations in the top N most likely segmentations (an N -best list), and detect new words according to the above criteria in all N segmentations.Many errors can be corrected by new word detection.
A popular discriminative model that have been used for this task is the conditional random fields (CRFs) (Lafferty et al, 2001), starting with the model of Peng et al (2004). $$$$$ Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation.

Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.
Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ AcknowledgmentsThis work was supported in part by the Center for Intelligent Information Retrieval, in part by The Cen tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS 0326249, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.
Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al, 2004), (Tseng et al, 2005), (Low et al., 2005), (Zhao et al, 2006). $$$$$ adjective (e.g.,?,?)

CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ How ever, a traditional maximum entropy tagger, as used in Xue (2003), labels characters without consideringdependencies among the predicted segmentation labels that is inherent in the state transitions of finite state sequence models.
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ Fea ture function are represented as f(yt?1, yt,x).
CRF is a statistical sequence modeling framework introduced by Lafferty et al (2001), and was first used for the Chinese word segmentation task by Peng et al (2004), who treated word segmentation as a binary decision task. $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.

We follow the format from Peng et al (2004). $$$$$ Table 6 shows the effect of new word detection on the closed tests.
We follow the format from Peng et al (2004). $$$$$ Given a word segmentation proposed by the CRF, we can compute a confidence in each segment.
We follow the format from Peng et al (2004). $$$$$ We measure the performance ofnew word detection by its improvements on seg mentation.
We follow the format from Peng et al (2004). $$$$$ Each entry is the performance of the given metric (precision, recall, F1, and Roov) on the test set.

represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ Chinese word segmentation is a difficult, im portant and widely-studied sequence modelingproblem.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ Thus, each training set is randomly split?80% used for training and theremaining 20% for validation?and based on vali dation set performance, choices are made for model structure, prior, and which word lexicons to include.
represents the CRF model from Peng et al (2004), and the last row represents our model. $$$$$ These beneficialproperties suggests that CRFs are a promising ap proach for Chinese word segmentation.New word detection is one of the most impor tant problems in Chinese information processing.Many machine learning approaches have been pro posed (Chen and Bai, 1998; Wu and Jiang, 2000; Nie et al, 1995).

RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ The contribution of this paper is three-fold.
RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ To avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters.
RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ The contribution of this paper is three-fold.
RMNs are the special case of Conditional Markov Networks (or Conditional Random Fields) in which graph structure and parameter tying are determined by SQL-like form. As for the marginal probability to use as a confidence measure shown in Figure 4, Peng et al (2004) has applied linear-chain CRFs to Chinese word segmentation. $$$$$ Furthermore, they arediscriminatively-trained, and are often more accurate than generative models, even with the same fea tures.

Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ S01 is one of the best segmentation systems in mainland China (Zhang et al., 2003).
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ Unlike heuristic methods, they are principled probabilistic finite state models onwhich exact inference over sequences can be ef ficiently performed.
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ We also present a probabilistic new word detection method, which further improves performance.
Aiming to improve both tasks, work by Peng et al (2004) and Sun et al (2012) conduct segmentation and detection sequentially, but in an iterative manner rather than joint. $$$$$ Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.

Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ defines a conditional proba bility for a state (label) sequence y = y1...yT (for example, labels indicating where words start or have their interior) given an input sequence x = x1...xT (for example, the characters of a Chinese sentence) to be P?(y|x) = 1Zx exp ( T?
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ Third, as far as we are aware, this is the first work to comprehensively evaluate on the four benchmark datasets, making a solid baseline for future research on Chinese word segmentation.
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ They can pro duce not only a segmentation, but also confidence in local segmentation decisions, which can be usedto find new, unfamiliar character sequences sur rounded by high-confidence segmentations.
Work by Peng et al (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. $$$$$ First, we apply CRFs to Chinese word segmentation and find that they achieve state-of-the art performance.Second, we propose a probabilistic new word de tection method that is integrated in segmentation, and show it to improve segmentation performance.

3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ AcknowledgmentsThis work was supported in part by the Center for Intelligent Information Retrieval, in part by The Cen tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS 0326249, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.
3.2.1 Results on Sighanbakeoff 2003 Experiments done while developing this system showed that its performance was significantly better than that of Peng et al (2004). $$$$$ number (e.g.,,) negative (e.g.,X,:) determiner (e.g.,?,?,Y) function (e.g. ?,?)

 $$$$$ Column SITE-AVG is the averageF1 performance over the datasets on which a site re ported results.
 $$$$$ They participated in two datasets, with an average of 93.8%.
 $$$$$ Third, consider a comparison of our results with site S12, who use a sliding-window maximum entropy model (Xue, 2003).
 $$$$$ A recent advance in this area is Xue (2003), in which the author uses a sliding-window maximum entropy classifier to tag Chinese characters into one of four position tags, and then covert these tags into a segmentation using rules.

Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ AcknowledgmentsThis work was supported in part by the Center for Intelligent Information Retrieval, in part by The Cen tral Intelligence Agency, the National Security Agencyand National Science Foundation under NSF grant #IIS 0326249, and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903.
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ This paper demonstrates the abil ity of linear-chain conditional random fields(CRFs) to perform robust and accurate Chinese word segmentation by providing a principled framework that easily supports the in tegration of domain knowledge in the form of multiple lexicons of characters and words.
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ pronoun (e.g.,fi,?,?)
Peng et al (2004) defined the word segmentation problem as labeling each character as whether or not the previous character boundary of the current character is a word boundary. $$$$$ However, our implemen tation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003).

We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. $$$$$ For example, person name ?????
We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. $$$$$ Overall, CRFs perform robustly well across all datasets.
We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. $$$$$ Conditional random fields are configured as a linear-chain (finite state machine) for this purpose,and tagging is performed using the Viterbi algorithm to efficiently find the most likely label se quence for a given character sequence.
We also used lexical features consulting a dictionary: one is to check if any of the above defined character n-grams appear in a dictionary (Peng et al, 2004), and the other is to check if there are any words in the dictionary that start or end at the current character boundary. $$$$$ Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al, 2001).

text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ They can pro duce not only a segmentation, but also confidence in local segmentation decisions, which can be usedto find new, unfamiliar character sequences sur rounded by high-confidence segmentations.
text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ See Peng and McCallum (2004) for more details and further experiments.
text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al, 2004). $$$$$ Our system is evaluated on four datasets usedin a recent comprehensive Chinese word segmentation competition.

Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ Domain knowledge is either not used, used in a limited way, or used in a complicated way spread across different components.
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ The standard forward-backward algorithm (Rabiner, 1990) calculates Zx, the total like lihood of all label sequences y given a sequence x. Constrained forward-backward algorithm calculates Z ?x, total likelihood of all paths passing through a constrained segment (in our case, a sequence of characters starting with a START tag followed by a few NONSTART tags before the next START tag).
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ preposition (e.g.,?)
Different from (Peng et al, 2004), we represent the positions of a hanzi (Chinese character) with four different tags: B for a hanzi 196 that starts a word, I for a hanzi that continues the word, F for a hanzi that ends the word, S for a hanzi that occurs as a single-character word. $$$$$ Unlike generative N-gram or hidden Markov models, they have the ability to straightforwardly combine rich domain knowledge, for example in this paper, in the form of multiple readily-available lexicons.
