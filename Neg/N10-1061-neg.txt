These abstract notions (lexical association, proximity, tendencies towards few or many relations, and allowing for unassociated items) play an important role in many relation-detection tasks (e.g., co-reference resolution, Haghighi and Klein 2010). $$$$$ As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009).
These abstract notions (lexical association, proximity, tendencies towards few or many relations, and allowing for unassociated items) play an important role in many relation-detection tasks (e.g., co-reference resolution, Haghighi and Klein 2010). $$$$$ By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.

Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. $$$$$ Coreference resolution is governed by syntactic, semantic, and discourse constraints.
Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. $$$$$ Performance on the A05RA dataset is generally lower because it includes articles from blogs and web forums where parser quality is significantly degraded.
Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. $$$$$ In general, the choice of entity types to prime with prototypes is a domain-specific question.
Many other existing systems applied supervised or unsupervised (Haghighi and Klein, 2010) learning models. $$$$$ Formally, this process is described as: For each mention position, i = 1, ... , n, Draw antecedent position Ai E {1, ... , i}: Here, K denotes the number of entities allocated in the first i-1 mention positions.

However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. $$$$$ We represent each mention M as a collection of key-value pairs.
However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. $$$$$ Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge.
However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. $$$$$ In this task, the plurality of errors are confusions between the GPE (geo-political entity) and ORG entity types, which have very similar distributions.
However, such structured knowledge bases are of limited scope, and, while Haghighi and Klein (2010) self-acquires knowledge about coreference, it does so only via reference constructions and on a limited scale. $$$$$ We now discuss errors and improvements made by our system.

Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). $$$$$ While Bengston and Roth (2008) do not report on the full system mention task, they do report on the more optimistic setting where mention detection is performed but non-gold mentions are removed for evaluation using an oracle.
Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). $$$$$ As a result, it leverages semantic constraints more effectively than systems operating at either level alone.
Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). $$$$$ Our model is able to acquire and exploit knowledge at either the level of individual entities (“Obama” is a “president”) and entity types (“company” can refer to a corporation).
Altogether, our final system produces the best numbers reported to date on end-to-end coreference resolution (with automatically detected system mentions) on multiple data sets (ACE 2004 and ACE2005) and metrics (MUC and B3), achieving significant improvements over the Reconcile DT baseline and over the state-of-the-art results of Haghighi and Klein (2010). $$$$$ Separately from the type-entity semantic module, a log-linear discourse model captures configurational effects.

In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). $$$$$ Of course enumerating and scoring all Zr hypotheses is intractable, so we instead utilize a left-to-right sequential beam search.
In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). $$$$$ N000140911081.
In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). $$$$$ Table 1 shows our results.
In ACE04 and ACE05, we have only the newswire portion (of the original ACE 2004 and 2005 training sets) and use the standard train/test splits reported in Stoyanov et al (2009) and Haghighi and Klein (2010). $$$$$ Entities: An entity is a specific individual or object in the world.

In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ To train, we included the text of all documents above, though of course not looking at either their mention boundaries or reference annotations in any way.
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ While it is not unusual for a single entity to admit multiple modifiers, the particular modifiers new and old are incompatible in a way that new and popular 9Note that we are still penalized for marking a spurious mention coreferent with an annotated one.
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ We utilized MUC (Vilain et al., 1995), B3All (Stoyanov et al., 2009), B3None (Stoyanov et al., 2009), and Pairwise F1.

 $$$$$ We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner.
 $$$$$ We considered the challenging end-to-end system mention setting, where in addition to predicting mention partitions, a system must identify the mentions themselves and their boundaries automatically.
 $$$$$ We also compared to the strong deterministic system of Haghighi and Klein (2009).10 Across all data sets, our model, despite being largely unsupervised, consistently outperforms these systems, which are the best previously reported results on end-to-end coreference resolution (i.e. including mention detection).
 $$$$$ We compared our output to the deterministic system of Haghighi and Klein (2009).

Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. $$$$$ The Stoyanov et al. (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahman and Ng (2009) numbers represent their highestperforming cluster ranking model.
Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. $$$$$ For example, analyst and it cannot corefer in our system because it is not a likely pronoun for the type PERSON.
Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. $$$$$ Our model yields 80.3.
Our main comparison is against Haghighi and Klein (2010), a mostly-unsupervised generative approach that models latent entity types, which generate specific entities that in turn render individual mentions. $$$$$ By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.

For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). $$$$$ Our model is able to acquire and exploit knowledge at either the level of individual entities (“Obama” is a “president”) and entity types (“company” can refer to a corporation).
For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). $$$$$ While the focus of our model is coreference resolution, we can also isolate and evaluate the type component of our model as an NER system.
For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). $$$$$ We first describe the variable groups, then the updates which optimize them in turn.
For the ACE05 and ACE05-ALL datasets, we revert to the 'AllPairs' (AP) setting of Reconcile because this gives us baselines competitive with Haghighi and Klein (2010). $$$$$ Similarly to Haghighi and Klein (2007) and Elsner et al. (2009), we biased these types’ pronoun distributions to the allowed set of pronouns.

Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. $$$$$ In conjunction with reasonable, but simple, factors capturing discourse and syntactic configurational preferences, our entity-centric semantic model lowers coreference error rate substantially, particularly on semantically disambiguated references, giving a sizable improvement over the state-of-the-art.11 Acknowledgements: This project is funded in part by the Office of Naval Research under MURI Grant No.
Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. $$$$$ While Bengston and Roth (2008) do not report on the full system mention task, they do report on the more optimistic setting where mention detection is performed but non-gold mentions are removed for evaluation using an oracle.
Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. $$$$$ For example, analyst and it cannot corefer in our system because it is not a likely pronoun for the type PERSON.
Our submission was a reduced version of the system described in Haghighi and Klein (2010), with extensions to improve mention detection to suit the OntoNotes annotation scheme. $$$$$ sociated with nodes in a parse tree and are typically realized as NPs.

Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). $$$$$ Note that this task is substantially more difficult than the unsupervised NER in Elsner et al. (2009) because the inventory of named entities is larger (7 vs. 3) and because we predict types over nominal mentions that are more difficult to judge from surface forms.
Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). $$$$$ The B3All and B3None are B3 variants (Bagga and Baldwin, 1998) that differ in their treatment of spurious mentions.
Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). $$$$$ By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.
Unlike previous work, we did not use the Bllip or Wikipedia data described in Haghighi and Klein (2010). $$$$$ We compared to two state-of-the-art supervised coreference systems.

The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). $$$$$ 10Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. are not.
The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). $$$$$ As an example of nominal headword compatibility, a “president” can be a “leader” but cannot be not an “increase.” Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007; Bengston and Roth, 2008; Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009).
The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). $$$$$ Many improvements arise from correctly identifying mentions which are semantically compatible but which do not explicitly appear in an appositive or predicatenominative configuration in the data.
The specific update methods vary for each set of parameters; for details see Section 4 of Haghighi and Klein (2010). $$$$$ In this work, we take a primarily unsupervised approach to coreference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue.

Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. $$$$$ The prior P(7rJσ2) is a zero-centered normal distribution with shared diagonal variance σ2, which is incorporated via L2 regularization during optimization.
Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. $$$$$ Br is a unigram distribution of words that are semantically licensed for property r. fr is a “fertility” distribution over the integers that characterizes entity list lengths.
Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. $$$$$ To produce our final coreference partitions, we assign each referring mention to the entity given by the δr factor and each pronoun to the most likely entity given by the ri.
Unlike Haghighi and Klein (2010), no extra data from Wikipedia or Bllip was used, a restriction that was necessary to be eligible for the closed part of the task. $$$$$ A nice consequence of this approach is that we can simply run our model on all mentions, discarding at evaluation time any which are of nonprototyped types.

Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). $$$$$ For example, for the type PERSON, Br for proper heads is quite flat (there are many last names) but fr is peaked at 1 (people have a single last name).
Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). $$$$$ Finally, a mention generation module independently renders the sequence of mentions (M) from their underlying entities.
Generative models are also used in unsupervised coreference (Haghighi and Klein, 2010). $$$$$ N000140911081.

While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). $$$$$ 11See nlp.cs.berkeley.edu and aria42.com/software.html for software release.
While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). $$$$$ We evaluated on multiple coreference resolution metrics, as no single one is clearly superior, partic8Meaning those headwords were assigned to the target type for more than 75% of their usages.
While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). $$$$$ We considered the challenging end-to-end system mention setting, where in addition to predicting mention partitions, a system must identify the mentions themselves and their boundaries automatically.
While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al, 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). $$$$$ On this more lenient setting, they report 78.4 B3 on the A04CU test set.

The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). $$$$$ 10Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. are not.
The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). $$$$$ We represent each type T as a mapping between properties r and pairs of multinomials (Br, fr).
The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). $$$$$ The discourse module is responsible for choosing an entity to evoke at each of the n mention positions.
The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). $$$$$ We evaluated on multiple coreference resolution metrics, as no single one is clearly superior, partic8Meaning those headwords were assigned to the target type for more than 75% of their usages.

This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. $$$$$ Note that this task is substantially more difficult than the unsupervised NER in Elsner et al. (2009) because the inventory of named entities is larger (7 vs. 3) and because we predict types over nominal mentions that are more difficult to judge from surface forms.
This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. $$$$$ Our model is able to acquire and exploit knowledge at either the level of individual entities (“Obama” is a “president”) and entity types (“company” can refer to a corporation).
This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. $$$$$ Our learning procedure involves finding parameters and assignments which are likely under our model’s posterior distribution P(E, Z, T, 9C|M, X).
This knowledge could be especially helpful for cross document coreference resolution systems (Haghighi and Klein, 2010), which actually represent concepts and track mentions of them across documents. $$$$$ For instance, for the PERSON type we might provide: Bush, Gore, Hussein president, minister, official he, his, she, him, her, you, ...

 $$$$$ We represent each mention M as a collection of key-value pairs.
 $$$$$ Our model is able to acquire and exploit knowledge at either the level of individual entities (“Obama” is a “president”) and entity types (“company” can refer to a corporation).
 $$$$$ On this more lenient setting, they report 78.4 B3 on the A04CU test set.

For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. $$$$$ In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009).
For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. $$$$$ To produce our final coreference partitions, we assign each referring mention to the entity given by the δr factor and each pronoun to the most likely entity given by the ri.
For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. $$$$$ We utilized MUC (Vilain et al., 1995), B3All (Stoyanov et al., 2009), B3None (Stoyanov et al., 2009), and Pairwise F1.
For instance, Haghighi and Klein (2010) include the governor of the head of nominal mentions as features in their model. $$$$$ With these variable groups, we would like to approximation our model posterior P(T, L, Zr, Ap, -r, 7rJM, X) using a simple factored representation.

Daume III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. $$$$$ 11See nlp.cs.berkeley.edu and aria42.com/software.html for software release.
Daume III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. $$$$$ In this task, the plurality of errors are confusions between the GPE (geo-political entity) and ORG entity types, which have very similar distributions.
Daume III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. $$$$$ “giant” may be a likely head for the Microsoft entity but not all ORGs).
