More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ Wikipedia covers a wider range of sense distributions, whereas Gigaword contains only newswire text and tends to employ fewer senses of most ambiguous words.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ Doing so significantly increases the accuracy of lexical-similarity computation as demonstrated by improved correlation with human similarity judgements and generation of better near synonyms according to human evaluators.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ Our experiments employ a mixture of von MisesFisher distributions (movMF) clustering method with first-order unigram contexts (Banerjee et al., 2005).

Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Such models have been widely studied in the Psychology literature (Griffiths et al., 2007; Love et al., 2004; Rosseel, 2002).
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Given the word cell in the context: “The book was published while Piasecki was still in prison, and a copy was delivered to his cell.” the standard approach produces protein while our method yields incarcerated.
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Rather, we only rely on clusters to capture meaningful variation in word usage.
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ It is possible to circumvent the model-selection problem (choosing the best value of K) by simply combining the prototypes from clusterings of different sizes.

It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ However, due to homonymy and polysemy, capturing the semantics of a word with a single vector is problematic.
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ Table 3 compares the inferred synonyms for several target words, generally demonstrating the ability of the multi-prototype model to improve the precision of inferred near-synonyms (e.g. in the case of singer or need) as well as its ability to include synonyms from less frequent senses (e.g., the experiment sense of research or the verify sense of prove).
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ Rater variance depends more directly on the actual word similarity: word pairs at the extreme ranges of similarity have significantly lower variance as raters are more certain.

As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ 1).
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ Individual exemplars can be quite noisy and the model can incur high computational overhead at prediction time since naively computing the similarity between two words using each occurrence in a textual corpus as an exemplar requires O(n2) comparisons.

In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ MaxSim, on the other hand, only requires a single pair of prototypes to be close for the words to be judged similar.
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ Occurrences are clustered and cluster centroids are used as prototype vectors.

Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ Furthermore, we show that, although performance is sensitive to the number of prototypes, combining prototypes across a large range of clusterings performs nearly as well as the ex-post best clustering.
Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ Given words w and w0, we define two noncontextual clustered similarity metrics to measure similarity of isolated words: where d(·, ·) is a standard distributional similarity measure.

Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Due to its modularity, the multiprototype approach can easily incorporate such advances in order to further improve its effectiveness. synonyms using the single- and multi-prototype approaches (with results merged).
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Current vector-space models of lexical semantics create a single “prototype” vector to represent the meaning of a word.

Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ Doing so significantly increases the accuracy of lexical-similarity computation as demonstrated by improved correlation with human similarity judgements and generation of better near synonyms according to human evaluators.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ Joint model: The current method independently clusters the contexts of each word, so the senses discovered for w cannot influence the senses discovered for w' 7� w. Sharing statistical strength across similar words could yield better results for rarer words.

Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ In AvgSim, word similarity is computed as the average similarity of all pairs of prototype vectors; In MaxSim the similarity is the maximum over all pairwise prototype similarities.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ This result is to be expected since the single prototype mainly reflects the majority sense, preventing it from predicting appropriate synonyms for a minority sense.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.

We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ However, the distributional approach can suffer from poor precision, as thematically similar words (e.g., singer and actor) and antonyms often occur in similar contexts (Lin et al., 2003).
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ In all experiments we prune tf-idf features by their overall weight, taking the top 5000.

Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Note that AvgSim and MaxSim can be thought of as special cases of AvgSimC and MaxSimC with uniform weight to each cluster; hence AvgSimC and MaxSimC can be used to compare words in context to isolated words as well.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Figure 1 gives an overview of this process.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).

The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Our approach is similar to standard vector-space models of word meaning, with the addition of a perword-type clustering step: Occurrences for a specific word type are collected from the corpus and clustered using any appropriate method (§3.1).
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Furthermore, we show that, although performance is sensitive to the number of prototypes, combining prototypes across a large range of clusterings performs nearly as well as the ex-post best clustering.
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ When contextual information is available, AvgSim and MaxSim can be modified to produce more precise similarity computations: where d�,w,k = d(v(c), 7rk(w)) is the likelihood of context c belonging to cluster 7rk(w), and �r(w) def= 7rargmax1<k<K dc,w,k(w), the maximum likelihood cluster for w in context c. Thus, AvgSimC corresponds to soft cluster assignment, weighting each similarity term in AvgSim by the likelihood of the word contexts appearing in their respective clusters.
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ We presented a resource-light model for vectorspace word meaning that represents words as collections of prototype vectors, naturally accounting for lexical ambiguity.

We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ The third edition English Gigaword corpus, with articles containing fewer than 100 words removed, leaving 6.6M articles and 3.913 words (Graff, 2003).
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ The top k most similar words were computed for each prototype of each target word.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012) $$$$$ Individual exemplars can be quite noisy and the model can incur high computational overhead at prediction time since naively computing the similarity between two words using each occurrence in a textual corpus as an exemplar requires O(n2) comparisons.

A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ Based on preliminary experiments comparing various clustering methods, we found movMF gave the best results.
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ Previous work on lexical semantic relatedness has focused on two approaches: (1) mining monolingual or bilingual dictionaries or other pre-existing resources to construct networks of related words (Agirre and Edmond, 2006; Ramage et al., 2009), and (2) using the distributional hypothesis to automatically infer a vector-space prototype of word meaning from large corpora (Agirre et al., 2009; Curran, 2004; Harris, 1954).
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ Most work has also focused on corpus-based distributional approaches, varying the vector-space representation, e.g. by incorporating syntactic and co-occurrence information from the words surrounding the target term (Pereira et al., 1993; Pantel and Lin, 2002).
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.

The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ Such models have been widely studied in the Psychology literature (Griffiths et al., 2007; Love et al., 2004; Rosseel, 2002).
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ Doing so significantly increases the accuracy of lexical-similarity computation as demonstrated by improved correlation with human similarity judgements and generation of better near synonyms according to human evaluators.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ Occurrences are clustered and cluster centroids are used as prototype vectors.

Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ 1).
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ Nonparametric clustering: The success of the combined approach indicates that the optimal number of clusters may vary per word.
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ We presented a resource-light model for vectorspace word meaning that represents words as collections of prototype vectors, naturally accounting for lexical ambiguity.
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ Finally, variance in the prototype similarities is found to correlate with inter-annotator disagreement, suggesting psychological plausibility.

Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ Tversky and Gati (1982) famously showed that conceptual similarity violates the triangle inequality, lending evidence for exemplar-based models in psychology.
Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ However we have not yet evaluated its performance when using more powerful feature representations such those based on Latent or Explicit Semantic Analysis (Deerwester et al., 1990; Gabrilovich and Markovitch, 2007).

 $$$$$ We posit that the finer-grained senses actually capture useful aspects of word meaning, leading to better correlation with WordSim-353.
 $$$$$ MaxSimC corresponds to hard assignment, using only the most probable cluster assignment.
 $$$$$ We present experimental comparisons to human judgements of semantic similarity for both isolated words and words in sentential context.
 $$$$$ A more principled approach to selecting the number of prototypes per word is to employ a clustering model with infinite capacity, e.g. the Dirichlet Process Mixture Model (Rasmussen, 2000).

Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ They are grouped into homonyms (words with very distinct senses) and polysemes (words with related senses).
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ Finally, variance in the prototype similarities is found to correlate with inter-annotator disagreement, suggesting psychological plausibility.
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ However, there are a number of ways it could be improved: Feature representations: Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009).
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ Similarity between two word types is then computed as a function of their cluster centroids (§3.2), instead of the centroid of all the word’s occurrences.
