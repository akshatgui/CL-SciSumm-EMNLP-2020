More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ However we have not yet evaluated its performance when using more powerful feature representations such those based on Latent or Explicit Semantic Analysis (Deerwester et al., 1990; Gabrilovich and Markovitch, 2007).
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ Instead, the standard ... chose Zbigniew Brzezinski for the position of ... ... thus the symbol s position on his clothing was ... ... writes call options against the stock position ... ... offered a position with ... ... a position he would hold until his retirement in ... to near-synonym discovery for a single target word independent of context.
More recently, Reisinger and Mooney (2010) present a method that uses clustering to produce multiple sense-specific vectors for each word. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.

Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ 2.
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ Such a model would allow naturally more polysemous words to adopt more flexible representations.
Following Reisinger and Mooney (2010), we also evaluated mixture models that combine the output of models with varying parameter settings. $$$$$ The similarity between two words in a multiprototype model can be computed straightforwardly, requiring only simple modifications to standard distributional similarity methods such as those presented by Curran (2004).

It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ Note that AvgSim and MaxSim can be thought of as special cases of AvgSimC and MaxSimC with uniform weight to each cluster; hence AvgSimC and MaxSimC can be used to compare words in context to isolated words as well.
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ We employed two corpora to train our models: 29th, 2009.
It is difficult to relate our results to Reisinger and Mooney (2010), due to differences in the training data and the vector representations it gives rise to. $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.

As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ In addition, a context-dependent meaning for a word is determined by choosing one of the vectors in its set based on minimizing the distance to the vector representing the current context.
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ Tversky and Gati (1982) famously showed that conceptual similarity violates the triangle inequality, lending evidence for exemplar-based models in psychology.
As a comparison, a baseline configuration with tf-idf weighting and the cosine similarity measure yields a correlation of 0.38 with our data and 0.49 in Reisinger and Mooney (2010). $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.

In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
In general, our approach is quite close to the multi prototype models of Reisinger and Mooney (2010). $$$$$ Correlations for each dataset are shown in Figure 4 left.

Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ We present experimental comparisons to human judgements of semantic similarity for both isolated words and words in sentential context.
Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ Furthermore, we show that, although performance is sensitive to the number of prototypes, combining prototypes across a large range of clusterings performs nearly as well as the ex-post best clustering.
Reisinger and Mooney (2010b) introduced a multi-prototype VSM where word sense discrimination is first applied by clustering contexts, and then prototypes are built using the contexts of the sense-labeled words. $$$$$ When raters are forced to chose between the top three predictions for each method (presented as top set in Figure 3 left), the effect of this noise is reduced and the multi-prototype approach remains dominant even for a large number of clusters.

Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
Instead of using only one representation per word, Reisinger and Mooney (2010b) proposed the multi prototype approach for vector-space models, which uses multiple representations to capture different senses and usages of a word. $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.

Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ Individual exemplars can be quite noisy and the model can incur high computational overhead at prediction time since naively computing the similarity between two words using each occurrence in a textual corpus as an exemplar requires O(n2) comparisons.
Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included. $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.

Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ Furthermore, we show that, although performance is sensitive to the number of prototypes, combining prototypes across a large range of clusterings performs nearly as well as the ex-post best clustering.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ In AvgSim, word similarity is computed as the average similarity of all pairs of prototype vectors; In MaxSim the similarity is the maximum over all pairwise prototype similarities.
Reisinger and Mooney (2010b) found pruning the low-value tf-idf features helps performance. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.

We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ Note the “hurricane” sense of position (cluster 3) is not typically considered appropriate in WSD. approach is to compute a single prototype vector for each word from its occurrences.
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ When asked to choose between the single best word for each method (top word), the multi-prototype prediction is chosen significantly more frequently (i.e. the result is above 0.5) when the number of clusters is small, but the two methods perform similarly for larger numbers of clusters (Wald test, α = 0.05.)
We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5). $$$$$ Multiple prototypes for each word w are generated by clustering feature vectors v(c) derived from each occurrence c E C(w) in a large textual corpus and collecting the resulting cluster centroids 7rk(w), k E [1, K].

Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Joint model: The current method independently clusters the contexts of each word, so the senses discovered for w cannot influence the senses discovered for w' 7� w. Sharing statistical strength across similar words could yield better results for rarer words.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
Reisinger and Mooney (2010b) combined the two approaches and applied them to vector-space models, which was further improved in Reisinger and Mooney (2010a). $$$$$ Consequently, the model supports judging the similarity of both words in isolation and words in context.

The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Rater variance depends more directly on the actual word similarity: word pairs at the extreme ranges of similarity have significantly lower variance as raters are more certain.
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ The similarity of two isolated words A and B is defined as the minimum distance between one of A’s vectors and one of B’s vectors.
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ Multiple prototypes for each word w are generated by clustering feature vectors v(c) derived from each occurrence c E C(w) in a large textual corpus and collecting the resulting cluster centroids 7rk(w), k E [1, K].
The multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al, 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al, 2010) can be classified into this second category. $$$$$ A more principled approach to selecting the number of prototypes per word is to employ a clustering model with infinite capacity, e.g. the Dirichlet Process Mixture Model (Rasmussen, 2000).

We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ By removing word pairs with similarity judgements in the middle two quartile ranges (4.4 to 7.5) we find significantly higher variance correlation (Figure 4 right).
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ This paper presents a multi-prototype vector space model for lexical semantics with a single parameter K (the number of clusters) that generalizes both prototype (K = 1) and exemplar (K = N, the total number of instances) methods.
We plan to extend this work by taking into account the notion of word sense as it is done in (Reisinger and Mooney, 2010) or (Huang et al, 2012): since we rely on occurrences of words in texts, this extension should be quite straightforward by turning our word-in-context classifiers into true word sense classifiers. $$$$$ Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.

A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ This paper presents a method that uses clustering to produce multiple “sense-specific” vectors for each word.
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.

The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ We would like to thank Katrin Erk for helpful discussions and making the USim data set available.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ We employed two corpora to train our models: 29th, 2009.
The top-down multi-prototype approach determines a number of senses for each word, and then clusters the occurrences of the word (Reisinger and Mooney, 2010) into these senses. $$$$$ This paper presents a multi-prototype vector space model for lexical semantics with a single parameter K (the number of clusters) that generalizes both prototype (K = 1) and exemplar (K = N, the total number of instances) methods.

Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ We next evaluated the multi-prototype approach on its ability to determine the most closely related words for a given target word (using the Wikipedia corpus with tf-idf features).
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ We presented a resource-light model for vectorspace word meaning that represents words as collections of prototype vectors, naturally accounting for lexical ambiguity.
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ Most work has also focused on corpus-based distributional approaches, varying the vector-space representation, e.g. by incorporating syntactic and co-occurrence information from the words surrounding the target term (Pereira et al., 1993; Pantel and Lin, 2002).
Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). $$$$$ Using a forced-choice setup, human subjects were asked to evaluate the quality of these near synonyms relative to those produced by a sincarrier, crane, cell, company, issue, interest, match, media, nature, party, practice, plant, racket, recess, reservation, rock, space, value cause, chance, journal, market, network, policy, power, production, series, trading, train gle prototype.

Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ Experimental comparisons to human judgements of semantic similarity for both isolated words as well as words in sentential contexts demonstrate the superiority of this approach over both prototype and exemplar based vector-space models.
Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ Experiments were run on the Mastodon Cluster, provided by NSF Grant EIA-0303609.
Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ The multi-prototype approach uses word sense discovery to partition a word’s contexts and construct “sense specific” prototypes for each cluster.
Related work to ours is (Reisinger and Mooney, 2010) where exemplars of a word are first clustered and then prototype vectors are built. $$$$$ MaxSim, on the other hand, only requires a single pair of prototypes to be close for the words to be judged similar.

 $$$$$ However, due to lexical ambiguity, encoding word meaning with a single vector is problematic.
 $$$$$ Note that AvgSim and MaxSim can be thought of as special cases of AvgSimC and MaxSimC with uniform weight to each cluster; hence AvgSimC and MaxSimC can be used to compare words in context to isolated words as well.
 $$$$$ Figure 1 gives an overview of this process.
 $$$$$ It is possible to circumvent the model-selection problem (choosing the best value of K) by simply combining the prototypes from clusterings of different sizes.

Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ The similarity between two words in a multiprototype model can be computed straightforwardly, requiring only simple modifications to standard distributional similarity methods such as those presented by Curran (2004).
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ This work was supported by an NSF Graduate Research Fellowship and a Google Research Award.
Although in previous work, researchers try to capture word senses using different vectors (Reisinger and Mooney, 2010) from the same text corpus, this is in fact difficult in practice. $$$$$ Joint model: The current method independently clusters the contexts of each word, so the senses discovered for w cannot influence the senses discovered for w' 7� w. Sharing statistical strength across similar words could yield better results for rarer words.
