Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ We use the entire Timebank Corpus as supervised training data, condensing the before and immediately-before relations into one before relation.
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ The Baseline performs better at first (years 1994-5), but as more data is seen, the Baseline worsens while the Protagonist improves.
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ From each document, the entity involved in the most events was selected as the protagonist.
Chambers and Jurafsky (2008) extracted narrative event chains based on common protagonists. $$$$$ Thus, we calculate the PMI across verbs that share arguments.

These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). $$$$$ This approach is called Protagonist.
These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). $$$$$ We have shown that it is possible to learn narrative event chains unsupervised from raw text.
These knowledge structures, comparable to scripts (Schank and Abelson, 1977) or narrative chains (Chambers and Jurafsky, 2008), describe typical sequences of events in a particular context. Given the number of potential scripts, their development by hand becomes a resource intensive process. In the past, some work has been devoted to automatically construct script-like structures from compiled corpora (Fujiki et al, 2003) (Chambers and Jurafsky, 2008). $$$$$ Of the 69 chains, 6 did not have any ordered events and were removed from the evaluation.

(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. $$$$$ used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge.
(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. $$$$$ Out approach gives higher scores to orders that coincide with the pairwise orderings classified in our gigaword training data.
(Chambers and Jurafsky, 2008) attempt to identify narrative chains in newspaper corpora. $$$$$ Each ranked list of candidate verbs for the missing event in Baseline/Protagonist contained approximately 9 thousand candidates.

This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. $$$$$ The full narrative model that includes the grammatical dependencies is called Typed Deps.
This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. $$$$$ We apply work in the area of temporal classification to create partial orders of our learned events.
This virtue of discourse structure of coherent stories has been described in (Trabasso et al, 1984) and applied by (Fujiki et al, 2003) as subject and object overlap and by (Chambers and Jurafsky, 2008) as following a common protagonist in a story. $$$$$ The narrative cloze is a sequence of narrative events in a document from which one event has been removed.

We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). $$$$$ A narrative chain, by definition, includes a partial ordering of events.
We utilize the definition of PMI described in (Chambers and Jurafsky, 2008). $$$$$ We thus modified the narrative model to ignore typed dependencies, but still count events with shared arguments.

The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ The Baseline performs better at first (years 1994-5), but as more data is seen, the Baseline worsens while the Protagonist improves.
The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ Our experiments found that PMI outperforms the t-test on this task by itself and when interpolated together using various mixture weights.
The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ Each focused protagonist chain offers one perspective on a narrative, similar to the multiple perspectives on a commercial transaction event offered by buy and sell.
The second metric M2 utilizes point wise mutual information as defined in (Chambers and Jurafsky, 2008). $$$$$ We apply work in the area of temporal classification to create partial orders of our learned events.

Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ We want to evaluate temporal order at the narrative level, across all events within a chain.
Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ The next step is to order events in the same narrative chain.
Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ Consider these two distinct narrative chains.
Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using point wise mutual information (PMI) between syntactic positions. $$$$$ In addition, we applied state of the art temporal classification to show that sets of events can be partially ordered.

We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. $$$$$ These terms can capture some narrative relations, but the model requires topic-sorted training data.
We could have obtained a more accurate ordering using a temporal classifier (see Chambers and Jurafsky 2008), however we leave this to future work. $$$$$ Finally, a global narrative score is built such that all events in the chain provide feedback on the event in question (whether for inclusion or for decisions of inference).

Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. $$$$$ If the original document contained (fired obj), this cloze test would score 3.
Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. $$$$$ This showed a 5.7% decrease in the verb-only results.
Chambers and Jurafsky (2008) define their event ranking function based on point wise mutual information. $$$$$ Each arrow indicates a before relation.

We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge.
We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ It is advantageous to consider a space of possible narrative events and the ordering within, not a closed list.
We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ When all training data is used (1994-2004), the average ranked position is 1826 for Baseline and 1160 for Protagonist (1 being most confident).
We follow the approach of Chambers and Jurafsky (2008), evaluating our models for predicting script events in a narrative cloze task. $$$$$ Each ranked list of candidate verbs for the missing event in Baseline/Protagonist contained approximately 9 thousand candidates.

In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ We report 75.2% accuracy, but 22 of the 63 had 5 or fewer pairs of ordered events.
In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ These are often none relations (events that have no explicit relation) or as is often the case, overlap relations where the two events have no Timebank-defined ordering but overlap in time.
In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ We use years 1994-2004 (1,007,227 documents) of the Gigaword Corpus (Graff, 2002) for training2.
In particular, it outperforms the state-of-the-art point wise mutual information method introduced by Chambers and Jurafsky (2008), and it does soby a large margin, more than doubling the Recall@ 50 on the Reuters corpus. $$$$$ We have shown that it is possible to learn narrative event chains unsupervised from raw text.

Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. $$$$$ We report 75.2% accuracy, but 22 of the 63 had 5 or fewer pairs of ordered events.
Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. $$$$$ When all training data is used (1994-2004), the average ranked position is 1826 for Baseline and 1160 for Protagonist (1 being most confident).
Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al, 2004), a knowledge-based strategy. $$$$$ As we would hope, the accuracy improves the larger the ordered narrative chain.

Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ We believe our model provides an important first step toward learning the rich causal, temporal and inferential structure of scripts and frames.
Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ Figures 6 and 7 show two learned chains after clustering and ordering.
Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ Bean and Riloff (2004) proposed the use of caseframe networks as a kind of contextual role knoweldge for anaphora resolution.
Here narrative event chains were defined by Chambers and Jurafsky (2008) as partially ordered sets of events involving the same protagonist. $$$$$ Particles are included with the verb.

Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ We show, using a new evaluation task called narrative cloze, that our protagonist-based method leads to better induction than a verb-only approach.
Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ In summarization, topic signatures are a set of terms indicative of a topic (Lin and Hovy, 2000).
Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ Processing the entire corpus produces a database of event pair counts where confidence of two generic events A and B can be measured by comparing how many before labels have been seen versus their inverted order B and A5.
Fabulas can be viewed as distributions over characters, events and other entities; this conceptualization of what constitutes a narrative is broader than Chambers and Jurafsky (2008). $$$$$ Judgements of coherence can then be made over chains within documents.

Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain $$$$$ Out approach gives higher scores to orders that coincide with the pairwise orderings classified in our gigaword training data.
Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain $$$$$ Duplicate arrows implied by rules of transitivity are removed.
Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain $$$$$ The core employment events are accurate, but clustering included life events (born, died, graduated) from obituaries of which some temporal information is incorrect.
Chambers and Jurafsky (2008) suggested inducing a similar structure called a narrative chain $$$$$ More recently, Brody (2007) proposed an approach similar to caseframes that discovers highlevel relatedness between verbs by grouping verbs that share the same lexical items in subject/object positions.

Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs.
Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ We assume that although a narrative has several participants, there is a central actor who characterizes a narrative chain: the protagonist.
Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ Of the 740 cloze tests, 714 of the removed events were present in their respective list of guesses.
Setup Following Chambers and Jurafsky (2008), we extracted and lemmatized the verbs from the New York Times section of the Gigaword Corpus using the Stanford POS tagger (Toutanova et al, 2004) and the Morphalemmatizer (Minnen et al, 2000). $$$$$ We could remove (threw subject) and use the remaining four events to rank this missing event.

Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). $$$$$ Perhaps due to data sparsity, this produces our best results as reported above.
Triples of verb tokens were sampled at random from the narrative cloze test set of Chambers and Jurafsky (2008). $$$$$ This showed a 5.7% decrease in the verb-only results.

The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). $$$$$ The Baseline performs better at first (years 1994-5), but as more data is seen, the Baseline worsens while the Protagonist improves.
The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). $$$$$ Out approach gives higher scores to orders that coincide with the pairwise orderings classified in our gigaword training data.
The primary research effort in event temporality has gone into ordering events with respect to one another (e.g., Chambers and Jurafsky (2008)), and detecting their typical durations (e.g., Pan et al (2006)). $$$$$ A narrative chain can be viewed as defining the semantic roles of an event, constraining it against roles of the other events in the chain.

Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ After initial evaluations, the baseline was performing very poorly due to the huge amount of data involved in counting all possible verb pairs (using a protagonist vastly reduces the number).
Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ Discrete sets have the drawback of shutting out unseen and unlikely events from consideration.
Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ Brody learns pairwise relations between clusters of related verbs, similar to the results with caseframes.
Utilizing verb co-occurrence at the document level, Chambers and Jurafsky (2008) estimate whether a pair of verbs is narratively related by counting the number of times the verbs share an argument in the same document. $$$$$ Narrative chains (such as Firing of Employee or Executive Resigns) offer the structure and power to directly infer these new subevents by providing critical background knowledge.

Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved. $$$$$ Experiments with varying sizes of training data are presented in figure 3.
Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved. $$$$$ This approach is called Protagonist.
Narrative score Chambers and Jurafsky (2008) suggested a method for learning sequences of actions or events (expressed by verbs) in which a single entity is involved. $$$$$ Formally, a narrative chain is a partially ordered set of narrative events that share a common actor.
