Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ We have in the worst case to visit all |Q |· |I |· |O| (q, i, o) pairs and have all |R |transducer rules match at each of them.
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ This new transducer produces derivation trees on its output instead of normal output trees.
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ The sources of a rule r = (q, l, rhs, w) ∈ R are the inputpath parts of the rhs nonterminals: If the sources of a rule refer to input paths that do not exist in the input, then the rule cannot apply (because a ↓ (i · (1) · i′) would not exist).
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling.

In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ Their generative model provides a formula for P(Japanese string  |English tree), in terms of individual parameters, and their appendix gives special EM re-estimation formulae for maximizing the product of these conditional probabilities across the whole tree/string corpus.
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ We note that this algorithm subsumes normal inside-outside training of PCFG on strings (Lari and Young, 1990), since we can always fix the input tree to some constant for all training examples.
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted).

Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ A rule says that to transform (with weight w) an input subtree matching pattern while in state q, replace it by the string of rhs with its nonterminal (Q × paths) letters replaced by their (recursive) transformation. xRS is the same as xR, except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves (so the intermediate derivation objects
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted).

In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ Each EM iteration takes time linear in the size of the transducer and linear in the size of the derivation tree grammars for the training examples.
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ (Eisner, 2003) discusses training for Synchronous TSG.
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ : - r NP(x0:CD, x1:NN) → q.NP.CD x0, q.NP.NN x1 - r NP(x0:CD, x1:NN) → q.NP.NN x1, q.NP.CD x0 The rhs sends the child subtrees back to state q for recursive processing.
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ It is possible to cast many current probabilistic natural language models as R-type tree transducers.

Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.
Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ Finally, given inside and outside weights, the sum of weights of trees using a particular production is γG((n, r, w) ∈ P) ≡ αG(n) · w · βG(r).
Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ Each EM iteration takes time linear in the size of the transducer and linear in the size of the derivation tree grammars for the training examples.
Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.

Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ It can copy whole subtrees, and transform those subtrees differently.
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ For a corpus of K examples with average input/output size M, an iteration takes (at worst) O(|Q |· |R |· K · M2) time—quadratic, like the forward-backward algorithm.
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).

Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ A weighted extended-lhs root-to-frontier tree transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATE × To(Q × paths) × IIB+ is a finite set of weighted transformation rules, written (q, pattern) →w rhs, meaning that an input subtree matching pattern while in state q is transformed into rhs, with Q × paths leaves replaced by their (recursive) transformations.
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ Algorithm 2 implements EM xR training, repeatedly computing inside-outside weights (using fixed transducer derivation wRTGs for each input/output tree pair) to efficiently sum each parameter contribution to likelihood over all derivations.
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ A frontier is a set of paths f that are pairwise prefix-independent: A frontier of t is a frontier f ⊆ pathst.
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string.

The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ We define xR, a convenience-oriented generalization of weighted R. Because of its good fit to natural language problems, xR is already briefly touched on, though not defined, in (Rounds, 1970).
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Much of natural language work over the past decade has employed probabilistic finite-state transducers (FSTs) operating on strings.
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Finally, given inside and outside weights, the sum of weights of trees using a particular production is γG((n, r, w) ∈ P) ≡ αG(n) · w · βG(r).

It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ Many probabilistic models for natural language are now written in terms of hierarchical tree structure.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ An input tree is transformed by starting at the root in the initial state, and recursively applying outputgenerating rules to a frontier of (copies of) input subtrees (each marked with their own state), until (in a complete derivation, finishing at the leaves with terminal rules) no states remain.

While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ However, language problems like machine translation break this mold, because they involve massive reordering of symbols, and because the transformation processes seem sensitive to hierarchical tree structure.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ 1 if di,o = false then warn(more rules are needed to explain (i,o)) compute inside/outside weights for di,o and remove all useless nonterminals n whose βdi,o(n) = 0 or αdi,o(n) = 0 itno ← 0, lastL ← −∞, δ ← ǫ for r = (q, pat, rhs, w) ∈ R do wr ← w
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ This new transducer produces derivation trees on its output instead of normal output trees.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ We note that this algorithm subsumes normal inside-outside training of PCFG on strings (Lari and Young, 1990), since we can always fix the input tree to some constant for all training examples.

Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ The weighted tree transduction given by X is XX ≡ {(i, o, w) ∈ TE × To × R+|WX(i, o) = w}.
Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ are strings containing state-marked input subtrees).

WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ For example, R is not closed under composition (Rounds, 1970), and neither are RL or F (the “frontier-to-root” cousin of R), but the non-copying FL is closed under composition.
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ A rule says that to transform (with weight w) an input subtree matching pattern while in state q, replace it by the string of rhs with its nonterminal (Q × paths) letters replaced by their (recursive) transformation. xRS is the same as xR, except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves (so the intermediate derivation objects
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ Each iteration is guaranteed to increase the likelihood until a local maximum is reached.

Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ Third, we can easily extend the model in interesting ways.
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ X′ is That is, the original rhs of rules are flattened into a tree of depth 1, with the root labeled by the original rule, and all the non-expanding A-labeled nodes of the rhs removed, so that the remaining children are the nonterminal yield in left to right order.
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ We have in the worst case to visit all |Q |· |I |· |O| (q, i, o) pairs and have all |R |transducer rules match at each of them.
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ Formally, a weighted extended-lhs root-to-frontier tree-to-string transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATΣ × (A ∪(Q × paths))⋆ × R+ are a finite set of weighted transformation rules, written (q, pattern) →w rhs.

The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0).
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ The tree S(NP(DT(the), N(sons)), VP(V(run))) comes out with probability 0.3.

The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ The size of the derivation trees is at worst O(|Q|·|I|·|O|·|R|).
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ are strings containing state-marked input subtrees).
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ For a corpus of K examples with average input/output size M, an iteration takes (at worst) O(|Q |· |R |· K · M2) time—quadratic, like the forward-backward algorithm.

Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ In the automata literature, these were first called generalized syntax-directed translations (Aho and Ullman, 1971) and used to specify compilers.
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ 1 if di,o = false then warn(more rules are needed to explain (i,o)) compute inside/outside weights for di,o and remove all useless nonterminals n whose βdi,o(n) = 0 or αdi,o(n) = 0 itno ← 0, lastL ← −∞, δ ← ǫ for r = (q, pat, rhs, w) ∈ R do wr ← w
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Our training algorithm is a generalization of forwardbackward EM training for finite-state (string) transducers, which is in turn a generalization of the original forwardbackward algorithm for Hidden Markov Models.
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Input: xR transducer X = (E, A, Q, Qd, R), observed weighted tree pairs T ∈ TE × TA × R+, normalization function Z({countr  |r ∈ R}, r′ ∈ R), minimum relative log-likelihood change for convergence ǫ ∈ R+, maximum number of iterations maxit ∈ N, and prior counts (for a so-called Dirichlet prior) {priorr  |r ∈ R} for smoothing each rule.

Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ The weighted regular tree language produced by G is LG ≡ {(t,w) ∈ TE × IIB+  |WG(t) = w}.
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ Many probabilistic models for natural language are now written in terms of hierarchical tree structure.

Such an algorithm is presented by Graehl and Knight (2004). $$$$$ Input: xR transducer X = (E, A, Q, Qd, R), observed weighted tree pairs T ∈ TE × TA × R+, normalization function Z({countr  |r ∈ R}, r′ ∈ R), minimum relative log-likelihood change for convergence ǫ ∈ R+, maximum number of iterations maxit ∈ N, and prior counts (for a so-called Dirichlet prior) {priorr  |r ∈ R} for smoothing each rule.
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ First, R productions have no lookahead capability—the left-handside of the S production consists only of q S(x0, x1), although we want our English-to-Arabic transformation to apply only when it faces the entire structure q S(PRO, VP(V, NP)).
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ Multiple initial states are not needed: we can use a single start state Qi, and instead of each initial state q with starting weight w add the rule (Qi, TRUE) →w (q, ()) (where TRUE(t) ≡ 1, ∀t).
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ (Eisner, 2003) discusses training for Synchronous TSG.

We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990).
We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ A rule says that to transform (with weight w) an input subtree matching pattern while in state q, replace it by the string of rhs with its nonterminal (Q × paths) letters replaced by their (recursive) transformation. xRS is the same as xR, except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves (so the intermediate derivation objects
We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ Leaves are nodes with no children.

The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0).
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Each EM iteration takes time linear in the size of the transducer and linear in the size of the derivation tree grammars for the training examples.
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Computing αG and βG for nonrecursive wRTG is a straightforward translation of the above recursive definitions (using memoization to compute each result only once) and is O(|G|) in time and space.
