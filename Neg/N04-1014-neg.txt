Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ (Eisner, 2003) discusses training for Synchronous TSG.
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ We make two copies of the English VP, and assign them different states: While general properties of R are understood, there are many algorithmic questions.
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ The path to the root is the empty sequence (), and p1 extended by p2 is p1 · p2, where · is concatenation.
Graehl and Knight (2004) defined training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.

In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ are strings containing state-marked input subtrees).
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ In the traditional statement of R, sources(rhs) is always {(1), ... , (n)}, writing xi instead of (i), but in xR, we identify mapped input subtrees by arbitrary (finite) paths.
In our experiments, we use a translation model based on T2S tree transducers (Graehl and Knight,2004), constructed using the Travatar toolkit (Neu big, 2013). $$$$$ XRPATE is the set of finite tree patterns: predicate functions f : TE → {0, 1} that depend only on the label and rank of a finite number of fixed paths their input. xR is the set of all such transducers.

Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ The path to the root is the empty sequence (), and p1 extended by p2 is p1 · p2, where · is concatenation.
Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. $$$$$ The tree S(NP(DT(the), N(sons)), VP(V(run))) comes out with probability 0.3.

In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ are strings containing state-marked input subtrees).
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ The tree S(NP(DT(the), N(sons)), VP(V(run))) comes out with probability 0.3.
In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). $$$$$ The inside and outside probabilities of this packed derivation structure are used to compute expected counts of the productions from the original, given transducer (Sections 6-7).

Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ Third, we can easily extend the model in interesting ways.
Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990).
Graehl and Knight (2004) present an implementation that runs in time O (V log V+ E) using the method described in Algorithm 5 to ensure that every hyperedge is visited only once (assuming the priority queue is implemented as a Fibonaaci heap; for binary heap, it runs in O ((V+ E) log V)). $$$$$ We now turn to tree-to-string transducers (xRS).

Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ Figure 3 shows derivation trees for a particular transducer.
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ A rule says that to transform (with weight w) an input subtree matching pattern while in state q, replace it by the string of rhs with its nonterminal (Q × paths) letters replaced by their (recursive) transformation. xRS is the same as xR, except that the rhs are strings containing some nonterminals instead of trees containing nonterminal leaves (so the intermediate derivation objects
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ We give an explicit tree-to-string transducer example in the next section.
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers. $$$$$ As organized in the rest of this paper, we accomplish this by intersecting the given transducer with each input/output pair in turn.

Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ A weighted regular tree grammar (wRTG) G is a quadruple (E, N, S, P), where E is the alphabet, N is the finite set of nonterminals, S ∈ N is the start (or initial) nonterminal, and P ⊆ N ×TΣ(N)×R+ is the finite set of weighted productions (R+ ≡ {r ∈ R  |r > 0}).
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ The inside weights using G are given by βG : TE → (R−R−), giving the sum of weights of all tree-producing derivatons from trees with nonterminal leaves: By definition, βG(S) gives the sum of the weights of all trees generated by G. For the wRTG generated by DERIV(X, I, O), this is exactly WX(I, O).
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ Given a wRTG G = (E, N, S, P), we can compute the sums of weights of trees derived using each production by adapting the well-known inside-outside algorithm for weighted context-free (string) grammars (Lari and Young,1990).
Graehl and Knight (2004) proposed the use of target tree-to-source-string transducers (xRs) to model translation. $$$$$ We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string.

The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ For every weighted context-free grammar, there is an equivalent wRTG that produces its weighted derivation trees with yields being the string produced, and the yields of regular tree grammars are context free string languages (Gécseg and Steinby, 1984).
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Many probabilistic models for natural language are now written in terms of hierarchical tree structure.
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ If enumerating rules matching transducer input-patterns and output-subtrees has cost L (constant given a transducer), then DERIV has time complexity O(L · |Q |· |I |· |O |· |R|).
The recomposed templates are then re-estimated using the EM algorithm described in Graehl and Knight (2004). $$$$$ Because the transformations of an input subtree depend only on that subtree and its state, we can (Algorithm 1) build a compact wRTG that produces exactly the weighted derivation trees corresponding to Xtransductions (I, ()) �� X(O, h) (with weight equal to wX(h)).

It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ What is sometimes called a forest in natural language generation (Langkilde, 2000; Nederhof and Satta, 2002) is a finite wRTG without loops, i.e., ∀n ∈ N(n, ()) ⇒∗G (t, h) =⇒ pathst({n}) = ∅.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ We have developed an xRS training procedure similar to the xR procedure, with extra computational expense to consider how different productions might map to different spans of the output string.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ The derived transducer X′ nicely produces derivation trees for a given input, but in explaining an observed (input/output) pair, we must restrict the possibilities further.
It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, since their theory has been worked out in an extensive literature and is well understood (see, e.g., (Graehl and Knight, 2004)). $$$$$ The size of the derivation trees is at worst O(|Q|·|I|·|O|·|R|).

While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ A weighted extended-lhs root-to-frontier tree transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATE × To(Q × paths) × IIB+ is a finite set of weighted transformation rules, written (q, pattern) →w rhs, meaning that an input subtree matching pattern while in state q is transformed into rhs, with Q × paths leaves replaced by their (recursive) transformations.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ We give an explicit tree-to-string transducer example in the next section.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ Tree substitution grammars or TSG (Schabes, 1990) are equivalent to regular tree grammars. xR transducers are similar to (weighted) Synchronous TSG, except that xR can copy input trees (and transform the copies differently), but does not model deleted input subtrees.
While it is infeasible to enumerate the millions of derivations in each forest, Graehl and Knight (2004) demonstrate an efficient algorithm. $$$$$ The nodes of a tree t are identified one-to-one with its paths: pathst ⊂ paths ≡ N* ≡ U i=0 Ni (A0 ≡ {()}).

Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ The weighted regular tree language produced by G is LG ≡ {(t,w) ∈ TE × IIB+  |WG(t) = w}.
Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ It is possible to cast many current probabilistic natural language models as R-type tree transducers.
Graehl and Knight (2004) and Melamed (2004), propose methods based on tree-to-tree mappings. $$$$$ Formally, a weighted extended-lhs root-to-frontier tree-to-string transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATΣ × (A ∪(Q × paths))⋆ × R+ are a finite set of weighted transformation rules, written (q, pattern) →w rhs.

WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ We note that this algorithm subsumes normal inside-outside training of PCFG on strings (Lari and Young, 1990), since we can always fix the input tree to some constant for all training examples.
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ Each iteration is guaranteed to increase the likelihood until a local maximum is reached.
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ Tree substitution grammars or TSG (Schabes, 1990) are equivalent to regular tree grammars. xR transducers are similar to (weighted) Synchronous TSG, except that xR can copy input trees (and transform the copies differently), but does not model deleted input subtrees.
WXTTs have been proposed by Graehl and Knight (2004) and Knight (2007) and are rooted in similar devices introduced earlier in the formal language literature (Arnold and Dauchet, 1982). $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ If we think of strings written down vertically, as degenerate trees, we can convert any FST into an R transducer by automatically replacing FST transitions with R productions. and state-based record keeping.
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ The size of the derivation trees is at worst O(|Q|·|I|·|O|·|R|).
Note that our semantics is equivalent to the classical term rewriting semantics, which is presented by Graehl and Knight (2004) and Graehl et al (2008), for example. $$$$$ are strings containing state-marked input subtrees).

The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted).
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ The nodes of a tree t are identified one-to-one with its paths: pathst ⊂ paths ≡ N* ≡ U i=0 Ni (A0 ≡ {()}).
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ Tree-based modeling still lacks many of the standard tools taken for granted in (finitestate) string-based modeling.

The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ Input: xR transducer X = (E, A, Q, Qd, R), observed weighted tree pairs T ∈ TE × TA × R+, normalization function Z({countr  |r ∈ R}, r′ ∈ R), minimum relative log-likelihood change for convergence ǫ ∈ R+, maximum number of iterations maxit ∈ N, and prior counts (for a so-called Dirichlet prior) {priorr  |r ∈ R} for smoothing each rule.
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ This new transducer produces derivation trees on its output instead of normal output trees.
The standard inside outside algorithm (Graehl and Knight, 2004) can be used to compute the expected counts of the TTS templates. $$$$$ Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted).

Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ The theory of tree transducer automata provides a possible framework to draw on, as it has been worked out in an extensive literature.
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ A weighted extended-lhs root-to-frontier tree transducer X is a quintuple (E, A, Q, Qi, R) where E is the input alphabet, and A is the output alphabet, Q is a finite set of states, Qi ∈ Q is the initial (or start, or root) state, and R ⊆ Q × XRPATE × To(Q × paths) × IIB+ is a finite set of weighted transformation rules, written (q, pattern) →w rhs, meaning that an input subtree matching pattern while in state q is transformed into rhs, with Q × paths leaves replaced by their (recursive) transformations.
Graehl and Knight (2004) describe training and decoding algorithms for both generalized tree-to-tree and tree-to-string transducers. $$$$$ Because the transformations of an input subtree depend only on that subtree and its state, we can (Algorithm 1) build a compact wRTG that produces exactly the weighted derivation trees corresponding to Xtransductions (I, ()) �� X(O, h) (with weight equal to wX(h)).

Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ Output: New rule weights W ≡ {wr  |r ∈ R}. begin for (i, o, w) ∈ T do di,o ← DERIV(X, i, o)//Alg.
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ In this section, we implement the translation model of (Yamada and Knight, 2001).
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ Fort ∈ TΣ, u ∈ E, u(t) is the tree whose root has label u and whose single child is t. The yield of X in t is yieldt(X), the string formed by reading out the leaves labeled with X in left-to-right order.
Graehl and Knight (2004) describe methods for training tree transducers. $$$$$ It can also delete subtrees without inspecting them (imagine by analogy an FST that quits and accepts right in the middle of an input string).

Such an algorithm is presented by Graehl and Knight (2004). $$$$$ We have in the worst case to visit all |Q |· |I |· |O| (q, i, o) pairs and have all |R |transducer rules match at each of them.
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ Derivation trees for a transducer X = (E, A, Q, Qi, R) are trees labeled by rules (R) that dictate the choice of rules in a complete X-derivation.
Such an algorithm is presented by Graehl and Knight (2004). $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.

We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-totree and tree-to-string transducers.
We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ In the automata literature, these were first called generalized syntax-directed translations (Aho and Ullman, 1971) and used to specify compilers.
We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ For example, the tree t = S(NP, VP(V, NP)) has labelandrankt((2)) = (VP, 2) and labelandrankt((2, 1)) = (V, 0).
We initially attempted to use the top-down DERIV algorithm of Graehl and Knight (2004), but as the constraints of the derivation forests are largely lexical, too much time was spent on exploring dead ends. $$$$$ Space limitations prohibit a detailed description; we refer the reader to a longer version of this paper (submitted).

The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Our problem statement is: Given (1) a particular transducer with productions P, and (2) a finite training set of sample input/output tree pairs, we want to produce (3) a probability estimate for each production in P such that we maximize the probability of the output trees given the input trees.
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ are strings containing state-marked input subtrees).
The actual running of EM iterations (which directly implements the TRAIN algorithm of Graehl and Knight (2004)). $$$$$ Each EM iteration takes time linear in the size of the transducer and linear in the size of the derivation tree grammars for the training examples.
