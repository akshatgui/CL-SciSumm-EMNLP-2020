 $$$$$ 3) 4 f is impressive too with another 5.5% performance improvement.
 $$$$$ 4 f is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.).
 $$$$$ Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.

Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ The rest of the features distinguish types of capitalization and all other words such as punctuation marks.
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence ti−1ti is valid / invalid while &quot;valid on&quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ 2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ 4) However, 3 f contributes only further 1.2% to the performance.

However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ The first eleven features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ Section 5 gives the experimental results of our system.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ During decoding, the NEs already recognized from the document are stored in a list.

Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ Moreover, the performance is even consistently better than those based on handcrafted rules.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ Assumption (2-2) is much looser than assumption (I-1) because assumption (I-1) has the same effect with the sum of assumptions (2-2) and (I-3)2.

The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ It also shows that our NER system can reach &quot;near human performance&quot;.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.

Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ Gazetters have been widely used in NER systems to improve performance.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ This may be because of the complex word feature and the corresponding sparseness problem existing in our system.

For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.
For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.
For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ Moreover, the performance is consistently better than those based on handcrafted rules.

The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.
The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.

In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ Moreover, the performance is even consistently better than those based on handcrafted rules.

Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ In the near feature, we would like to incorporate the following into our system:
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Here, a NE is regarded as a chunk, named &quot;NE-Chunk&quot;.
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Assumption (2-2) is much looser than assumption (I-1) because assumption (I-1) has the same effect with the sum of assumptions (2-2) and (I-3)2.

This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ Moreover, the performance is even consistently better than those based on handcrafted rules.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.

Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ With any learning technique, one important question is how much training data is required to achieve acceptable performance.
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (  |) Then we assume conditional probability word sequence and F1n = f 1 f2 ... fn is the word-feature sequence.

In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ 2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ Another difference is that our model assumes mutual information independence (2-2) while traditional HMM assumes conditional probability independence (I-1).

Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.

An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ In this way, our model can apply more context information to determine the tag of current token.
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.

Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ Our model captures three types of internal sub-features: 1) 1 f : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2: internal semantic feature of important triggers; 3) f 3: internal gazetteer feature. f is the basic sub-feature exploited in this model, as shown in Table 2 with the descending order of priority.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ This sub-feature is unique to our system.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ 2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .

Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ Gazetters have been widely used in NER systems to improve performance.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.

The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ However, the experimental result is disappointing that incorporation of POS even decreases the performance by 2%.
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ 's system in [Sekine98] and SRA's system in [Bennett+97]).
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.

In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ In particular, the FirstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that AllCaps and CapPeriod are computed before FirstWord, and take precedence.)
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ From equation (2-4), we can see that: We will not discuss both the first and second items further in this paper.

Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ In this way, the NER problem can be resolved effectively.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.
