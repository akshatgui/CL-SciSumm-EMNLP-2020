 $$$$$ It also shows that our system still has some room for performance improvement.
 $$$$$ In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &quot;valid&quot; / &quot;invalid&quot; means the tag sequence ti−1ti is valid / invalid while &quot;valid on&quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .
 $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.

Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ This may be because information included in 3 f has already been captured by 2 f and f4 .
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ Given a token sequence G1n = g1g2 g , the goal The second item in (2-1) is the mutual information between T1n and n simplify the computation of this item, we assume mutual information independence: The basic premise of this model is to consider the raw text, encountered when decoding, as though it had passed through a noisy channel, where it had been originally marked with NE tags.
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ The result is shown in Figure 2 for MUC-7 NE task.
Zhou and Su (2002) integrated four different kinds of features, which convey different semantic information, for a classification model based on the Hidden Markov Model (HMM). $$$$$ With any learning technique, one important question is how much training data is required to achieve acceptable performance.

However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ This sub-feature is language dependent.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ 4) However, 3 f contributes only further 1.2% to the performance.
However, Zhou and Su (2002) have reported state of the art results on the MUC-6 and MUC-7 data using a HMM-based tagger. $$$$$ Table 8 answers the question on MUC-7 NE task: 1) Applying only 1 f gives our system the performance of 77.6%.

Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ This sub-feature is language dependent.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher.
Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. $$$$$ In the near feature, we would like to incorporate the following into our system:

The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ The layout of this paper is as follows.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.
The additional orthographic features have proved useful in other systems, for example Carreras et al (2002), Borthwick (1999) and Zhou and Su (2002). $$$$$ Moreover, the performance is consistently better than those based on handcrafted rules.

Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ The first eleven features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ The layout of this paper is as follows.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ It also shows that our NER system can reach &quot;near human performance&quot;.
Among them, NE recognition, part-of-speech tagging and text chunking adopt the same HMM based engine with error-driven learning capability (Zhou and Su, 2002). $$$$$ 4) However, 3 f contributes only further 1.2% to the performance.

For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .
For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ NER can also be used as the first step in a chain of processors: a next level of processing could relate two or more NEs, or perhaps even give semantics to that relationship using a verb.
For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). $$$$$ The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text.

The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ Microsoft, IBM and Bach (a composer), which are introduced in texts without much helpful context.
The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (  |) Then we assume conditional probability word sequence and F1n = f 1 f2 ... fn is the word-feature sequence.
The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .
The named entity recognition component (Zhou and Su 2002) recognizes various types of MUC-style named entities, that is, organization, location, person, date, time, money and percentage. $$$$$ In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .

In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ Section 2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ As stated above, token is denoted as ordered pairs of word-feature and word itself: gi =< fi , wi > .
In this paper, it is tackled by a named entity recognition component, as in Zhou and Su (2002), using the following name alias algorithm in the ascending order of complexity. $$$$$ It also shows that our NER system can reach &quot;near human performance&quot;.

Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ It is based on the intuitions that important triggers are useful for NER and can be classified according to their semantics.
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ It shows that the performance is significantly better than reported by any other machine-learning system.
Research on named-entity recognition was addressed in the nineties at the Message Understanding Conferences (Chinchor, 1998) and is continued for example in (Zhou and Su, 2002). $$$$$ Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.

This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ It is obvious that our generative model is reverse to the generative model of traditional HMM1, as used in BBN's IdentiFinder, which models the original process that generates the NE-class annotated words from the original NE tags.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ It also shows that our NER system can reach &quot;near human performance&quot;.
This will be done by integrating the relation extraction system with our previously developed NER system as described in Zhou and Su (2002). $$$$$ Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature.

Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ In this way, the NER problem can be resolved effectively.
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.
Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.

In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ In this way, the NER problem can be resolved effectively.
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ Fortunately, the feature computation is an extremely small part of the implementation.
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ As stated above, token is denoted as ordered pairs of word-feature and word itself: gi =< fi , wi > .
In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. $$$$$ The atomic elements of information extraction -- indeed, of language as a whole -- could be considered as the &quot;who&quot;, &quot;where&quot; and &quot;how much&quot; in a sentence.

Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ Here, a NE is regarded as a chunk, named &quot;NE-Chunk&quot;.
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ In the near feature, we would like to incorporate the following into our system:
Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). $$$$$ The layout of this paper is as follows.

An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.
An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). $$$$$ As stated above, token is denoted as ordered pairs of word-feature and word itself: gi =< fi , wi > .

Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ From equation (2-4), we can see that: We will not discuss both the first and second items further in this paper.
Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). $$$$$ In this way, our model can apply more context information to determine the tag of current token.

Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher.
Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). $$$$$ Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence.

The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &quot;none-of-the-above&quot;.
The reported result of the simple deterministic features used in MUC can achieve F measure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. $$$$$ This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities.

In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ It shows that the performance is significantly better than reported by any other machine-learning system.
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ 4 f is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.).
In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). $$$$$ More generally how does the performance vary as the training data size changes?

Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ It is obvious that our generative model is reverse to the generative model of traditional HMM1, as used in BBN's IdentiFinder, which models the original process that generates the NE-class annotated words from the original NE tags.
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ 2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .
Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). $$$$$ This may be because of the complex word feature and the corresponding sparseness problem existing in our system.
