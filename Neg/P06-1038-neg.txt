 $$$$$ Categories are generated using a novel graph clique-set algorithm.
 $$$$$ We first define the singlepattern graph G(P) as follows.
 $$$$$ A nice verb category example is {dive, snorkel, swim, float, surf, sail, canoe, kayak, paddle, tube, drift}.

In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. $$$$$ Manual compilation of lexical resources is labor intensive, error prone, and susceptible to arbitrary human decisions.
In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. $$$$$ Evaluation is presented in Section 5, and a discussion in Section 6.
In Section 2, we describe a corpus statistics approach, previously applied for web mining (Davidov and Rappoport, 2006), which we extend for relation discovery. $$$$$ We find all strong n-cliques (subgraphs containing n nodes that are all bidirectionally interconnected.)

To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). $$$$$ Our first step is the discovery of patterns that are useful for lexical category acquisition.
To discover them, we use a slightly modified version of the method presented in (Davidov and Rappoport, 2006). $$$$$ Evaluation is presented in Section 5, and a discussion in Section 6.

It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). $$$$$ This metric was reported to be 82% in (Widdows and Dorow, 2002).
It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). $$$$$ It should be noted that our algorithm can be viewed as one for automatic discovery of word senses, because it allows a word to participate in more than a single category.
It also significantly outperforms the single-language pattern-based method introduced by (Davidov and Rappoport, 2006), which achieves average precision of 79.3 on a similar set in English (in comparison to 86.7 in this study). $$$$$ Additional patterns include ‘from x to y’, ‘x and/or y’ (punctuation is treated here as white space), ‘x and a y’, and ‘neither x nor y’.

Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ When merged properly, the different categories containing a word can be viewed as the set of its senses.
Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ The technique is also quite demanding computationally.
Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ Such matrix decomposition is computationally heavy and has not been proven to scale well when the number of words assigned to categories grows.
Following (Davidov and Rappoport, 2006), we classified words into high-frequency words (HFWs) and content words (CWs). $$$$$ We discover categories of different parts of speech.

Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. $$$$$ Our algorithm is also easy to implement.
Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. $$$$$ Computational efficiency and specifically lack of annotation are important criteria, because they allow usage of huge corpora, which are presently becoming available and growing in size.
Unlike (Davidov and Rappoport, 2006), we consider all punctuation characters as HFWs. $$$$$ We are planning an evaluation according to this measure after improving the merge stage.

To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). $$$$$ They report good results.
To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). $$$$$ In order to find a usable subset, we focus on the symmetric patterns.
To specify patterns, following (Davidov and Rappoport, 2006) we classify words into high frequency words (HFWs) and content words (CWs). $$$$$ For example, any pattern of the form CHC is contained in a pattern of the form HCHC (‘x and y’, ‘both x and y’.)

We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. $$$$$ They also introduce an elegant graph representation that we adopted.
We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. $$$$$ We evaluate our algorithm on very large corpora in two languages, using both human judgments and WordNetbased evaluation.
We discover such words by scanning our corpora and querying the web for symmetric patterns (obtained automatically from the corpus as in (Davidov and Rappoport, 2006)) that contain w1 or w2. $$$$$ Our approach to category discovery is based on connectivity structures in the all-pattern word relationship graph G, resulting from merging all of the single-pattern graphs into a single unified graph.

Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ Our human-evaluated and WordNet-based results are better than the baseline and previous work respectively.
Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ Our algorithm was evaluated using both human judgment and automatic comparisons with WordNet, and results were superior to previous work (although it used a POS tagged corpus) and more efficient computationally.
Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ We did this in order to disambiguate our categories.
Davidov and Rappoport (2006) developed a framework which discovers concepts based on high frequency words and symmetry-based pattern graph properties. $$$$$ Our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words.

Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ We have presented a novel method for patternbased discovery of lexical semantic categories.
Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ We first define the singlepattern graph G(P) as follows.
Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ It should be noted that our algorithm can be viewed as one for automatic discovery of word senses, because it allows a word to participate in more than a single category.
Our algorithm is based on the concept acquisition method of (Davidov and Rappoport, 2006). $$$$$ The Table also shows the number of WN words (:WN), in order to get a feeling by how much WN could be improved here.

 $$$$$ Manual compilation of lexical resources is labor intensive, error prone, and susceptible to arbitrary human decisions.
 $$$$$ In order to cover as many words as possible, we use the smallest clique, a single symmetric arc.
 $$$$$ This process has only a negligible effect on running times, because each iteration is run only on a single window, not on the whole corpus.
 $$$$$ For identifying symmetric patterns, we use a version of the graph representation of (Widdows and Dorow, 2002).

Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ (Sch¨utze, 1998; Deerwester et al, 1990).
Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ Our human-evaluated and WordNet-based results are better than the baseline and previous work respectively.
Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ Patterns are a common approach in lexical acquisition.
Our basic comparison was to (Davidov and Rappoport, 2006) (we have obtained their data and utilized their algorithm), where we can estimate if incorporation of parser data can solve some fundamental weaknesses of their framework. $$$$$ We have presented a novel method for patternbased discovery of lexical semantic categories.

The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. $$$$$ Our fully unsupervised results are superior to previous work that used a POS tagged corpus, and computation time for huge corpora are orders of magnitude faster than previously reported.
The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. $$$$$ We took the exact 10 WN subsets referred to as ‘subjects’ in (Widdows and Dorow, 2002), and removed all multi-word items.
The Russian corpus (Davidov and Rappoport, 2006) was assembled from web-based Russian repositories, to yield 33GB and 4G words. $$$$$ We used 8 subjects for evaluation of the English categories and 15 subjects for evaluation of the Russian ones.

All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ It should be noted that our algorithm can be viewed as one for automatic discovery of word senses, because it allows a word to participate in more than a single category.
All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ We did not know why these words were grouped together; if asked in an evaluation, we would give the category a very low score.
All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ Categories are generated using a novel graph clique-set algorithm.
All of these corpora were also used by (Davidov and Rappoport, 2006) and BNC was used in similar settings by (Widdows and Dorow, 2002). $$$$$ The reasoning here is that if a pattern allows a large percentage of words to participate in both slots, its chances of being a symmetric pattern are greater: M1 filters well patterns that connect words having different parts of speech.

We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering.
We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ 20 of our categories were selected at random from the non-overlapping categories we have discovered, and three words were selected from each of these at random.
We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ A nice verb category example is {dive, snorkel, swim, float, surf, sail, canoe, kayak, paddle, tube, drift}.
We have improved the evaluation framework for Russian by using the Russian WordNet (Gelfenbeyn and et al, 2003) instead of back-translations as done in (Davidov and Rappoport, 2006). $$$$$ Among the patterns discovered are the ubiquitous ‘x and y’, ‘x or y’ and many patterns containing them.

We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ Nodes A and C appear in both categories.
We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ For the baseline, we implemented k-means as follows.
We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ Lexical acquisition algorithms are notoriously hard to evaluate.
We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006). $$$$$ Named Entity Recognition can be viewed as an instance of our problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using coclustering.

It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ We will relax these constraints in future papers.
It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ Our approach towards unsupervised pattern induction is to find such words and utilize them.
It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ However, we are not aware of work in this direction that has been evaluated with good results on lexical category acquisition.
It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics). $$$$$ We present a novel approach for discovering word categories, sets of words sharing a significant aspect of their meaning.

Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ In general, the notion of a category is a fundamental one in cognitive psychology (Matlin, 2005).
Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ In order to focus on patterns that are more likely to provide high quality categories, we removed patterns that appear in the corpus less than TP times per million words.
Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al, 2004).
Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes. $$$$$ Patterns are a common approach in lexical acquisition.

For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ Both use information obtained by parsing.
For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ In this section we describe the graph clique-set method for generating initial categories, and category pruning techniques for increased quality.
For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ Second, given two categories Q, R, we merge them iff there’s more than a 50% overlap between them: (|Q n R |> |Q|/2) n (|Q n R |> |R|/2).
For automated extraction of patterns, we followed the pattern definitions given in (Davidov and Rappoport, 2006). $$$$$ We are planning an evaluation according to this measure after improving the merge stage.

Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs. $$$$$ Symmetric patterns are then identified using graph-based measures, and word categories are created based on graph clique sets.
Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs. $$$$$ For example, suppose a graph G containing five nodes and seven arcs that define exactly three strongly connected triangles, ABC, ABD, ACE.
Unlike (Davidov and Rappoport, 2006), we consider all single punctuation characters or consecutive sequences of punctuation characters as HFWs. $$$$$ In order to cover as many words as possible, we use the smallest clique, a single symmetric arc.
