 $$$$$ We constructed a class bigram model, using absolute interpolation with a singleton generalised distribution for the transition weights, and using absolute discounting with backing off for the membership/output function.
 $$$$$ Intuitively we have used all of the different types of information available - when we encounter a new word, we know three things about it: first, the context that it has appeared in, secondly the string of characters that it is made of, and thirdly that it is a new word and therefore rare.
 $$$$$ As can be seen the use of morphological information improves the preformance markedly for rare words, and that this effect reduces as the frequency increases.
 $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.

 $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
 $$$$$ Table 6 shows the results of this evaluation on some English data for various numbers of states.
 $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
 $$$$$ In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.

We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ Clearly as the number of clusters increases, the conditional entropy will decrease, as is demonstrated below.
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ Here DO refers to the distributional clustering algorithm where all words are clustered; D5 leaves all words with frequency at most 5 in a seperate cluster, DM uses morphological information as well, DF uses frequency information and DMF uses morphological and frequency information.

Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ A more important variation for our purposes is how the rare words are treated.
Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ This algorithm iteratively improves the likelihood of a given clustering by moving each word from its current cluster to the cluster that will give the maximum increase in likelihood, or leaving it in its original cluster if no improvement can be found.
Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ A number of different approaches have been presented for this task using exclusively distributional evidence to cluster the words together, starting with (Lamb, 1961) and these have been shown to produce good results in English, Japanese and Chinese.
Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ We then briefly discuss the use of ambiguous models or soft clustering in Section 6, and then finish with our conclusions and proposals for future work.

As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ We can assume an additional sentence boundary token.
As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ Suppose we have a corpus of length N, , wN.

Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ First, early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories.
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ Since the data sets are so small we decided to use the conditional entropy evaluation.
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ We are particularly interested in rare words: as (Rosenfeld, 2000, pp.1313-1314) points out, it is most important to cluster the infrequent words, as we will have reliable information about the frequent words; and yet it is these words that are most difficult to cluster.

We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ This relates to two other approaches that we are aware of (Fine et al., 1998) and (Weber et al., 2001).
We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ As is often the case in machine learning of natural language, there are two parallel motivations: first a simple engineering one — the induction of these categories can help in smoothing and generalising other models, particularly in language modelling for speech recognition as explored by (Ney et al., 1994) and secondly a cognitive science motivation — exploring how evidence in the primary linguistic data can account for first language acquisition by infant children (Finch and Chater, 1992a; Finch and Chater, 1992b; Redington et al., 1998).

Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).
Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ First, early work used an informal evaluation of manually comparing the clusters or dendrograms produced by the algorithms with the authors' intuitive judgment of the lexical categories.
Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ We will then discuss the basic algorithm that is the starting point for our research in Section 3.

In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ We will then discuss the basic algorithm that is the starting point for our research in Section 3.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ Frequent word baseline take the n — 1 most frequent words and assign them each to a separate class, and put all remaining words in the remaining class.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.

 $$$$$ To take a trivial example, if we encounter an unknown word, say £212,000 then merely looking at the sequence of characters that compose it is enough to enable us to make a good guess as to its part of speech.
 $$$$$ In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.
 $$$$$ Again, this approach has several weaknesses: there is not a unique well-defined set of part-ofspeech tags, but rather many different possible sets that reflect rather arbitrary decisions by the annotators.
 $$$$$ We follow (Ney et al., 1994; Martin et al., 1998) and use an exchange algorithm similar to the k-means algorithm for clustering.

For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ Note this is different from the common task of guessing the word category of an unknown word given a pre-existing set of parts-of-speech, a task which has been studied extensively (Mikheev, 1997).
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ Our task is to learn a deterministic clustering, that is to say a class membership function g from V into the set of class labels , n}.
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ One way to incorporate this simple source of information would be to use a mixture of string models alone, without distributional evidence.
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ Table 5 shows a qualitative evaluation of some of the clusters produced by the best performing model for 64 clusters on the WSJ data set.

 $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).
 $$$$$ As can be seen they cover a wide range of language families; furthermore Bulgarian is written in Cyrillic, which slightly stretches the range.

We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We have demonstrated that the use of morphological information can improve the performance of the algorithm with rare words quite substantially.
We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.

In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.
In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ Table 4 shows the results of the perplexity evaluation on the WSJ data.
In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.

 $$$$$ We can assume an additional sentence boundary token.
 $$$$$ Recall that Thus low conditional entropy means that the mutual information between the gold and induced tags will be high.
 $$$$$ We trained the model on the various sentences in the model, on WSJ data.
 $$$$$ The tags used are extremely fine-grained, and incorporate a great deal of information about case, gender and so on — in Hungarian for example 400 tags are used with 86 tags used only once.

Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ We have demonstrated that the use of morphological information can improve the performance of the algorithm with rare words quite substantially.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ We can assume an additional sentence boundary token.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ Intuitively we have used all of the different types of information available - when we encounter a new word, we know three things about it: first, the context that it has appeared in, secondly the string of characters that it is made of, and thirdly that it is a new word and therefore rare.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ Moreover, we wish to have comparatively weak models otherwise the algorithm will capture irrelevant orthotactic regularities — such as a class of words starting with &quot;st&quot; in English.

Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ With 5 substates, 20 iterations corpus, and then tagged the data with the most likely (Viterbi) tag sequence.
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ This clustering can be used to define a number of simple statistical models.
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information.

As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ In addition we will use a very limited sort of frequency information, since rare words tend to belong to open class categories.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ The tags used are extremely fine-grained, and incorporate a great deal of information about case, gender and so on — in Hungarian for example 400 tags are used with 86 tags used only once.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ We then evaluated the conditional entropy of the gold standard tags given the derived HMM tags.

 $$$$$ We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.
 $$$$$ Additionally we have tested this on a wide range of languages.
 $$$$$ Clearly, because of lexical ambiguity, we would like to be able to assign some words to more than one class.
 $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).

Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ Additionally we have tested this on a wide range of languages.
Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ This will give a higher probability to partitions where morphologically similar strings are in the same cluster.
Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ In addition we can modify this to incorporate information about frequency.
