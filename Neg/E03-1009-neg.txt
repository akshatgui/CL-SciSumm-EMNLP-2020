 $$$$$ In addition we will use a very limited sort of frequency information, since rare words tend to belong to open class categories.
 $$$$$ Since the data sets are so small we decided to use the conditional entropy evaluation.
 $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).

 $$$$$ We have applied several different algorithms to the task of identifying parts of speech.
 $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
 $$$$$ We know that rare words are more likely to be nouns, proper nouns or members of some other open word class rather than say pronouns or articles.
 $$$$$ The task studied in this paper is the unsupervised learning of parts-of-speech, that is to say lexical categories corresponding to traditional notions of, for example, nouns and verbs.

We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ (Martin et al., 1998) leave all words with a frequency of less than 5 in a particular class, from which they may not be moved.
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ One way to incorporate this simple source of information would be to use a mixture of string models alone, without distributional evidence.
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ We have also demonstrated that a very simple use of frequency can provide further improvements.
We used Alexander Clarke's software, based on (Clark, 2003), to cluster the words, and then allow each word to be labeled with any part of speech tag seen in the data with any other word in the same cluster. $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.

Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
Since induction is founded to some extent upon disambiguating contexts, this work has some bearing on the evaluation of induced categories with corpus annotation; not only is there more than one tag set in existence (see discussion in Clark, 2003), but annotation schemes make distinctions that morphosyntactic contexts can not readily capture. $$$$$ Table 4 shows the results of the perplexity evaluation on the WSJ data.

As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ We use the symbol W to refer to the random variable related to the word, G for the associated gold standard tag, and T for the tag produced by one of our algorithms.
As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ Frequent word baseline take the n — 1 most frequent words and assign them each to a separate class, and put all remaining words in the remaining class.
As a base tagger, we modify a leading unsupervised POS tagger (Clark, 2003) to constrain the distributions of word types across clusters to be Zipfian, allowing us to utilize a perplexity-based quality test. $$$$$ We chose sections 0 — 19, a total of about 500,000 words.

Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ This will give a higher probability to partitions where morphologically similar strings are in the same cluster.
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).
Figure 1 demonstrates this phenomenon for a leading POS induction algorithm (Clark, 2003). $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).

We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ This just has the effect of discriminating between classes that will have lots of types (i.e. open class clusters) and clusters that tend to have few types (corresponding to closed class words).
We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ We assume a vocabulary of words V = {W1, ... }.
We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ We are particularly interested in rare words: as (Rosenfeld, 2000, pp.1313-1314) points out, it is most important to cluster the infrequent words, as we will have reliable information about the frequent words; and yet it is these words that are most difficult to cluster.
We focus here on Clark's tagger (Clark, 2003) (CT), probably the leading POS induction algorithm (see Table 3). $$$$$ The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).

Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ We performed experiments on parts of the Wall Street Journal corpus, using the corpus tags.
Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ In addition we are interested primarily in inducing small numbers of clusters (at most 128) from comparatively small amounts of data using limited or no sources of external knowledge, and in approaches that will work across a wide range of languages, rather than inducing large numbers (say 1000) from hundreds of millions of words.
Clark (2003) proposed a perplexity based test for the quality of his POS induction algorithm. $$$$$ Then the class bigram model defines the probability of the next word given the history as P(wi IOC') = P(wilg(wi))P(9(wi-1)1g(wi-2)) It is not computationally feasible to search through all possible partitions of the vocabulary to find the one with the highest value of the likelihood; we must therefore use some search algorithm that will give us a local optimum.

In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ However it is not clear that this directly measures the linguistic plausibility of the classification.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ Moreover, we wish to have comparatively weak models otherwise the algorithm will capture irrelevant orthotactic regularities — such as a class of words starting with &quot;st&quot; in English.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ Additionally we have tested this on a wide range of languages.
In this paper we show that for the tagger of (Clark, 2003) such a method provides mediocre results (Table 2) even when the training criterion (likelihood or data probability for this tagger) is evaluated on the test set. $$$$$ The most principled way is to use Hidden Markov Models: these provide the formal and technical apparatus required to train when the tags might be ambiguous.

 $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).
 $$$$$ We evaluated it for all words, and also for words with frequency at most 5.
 $$$$$ However it is not clear that this directly measures the linguistic plausibility of the classification.

For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ We evaluated it for all words, and also for words with frequency at most 5.
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ Clearly, because of lexical ambiguity, we would like to be able to assign some words to more than one class.
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ We use the same formula for the probability of the data given the model, but include an additional term for the probability of the model, that depends on the strings used in each cluster.
For example, (Sch?utze, 1993) induces 200 clusters and (Clark, 2003) chooses between 16-128; and most of these induced categories are difficult to associate with a specific POS tag. $$$$$ Section 5 presents the results of our evaluations on a number of data sets drawn from typologically distinct languages.

 $$$$$ To put the scores we present below in context, we note that using some data sets prepared for the AMALGAM project (Atwell et al., 2000) the conditional entropies between some data manually tagged with different tag sets varied from 0.22 (between Brown and LOB tag sets) to 1.3 (between LLC and Unix Parts tag sets).
 $$$$$ To put the scores we present below in context, we note that using some data sets prepared for the AMALGAM project (Atwell et al., 2000) the conditional entropies between some data manually tagged with different tag sets varied from 0.22 (between Brown and LOB tag sets) to 1.3 (between LLC and Unix Parts tag sets).

We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We have demonstrated that the use of morphological information can improve the performance of the algorithm with rare words quite substantially.
We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ The models we will use here for the cluster dependent word string probabilities will be letter Hidden Markov Models (HMMs).
We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
We continue by tagging the corpus using Clark's unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abendetal., 2010). $$$$$ We can do this simply by adding prior class probabilities ai to the above equation giving We can use the maximum likelihood estimates for ozi which are just the number of distinct types in cluster i, divided by the total number of types in the corpus.

In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ Secondly, because of the Zipfian distribution of word frequencies, simple baselines that assign each frequent word to a different class, can score rather highly, as we shall see below.
In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ Intuitively we have used all of the different types of information available - when we encounter a new word, we know three things about it: first, the context that it has appeared in, secondly the string of characters that it is made of, and thirdly that it is a new word and therefore rare.
In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
In the 'Fully Unsupervised' scenario, prepositions and verbs were identified using Clark's tagger (Clark, 2003). $$$$$ As can be seen, increasing the number of states of the model does not reduce the conditional entropy of the gold standard tags; rather it increases the lexical ambiguity of the model H(TIW).

 $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
 $$$$$ Table 1 shows that the residual conditional entropy with the word baseline is only 0.12.
 $$$$$ This gives us a two-level HMM: a HMM where each state corresponds to a word, and where the output function is a HMM where each state corresponds to a letter.
 $$$$$ We trained the model on the various sentences in the model, on WSJ data.

Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ In addition we will use a very limited sort of frequency information, since rare words tend to belong to open class categories.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ Additionally we have tested this on a wide range of languages.
Using a large corpus of abstracts from PubMed (30,963,886 word tokens of 335,811 word types), we cluster words by their syntactic contexts and morphological contents (Clark, 2003). $$$$$ We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.

Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ We plan to use this stronger form of information using Pair Hidden Markov Models as described in (Clark, 2001).
Toutanova et al. (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporating some of these in an unsupervised tagging model. $$$$$ We can assume an additional sentence boundary token.

As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ (Martin et al., 1998) leave all words with a frequency of less than 5 in a particular class, from which they may not be moved.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ We performed experiments on parts of the Wall Street Journal corpus, using the corpus tags.
As Clark (2003) points out, many-to-1 accuracy has several defects. $$$$$ We assume a vocabulary of words V = {W1, ... }.

 $$$$$ We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
 $$$$$ Additionally we have tested this on a wide range of languages.
 $$$$$ We have so far used only a limited form of morphological information that relies on properties of individual strings, and does not relate particular strings to each other.
 $$$$$ This suggests a solution that is to replace the multinomial distribution by a weaker distribution such as the Hidden Markov Models we have used before.

Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ We examined each cluster and chose a simple regular expression to describe it, and calculated the precision and recall for words of all frequency, and for words of zero frequency.
Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ Some preliminary experiments not reported here established that this approach could only separate out the most basic differences, such as sequences of numbers.
Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ Note that the use of the frequency information worsens the performance for rare words according to this evaluation — this is because the rare words are much more tightly grouped into just a few clusters, thus the entropy of the cluster tags is lower.
Both of the older systems discussed by Christodoulopoulos et al (2010), i.e., Clark (2003) and Brown et al (1992), included this constraint and achieved very good performance relative to token-based systems. $$$$$ The second sort of information is information about the sequence of letters or phones that form each word.
