Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ This may underestimate the difficulty of the Brown corpus by including sentences from the same documents in training and test sets.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ The first distribution gives probability of the syntactic category H of the head child of a parent node with category P, head word Hhw with the head tag (the part of speech tag of the head word) Hht: The head word and head tag of the new node H are defined to be the same as those of its parent.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.

Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ A special #STOP# symbol is generated to terminate the sequence of children for a given parent.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ We take as our baseline parser the statistical model of Model 1 of Collins (1997).
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ The results show a gradual degradation as more parameters are pruned.

Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ In order to ensure that the model's probabilities still sum to one, the backoff weight a must be adjusted whenever a parameter is removed from the model.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ The past several years have seen great progress in the field of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.

A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ In each case, the corpus used was the Penn Treebank's hand-annotated parses of Wall Street Journal articles.
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.

Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ This subset consists primarily of various fiction genres.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.

When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ It can be thought of as a variety of lexicalized probabilistic context-free grammar, with the rule probabilities factored into three distributions.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ For the WSJ data, we observed the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ In Collins' Model 1, the word pair statistics occur in the distribution where Hhw represent the head word of a parent node in the tree and Chw the head word of its (non-head) child.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS).

See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.
See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.

We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ The less specific distribution P2 of the backoff model is Pcw2 of equation 2, an interpolation of two empirical distributions.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ The past several years have seen great progress in the field of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.

Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) differ in their details but are based on similar features.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are reflected in a statistical parser's probability model.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ The standard WSJ task seems to be simplified by its homogenous style.

Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ Perhaps the most striking result is just how little the elimination of lexical bigrams affects the baseline system: performance on the WSJ corpus decreases by less than 0.5% absolute.
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.

Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS).
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.

For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ For the WSJ data, we observed the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets.
For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ (The head word of a parent is the same as the head word of its head child.)
For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ Because this is the only part of the model that involves pairs of words, it is also where the bulk of the parameters are found.

The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ This subset consists primarily of various fiction genres.
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ In the case of n-gram language modeling, e is the next word to be predicted, and the conditioning events are the n — 1 preceding words.

See Gildea (2001) for the exact setup. $$$$$ The parsing models of Charniak (2000) and Collins (2000) add more complex features to the parsing model that we use as our baseline.
See Gildea (2001) for the exact setup. $$$$$ The results show a gradual degradation as more parameters are pruned.
See Gildea (2001) for the exact setup. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.

For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ This leads us to a technique for pruning parameters to reduce the size of the parsing model.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ Results are shown in Table 3.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ Our selective pruning technique allows for a more fine grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ The WSJ bigrams are almost all specific to finance, are all word pairs that are likely to appear immediately adjacent to one another, and are all children of the base NP syntactic category.

For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus.

In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ The lexical bigrams are contained in the most specific distribution for P,,,,.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ The vocabularies used in corpora vary, as do the word frequencies.

Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ Our results show strong corpus effects for statistical parsing models: a small amount of matched training data appears to be more useful than a large amount of unmatched data.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ This subset consists primarily of various fiction genres.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ In order to test this hypothesis, we removed the distribution �P(ChwlP, H, Hht, Hhw, C, Cht) from the parsing model entirely, relying on the interpolation of the two less specific distributions in the parser: We performed cross-corpus experiments as before to determine whether the simpler parsing model might be more robust to corpus effects.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ The interpolation weights A1, A2 are chosen as a function of the number of examples seen for the conditioning events and the number of unique values seen for the predicted variable.

They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ We conducted separate experiments using WSJ data, Brown data, and a combination of the two as training material.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ The only overlap between the two sets is for pairs of unknown word tokens.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ The remaining two distributions generate the non-head children one after the other.

Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ Our selective pruning technique allows for a more fine grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration.
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ This technique applies to any statistical model which estimates probabilities by backing off, that is, using probabilities from a less specific distribution when no data are available are available for the full distribution, as the following equations show for the general case: Here e is the event to be predicted, h is the set of conditioning events or history, a is a backoff weight, and h' is the subset of conditioning events used for the less specific backoff distribution.
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are reflected in a statistical parser's probability model.
