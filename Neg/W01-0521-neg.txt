Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ In this paper we examine the following questions: Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ It is reasonable to expect word co-occurrences to vary as well.
Another approach to this topic, examining the effect of using lexical bigram information, which is very corpus specific, appears in (Gildea, 2001). $$$$$ While this does not reflect the state-of-the-art performance on the WSJ task achieved by the more the complex models of Charniak (2000) and Collins (2000), we regard it as a reasonable baseline for the investigation of corpus effects on statistical parsing.

Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ Because the word pair feature is the most specific in the model, it is likely to be the most corpusspecific.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ In order to ensure that the model's probabilities still sum to one, the backoff weight a must be adjusted whenever a parameter is removed from the model.
Lexicalization allows us to capture bi-lexical relationships along dependency arcs, but it has been previously shown that these add only marginal benefit to Collins's model anyway (Gildea, 2001). $$$$$ Models 2 and 3 of Collins (1997) add some slightly more elaborate features to the probability model, as do the additions of Charniak (2000) to the model of Charniak (1997).

Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ Results are shown in Table 3.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.
Gildea (2001) and Bacchiani et al (2006) show that out-of-domain training data can improve parsing accuracy. $$$$$ For the WSJ data, we observed the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets.

A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ The first distribution gives probability of the syntactic category H of the head child of a parent node with category P, head word Hhw with the head tag (the part of speech tag of the head word) Hht: The head word and head tag of the new node H are defined to be the same as those of its parent.
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ Similarly, adding the Brown data to the WSJ model increased performance on WSJ by less than 0.5%.
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data.
A statistical syntactic parser is known to perform badly if a text to be parsed belongs to a domain which differs from a domain on which the parser is trained (Gildea, 2001). $$$$$ The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.

Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ The vocabularies used in corpora vary, as do the word frequencies.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ The basic mismatch between the two corpora is shown in the significantly lower performance of the WSJtrained model on Brown data than on WSJ data (rows 1 and 2).
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.
Previous work has shown that parsers typically perform poorly outside of their training domain (Gildea, 2001). $$$$$ Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.

When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ Only the first distribution in this interpolation scheme involves pairs of words, and the third component is simply the probability of a word given its part of speech.
When applying parsers out of domain they are typically slower and less accurate (Gildea, 2001). $$$$$ As shown by Stolcke (1998), this criterion is an approximation of the relative entropy between the original and pruned distributions, but does not take into account the effect of changing the backoff weight on other events' probabilities.

See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ The standard WSJ task seems to be simplified by its homogenous style.
See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ Combining the WSJ and Brown training data in one model improves performance further, but by less than 0.5% absolute.
See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ In this paper we examine the following questions: Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance.
See Gildea (2001) and Petrov and Klein (2007) for the exact experimental setup that we followed here. $$$$$ In order to avoid assigning zero probability to unseen events, it is necessary to smooth the training data.

We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language.
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).
We therefore, following Gildea (2001) and others, consider S, and also the baseline training data, B, as out-of domain training data. $$$$$ It can be thought of as a variety of lexicalized probabilistic context-free grammar, with the rule probabilities factored into three distributions.

Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ In this paper we examine the following questions: Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ It can be thought of as a variety of lexicalized probabilistic context-free grammar, with the rule probabilities factored into three distributions.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.
Lexical information has a lot of promise for parse selection in theory, but there are practical problems such as sparse data and genre effects (Gildea, 2001). $$$$$ This leads us to a technique for pruning parameters to reduce the size of the parsing model.

Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ The Collins model uses linear interpolation to estimate probabilities from empirical distributions of varying specificities: where P� represents the empirical distribution derived directly from the counts in the training data.
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ Aside from cross-corpus considerations, this is an important finding if a lightweight parser is desired or memory usage is a consideration.
Second, it is well known that the accuracy of parsers trained on the Penn Treebank degrades when they are applied to different genres and domains (Gildea, 2001). $$$$$ The Seymore/Rosenfeld pruning technique can be used to prune backoff probability models regardless of whether the backoff weights are derived from linear interpolation weights or discounting techniques such as Good-Turing.

Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ (The head word of a parent is the same as the head word of its head child.)
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.
Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. $$$$$ The interpolation weights A1, A2 are chosen as a function of the number of examples seen for the conditioning events and the number of unique values seen for the predicted variable.

For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ These word pair relations, also called lexical bigrams (Collins, 1996), are reminiscent of dependency grammars such as Melcuk (1988) and the link grammar of Sleator and Temperley (1993).
For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ Results for different values of 0 are shown in Table 4.
For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). $$$$$ Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.

The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ The basic mismatch between the two corpora is shown in the significantly lower performance of the WSJtrained model on Brown data than on WSJ data (rows 1 and 2).
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.
The genre dependency of parsers is an accepted fact and has been described by, among others, Sekine (1997) and Gildea (2001). $$$$$ In each case, the corpus used was the Penn Treebank's hand-annotated parses of Wall Street Journal articles.

See Gildea (2001) for the exact setup. $$$$$ In the Seymore/Rosenfeld approach, parameters are pruned according to the following criterion: where p'(elh') represents the new backed off probability estimate after removing p(eIh) from the model and adjusting the backoff weight, and N(e, h) is the count in the training data.
See Gildea (2001) for the exact setup. $$$$$ The Brown bigrams, which have lower correlation values by our metric, include verb/subject and preposition/object relations and seem more broadly applicable as a model of English.
See Gildea (2001) for the exact setup. $$$$$ We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
See Gildea (2001) for the exact setup. $$$$$ This technique applies to any statistical model which estimates probabilities by backing off, that is, using probabilities from a less specific distribution when no data are available are available for the full distribution, as the following equations show for the general case: Here e is the event to be predicted, h is the set of conditioning events or history, a is a backoff weight, and h' is the subset of conditioning events used for the less specific backoff distribution.

For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ (1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS).
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.
For the lexicalized approach, the annotation of symbols with lexical head is known to be rarely fully used in practice (Gildea, 2001), what is really used being the category of the lexical head. $$$$$ A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.

For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ In the case of n-gram language modeling, e is the next word to be predicted, and the conditioning events are the n — 1 preceding words.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ The ten lexical bigrams with the highest scores for the pruning metric are shown in Table 5 for WSJ and Table 6.
For this reason we do not run experiments on the task considered in (Gildea, 2001) and (Roark and Bacchiani, 2003), where they are porting from the restricted domain of the WSJ corpus to the more varied domain of the Brown corpus as a whole. $$$$$ The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.

In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus.
In addition to portability experiments with the parsing model of (Collins, 1997), (Gildea, 2001) provided a comprehensive analysis of parser portability. $$$$$ This leads us to a technique for pruning parameters to reduce the size of the parsing model.

Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ The relatively high performance of a parsing model with no lexical bigram statistics on the WSJ task led us to explore whether it might be possible to significantly reduce the size of the parsing model by selectively removing parameters without sacrificing performance.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ Each child is generated in two steps: first its syntactic category C and head tag Cht are chosen given the parent's and head child's features and a function A representing the distance from the head child: Then the new child's head word Chw is chosen: For each of the three distributions, the empirical distribution of the training data is interpolated with less specific backoff distributions, as we will see in Section 5.
Gildea (2001) only reports results on sentences of 40 or less words on all the Brown corpus sections combined, for which he reports 80.3%/81.0% recall/precision when training only on data from the WSJ corpus, and 83.9%/84.8% when training on data from the WSJ corpus and all sections of the Brown corpus. $$$$$ Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).

They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ For the WSJ data, we observed the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets.
They achieved very good improvement over their baseline and over (Gildea, 2001), but the absolute accuracies were still relatively low (as discussed above). $$$$$ However, the pairs are not strongly related semantically, no doubt because the first term of the pruning criterion favors the most frequent words, such as forms of the verbs &quot;be&quot; and &quot;have&quot;.

Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ Similarly, adding the Brown data to the WSJ model increased performance on WSJ by less than 0.5%.
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ We take as our baseline parser the statistical model of Model 1 of Collins (1997).
Past research in a variety of NLP tasks, like parsing (Gildea, 2001) and chunking (Huang and Yates, 2009), has shown that systems suffer from a drop-off in performance on out-of-domain tests. $$$$$ Thus, even a large amount of additional data seems to have relatively little impact if it is not matched to the test material.
