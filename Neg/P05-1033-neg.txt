Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ We have separated these factors out from the rule weights for notational convenience, but it is conceptually cleaner (and necessary for polynomial-time decoding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG.
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ Each cell contains all the items standing for X spanning fi/.

 $$$$$ These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit.
 $$$$$ The maximum initial phrase length is currently 10; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy, but requires more memory.
 $$$$$ Och’s method gives equal weight to all the extracted phrase occurences.
 $$$$$ Och’s method gives equal weight to all the extracted phrase occurences.

We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ The exceptions to the above are the two glue rules, (13), which has weight one, and (14), which has weight (16) w(S _� (S 1 X 2 , S 1 X 2 )) = exp(—λg) the idea being that λg controls the model’s preference for hierarchical phrases over serial combination of phrases.
We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.
We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ 5.
We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.

Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ S. D. G.
Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton.
Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.
Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.

Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ Och’s method gives equal weight to all the extracted phrase occurences.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ S. D. G.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ Our primary goal for the future is to move towards a more syntactically-motivated grammar, whether by automatic methods to induce syntactic categories, or by better integration of parsers trained on annotated data.

Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ Our primary goal for the future is to move towards a more syntactically-motivated grammar, whether by automatic methods to induce syntactic categories, or by better integration of parsers trained on annotated data.
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005).
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.

For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. $$$$$ For a partial example of a synchronous CFG derivation, see Figure 1.
For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. $$$$$ We also prune rules that have the same French side (b = 100).
For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. $$$$$ The parser only operates on the French-side grammar; the English-side grammar affects parsing only by increasing the effective grammar size, because there may be multiple rules with the same French side but different English sides, and also because intersecting the language model with the English-side grammar introduces many states into the nonterminal alphabet, which are projected over to the French side.

(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ The use of hierarchical structures opens the possibility of making the model sensitive to syntactic structure.
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ ]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases.
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.

More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ Hierarchical phrase pairs, which can be learned without any syntactically-annotated training data, improve translation accuracy significantly compared with a state-of-the-art phrase-based system.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ At each step, two coindexed nonterminals are rewritten using the two components of a single rule, such that none of the newly introduced symbols is linked to any symbols already present.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.

 $$$$$ This is faster than our Python implementation of a standard phrase-based decoder, so we expect that a future optimized implementation of the hierarchical decoder will run at a speed competitive with other phrase-based systems.
 $$$$$ S. D. G.
 $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
 $$$$$ The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton.

To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ Each cell contains all the items standing for X spanning fi/.
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ The training process begins with a word-aligned corpus: a set of triples hf, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004).

Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ English sentence e is modeled as: The translation model P(f  |e) “encodes” e into f by the following steps: Other phrase-based models model the joint distribution P(e, f) (Marcu and Wong, 2002) or made P(e) and P(f  |e) into features of a log-linear model (Och and Ney, 2002).
Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ Finally, we ran the decoder on the test set, pruning the phrase table with b = 100, pruning the chart with b = 100,/3 = 10−5, and limiting distortions to 4.
Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.
Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.

However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ Note that the parser was run only on the test data and not the (much larger) training data.
However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ This would potentially improve both accuracy and efficiency.

However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ ]1 [with] [North Korea] [is] [one of the few countries] where we have used subscripts to indicate the reordering of phrases.
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ We compared a baseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system.
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.

This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ The set of rules of hf, e, ∼i is the smallest set satisfying the following: is a rule, where k is an index not used in r. The above scheme generates a very large number of rules, which is undesirable not only because it makes training and decoding very slow, but also because it creates spurious ambiguity—a situation where the decoder produces many derivations that are distinct yet have the same model feature vectors and give the same translation.
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ In the grammar we extract from a bitext (described below), all of our rules use only X, except for two special “glue” rules, which combine a sequence of Xs to form an S: These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a standard phrase-based model.
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.

Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ Finally, the decoder has a constraint that prohibits any X from spanning a substring longer than 10 on the French side, corresponding to the maximum length constraint on initial rules during training.
Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ This would capture the fact that Chinese PPs almost always modify VP on the left, whereas English PPs usually modify VP on the right.
Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation.
Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.

Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ These hierarchical phrase pairs are formally productions of a synchronous context-free grammar (defined below).
Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ In a synchronous CFG the elementary structures are rewrite rules with aligned pairs of right-hand sides: where X is a nonterminal, γ and α are both strings of terminals and nonterminals, and — is a one-to-one correspondence between nonterminal occurrences in γ and nonterminal occurrences in α. Rewriting begins with a pair of linked start symbols.

Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ Thus, our decoder’s search space is many times larger than a monolingual parser’s would be.
Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ Our decoder is a CKY parser with beam search together with a postprocessor for mapping French derivations to English derivations.

(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ ]2 with [North Korea]1]]] The system we describe below uses rules like this, and in fact is able to learn them automatically from a bitext without syntactic annotation.
(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ Moreover, reducing the grammar size would allow more ambitious training settings.

While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.
While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. $$$$$ The decoder is implemented in Python, an interpreted language, with C++ code from the SRI Language Modeling Toolkit (Stolcke, 2002).
While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. $$$$$ These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit.
