Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ Streamlining the grammar would allow further experimentation in these directions.
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ A system like that of Yamada and Knight (2001) is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank).
Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. $$$$$ The use of hierarchical structures opens the possibility of making the model sensitive to syntactic structure.

 $$$$$ In the grammar we extract from a bitext (described below), all of our rules use only X, except for two special “glue” rules, which combine a sequence of Xs to form an S: These give the model the option to build only partial translations using hierarchical phrases, and then combine them serially as in a standard phrase-based model.
 $$$$$ Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases.

We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ The parser only operates on the French-side grammar; the English-side grammar affects parsing only by increasing the effective grammar size, because there may be multiple rules with the same French side but different English sides, and also because intersecting the language model with the English-side grammar introduces many states into the nonterminal alphabet, which are projected over to the French side.
We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ To reduce this effect, we apply the following heuristic when filling a cell: if an item falls outside the beam, then any item that would be generated using a lowerscoring rule or a lower-scoring antecedent item is also assumed to fall outside the beam.
We also implemented Algorithms 2 and 3 in a parsing-based MT decoder (Chiang, 2005) and report results on decoding speed. $$$$$ Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty.

Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ S. D. G.
Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ S. D. G.
Our second experiment was on a CKY-based decoder for a machine translation system (Chiang, 2005), implemented in Python 2.4 accelerated with Psyco 1.3 (Rigo, 2004). $$$$$ The weight of each rule is: where the φi are features defined on rules.

Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ S. D. G.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution.
Following this WSD reformulation for SMT, Chan et al (2007) integrate a state-of-the-art WSD system into a hierarchical phrase-based system (Chiang, 2005). $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.

Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ These hierarchical phrase pairs are formally productions of a synchronous context-free grammar (defined below).
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ However, our method may extract many rules from a single initial phrase pair; therefore we distribute weight equally among initial phrase pairs, but distribute that weight equally among the rules extracted from each.
Whenever we combine two dynamic programming items, we need to score the fluency of their concatentation by incorporating the score of any language model features which cross the target side boundaries of the two concatenated items (Chiang, 2005). $$$$$ The maximum initial phrase length is currently 10; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy, but requires more memory.

For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. $$$$$ Thus the hierarchical phrase pairs from our above example could be formalized in a synchronous CFG as: where we have used boxed indices to indicate which occurrences of X are linked by —.
For instance, Zollmann et al (2006) follow Chiang (2005) in disallowing adjacent non terminals. $$$$$ Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate the phrase translation parameters from the hypothesized distribution.

(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ The maximum initial phrase length is currently 10; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy, but requires more memory.
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an from position i to position j inclusive, and similarly for eji .
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
(Cherry, 2008) and (Marton and Resnik, 2008) introduce syntactic constraints into the standard phrase-based decoding (Koehn et al, 2003) and hierarchical phrase-based decoding (Chiang, 2005) respectively by using a counting feature which accumulates whenever hypotheses violate syntactic boundaries of source-side parse trees. $$$$$ Note that Ag penalizes the glue rule much less than App does ordinary rules.

More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ We used the default feature set: language model (same as above), p(f¯  |¯e), p(¯e  |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalty.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ Let D be a derivation of the grammar, and let f(D) and e(D) be the French and English strings generated by D. Let us represent D as a set of triples (r, i, j), each of which stands for an application of a grammar rule r to rewrite a nonterminal that spans f(D)ji on the French side.3 Then the weight of D hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 ,X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i is the product of the weights of the rules used in the translation, multiplied by the following extra factors: where plm is the language model, and exp(−λwp|e|), the word penalty, gives some control over the length of the English output.
More previously, (Chiang, 2005) rewards hypotheses whenever they exactly match constituent boundaries of parse trees on the source side. $$$$$ This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence.

 $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.
 $$$$$ Och’s method gives equal weight to all the extracted phrase occurences.

To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ I would like to thank Philipp Koehn for the use of the Pharaoh software; and Adam Lopez, Michael Subotin, Nitin Madnani, Christof Monz, Liang Huang, and Philip Resnik.
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
To better leverage syntactic constraint yet still allow non-syntactic translations, Chiang (2005) introduces a count for each hypothesis and accumulates it whenever the hypothesis exactly matches syntactic boundaries on the source side. $$$$$ Similarly, (6) ( 1 de 2 , the 2 that 1 ) would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative clauses modify on the right; and (7) ( 1 zhiyi, one of 1 ) would render the construction zhiyi in English word order.

Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ We have separated these factors out from the rule weights for notational convenience, but it is conceptually cleaner (and necessary for polynomial-time decoding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG.
Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ This would potentially improve both accuracy and efficiency.
Chiang (2005)'s hierarchal phrase-based model achieves significant performance improvement. $$$$$ Finally, we ran the decoder on the test set, pruning the phrase table with b = 100, pruning the chart with b = 100,/3 = 10−5, and limiting distortions to 4.

However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ Note that we have used only a single nonterminal symbol X instead of assigning syntactic categories to phrases.
However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ Our model is based on a weighted synchronous CFG (Aho and Ullman, 1969).
However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30).
However, no further significant improvement is achieved when the model is made sensitive to syntactic structures by adding a constituent feature (Chiang, 2005). $$$$$ Finally, the decoder has a constraint that prohibits any X from spanning a substring longer than 10 on the French side, corresponding to the maximum length constraint on initial rules during training.

However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information.
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ This suggests that the model will prefer serial combination of phrases, unless some other factor supports the use of hierarchical phrases (e.g., a better language model score).
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ Hierarchical phrase pairs, which can be learned without any syntactically-annotated training data, improve translation accuracy significantly compared with a state-of-the-art phrase-based system.
However, formal SCFG show much better performance in the formally syntax-based translation framework (Chiang, 2005). $$$$$ Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model.

This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ We have separated these factors out from the rule weights for notational convenience, but it is conceptually cleaner (and necessary for polynomial-time decoding) to integrate them into the rule weights, so that the whole model is a weighted synchronous CFG.
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ We ran the training process of Section 3 on the same data, obtaining a grammar of 24M rules.
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ Treating this distribution as our observed data, we use relativefrequency estimation to obtain P(y  |α) and P(α  |y).
This is because the formal syntax is learned from phrases directly without relying on any linguistic theory (Chiang, 2005). $$$$$ A system like that of Yamada and Knight (2001) is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank).

Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ The weight of each rule is: where the φi are features defined on rules.
Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.
Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models (Chiang, 2005). $$$$$ This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence.

Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ This heuristic greatly increases decoding speed, at the cost of some search errors.
Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ Moreover, we prohibit nonterminals that are adjacent on the French side, a major cause of spurious ambiguity.
Hiero (Chiang, 2005) is a hierarchical, string-to-string translation system. $$$$$ We present a statistical phrase-based translamodel that uses phrases that contain subphrases.

Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ The weight of each rule is: where the φi are features defined on rules.
Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ It translates the above example almost exactly as we have shown, the only error being that it omits the word ‘that’ from (6) and therefore (8).
Recent work in machine translation has evolved from the traditional word (Brown et al, 1993) and phrase based (Koehn et al, 2003a) models to include hierarchical phrase models (Chiang, 2005) and bilingual synchronous grammars (Melamed, 2004). $$$$$ This work was partially supported by ONR MURI contract FCPO.810548265 and Department of Defense contract RD-02-5700.

(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ The word penalty is easy; the language model is integrated by intersecting the English-side CFG with the language model, which is a weighted finitestate automaton.
(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ The maximum initial phrase length is currently 10; preliminary experiments show that increasing this limit to as high as 15 does improve accuracy, but requires more memory.
(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.
(Chiang, 2005) generates synchronous context free grammar (SynCFG) rules from an existing phrase translation table. $$$$$ Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty.

While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. $$$$$ The results of the experiments are summarized in Table 1.
While (Chiang, 2005) uses only two nonterminal symbols in his grammar, we introduce multiple syntactic categories, taking advantage of a target language parser for this information. $$$$$ Thus it can be seen as shift to the of syntaxtranslation systems without any lin- In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.
