Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ However, the dependency structures with high enough PCS to be among the highest probability structures are likely to have similar category sequences.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Collins (1999), Charniak (2000)).
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ This impacts on how best to define a probability model for CCG, since the “spurious ambiguity” of CCG derivations may lead to an exponential number of derivations for a given constituent.

The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ The parser is able to capture a number of longrange dependencies that are not dealt with by existing treebank parsers.
The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ The word-category sequences needed for estimating the probabilities in equation 8 can be read directly from the CCGbank.
The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ Note that we encode the subject argument of the to category as a dependency relation (Marks is a “subject” of to), since our philosophy at this stage is to encode every argument as a dependency, where possible.

Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Note that the supertagger does not provide a single category sequence for each sentence, and the final sequence returned by the parser (along with the dependencies) is determined by the probability model described in the previous section.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ This will be important for applying the parser to tasks such as language modelling, for which the possibility of incremental processing of CCG appears particularly attractive.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ A CCG parser can directly build derived structures, including longrange dependencies.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Collins (1996), Collins (1999)), although our hypothesis was that it would be less useful here, because the CCG grammar provides many of the constraints given by A, and distance measures are biased against long-range dependencies.

This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ As an additional experiment, we conditioned the dependency probabilities in 10 on a “distance measure” (A).
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.

Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ Among the constructions that project unbounded dependencies are relativisation and right node raising.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ For the functors, the category specifies the type and directionality of the arguments and the type of the result.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ The corresponding dependencies are given in the following figure, with the convention that arcs point away from arguments.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ To return a single dependency structure, we chose the most probable structure from the S dcl categories spanning the whole sentence.

This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ This tactic also has the effect of eliminating “spuriously ambiguous” entries from the chart— cf.
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ As an additional experiment, we conditioned the dependency probabilities in 10 on a “distance measure” (A).
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.

The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ Chiang (2000) uses Tree Adjoining Grammar as an alternative to context-free grammar, and here we use another “mildly context-sensitive” formalism, Combinatory Categorial Grammar (CCG, Steedman (2000)), which arguably provides the most linguistically satisfactory account of the dependencies inherent in coordinate constructions and extraction phenomena.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.

Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ CCG is unlike other formalisms in that the standard predicate-argument relations relevant to interpretation can be derived via extremely non-standard surface derivations.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ An advantage of our approach is that the recovery of long-range dependencies is fully integrated with the grammar and parser, rather than being relegated to a post-processing phase.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).

 $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
 $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
 $$$$$ The number of dependency types may be reduced in future work.

The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ This paper has shown that accurate, efficient widecoverage parsing is possible with CCG.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ For the functors, the category specifies the type and directionality of the arguments and the type of the result.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ However, the dependencies are typically derived from a context-free phrase structure tree using simple head percolation heuristics.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ For our purposes, a dependency structure n is a C D pair, where C c1 c2 cn is the sequence of categories assigned to the words, and D hfi fi si hai i 1 m is the set of dependencies.

This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ Not all trees produced dependency structures, since not all categories and type-changing rules in the CCGbank are encoded in the parser.
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ Chiang (2000) uses Tree Adjoining Grammar as an alternative to context-free grammar, and here we use another “mildly context-sensitive” formalism, Combinatory Categorial Grammar (CCG, Steedman (2000)), which arguably provides the most linguistically satisfactory account of the dependencies inherent in coordinate constructions and extraction phenomena.
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ Komagata (1997).
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.

Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ To measure the performance of the parser, we compared the dependencies output by the parser with those in the gold standard, and computed precision and recall figures over the dependencies.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ This paper has shown that accurate, efficient widecoverage parsing is possible with CCG.

Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ Initially the parser was run with 0 001 for the supertagger (an average of 38 categories per word), K 20 for the category dictionary, and a 0001 for the parser.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ Not all trees produced dependency structures, since not all categories and type-changing rules in the CCGbank are encoded in the parser.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ The coverage was increased to ensure the test set was representative of the full section.

We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ Komagata (1997).
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ The parser is able to capture a number of longrange dependencies that are not dealt with by existing treebank parsers.
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ The DAG-like nature of the dependency structures makes it difficult to apply generative modelling techniques (Abney, 1997; Johnson et al., 1999), so we have defined a conditional model, similar to the model of Collins (1996) (see also the conditional model in Eisner (1996b)).

See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.

Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ Collins (1999), Charniak (2000)).
Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ There is also a coordination rule which conjoins categories of the same type.6 Type-raising is applied to the categories NP, PP, and Sadj NP (adjectival phrase); it is currently implemented by simply adding pre-defined sets of type-raised categories to the chart whenever an NP, PP or Sadj NP is present.
Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.

This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ Komagata (1997).
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ The corresponding dependencies are given in the following figure, with the convention that arcs point away from arguments.

The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ So overall, almost 98% of the 2 352 unseen sentences were given some analysis.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ We emphasise that these additional rules and categories were not made available to the parser during testing, or used for training.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ Along with Hockenmaier and Steedman (2002b), this is the first CCG parsing work that we are aware of in which almost 98% of unseen sentences from the CCGbank can be parsed.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ To deal with the 48 no-analysis cases, the cut-off for the category-dictionary, K, was increased to 100.

Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ With these parameters, 2 098 of the 2 352 sentences received some analysis, with 206 timing out and 48 failing to parse.
Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ For our purposes, a dependency structure n is a C D pair, where C c1 c2 cn is the sequence of categories assigned to the words, and D hfi fi si hai i 1 m is the set of dependencies.

Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ CCG is unlike other formalisms in that the standard predicate-argument relations relevant to interpretation can be derived via extremely non-standard surface derivations.
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ The coverage was increased to ensure the test set was representative of the full section.
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.
