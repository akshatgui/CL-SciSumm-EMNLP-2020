Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Most recent wide-coverage statistical parsers have used models based on lexical dependencies (e.g.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Figure 2 gives part of a dependency structure returned by the parser for a sentence from section 00 (with the relations omitted).11 Notice that both respect and confidence are objects of had.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ If a word appears at least K times in the data, the supertagger only considers categories that appear in the word’s category set, rather than all lexical categories.
Also, a wide-coverage statistical parser which produces syntactic dependency structures for English is available for CCG (Clark et al, 2002). $$$$$ Initially the parser was run with 0 001 for the supertagger (an average of 38 categories per word), K 20 for the category dictionary, and a 0001 for the parser.

The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ The parser is able to capture a number of longrange dependencies that are not dealt with by existing treebank parsers.
The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ Derivations are written as follows, with underlines indicating combinatory reduction and arrows indicating the direction of the application: Formally, a dependency is defined as a 4-tuple: hf f s ha , where hf is the head word of the functor,2 f is the functor category (extended with head and dependency information), s is the argument slot, and ha is the head word of the argument—for example, the following is the object dependency yielded by the first step of derivation (3): The head of the infinitival complement’s subject is identified with the head of the object, using the variable X. Unification then “passes” the head of the object to the subject of the infinitival, as in standard unification-based accounts of control.3 The kinds of lexical items that use the head passing mechanism are raising, auxiliary and control verbs, modifiers, and relative pronouns.
The early dependency model of Clark et al (2002), in which model features were defined over only dependency structures, was partly motivated by these theoretical observations. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.

Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ The training and testing material for this CCG parser is a treebank of dependency structures, which have been derived from a set of CCG derivations developed for use with another (normal-form) CCG parser (Hockenmaier and Steedman, 2002b).
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head-dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ Not all trees produced dependency structures, since not all categories and type-changing rules in the CCGbank are encoded in the parser.

This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ The categories are combined using a small set of typed combinatory rules, such as functional application and composition (see Steedman (2000) for details).
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ The estimate in equation 10 suffers from sparse data problems, and so a backing-off strategy is employed.
This paper assumes a basic understanding of CCG; see Steedman (2000) for an introduction, and Clark et al (2002) and Hockenmaier (2003a) for an introduction to statistical parsing with CCG. $$$$$ We emphasise that these additional rules and categories were not made available to the parser during testing, or used for training.

Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ As far as long-range dependencies are concerned, it is similarly hard to give a precise evaluation.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ Such measures have been criticised by Lin (1995) and Carroll et al. (1998), who propose recovery of headdependencies characterising predicate-argument relations as a more meaningful measure.
Clark et al (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures. $$$$$ This will be important for applying the parser to tasks such as language modelling, for which the possibility of incremental processing of CCG appears particularly attractive.

This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ Heads and dependencies are always marked up on atomic categories (S, N, NP, PP, and conj in our implementation).
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ This impacts on how best to define a probability model for CCG, since the “spurious ambiguity” of CCG derivations may lead to an exponential number of derivations for a given constituent.
This extends the approach of Clark et al (2002) who modelled the dependency structures directly, not using any information from the derivations. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.

The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ Thus we ignore the normalisation factor, thereby simplifying the parsing process.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ To deal with the 48 no-analysis cases, the cut-off for the category-dictionary, K, was increased to 100.
The dependency structures considered in this paper are described in detail in Clark et al (2002) and Clark and Curran (2003). $$$$$ An advantage of our approach is that the recovery of long-range dependencies is fully integrated with the grammar and parser, rather than being relegated to a post-processing phase.

Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ To return a single dependency structure, we chose the most probable structure from the S dcl categories spanning the whole sentence.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
Following Clark et al (2002), evaluation is by precision and recall over dependencies. $$$$$ Capturing such dependencies is necessary for any parser that aims to support wide-coverage semantic analysis—say to support question-answering in any domain in which the difference between questions like Which company did Marks sue? and Which company sued Marks? matters.

 $$$$$ The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).
 $$$$$ The treebank of derivations, which we call CCGbank (Hockenmaier and Steedman, 2002a), was in turn derived (semi-)automatically from the handannotated Penn Treebank.
 $$$$$ The following category for the relative pronoun category (for words such as who, which, that) shows how heads are co-indexed for object-extraction: The derivation for the phrase The company that Marks wants to buy is given in Figure 1 (with the features on S categories removed to save space, and the constant heads reduced to the first letter).
 $$$$$ In future work we will present an evaluation which teases out the differences in extracted and insitu arguments.

The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ In an attempt to obtain a more thorough analysis, we analysed the performance of the parser on the 24 cases of extracted objects in the goldstandard Section 00 (development set) that were passed down the object relative pronoun category Sdcl✟ NPX NPX NPX .10 Of these, 10 (41.7%) were recovered correctly by the parser; 10 were incorrect because the wrong category was assigned to the relative pronoun, 3 were incorrect because the relative pronoun was attached to the wrong noun, and 1 was incorrect because the wrong category was assigned to the predicate from which the object was 9Currently all the modifiers in nominal compounds are analysed in CCGbank as N N, as a default, since the structure of the compound is not present in the Penn Treebank.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ Note that we encode the subject argument of the to category as a dependency relation (Marks is a “subject” of to), since our philosophy at this stage is to encode every argument as a dependency, where possible.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ As well as having a potential impact on the accuracy of the parser, recovering such dependencies may make the output more useful.
The results of Clark et al (2002) and Hockenmaier (2003a) are shown for comparison. $$$$$ To deal with the 48 no-analysis cases, the cut-off for the category-dictionary, K, was increased to 100.

This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ One solution is to consider only the normal-form (Eisner, 1996a) derivation, which is the route taken in Hockenmaier and Steedman (2002b).1 Another problem with the non-standard surface derivations is that the standard PARSEVAL performance measures over such derivations are uninformative (Clark and Hockenmaier, 2002).
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ Note that the supertagger does not provide a single category sequence for each sentence, and the final sequence returned by the parser (along with the dependencies) is determined by the probability model described in the previous section.
This paper assumes a basic knowledge of CCG; see Steedman (2000) and Clark et al (2002) for an introduction. $$$$$ The supertagger (described in Clark (2002)) assigns to each word all categories whose probabilities are within some constant factor, β, of the highest probability category for that word, given the surrounding context.

Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ To return a single dependency structure, we chose the most probable structure from the S dcl categories spanning the whole sentence.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ C ab cd for ac and bd is the number of times that word-category pairs ab and cd are in the same word-category sequence in the training data.
Following Clark et al (2002), we augment CCG lexical categories with head and dependency information. $$$$$ To obtain a point for an unlabelled dependency, the heads of the functor and argument must appear together in some relation (either as functor or argument) for the relevant sentence in the gold standard.

Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ We obtained dependency structures for roughly 95% of the trees in the data.
Clark et al (2002) give examples showing how heads can fill dependency slots during a derivation, and how long-range dependencies can be recovered through unification of co-indexed head variables. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.

We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ The parser is able to capture a number of longrange dependencies that are not dealt with by existing treebank parsers.
We have just begun the process of evaluating parsing performance using the same test data as Clark et al (2002). $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.

See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ The corresponding dependencies are given in the following figure, with the convention that arcs point away from arguments.
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ To deal with the 206 time-out cases, 0 was increased to 005, which resulted in 181 of the 206 sentences then receiving an analysis, with 18 failing to parse, and 7 timing out.
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.
See Steedman (2000) for an introduction to CCG, and see Clark et al (2002) and Hockenmaier (2003) for an introduction to wide-coverage parsing using CCG. $$$$$ So overall, almost 98% of the 2 352 unseen sentences were given some analysis.

Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ An advantage of our approach is that the recovery of long-range dependencies is fully integrated with the grammar and parser, rather than being relegated to a post-processing phase.
Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ We have explained elsewhere (Clark, 2002) how suitable features can be defined in terms of the word, pos-tag pairs in the context, and how maximum entropy techniques can be used to estimate the probabilities, following Ratnaparkhi (1996).
Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures. $$$$$ The tendency for the parser to assign the wrong category to the relative pronoun in part reflects the fact that complementiser that is fifteen times as frequent as object relative pronoun that.

This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ Thanks to Miles Osborne and the ACL-02 referees for comments.
This paper argues that probabilistic parsers should therefore model the dependencies in the predicate-argument structure, as in the model of Clark et al (2002), and defines a generative model for CCG derivations that captures these dependencies, including bounded and unbounded long-range dependencies. $$$$$ We would like to compare these results with those of other parsers that have presented dependencybased evaluations.

The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ An advantage of the dependency-based evaluation is that results can be given for individual dependency relations.
The conditional model used by the CCG parser of Clark et al (2002) also captures dependencies in the predicate-argument structure; however, their model is inconsistent. $$$$$ Various parts of the research were funded by EPSRC grants GR/M96889 and GR/R02450 and EU (FET) grant MAGICSTER.

Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ Because of the extreme naivety of the statistical model, these results represent no more than a first attempt at combining wide-coverage CCG parsing with recovery of deep dependencies.
Like Clark et al (2002), we define predicate argument structure for CCG in terms of the dependencies that hold between words with lexical functor categories and their arguments. $$$$$ The second parsing stage applies a CKY bottom-up chart-parsing algorithm, as described in Steedman (2000).

Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ There is also a coordination rule which conjoins categories of the same type.6 Type-raising is applied to the categories NP, PP, and Sadj NP (adjectival phrase); it is currently implemented by simply adding pre-defined sets of type-raised categories to the chart whenever an NP, PP or Sadj NP is present.
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ The corresponding dependencies are given in the following figure, with the convention that arcs point away from arguments.
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ As an additional experiment, we conditioned the dependency probabilities in 10 on a “distance measure” (A).
Like Clark et al. (2002), we do not take the lexical category of the dependent into account, and evaluate hhc; wi; i; h ; w0ii for labelled, and hh ; wi; ; h ; w0ii for unlabelled recovery. $$$$$ Chiang (2000) uses Tree Adjoining Grammar as an alternative to context-free grammar, and here we use another “mildly context-sensitive” formalism, Combinatory Categorial Grammar (CCG, Steedman (2000)), which arguably provides the most linguistically satisfactory account of the dependencies inherent in coordinate constructions and extraction phenomena.
