 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.
 $$$$$ This averaging effect has been shown to reduce overfitting and produce much more stable results (Collins, 2002).
 $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.

All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ The development set and the test set are parsed with a model trained on all 39832 training sentences.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ But despite its apparent success, there remains a major drawback: this method suffers from the limited scope of the nbest list, which rules out many potentially good alternatives.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.

From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ In other words, the optimal F-score tree in a forest is not guaranteed to be composed of two optimal F-score subtrees.
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ A priority queue (heap in Pseudocode 3) is used to hold the candidates for the next-best derivation, which is initialized to the set of best derivations along each hyperedge (lines 7 to 9).
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ Our final result outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.

The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ In other words, the optimal F-score tree in a forest is not guaranteed to be composed of two optimal F-score subtrees.
The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ The time complexity of this algorithm for a sentence of l words is O(JEJ · l2(a−1)) where a is the arity of the forest.

A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ For example, the WordEdges feature in Fig.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.

See Huang (2008) for more details. $$$$$ For example, the hyperedge for deduction (*) is notated: We also denote IN(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN(VP1,6) is {e1, e2}, with e2 = ((VBD1,2, NP2,6), VP1,6).
See Huang (2008) for more details. $$$$$ Shown in Figure 1, this sentence has (at least) two derivations depending on the attachment of the prep. phrase PP3,6 “with a mirror”: it can either be attached to the verb “saw”, or be attached to “him”, which will be further combined with the verb to form the same VP as above.
See Huang (2008) for more details. $$$$$ A CKY forest has an arity of 2, since the input grammar is required to be binary branching (cf.
See Huang (2008) for more details. $$$$$ More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges.

However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ Our final result outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features.

 $$$$$ We instead propose a method that reranks a packed forest of exponentially many parses.
 $$$$$ With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation.
 $$$$$ But despite its apparent success, there remains a major drawback: this method suffers from the limited scope of the nbest list, which rules out many potentially good alternatives.
 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.

With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ We can also express Eq.
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ A priority queue (heap in Pseudocode 3) is used to hold the candidates for the next-best derivation, which is initialized to the set of best derivations along each hyperedge (lines 7 to 9).
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ If there is another hyperedge e′ deriving node w, we also need to combine the resulting oracle functions from both hyperedges, for which we define a pointwise addition operator ®: Shown in Pseudocode 4, we perform these computations in a bottom-up topological order, and finally at the root node TOP, we can compute the best global F-score by maximizing over different numbers of test brackets (line 7).

That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ It is worth noting that some features which seem complicated at the first sight are indeed local.
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ In other words, the optimal F-score tree in a forest is not guaranteed to be composed of two optimal F-score subtrees.
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length.

Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005).

 $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
 $$$$$ With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation.
 $$$$$ In practice it takes on average 0.05 seconds for forests pruned by p = 10 (see Section 4.2), but we can pre-compute and store the oracle for each forest before training starts.
 $$$$$ Our final result (91.7) is better than any previously reported system trained on the Treebank, although 7It is surprising that 50-best reranking with local features achieves an even higher F-score of 91.28, and we suspect this is due to the aggressive updates and instability of the perceptron, as we do observe the learning curves to be non-monotonic.

Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and non-local features.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ We instead propose a dynamic programming algorithm which optimizes the number of matched brackets for a given number of test brackets.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.

Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ We also use a refinement called “averaged parameters” where the final weight vector is the average of weight vectors after each sentence in each iteration over the training data.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ In n-best reranking, all features are treated equivalently by the decoder, which simply computes the value of each one on each candidate parse.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it.

 $$$$$ 9 in a purely functional form where 1� is a translation operator which shifts a function along the axes: Above we discussed the case of one hyperedge.
 $$$$$ In this work we use the averaged perceptron algorithm (Collins, 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods.
 $$$$$ The oracle tree y+ can be recursively restored by keeping backpointers for each ora[v](t), which we omit in the pseudocode.
 $$$$$ For example, the arity of the forest in Figure 1 is 3.

In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ We instead propose a method that reranks a packed forest of exponentially many parses.
In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ The key difference from (Charniak and Johnson, 2005) is that in this algorithm, a node can “partially” survive the beam, with a subset of its hyperedges pruned.
In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ The key idea is to compute non-local features incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1).
In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ In addition, the feature extraction step in 100-best reranking produces huge data files and takes 44 hours in total, though this part can be parallelized.8 On two CPUs, 100-best reranking takes 25 hours, while our forest-reranker can also finish in 26 hours, with a much smaller disk space.

We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005).
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope.

Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ With efficient approximate decoding, perceptron training on the whole Treebank becomes practical, which can be done in about a day even with a Python implementation.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ Our final result outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ For example, the hyperedge for deduction (*) is notated: We also denote IN(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN(VP1,6) is {e1, e2}, with e2 = ((VBD1,2, NP2,6), VP1,6).

To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ We instead propose a dynamic programming algorithm which optimizes the number of matched brackets for a given number of test brackets.
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it.

For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance.
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ For example, the WordEdges feature in Fig.
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ Non-local features, however, can not be precomputed, but we still prefer to compute them as early as possible, which we call “on-the-fly” computation, so that our decoder can be sensitive to them at internal nodes.
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.
