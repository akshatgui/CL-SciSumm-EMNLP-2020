 $$$$$ We believe this general framework could also be applied to other problems involving forests or lattices, such as sequence labeling and machine translation.
 $$$$$ reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives.
 $$$$$ For example, the arity of the forest in Figure 1 is 3.

All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ These two derivations can be represented as a single forest by sharing common sub-derivations.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ However, we miss the benefits of non-local features that are not representable here.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the baseline system.
All those methods fall short of re ranking parsers like Charniak and Johnson (2005) and Huang (2008), which, however, have access to many additional features, that can not be used in our dynamic program. $$$$$ We can also express Eq.

From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ 9 in a purely functional form where 1� is a translation operator which shifts a function along the axes: Above we discussed the case of one hyperedge.
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ As a result, we often see very few variations among the n-best trees, for example, 50best trees typically just represent a combination of 5 to 6 binary ambiguities (since 25 < 50 < 26).
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ reranking techniques ofsuffer from the limited scope of the best list, which rules out many potentially alternatives.
From the presented data, we can see that indirect re ranking on LAS may not seem as good as direct re ranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. $$$$$ Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope.

The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ For both systems, we first use only the local features, and then all the features.
The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ The oracle tree y+ can be recursively restored by keeping backpointers for each ora[v](t), which we omit in the pseudocode.
The combination of n-best lists would not scale up and working on the ambiguous structure itself, the packed forest as in (Huang, 2008), might be necessary. $$$$$ Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.

A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ We instead propose a method that reranks a packed forest of exponentially many parses.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope.
A common objection to re ranking is that the candidate set may not be diverse enough to allow for much improvement unless it is very large; the candidates may be trivial variations that are all very similar to the top-scoring candidate (Huang, 2008). $$$$$ We instead propose a method that reranks a packed forest of exponentially many parses.

See Huang (2008) for more details. $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.
See Huang (2008) for more details. $$$$$ The oracle tree y+ can be recursively restored by keeping backpointers for each ora[v](t), which we omit in the pseudocode.
See Huang (2008) for more details. $$$$$ Then we have: Intuitively, we compute the unit non-local features at each subtree from bottom-up.
See Huang (2008) for more details. $$$$$ Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005).

However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ So we first classify features into local and non-local, which the decoder will process in very different fashions.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ There are 0.8M features in our final set, considerably fewer than that of Charniak and Johnson which has about 1.3M features in the updated version.5 However, our initial experiments show that, even with this much simpler feature set, our 50-best reranker performed equally well as theirs (both with an F-score of 91.4, see Tables 3 and 4).
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.
However, Huang (2008) shows that the use of non-local features does in fact contribute substantially to parser performance. $$$$$ The baseline parser is the Charniak parser, which we modified to output a packed forest for each sentence.3 We use the standard split of the Treebank: sections 02-21 as the training data (39832 sentences), section 22 as the development set (1700 sentences), and section 23 as the test set (2416 sentences).

 $$$$$ To put our results in perspective, we also compare them with other best-performing systems in Table 4.
 $$$$$ For a CKY forest, this amounts to O(l3 · l2) = O(l5), but for general forests like those in our experiments the complexities are much higher.
 $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.

With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ Analogous to the language model cost in forest rescoring, the unit feature cost here is a non-monotonic score in the dynamic programming backbone, and the derivations may thus be extracted out-of-order.
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length.
With the ability to incorporate non-local phrase-structure parse features (Huang, 2008), we can recognize dependency features of arbitrary order. $$$$$ For instance, the NGramTree feature in Fig.

That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges.
That is, not only is phrase+deps better at dependency recovery than its component parts, but phrase+deps+gen is also considerably better on dependency recovery than phrase+gen, which represents the previous state-of-the-art in this vein of research (Huang, 2008). $$$$$ 2 (b) can be computed when the S subtree is formed.

Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ In practice, this method prunes on average 15% more hyperedges than their method.
Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ Our final result outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
Note that our model phrase+gen uses essentially the same features as Huang (2008), so the fact that our phrase+gen is noticeably more accurate on F1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy. $$$$$ Since exact inference is intractable with non-local features, we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole Treebank.

 $$$$$ We also devised a dynamic programming algorithm for forest oracles, an interesting problem by itself.
 $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.
 $$$$$ For example, the hyperedge for deduction (*) is notated: We also denote IN(v) to be the set of incoming hyperedges of node v, which represent the different ways of deriving v. For example, in the forest in Figure 1, IN(VP1,6) is {e1, e2}, with e2 = ((VBD1,2, NP2,6), VP1,6).

Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ 2(a) is local, while the ParentRule feature in Fig.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ For example, our algorithm will ask questions like, “when a test parse has 5 brackets, what is the maximum number of matched brackets?” More formally, at each node v, we compute an oracle function ora[v] : N H N, which maps an integer t to ora[v](t), the max. number of matched brackets Pseudocode 4 Forest Oracle Algorithm When node v is combined with another node u along a hyperedge e = ((v, u), w), we need to combine the two oracle functions ora[v] and ora[u] by distributing the test brackets of w between v and u, and optimize the number of matched bracktes.
Huang (2008) proposed to use a parse forest to incorporate non-local features. $$$$$ Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005).

Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ However, we miss the benefits of non-local features that are not representable here.
Our parser achieved an f-score of 88.4on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). $$$$$ Chomsky Normal Form) to ensure cubic time parsing complexity.

 $$$$$ This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing.
 $$$$$ Moreover, their technique is orthogonal to ours, and we suspect that replacing their n-best reranker by our forest reranker might get an even better performance.
 $$$$$ A priority queue (heap in Pseudocode 3) is used to hold the candidates for the next-best derivation, which is initialized to the set of best derivations along each hyperedge (lines 7 to 9).
 $$$$$ This result is also better than any previously reported systems trained on the Treebank.

In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ This situation becomes worse with longer sentences because the number of possible interpretations usually grows exponentially with the sentence length.
In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists.
In the second pass, we use the hyper graph reranking algorithm (Huang, 2008) to find promising translations using additional dependency features (i.e., features 810 in the list). $$$$$ There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length.

We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ For a given sentence s, a generic reranker selects the best parse y� among the set of candidates cand(s) according to some scoring function: In n-best reranking, cand(s) is simply a set of n-best parses from the baseline parser, that is, cand(s) = {y1, y2, ... , yn}.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ For example, consider the following sentence 0 I 1 saw 2 him 3 with 4 a 5 mirror 6 where the numbers between words denote string positions.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ For example, our algorithm will ask questions like, “when a test parse has 5 brackets, what is the maximum number of matched brackets?” More formally, at each node v, we compute an oracle function ora[v] : N H N, which maps an integer t to ora[v](t), the max. number of matched brackets Pseudocode 4 Forest Oracle Algorithm When node v is combined with another node u along a hyperedge e = ((v, u), w), we need to combine the two oracle functions ora[v] and ora[u] by distributing the test brackets of w between v and u, and optimize the number of matched bracktes.
We follow Mi and Huang (2008) to estimate the fractional count of a rule extracted from an aligned forest pair. $$$$$ 3, the unit NGramTree instance is for the pair (wj−1, wj) on the boundary between the two subtrees, whose smallest common ancestor is the current node.

Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the baseline system.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ 9 in a purely functional form where 1� is a translation operator which shifts a function along the axes: Above we discussed the case of one hyperedge.
Then, we ran the Python scripts (Huang, 2008) provided by Liang Huang to output packed forests. $$$$$ There is also a distinguished root node TOP in each forest, denoting the goal item in parsing, which is simply S0,l where S is the start symbol and l is the sentence length.

To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ 23.
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ Our final result, an F-score of 91.7, outperforms both 50-best and 100-best reranking baselines, and is better than any previously reported systems trained on the Treebank.
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ Such a Treebank-style forest is easier to work with for reranking, since many features can be directly expressed in it.
To prune the packed forests, Huang (2008) uses inside and outside probabilities to compute the distance of the best derivation that traverses a hyper edge away from the globally best derivation. $$$$$ Plus, except for n-best reranking, most discriminative methods require repeated parsing of the training set, which is generally impratical (Petrov and Klein, 2008).

For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ Discriminative reranking has become a popular technique for many NLP problems, in particular, parsing (Collins, 2000) and machine translation (Shen et al., 2005).
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like PP3,6 are called nodes, and deductive steps like (*) correspond to hyperedges.
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as "NGramTree" features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j+1 when two constituents CY, i, j and CZ, j, k are combined. $$$$$ The time complexity of this algorithm for a sentence of l words is O(JEJ · l2(a−1)) where a is the arity of the forest.
