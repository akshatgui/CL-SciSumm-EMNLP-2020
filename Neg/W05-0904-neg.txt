Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. $$$$$ Our syntax-based measures require the existence of a parser for the language in question, however it is worth noting that a parser is required for the target language only, as all our measures of similarity are defined across hypotheses and references in the same language.
Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. $$$$$ Dependency trees consist of trees of head-modifier relations with a word at each node, rather than just at the leaves.
Inspired in the work by Liu and Gildea (2005), who introduced a series of metrics based on con stituent/dependency syntactic matching, we have designed three subgroups of syntactic similarity metrics. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.

Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ BLEU and NIST have been shown to correlate closely with human judgments in ranking MT systems with different qualities (Papineni et al., 2002; Doddington, 2002).
Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ The hypothesis is missing the word “fifth”, but was nonetheless assigned a high score by human judges.
Owczarzak et al (2007a, b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syn tactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. $$$$$ Thus the evaluation of fluency can be transformed as computing the syntactic similarity of the hypothesis and the references.

Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ This means that the syntax-based evaluation measures can succeed even when the tree structure for a poor hypothesis looks reasonable on its own, as long as it is sufficiently distinct from the structures used in the references.
Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years.
Liu and Gildea (2005) measure the syntactic similarity between MT output and reference translation. $$$$$ The kernel based metrics, though poor in sentence-level evaluation, achieve the best results in corpus-level evaluation, where sparse data are less of a barrier.

Similarities are captured from different viewpoints $$$$$ Consider the following simple example:
Similarities are captured from different viewpoints $$$$$ Figure 1 shows syntactic trees for the example sentences, from which we can see that hypothesis 1 has exactly the same syntactic structure with the reference, while hypothesis 2 has a very different one.
Similarities are captured from different viewpoints $$$$$ The translations were generated by the alignment template system of Och (2003).
Similarities are captured from different viewpoints $$$$$ Thus hypothesis 2 will get a higher score than hypothesis 1.

This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ For the 1-depth subtrees, we get S, NP, VP, PRON, V, NP which also appear in the reference syntax tree.
This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ This shows that corpus-level evaluation, compared with the sentence-level evaluation, is much less sensitive to the sparse data problem and thus leaves more space for making use of comprehensive evaluation metrics.
This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ One part is a set of 665 English sentences generated by a ChineseEnglish MT system.
This metric corresponds to the STM metric presented by Liu and Gildea (2005). $$$$$ Thus hypothesis 2 will get a higher score than hypothesis 1.

Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. $$$$$ This type of feature cannot capture the grammaticality of the sentence, in part because they do not take into account sentence-level information.
Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement. $$$$$ While the machine learning approach improves correlation with human judgments, all the metrics discussed are based on the same type of information: n-gram subsequences of the hypothesis translations.

While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ Often the intended meaning can be inferred; often it cannot.
While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ And for each MT hypothesis, three reference translations are associated with it.
While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a Lexical Functional Grammar (LFG) parser. $$$$$ For the overall evaluation of sentences for fluency and adequacy, the metric based on headword chain performs better than BLEU in both sentencelevel and corpus-level correlation with human judgments.

These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ Acknowledgments We are very grateful to Alex Kulesza for assistance with the JHU data.
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc. $$$$$ This means that the syntax-based evaluation measures can succeed even when the tree structure for a poor hypothesis looks reasonable on its own, as long as it is sufficiently distinct from the structures used in the references.

Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ However, if we evaluate their fluency based on the syntactic similarity with the reference, we will get our desired results.
Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ For the same reason, the subtree NP→PRON can only be counted once.
Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ Thus hypothesis 2 will get a higher score than hypothesis 1.
Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005). $$$$$ The dependency tree contains both the lexical and syntactic information, which inspires us to use it for the MT evaluation.

This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ For example, if the same subtree occurs 10 times in both the hypothesis and the reference, this contributes a term of 100 to the dot product, rather than 10 in the clipped count used by BLEU and by our subtree metric STM.
This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ Modern statistical parsers have been tuned to discriminate good structures from bad rather than good sentences from bad.
This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ Thus the evaluation of fluency can be transformed as computing the syntactic similarity of the hypothesis and the references.
This finding has been previously reported, among others, in Liu and Gildea (2005). $$$$$ Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.

The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ Indeed, in some recent work on re-ranking machine translation hypotheses (Och et al., 2004), parserproduced structures were not found to provide helpful information, as a parser is likely to assign a goodlooking structure to even a lousy input hypothesis.
The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ The last row of the each table shows the treekernel-based measures, which have no depth parameter to adjust, but implicitly consider all depths.
The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ Consider the following simple example:
The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric. $$$$$ For every hypothesis, BLEU computes the fraction of n-grams which also appear in the reference sentences, as well as a brevity penalty.

These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ The fluency and adequacy scores both range from 1 to 5.
These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
These metrics are similar to the Syntac tic Tree Matching metric defined by Liu and Gildea (2005), in this case applied to DRSsinstead of constituent trees. $$$$$ Similarly, the fraction of 3-depth subtrees is 1 out of 2.

We use three different kinds of metrics $$$$$ Our syntactic metrics are motivated by a desire to better capture grammaticality in MT evaluation, and thus we are most interested in how well they correlate with human judgments of sentences’ fluency, rather than the adequacy of the translation.
We use three different kinds of metrics $$$$$ To do this, the syntactic metrics (computed with the Collins (1999) parser) as well as BLEU were used to evaluate hypotheses in the test set from ACL05 MT workshop, which provides both fluency and adequacy scores for each sentence, and their Pearson coefficients of correlation with the human fluency scores were computed.

The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ However, if we evaluate their fluency based on the syntactic similarity with the reference, we will get our desired results.
The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ From the results, we can see that with the increase of n-grams length, the performance of BLEU and HWCM will first increase up to length 5, and then starts decreasing, where the optimal n-gram length of 5 corresponds to our usual setting for BLEU algorithm.
The usual practice to model the wellformedness of a sentence is to employ the n-gram language model or compute the syntactic structure similarity (Liu and Gildea 2005). $$$$$ For the overall evaluation of sentences for fluency and adequacy, the metric based on headword chain performs better than BLEU in both sentencelevel and corpus-level correlation with human judgments.

coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. $$$$$ As discussed above, the existing word-based metrics can not give a clear evaluation for the hypothesis’ fluency.
coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. $$$$$ This paper develops a number of syntactically motivated evaluation metrics computed by automatically parsing both reference and hypothesis sentences.
coefficients reported in Albrecht and Hwa (2007) including smoothed BLEU (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), HWCM (Liu and Gildea 2005), and the metric proposed in Albrecht and Hwa (2007) using the full feature set. $$$$$ Clipped here means that, for a given subtree, the count computed from the hypothesis syntax tree can not exceed the maximum number of times the subtree occurs in any single reference’s syntax tree.

Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ This paper develops a number of syntactically motivated evaluation metrics computed by automatically parsing both reference and hypothesis sentences.
Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on the assumption that “The closer a machine translation is to a professional human translation, the better it is” (Papineni et al., 2002).
Syntactic Score (SC) Some erroneous sentences often contain words and concepts that are locally correct but can not form coherent sentences (Liu and Gildea, 2005). $$$$$ The hypothesis is missing the word “fifth”, but was nonetheless assigned a high score by human judges.

For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. $$$$$ The problem can be explained by the limitation of the reference translations.
For example, Liu and Gildea (2005) developed the Sub-Tree Metric (STM) over constituent parse trees and the Head-Word Chain Metric (HWCM) over dependency parse trees. $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.

This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ In this particular sentence, the common head-modifier relations “aboard +– plane” and “plane +– the” caused a high headword chain overlap, but did not appear as common n-grams counted by BLEU.
This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ As MT systems improve, the shortcomings of n-gram based evaluation are becoming more apparent.
This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ Figure 3 gives an algorithm which recursively extracts the headword chains in a dependency tree from short to long.
This phenomenon has been previously observed by Liu and Gildea (2005). $$$$$ We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.

This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ The other set of testing data is from MT evaluation workshop at ACL05.
This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ The results show that in both systems our syntactic metrics all achieve a better performance in the correlation with human judgments of fluency.
This direction was first explored by (Liu and Gildea, 2005), who used syntactic structure and dependency information to go beyond the surface level matching. $$$$$ The result is obviously incorrect.

With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement. $$$$$ Just as with the fluency evaluation, HWCM and other syntactic metrics present more stable performance as the n-gram’s length (subtree’s depth) increases.
With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement. $$$$$ However, there is an important distinction between the use of parsers in re-ranking and evaluation – in the present work we are looking for similarities between pairs of parse trees rather than at features of a single tree.
With the addition of partial matching and n-best parses, Owczarzak et al (2007)'s method considerably outperforms Liu and Gildea's (2005 )w.r.t. correlation with human judgement. $$$$$ For the example of the reference syntax tree in Figure 2, the whole tree with the root S represents a sentence; and the subtree NP—*ART N represents a noun phrase.
