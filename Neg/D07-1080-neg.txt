Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ L(?)
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ 4.1 Margin Infused Relaxed Algorithm.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ The model wastrained on a small development set.

Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ ?b? is a corresponding target side where ?b is a string of termi nals, or a phrase, and ? is a (possibly empty) stringof non-terminals.
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ We applied an Earley-style top-down parsing approach(Wu and Wong, 1998; Watanabe et al, 2006b; Zoll mann and Venugopal, 2006).
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ In Section 3, a set of binary sparse features are defined including numeric features for our baseline system.

MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ Experiments on Arabic-to English translation indicated that a modeltrained with sparse binary features outper formed a conventional SMT system with a small number of features.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ For ex ample, the word ?2007/6/27?
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ This result indicates that we can easily experiment many alternative features evenwith a small data set, but we believe that our ap proach can scale well to a larger data set for furtherimproved performance.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences.

The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ When decoding, a 1000-best list is generated to achieve better oracle translations.
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ New wi+1 is computed using the k-best list Ct with respect to the oracle translations Ot (line 5).
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ For ex ample, the word ?2007/6/27?
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ Therefore, insertion fea tures are integrated in which no word alignment isassociated in the target.

Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus.
Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ The optimiza tion was carried out by MIRA, which is an onlineversion of the large-margin training algorithm.
Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ Liang et al (2006) presented a similar up dating strategy in which parameters were updated toward an oracle translation found in Ct, but ignored potentially better translations discovered in the past iterations.

Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ si+1( f t, e?)
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ and by updating ?(?)
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ We achieved a state of the art performance in statistical machine translation by using a large number of features with an onlinelarge-margin training algorithm.

(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ After N iterations, the algorithm returns an averaged weight vector to avoid overfitting (line 9).
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ Section 4 introduces an online large-margin training algorithm using MIRA with our key components.
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ The digit sequence normalization provides a similar generaliza tion capability despite of the moderate increase in the active feature size.
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ Such a quasi-syntactic structure can naturally capture the reordering of phrases that is notdirectly modeled by a conventional phrase-based approach (Koehn et al, 2003).

In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ Mil lions of sparse features are intuitively considered prone to overfitting, especially when trained on a small development set.
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ Digits replaced by a sequence of ?@?.
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ An incorrect translation was updated towardan oracle translation found in a k-best list, but discarded potentially better translations in the past iter ations.
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ The use of phrase?b as a prefix maintains the strength of the phrase base framework.

The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ (2) where X is a non-terminal, ? is a source side string ofarbitrary terminals and/or non-terminals.
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ Future work involves scal ing up to larger data and more features.
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ by taking the prefix and suffix, respectively.?
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ The hierarchical features are extracted only for those source wordsthat are aligned with the target side to limit the fea ture size.
