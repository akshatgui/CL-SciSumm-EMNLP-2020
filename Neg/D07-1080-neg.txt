Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ 1 is a brevity penalty.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based ei ther on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al, 2003; Chiang, 2005) orsyntax-based translation (Galley et al, 2006).
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences.
Sentence-level approximations to BLUE exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform BLUE computations in the context of a set O of previously-translated sentences, following Watanabe et al (2007). $$$$$ We exploited a large number of binary features for statistical machine translation.

Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ word ?tnthk?
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ + ?(e?, e?)
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ Chiang (2005) introduced the hierarchical phrase based translation approach, in which non-terminalsare embedded in each phrase.
Work has been done to investigate a perceptron-like online margin training for statisitical machine translation (Watanabe et al, 2007). $$$$$ Digits replaced by a sequence of ?@?.

MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ The mil lions of parameters were tuned only on a small development set consisting of less than1K sentences.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ Second, the word alignment is refined by a grow-diag-final heuristic (Koehn et al, 2003).
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ The hierarchical features are extracted only for those source wordsthat are aligned with the target side to limit the fea ture size.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.

The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ We may includeas many constraints as possible, like m-oracle con straints in our experiments.
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ Thedevelopment set comes from the MT2003 Arabic English NIST evaluation test set consisting of 663 sentences in the news domain with four reference translations.
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ by taking the prefix and suffix, respectively.?
The incorporation of a large number of sparse feature functions is described in (Watanabe et al, 2007). $$$$$ How ever, it does not scale well with a large number of features of the order of millions.

Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ The basic idea is to perform top-down parsing so that the projected target side is generated in a left-to-right manner.
Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ However, our algorithm with 771millions of features achieved very significant im provements over a conventional method with a small number of features.
Along with MIRA (Margin Infused Relaxed Algorithm) (Watanabe et al, 2007), MERT is the most widely used algorithm for system optimization. $$$$$ Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.

Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ Hierarchical phrase translation probabilities in both directions, h(?|?b?) and h(?b?|?), estimated by relative counts, count(?, ?b?).
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ Mil lions of sparse features are intuitively considered prone to overfitting, especially when trained on a small development set.
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ From theword alignment encoded in this phrase, we can ex tract word pair features of (ei, f j+1), (ei+2, f j+2) and (ei+3, f j).
Sparse features used in reranking are extracted according to (Watanabe et al, 2007). $$$$$ We achieved a state of the art performance in statistical machine translation by using a large number of features with an onlinelarge-margin training algorithm.

(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ = argmax e wT ? h( f , e) (1) where h( f , e) is a large-dimension feature vector.
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ The larger m-oracle size seems to be harmful if coupled with the 1-best list.As indicated by the reduced active feature size, 1 best translation seems to be updated toward worse translations in 10-oracles that are ?close?
(Watanabe et al, 2007) also reports the possibility of overfitting in their dataset (Arabic-English newswire translation), especially when domain differences are present. $$$$$ Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.

In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ However, our algorithm with 771millions of features achieved very significant im provements over a conventional method with a small number of features.
In fact, some structured prediction algorithms, such as the MIRA algorithm used in dependency parsing (McDonald et al, 2005) and MT (Watanabe et al, 2007) uses iterative sets of N-best lists in its training process. $$$$$ If multiple word alignments are observed ei?1 ei ei+1 ei+2 ei+3 ei+4 f j?1 f j f j+1 f j+2 f j+3 Figure 1: An example of sparse features for a phrase translation.with the same source and target sides, only the fre quently observed word alignment is kept to reduce the grammar size.

The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ Tillmann and Zhang (2006), Liang et al (2006) and Bangalore et al (2006) introduced sparse binary features for statistical machine translation trained ona large training corpus.
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ 1 N N ? n=1 log pn(E, E) ? ?
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ The setting severely overfit to the development data, and therefore resulted in worse results in open tests.
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems (Liang et al 2006), and extended to use large-margin training in an online frame work (Watanabe et al 2007). $$$$$ Our online large-margin training with 10-oracle and 10 best constraints and the approximated BLEU lossfunction significantly outperformed the baseline sys tem in the open test.
