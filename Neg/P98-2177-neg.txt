We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ Note that while the head word tuples represent correct attachments only 69% of the time, their quantity is about 45 times greater than the quantity of data used in previous supervised approaches.
We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.
We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ However in sentence 2, with modifies the verb washed since with soap describes how the shirt is washed.

(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ Most of the previous successful approaches to this problem have been statistical or corpusbased, and they consider only prepositions whose attachment is ambiguous between a preceding noun phrase and verb phrase.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ Then p(cb = trueln) is the conditional probability that a particular noun n in free text has an unambiguous prepositional phrase attachment.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ This may be due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ This may be due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence.

 $$$$$ We successfully demonstrated the portability of our approach by applying it to the prepositional phrase attachment task in the Spanish language.
 $$$$$ Then p(cb = trueln) is the conditional probability that a particular noun n in free text has an unambiguous prepositional phrase attachment.
 $$$$$ Note that while the head word tuples represent correct attachments only 69% of the time, their quantity is about 45 times greater than the quantity of data used in previous supervised approaches.

The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ The chunker, implemented with two small regular expressions, then replaces simple noun phrases and quantifier phrases with their head words.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ Chunking was not performed on the Spanish data.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ The order in which these tools are applied to raw text is shown in Table 1.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ The performance of the classifiers Clbigram, dinterp) and c/base, when trained and tested on Spanish language data, are shown in Table 6.

The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ We present results for prepositional phrase attachment in both English and Spanish.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ Then p(cb = trueln) is the conditional probability that a particular noun n in free text has an unambiguous prepositional phrase attachment.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ This technique uses the bigram counts of the extracted head word tuples, and backs off to the uniform distribution when the denominator is zero. where 7) is the set of possible prepositions, where all the counts c(...) are from the extracted head word tuples.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ Tables 2 and 3 list the most frequent extracted head word tuples for unambiguous verb and noun attachments, respectively.

The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ Since only a small part of the extraction heuristic is specific to English, and since partof-speech taggers and morphology databases are widely available in other languages, our approach is far more portable than previous approaches for this problem.
The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy.
The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.
The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ The performance of the classifiers Clbigram, dinterp) and c/base, when trained and tested on Spanish language data, are shown in Table 6.

As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ The unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ We successfully demonstrated the portability of our approach by applying it to the prepositional phrase attachment task in the Spanish language.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ This technique uses the bigram counts of the extracted head word tuples, and backs off to the uniform distribution when the denominator is zero. where 7) is the set of possible prepositions, where all the counts c(...) are from the extracted head word tuples.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ They annotated two sets (using the full sentence context); one set consisted of all ambiguous prepositional phrase attachments of the form (v, n,p, n2), and the other set consisted of cases where p = con.

 $$$$$ The main idea of the extraction heuristic is that an attachment site of a preposition is usually within a few words to the left of the preposition.
 $$$$$ The unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser.
 $$$$$ While the extracted tuples of the form (n, p, n2) and (v, p, n2) represent unambiguous noun and verb attachments in which either the verb or noun is known, our eventual goal is to resolve ambiguous attachments in the test data of the form (v, n,p, n2), in which both the noun n and verb v are always known.

 $$$$$ We present results for prepositional phrase attachment in both English and Spanish.
 $$$$$ Three native Spanish speakers were asked to extract and annotate ambiguous instances of Spanish prepositional phrase attachments.
 $$$$$ For example, the task in the following examples is to decide whether the preposition with modifies the preceding noun phrase (with head word shirt) or the preceding verb phrase (with head word bought or washed).

 $$$$$ We use word statistics from both the tagged corpus and the set of extracted head word tuples to estimate the probability of generating 0 = true, p, and n2.
 $$$$$ (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy.
 $$$$$ They annotated two sets (using the full sentence context); one set consisted of all ambiguous prepositional phrase attachments of the form (v, n,p, n2), and the other set consisted of cases where p = con.

More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ Prepositional phrase attachment is the task of deciding, for a given preposition in a sentence, the attachment site that corresponds to the interpretation of the sentence.
More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ The Penn Treebank is drawn from the 1989 Wall St. Journal data, so there is no possibility of overlap with our training data.
More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ The quantities Pr(trueln) and Pr(truelv) denote the conditional probability that n or v will occur with some unambiguously attached preposition, and are estimated as follows: p, and define cN Ep cAp) as the number of noun attached tuples.

Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ They annotated two sets (using the full sentence context); one set consisted of all ambiguous prepositional phrase attachments of the form (v, n,p, n2), and the other set consisted of cases where p = con.
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information.

Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ We used 450k sentences of raw text from the Linguistic Data Consortium's Spanish News Text Collection to extract a training set, and we used a non-overlapping set of 50k sentences from the collection to create test sets.
Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ We successfully demonstrated the portability of our approach by applying it to the prepositional phrase attachment task in the Spanish language.

A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ The heuristic ignores cases where p = of, since such cases are rarely ambiguous, and we opt to model them deterministically as noun attachments.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Analogously, define cv(p) = E, c(v, p, true) and cv = Ep cv (P)â€¢ The counts c(n,p,true) and c(v,p,true) are from the extracted head word tuples.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Since only a small part of the extraction heuristic is specific to English, and since partof-speech taggers and morphology databases are widely available in other languages, our approach is far more portable than previous approaches for this problem.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ For testing our classifier, we used only those judgments on which all three annotators agreed.

The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ While previous corpus-based methods are highly accurate for this task, they are difficult to port to other languages because they require resources that are expensive to construct or simply nonexistent in other languages.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ We present results for prepositional phrase attachment in both English and Spanish.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ Therefore, there is only one possible attachment site for the preposition, and either the verb v or the noun n does not exist, in the case of noun-attached preposition or a verb-attached preposition, respectively.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ In that step, unambiguous attachments from the FIDDITCH parser's output are initially used to resolve some of the ambiguous attachments, and the resolved cases are iteratively used to disambiguate the remaining unresolved cases.

equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ The exact task of our algorithm will be to construct a classifier c/ which maps an instance of an ambiguous prepositional phrase (v, n, p, n2) to either N or V, corresponding to noun attachment or verb attachment, respectively.
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ Both classifiers c/interp and c/bigram clearly outperform the. baseline, but the classifier does not outperform dbigraml even though it interpolates between the less specific evidence (the preposition counts) and more specific evidence (the bigram counts).
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ For testing our classifier, we used only those judgments on which all three annotators agreed.

(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ Approximately 970K unannotated sentences from the 1988 Wall St. Journal were processed in a manner identical to the example sentence in Table 1.
(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.
