We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
We rely on Gsearch to provide moderately accurate information about verb frames in the same way that Hindle and Rooth (1993) relied on Fidditch to provide moderately accurate information about syntactic structure, and Ratnaparkhi (1998) relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that provided by Fidditch. $$$$$ Since only a small part of the extraction heuristic is specific to English, and since partof-speech taggers and morphology databases are widely available in other languages, our approach is far more portable than previous approaches for this problem.

(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ In that step, unambiguous attachments from the FIDDITCH parser's output are initially used to resolve some of the ambiguous attachments, and the resolved cases are iteratively used to disambiguate the remaining unresolved cases.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ This may be due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence.
(Ratnaparkhi, 1998) first assumes noun attachment for all of-PPs and then applies his disambiguation methods to all remaining PPs. $$$$$ They annotated two sets (using the full sentence context); one set consisted of all ambiguous prepositional phrase attachments of the form (v, n,p, n2), and the other set consisted of cases where p = con.

 $$$$$ For example, the (v,n,p, n2) tuples corresponding to the example sentences are The correct classifications of tuples 1 and 2 are N and V, respectively.
 $$$$$ For example, the (v,n,p, n2) tuples corresponding to the example sentences are The correct classifications of tuples 1 and 2 are N and V, respectively.
 $$$$$ Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary.

The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ In that step, unambiguous attachments from the FIDDITCH parser's output are initially used to resolve some of the ambiguous attachments, and the resolved cases are iteratively used to disambiguate the remaining unresolved cases.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ The unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser.
The most prominent unsupervised methods are the Lexical Association score by Hindle and Rooth (1993) and the co occurrence values by Ratnaparkhi (1998). $$$$$ Finally, numbers are replaced by a single token, the text is converted to lower case, and the morphology database is used to find the base forms of the verbs and nouns.

The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ We present an unsupervised algorithm for prepositional phrase attachment in English that requires only an part-of-speech tagger and a morphology database, and is therefore less resourceintensive and more portable than previous approaches, which have all required either treebanks or partial parsers.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ In sentence 1, with modifies the noun shirt, since with pockets describes the shirt.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ Since only a small part of the extraction heuristic is specific to English, and since partof-speech taggers and morphology databases are widely available in other languages, our approach is far more portable than previous approaches for this problem.
The co occurrence values for verb V and noun N correspond to the probability estimates in (Ratnaparkhi, 1998) except that Ratnaparkhi includes a back-off to the uniform distribution for the zero denominator case. $$$$$ It is the hypothesis of this approach that the information in just the unambiguous attachment events can resolve the ambiguous attachment events of the test data.

The current unsupervised state of the art achieves 81.9% attachment accuracy (Ratnaparkhi, 1998). $$$$$ Our result shows that the information in imperfect but abundant data from unambiguous attachments, as shown in Tables 2 and 3, is sufficient to resolve ambiguous prepositional phrase attachments at accuracies just under the supervised state-of-the-art accuracy.

As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ We use word statistics from both the tagged corpus and the set of extracted head word tuples to estimate the probability of generating 0 = true, p, and n2.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ We present an unsupervised algorithm for prepositional phrase attachment in English that requires only an part-of-speech tagger and a morphology database, and is therefore less resourceintensive and more portable than previous approaches, which have all required either treebanks or partial parsers.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ We present an unsupervised algorithm for prepositional phrase attachment in English that requires only an part-of-speech tagger and a morphology database, and is therefore less resourceintensive and more portable than previous approaches, which have all required either treebanks or partial parsers.
As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous. $$$$$ We present results for prepositional phrase attachment in both English and Spanish.

 $$$$$ For testing our classifier, we used only those judgments on which all three annotators agreed.
 $$$$$ Also, the heuristic excludes examples with the verb to be from the training set (but not the test set) since we found them to be unreliable sources of evidence.
 $$$$$ Note that while the head word tuples represent correct attachments only 69% of the time, their quantity is about 45 times greater than the quantity of data used in previous supervised approaches.

 $$$$$ several unsupervised statistical models for the prepositional phrase attachment task that approach the accuracy of the best supervised methods for this task.
 $$$$$ The unsupervised algorithm for prepositional phrase attachment presented here is the only algorithm in the published literature that can significantly outperform the baseline without using data derived from a treebank or parser.

 $$$$$ While this form of attachment ambiguity is usually easy for people to resolve, a computer requires detailed knowledge about words (e.g., washed vs. bought) in order to successfully resolve such ambiguities and predict the correct interpretation.
 $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
 $$$$$ Most of the previous successful approaches to this problem have been statistical or corpusbased, and they consider only prepositions whose attachment is ambiguous between a preceding noun phrase and verb phrase.

More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ We present an unsupervised algorithm for prepositional phrase attachment in English that requires only an part-of-speech tagger and a morphology database, and is therefore less resourceintensive and more portable than previous approaches, which have all required either treebanks or partial parsers.
More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ We successfully demonstrated the portability of our approach by applying it to the prepositional phrase attachment task in the Spanish language.
More recently, Ratnaparkhi (1998) developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms. $$$$$ Analogously, define cv(p) = E, c(v, p, true) and cv = Ep cv (P)• The counts c(n,p,true) and c(v,p,true) are from the extracted head word tuples.

Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ The accuracy of our technique approaches the accuracy of the best supervised methods, and does so with only a tiny fraction of the supervision.
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ (Hindle and Rooth, 1993) describes a partially supervised approach in which the FIDDITCH partial parser was used to extract (v, n, p) tuples from raw text, where p is a preposition whose attachment is ambiguous between the head verb v and the head noun n. The extracted tuples are then used to construct a classifier, which resolves unseen ambiguities at around 80% accuracy.
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ Later work, such as (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Merlo et al., 1997; Zavrel and Daelemans, 1997; Franz, 1997), trains and tests on quintuples of the form (v, n,p, n2, a) extracted from the Penn treebank(Marcus et al., 1994), and has gradually improved on this accuracy with other kinds of statistical learning methods, yielding up to 84.5% accuracy(Collins and Brooks, 1995).
Ratnaparkhi (1998) notes that the test set contains errors, but does not correct them. $$$$$ The result was approximately 910,000 head word tuples of the form (v ,p, n2) or (n,p, n2).

Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ We claim that our approach is portable to languages with similar word order, and we support this claim by demonstrating our approach on the Spanish language.
Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ We use word statistics from both the tagged corpus and the set of extracted head word tuples to estimate the probability of generating 0 = true, p, and n2.
Using an adaptation of the algorithm proposed by Ratnaparkhi (1998) for PP-attachment, she achieves P=72% (baseline P=64%), R=100.00%. $$$$$ Since only a small part of the extraction heuristic is specific to English, and since partof-speech taggers and morphology databases are widely available in other languages, our approach is far more portable than previous approaches for this problem.

A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Finally, numbers are replaced by a single token, the text is converted to lower case, and the morphology database is used to find the base forms of the verbs and nouns.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Analogously, define cv(p) = E, c(v, p, true) and cv = Ep cv (P)• The counts c(n,p,true) and c(v,p,true) are from the extracted head word tuples.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Using the above notation, we can interpolate as follows: where c(n) and c(v) are counts from the tagged corpus, and where c(n, true) and c(v, true) are counts from the extracted head word tuples.
A second model relevant to our discussion is the one proposed in (Ratnaparkhi,1998), addressing the problem of unsupervised learning for PP attachment resolution in VERB NOUN PP sequences. $$$$$ Our procedure differs critically from (Hindle and Rooth, 1993) in that we do not iterate, we extract unambiguous attachments from unparsed input sentences, and we totally ignore the ambiguous cases.

The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ In the supervised case, both of the potential sites, namely the verb v and the noun n are known before the attachment is resolved.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ Previous work has framed the problem as a classification task, in which the goal is to predict N or V, corresponding to noun or verb attachment, given the head verb v, the head noun n, the preposition p, and optionally, the object of the preposition n2.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ Our unsupervised approach uses a heuristic based on attachment proximity and trains from raw text that is annotated with only part-of-speech tags and morphological base forms, as opposed to attachment information.
The model proposed in (Ratnaparkhi, 1998) is similar to a version of our model based solely on equation (9), with no semantic information. $$$$$ Recently, (Stetina and Naga,o, 1997) have reported 88% accuracy by using a corpus-based model in conjunction with a semantic dictionary.

equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ For example, the (v,n,p, n2) tuples corresponding to the example sentences are The correct classifications of tuples 1 and 2 are N and V, respectively.
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ (Our goal is to replicate the supervision of a treebank, but not a semantic dictionary, so we do not compare against (Stetina and Nagao, 1997).)
equation (9) captures both the contribution from the random variable used in (Ratnaparkhi, 1998) to denote the presence or absence of any preposition that is unambiguously attached to the noun or the verb in question, and the contribution from the conditional probability that a particular preposition will occur as unambiguous attachment to the verb or to the noun. $$$$$ Therefore, there is only one possible attachment site for the preposition, and either the verb v or the noun n does not exist, in the case of noun-attached preposition or a verb-attached preposition, respectively.

(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ However in sentence 2, with modifies the verb washed since with soap describes how the shirt is washed.
(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ It is therefore less resource-intensive and more portable than previous corpus-based algorithm proposed for this task.
(Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. $$$$$ Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information.
