In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ 4.1-4.2, we use abaseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search quality; in Sec.
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ Mark-Jan Nederhof inspired the use of prefix cost.

While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ So in general we write if the feature window looks at top d + 1 trees on stack, and where fz(sz) extracts kernel features from tree sz (0 ≤ i ≤ d).
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality.
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ The deductive system is optimal and runs in worst-case polynomial time as long as the kernel feature function satisfies two properties: Intuitively, boundedness means features can only look at a local window and can only extract bounded information on each tree, which is always the case in practice since we can not have infinite models.

Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ 1(a) for the list of feature templates used in the full model.
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ Our final parser outperforms all previously reported dependency parsers trained on the Penn Treebanks for both English and Chinese, and is much faster in speed (even with a Python implementation).
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0: w and s1: w, respectively. $$$$$ Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.

In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ Figure 3 shows the new deductive system with dynamic programming and GSS.
In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ (i.e., same step) if they have the same feature values, because they will have the same costs as shown in the deductive system in Figure 1.
In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).

Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ We also test our final parser on the Penn Chinese Treebank (CTB5).
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ Time is in seconds per sentence.

Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Note the prefix cost of q is irrelevant.
Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Shift-reduce parsing performs a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items at the end of the stack (Aho and Ullman, 1972).
Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.

The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ Table 3 presents the final test results of our DP parser on the Penn English Treebank, compared with other state-of-the-art parsers.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ HR0011-06-C-0022 under subcontract to BBN Technologies, and by the U.S. Army Research, Development, and Engineering Command (RDECOM).

The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ 10Parser combination in Zhang and Clark (2008) achieves a higher word accuracy of 85.77%, but again, it is not directly comparable to our work. that it is not restricted to LR (a special case of shift-reduce), and thus does not require building an LR table, which is impractical for modern grammars with a large number of rules or features.
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ However, our formalism is more flexible and our algorithm more practical.
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ Our final parser outperforms all previously reported dependency parsers trained on the Penn Treebanks for both English and Chinese, and is much faster in speed (even with a Python implementation).
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ We evaluate their performances on the standard Penn Treebank (PTB) English dependency parsing task7 using the standard split: secs 02-21 for training, 22 for development, and 23 for testing.

However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec.
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Dynamic programming turns out to be a great fit for early updating (see Section 4.3 for details).
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.

To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ However, our formalism is more flexible and our algorithm more practical.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ In terms of search strategy, most parsing algorithms in current use for data-driven parsing can be divided into two broad categories: dynamic programming which includes the dominant CKY algorithm, and greedy search which includes most incremental parsing methods such as shift-reduce.1 Both have pros and cons: the former performs an exact search (in cubic time) over an exponentially large space, while the latter is much faster (in linear-time) and is psycholinguistically motivated (Frazier and Rayner, 1982), but its greedy nature may suffer from severe search errors, as it only explores a tiny fraction of the whole space even with a beam.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989).

The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ At each step we extend the states in the current beam by applying one of the three actions, and then choose the best b resulting states for the next step.
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ Our work advances this line of research in two aspects.
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality.
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity.

The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ Feature templates are instantiated for a specific state.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ The difference here is that the annotations are not vertical ((grand-)parent), but rather horizontal (left context).
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ This work is supported in part by DARPA GALE Contract No.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ To adapt it to dependency parsing, we split the reduce action into two cases, rex and rey, depending on which one of the two items becomes the head after reduction.

W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ This work is supported in part by DARPA GALE Contract No.

Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). $$$$$ Can we combine the advantages of both approaches, that is, construct an incremental parser that runs in (almost) linear-time, yet searches over a huge space with dynamic programming?
Dep?: the state-of-the-art dependency parser by Huang and Sagae (2010). $$$$$ Now that we have the kernel feature functions, it is intuitive that we might only need to remember the relevant bits of information from only the last (d + 1) trees on stack instead of the whole stack, because they provide all the relevant information for the features, and thus determine the costs.

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ This procedure is known as “arc-standard” (Nivre, 2004), and has been engineered to achieve state-of-the-art parsing accuracy in Huang et al. (2009), which is also the reference parser in our experiments.2 More formally, we describe a parser configuration by a state (j, S) where S is a stack of trees s0, s1, ... where s0 is the top tree, and j is the queue head position (current word q0 is wj).
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Our final parser outperforms all previously reported dependency parsers trained on the Penn Treebanks for both English and Chinese, and is much faster in speed (even with a Python implementation).
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ This work is supported in part by DARPA GALE Contract No.

H&S10 refers to the results of Huang and Sagae (2010). $$$$$ The forest itself has an oracle of 98.15 (as if k → ∞), computed a` la Huang (2008, Sec.
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ 2, the above template f100 will generate a feature instance More formally, we denote f to be the feature function, such that f(j, S) returns a vector of feature instances for state (j, S).
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ Our parser achieves the highest (unlabeled) dependency accuracy among dependency parsers trained on the Treebank, and is also much faster than most other parsers even with a pure Python implementation parser (in pure Python) has the highest accuracy among dependency parsers trained on the Treebank, and is also much faster than major parsers.

In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ Thus we can define two states (j, S) and (j′, S′) to be equivalent, notated (j, S) — (j′, S′), iff. j = j′ and f(j, S) = f(j′, S′).
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ To improve on strictly greedy search, shift-reduce parsing is often enhanced with beam search (Zhang and Clark, 2008), where b states develop in parallel.
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ HR0011-06-C-0022 under subcontract to BBN Technologies, and by the U.S. Army Research, Development, and Engineering Command (RDECOM).
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ For instance, a context-free rule A —* B C would become DA —* DB BC for some D if there exists a rule E —* αDAQ.

Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.

We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). $$$$$ Can we combine the advantages of both approaches, that is, construct an incremental parser that runs in (almost) linear-time, yet searches over a huge space with dynamic programming?
We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
We implement three transition-based dependency parsers with three different parsing algorithms: Nivre's arc standard, Nivre's arc eager (see Nivre (2004) for a comparison between the two Nivre algorithms), and Liang's dynamic algorithm (Huang and Sagae, 2010). $$$$$ The parser runs in linear-time as there are exactly 2n−1 steps for a sentence of n words.

This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ For example, for the full model in Table 1(a) we have where d = 2, f2(x) = x.t, and f1(x) = f0(x) = (x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ First, ours is more general than GLR in 9Duan et al. (2007) and Zhang and Clark (2008) did not report word accuracies, but those can be recovered given nonroot and root ones, and the number of non-punctuation words.
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ In that case, we tag the dev and test sets using an automatic POS tagger (at 97.2% accuracy), and tag the training set using four-way jackknifing similar to Collins (2000), which contributes another +0.1% improvement in accuracy on the test set.
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.
