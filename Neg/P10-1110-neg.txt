In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ In fact, Tomita’s GLR is an instance of techniques for tabular simulation of nondeterministic pushdown automata based on deductive systems (Lang, 1974), which allow for cubictime exhaustive shift-reduce parsing with contextfree grammars (Billot and Lang, 1989).
In this paper, we follow the line of investigation started by Huang and Sagae (2010) and apply dynamic programming to (projective) transition-based dependency parsing (Nivre, 2008). $$$$$ C=C/C++, Py=Python, Ja=Java.

While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ HR0011-06-C-0022 under subcontract to BBN Technologies, and by the U.S. Army Research, Development, and Engineering Command (RDECOM).
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ 4.1-4.2, we use abaseline model trained with non-DP for both DP and non-DP, so that we can do a side-by-side comparison of search quality; in Sec.
While our general approach is the same as the one of Huang and Sagae (2010), we depart from their framework by not representing the computations of a parser as a graph-structured stack in the sense of Tomita (1986). $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).

Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0 $$$$$ However, our formalism is more flexible and our algorithm more practical.
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0 $$$$$ To the best of our knowledge, our work is the first linear-time incremental parser that performs dynamic programming.
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0 $$$$$ 4.1).
Two examples of feature functions are the word form associated with the topmost and second-topmost node on the stack; adopting the notation of Huang and Sagae (2010), we will write these functions as s0 $$$$$ To the best of our knowledge, our work is the first linear-time incremental parser that performs dynamic programming.

In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ Can we combine the advantages of both approaches, that is, construct an incremental parser that runs in (almost) linear-time, yet searches over a huge space with dynamic programming?
In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.
In the following, we take this second approach, which is also the approach of Huang and Sagae (2010). $$$$$ We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values.

Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity.
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ Both DP and non-DP parsers use the same feature templates in Table 1.
Huang and Sagae (2010) use this quantity to order the items in a beam search on top of their dynamic programming method. $$$$$ 3, the prefix cost c is the total cost of the best action sequence from the initial state to the end of state p, i.e., it includes both the inside cost v (for Viterbi inside derivation), and the cost of the (best) path leading towards the beginning of state p. We say that a state p with prefix cost c is better than a state p′ with prefix cost c′, notated p � p′ in Fig.

Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Mark-Jan Nederhof inspired the use of prefix cost.
Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.
Alternatively, we can also use the more involved calculation employed by Huang and Sagae (2010), which allows them to get rid of the left context vector from their items. $$$$$ We instead propose a dynamic programming alogorithm for shift-reduce parsing which runs in polynomial time in theory, but linear-time (with beam search) in practice.

The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ 3.
The essential idea in the calculation by Huang and Sagae (2010) is to delegate (in the computation of the Viterbi score) the scoring of sh transitions to the inference rules for la/ra. $$$$$ When two equivalent shifted states get merged, their predictor states get combined.

The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ At each step, we choose one of the three actions: Note that the shorthand notation txt′ denotes a new tree by “attaching tree t′ as the leftmost child of the root of tree t”.
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.
The basic idea behind our technique is the same as the one implemented by Huang and Sagae (2010) for the special case of the arc-standard model, but instead of their graph-structured stack representation we use a tabulation akin to Lang's approach to the simulation of pushdown automata (Lang, 1974). $$$$$ Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.

However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Table 4 summarizes the final test results, where our work performs the best in all four types of (unlabeled) accuracies: word, non-root, root, and complete match (all excluding punctuations).
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ Our dynamic programming algorithm also runs on top of beam search in practice.
However, Huang and Sagae (2010) have provided evidence that the use of dynamic programming on top of a transition-based dependency parser can improve accuracy even without exhaustive search. $$$$$ 2.3), which happen much more often with DP, because a goldstandard state p is often merged with an equivalent (but incorrect) state that has a higher model score, which triggers update immediately.

To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ 5c), since the less expressive feature set makes more states “equivalent” and mergeable in DP.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ States are organized according to step ℓ, which denotes the number of actions accumulated.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.
To handle the increased computational complexity, we adopt the incremental parsing framework with dynamic programming (Huang and Sagae, 2010), and propose an efficient method of character-based decoding over candidate structures. $$$$$ Following Collins and Roark (2004) we also use the “early-update” strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.3 The intuition behind this strategy is that later mistakes are often caused by previous ones, and are irrelevant when the parser is on the wrong track.

The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ For example, for the full model in Table 1(a) we have where d = 2, f2(x) = x.t, and f1(x) = f0(x) = (x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).
The incremental framework of our model is based on the joint POS tagging and dependency parsing model for Chinese (Hatori et al, 2011), which is an extension of the shift-reduce dependency parser with dynamic programming (Huang and Sagae, 2010). $$$$$ Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.

The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ Our work advances this line of research in two aspects.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.
The feature set of our model is fundamentally a combination of the features used in the state-of-the-art joint segmentation and POS tagging model (Zhang and Clark, 2010) and dependency parser (Huang and Sagae, 2010), both of which are used as baseline models in our experiment. $$$$$ DP achieves better search quality because it expores an exponentially large search space rather than only b trees allowed by the beam (see Fig.

W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ Specifically, we make the following contributions: input: w0 ... w,,,−1 axiom 0 : (0, ǫ): 0 where ℓ is the step, c is the cost, and the shift cost ξ and reduce costs λ and ρ are: For convenience of presentation and experimentation, we will focus on shift-reduce parsing for dependency structures in the remainder of this paper, though our formalism and algorithm can also be applied to phrase-structure parsing.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ The parser runs in linear-time as there are exactly 2n−1 steps for a sentence of n words.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality.
W21, and T01? 05 are taken from Zhang and Clark (2010), and P01? P28 are taken from Huang and Sagae (2010). $$$$$ Empirical results on a state-the-art dependency parser confirm the advantage of DP in many aspects: faster speed, larger search space, higher oracles, and better and faster learning.

Dep? $$$$$ Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.
Dep? $$$$$ Yue Zhang helped with Chinese datasets, and Wenbin Jiang with feature sets.
Dep? $$$$$ 4).
Dep? $$$$$ Tomita uses GSS for exhaustive LR parsing, where the GSS is equivalent to a dynamic programming chart in chart parsing (see Footnote 4).

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ For future work we plan to extend it to constituency parsing.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931 and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.

H&S10 refers to the results of Huang and Sagae (2010). $$$$$ 1, which is a subset of those in McDonald et al. (2005b).
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ This work is supported in part by DARPA GALE Contract No.
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ This work is supported in part by DARPA GALE Contract No.
H&S10 refers to the results of Huang and Sagae (2010). $$$$$ Second, unlike previous theoretical results about cubic-time complexity, we achieved linear-time performance by smart beam search with prefix cost inspired by Stolcke (1995), allowing for state-of-the-art data-driven parsing.

In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ We have presented a dynamic programming algorithm for shift-reduce parsing, which runs in linear-time in practice with beam search.
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ Our final parser outperforms all previously reported dependency parsers trained on the Penn Treebanks for both English and Chinese, and is much faster in speed (even with a Python implementation).
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ (1-3) in Fig.
In particular, "Huang 10" and "Zhang 11" denote Huang and Sagae (2010) and Zhang and Nivre (2011), respectively. $$$$$ Our work advances this line of research in two aspects.

Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ However, our formalism is more flexible and our algorithm more practical.
Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ Additional techniques such as semi-supervised learning (Koo et al., 2008) and parser combination (Zhang and Clark, 2008) do achieve accuracies equal to or higher than ours, but their results are not directly comparable to ours since they have access to extra information like unlabeled data.
Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ This work was inspired in part by Generalized LR parsing (Tomita, 1991) and the graph-structured stack (GSS).
Since our algorithm is transition-based, many existing techniques such as k-best ranking (Zhang and Clark, 2008) or dynamic programming (Huang and Sagae, 2010) designed to improve transition-based parsing can be applied. $$$$$ 4).

We implement three transition-based dependency parsers with three different parsing algorithms $$$$$ The parser of Roark and Hollingshead (2009) is also almost linear time, but they achieved this by discarding parts of the CKY chart, and thus do achieve incrementality.
We implement three transition-based dependency parsers with three different parsing algorithms $$$$$ This framework is general and applicable to a largeclass of shift-reduce parsers, as long as the feature functions satisfy boundedness and monotonicity.
We implement three transition-based dependency parsers with three different parsing algorithms $$$$$ Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.
We implement three transition-based dependency parsers with three different parsing algorithms $$$$$ For example, for the full model in Table 1(a) we have where d = 2, f2(x) = x.t, and f1(x) = f0(x) = (x.w, x.t, x.lc.t, x.rc.t) (see Table 1(c)).

This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ We thank David Chiang, Yoav Goldberg, Jonathan Graehl, Kevin Knight, and Roger Levy for helpful discussions and the three anonymous reviewers for comments.
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a problem: the search is only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ (1-3) in Fig.
This has led to the development of various data-driven dependency parsers, such as those by Yamada and Matsumoto (2003), Nivre et al2004), McDonald et al2005), Martins et al2009), Huang and Sagae (2010) or Tratz and Hovy (2011), which can be trained directly from annotated data and produce ac curate analyses very efficiently. $$$$$ Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.
