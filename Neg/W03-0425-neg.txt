Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ For the German task, the improvement yielded by classifier combination is smaller.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ For the German task, the improvement yielded by classifier combination is smaller.

We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.

 $$$$$ The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.
 $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
 $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.

 $$$$$ To facilitate comparison with other classifiers for this task, most reported results 3 The method of retaining only the boundaries and reclassifying the entities was shown to improve the performance of 11 of the 12 systems participating in the CoNLL-2002 shared tasks, in both languages (Florian, 2002b). are obtained by using features exclusively extracted from the training data.
 $$$$$ To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.
 $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
 $$$$$ This section describes only briefly the classifiers used in combination in Section 4; a full description of the algorithms and their properties is beyond the scope of this paper – the reader is instead referred to the original articles.

 $$$$$ In the voting methods, each classifier gave its entire vote to one class – its own output.
 $$$$$ The RRM system alone obtains an Fmeasure of 92.1, and can effectively integrate these information streams with the output of the four classifiers, gazetteers and the two additional classifiers into obtaining 93.9 F-measure, as detailed in Table 4, a 21% reduction in F-measure error.
 $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
 $$$$$ Transformation-based learning is an error-driven algorithm which has two major steps: it starts by assigning some classification to each example, and then automatically proposing, evaluating and selecting the classification changes that maximally decrease the number of errors.

Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling, it is error–driven, and has an inherently dynamic behavior.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

 $$$$$ Within each of the regions, a statistical bigram language model is used to compute the likelihood of words occurring within that region (named entity type).
 $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.

Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ For the German task, the improvement yielded by classifier combination is smaller.

 $$$$$ For the German task, the improvement yielded by classifier combination is smaller.
 $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.
 $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
 $$$$$ Table 2 presents the combination results, for different ways of estimating the interpolation parameters.

Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.

It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ At training time, the system was fed the output of each classifier on the cross-classified data, the part-of-speech and chunk boundary tags.
It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.
It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). $$$$$ To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.
Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.
Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al, 2003), the SVM-based system (Mayfield et al, 2003) and the Perceptron-based system (Carreras et al, 2003). $$$$$ To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.

Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ In accordance with this observation, the classifiers used in this research can access a diverse set of features when examining a word in context, including: In addition, a ngram-based capitalization restoration algorithm has been applied on the sentences that appear in all caps2, for the English task.

Florian et al (2003) employed the same technique in a combination of learners. $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.

Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ Feature design and integration is of utmost importance in the overall classifier design – a rich feature space is the key to good performance.

Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.
Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.
Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ For the German task, the improvement yielded by classifier combination is smaller.
One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.
One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ German poses a completely different problem for named entity recognition: the data is considerably sparser.

The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ At decoding time, the best sequence of classifications is identified with the Viterbi algorithm.
The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.
The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).

For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ The particular setup in which fnTBL is used in this work is described in Florian (2002a): in a first phase, TBL is used to identify the entity boundaries, followed by a sequence classification stage, where the entities identified at the first step are classified using internal and external clues3.

Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ This section describes only briefly the classifiers used in combination in Section 4; a full description of the algorithms and their properties is beyond the scope of this paper – the reader is instead referred to the original articles.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
