Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ A widely used combination scheme is through linear interpolation of the classifiers’ class probability distribution The weights Ai (w) encode the importance given to classifier i in combination, for the context of word w, and Pi (C|w, Ci) is an estimation of the probability that the correct classification is C, given that the output of the classifier i on word w is Ci.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.
Classifier combination has been shown to be effective in improving the performance of NLP applications, and have been investigated by Brill and Wu (1998) and van Halteren et al (2001) for part-of-speech tagging, Tjong Kim Sang et al (2000) for base noun phrase chunking, and Florian et al (2003a) for word sense disambiguation. $$$$$ At decoding time, the best sequence of classifications is identified with the Viterbi algorithm.

We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
We also applied the classifier combination technique discussed in this paper to English and German (Florian et al, 2003b). $$$$$ German poses a completely different problem for named entity recognition: the data is considerably sparser.

 $$$$$ Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).
 $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
 $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
 $$$$$ The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.

 $$$$$ At decoding time, the best sequence of classifications is identified with the Viterbi algorithm.
 $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

 $$$$$ All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.
 $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
 $$$$$ The particular implementation we used underperformed consistently all the other classifiers on German, and is not included.

Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ The particular implementation we used underperformed consistently all the other classifiers on German, and is not included.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al, 2000), named entity extraction (Florian et al, 2003), etc. $$$$$ The results obtained by each individual classifier, broken down by entity type, are presented in Table 1.

 $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
 $$$$$ This paper presents a classifier-combination experimental framework for named entity recognition in which four diverse classifiers (robust linear classifier, maximum entropy, transformation-based learning, and hidden Markov model) are combined under different conditions.
 $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.

Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ In accordance with this observation, the classifiers used in this research can access a diverse set of features when examining a word in context, including: In addition, a ngram-based capitalization restoration algorithm has been applied on the sentences that appear in all caps2, for the English task.
Boosting (Freund and Schapire, 1996) has been applied to optimize chunking systems (Carreras et al, 2002), as well as voting over sets of different classifiers (Florian et al, 2003). $$$$$ Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).

 $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.
 $$$$$ To facilitate comparison with other classifiers for this task, most reported results 3 The method of retaining only the boundaries and reclassifying the entities was shown to improve the performance of 11 of the 12 systems participating in the CoNLL-2002 shared tasks, in both languages (Florian, 2002b). are obtained by using features exclusively extracted from the training data.
 $$$$$ As a machine learning method, the RRM algorithm seems especially suited to handle additional feature streams, and therefore is a good candidate for classifier combination.
 $$$$$ Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).

Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ For the German task, the improvement yielded by classifier combination is smaller.
Next, we compare the extraction quality of the customized CoreNER for CoNLL03 and Enron3 with the corresponding best published results by (Florian et al, 2003) and (Minkov et al, 2005). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ The remainder of the paper is organized as follows: Section 2 describes the features used by the classifiers, Section 3 briefly describes the algorithms used by each classifier, and Section 4 analyzes in detail the results obtained by each classifier and their combination.
It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ At decoding time, the best sequence of classifications is identified with the Viterbi algorithm.
It is worthwhile noting that the best published results for CoNLL03 (Florian et al, 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden MarkovModel) and trying six different classifier combination methods. $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task $$$$$ To estimate the parameters in Equation (2), the provided training data was split into 5 equal parts, and each classifier was trained, in a round-robin fashion, on 4 fifths of the data and applied on the remaining fifth.
Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task $$$$$ The states in the HMM are organized into regions, one region for each type of named entity plus one for NOTA-NAME.
Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL 2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task $$$$$ This paper investigates the combination of a set of diverse statistical named entity classifiers, including a rule-based classifier – the transformation-based learning classifier (Brill, 1995; Florian and Ngai, 2001, henceforth fnTBL) with the forward-backward extension described in Florian (2002a), a hidden Markov model classifier (henceforth HMM), similar to the one described in Bikel et al. (1999), a robust risk minimization classifier, based on a regularized winnow method (Zhang et al., 2002) (henceforth RRM) and a maximum entropy classifier (Darroch and Ratcliff, 1972; Berger et al., 1996; Borthwick, 1999) (henceforth MaxEnt).

Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic ,morpho logical, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

Florian et al (2003) employed the same technique in a combination of learners. $$$$$ Often, high performing classifiers operating in an impoverished space are surpassed by a lower performing classifier when the latter has access to enhanced feature spaces (Zhang et al., 2002; Florian, 2002a).
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Florian et al (2003) employed the same technique in a combination of learners. $$$$$ TBL has some attractive qualities that make it suitable for the language-related tasks: it can automatically integrate heterogeneous types of knowledge, without the need for explicit modeling, it is error–driven, and has an inherently dynamic behavior.

Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ This section describes only briefly the classifiers used in combination in Section 4; a full description of the algorithms and their properties is beyond the scope of this paper – the reader is instead referred to the original articles.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ The HMM classifier used in the experiments in Section 4 follows the system description in (Bikel et al., 1999), and it performs sequence classification by assigning each word either one of the named entity types or the label NOT-A-NAME to represent &quot;not a named entity&quot;.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al, 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. $$$$$ All algorithms described in this paper identify the named entities in the text by labeling each word with a tag corresponding to its position relative to a named entity: whether it starts/continues/ends a specific named entity, or does not belong to any entity.

Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ The MaxEnt classifier computes the posterior class probability of an example by evaluating the normalized product of the weights active for the particular example.
Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ For the German task, the improvement yielded by classifier combination is smaller.
Florian et al (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. $$$$$ To avoid running in severe over-training problems, a feature cutoff of 4 is applied before the model weights are learned.

One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al, 2003). $$$$$ When no gazetteer or other additional training resources are used, the combined system attains a performance of 91.6F on the English development data; integrating name, location and person gazetteers, and named entity systems trained on additional, more general, data reduces the F-measure error by a factor of 15 to 21% on the English data.

The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ RRM, MaxEnt, and fnTBL treat the problem entirely as a tagging task, while the HMM algorithm used here is constraining the transitions between the various phases, similar to the method described in (Bikel et al., 1999).
The inclusion of extra named entity recognition systems seems to have worked well (Florian et al, 2003). $$$$$ Feature design and integration is of utmost importance in the overall classifier design – a rich feature space is the key to good performance.

For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ The particular implementation we used underperformed consistently all the other classifiers on German, and is not included.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ In accordance with this observation, the classifiers used in this research can access a diverse set of features when examining a word in context, including: In addition, a ngram-based capitalization restoration algorithm has been applied on the sentences that appear in all caps2, for the English task.
For English, the combined classifier of Florian et al (2003) achieved the highest overall F1 rate. $$$$$ This particular set of classifiers is diverse across multiple dimensions, making it suitable for combination: decision arbitrary feature types, while HMM is dependent on a prespecified back-off path.

Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ In conclusion, we have shown results on a set of both well-established and novel classifier techniques which improve the overall performance, when compared with the best performing classifier, by 17-21% on the English task.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ The error-based classifiers (RRM and fnTBL) tend to obtain balanced precision/recall numbers, while the other two tend to be more precise at the expense of recall.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ We note that the numbers are roughly twice as large for the development data in German as they are for English.
Florian et al (2003) have also obtained the highest F1 rate for the German data. $$$$$ The error-based classifiers (RRM and fnTBL) tend to obtain balanced precision/recall numbers, while the other two tend to be more precise at the expense of recall.
