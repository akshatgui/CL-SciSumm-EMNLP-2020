Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words. $$$$$ 3 For each critical value of we show the relative entropy with respect to the asymmetric model based on Cp of the training set (set train), of randomly selected held-out test set (set iesi), and of held-out data for a further 1000 nouns that were not clustered (set new).
Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words. $$$$$ However, this is not very satisfactory because one of the goals of our work is precisely to avoid the problems of data sparseness by grouping words into classes.
Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words. $$$$$ Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data.
Class based methods (Pereira et al 1993) approximate the likelihood of unobserved words based on similar words. $$$$$ We use for this purpose the relative entropy or Kullback-Leibler (KL) distance between two distributions This is a natural choice for a variety of reasons, which we will just sketch here.'

 $$$$$ The resulting training set was used to build a sequence of cluster models as before.
 $$$$$ While the clusters derived by the proposed method seem in many cases semantically significant, this intuition needs to be grounded in a more rigorous assessment.
 $$$$$ In other words, the exceptional cases are those in which predictions based just on the marginal frequencies, which the initial one-cluster model represents, would be consistently wrong.

Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. $$$$$ We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts.
Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. $$$$$ While the clusters derived by the proposed method seem in many cases semantically significant, this intuition needs to be grounded in a more rigorous assessment.
Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. $$$$$ Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
Distributional clustering of words was first proposed by Pereira Tishby and Leein (Pereira et al, 1993): They cluster nouns according to their conditional verb distributions. $$$$$ Notice that d(n, c) does not need to be symmetric for this derivation, as the two distributions are simply related by Bayes's rule.

The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). $$$$$ In general, we are interested in how to organize a set of linguistic objects such as words according to the contexts in which they occur, for instance grammatical constructions or n-grams.
The cluster output can then be used as classes for selectional preferences (Pereira et al, 1993), or one can directly use frequency information from distributionally similar words for smoothing (Grishman and Sterling, 1994). $$$$$ This collection process yielded 1112041 verb-object pairs.

This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. $$$$$ As a first experiment, we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb &quot;fire&quot; in one year (1988) of Associated Press newswire.
This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. $$$$$ The test set relative entropy decreases to a minimum at 206 clusters, and then starts increasing, suggesting that larger models are overtrained.
This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. $$$$$ We can thus use the relative entropy between the context distributions for two words to measure how likely they are to be instances of the same cluster centroid.
This indicates that techniques for differentiating between different senses are needed e.g., using a soft clustering technique as in (Pereira et al 1993) instead of a hard clustering technique. $$$$$ The test set relative entropy decreases to a minimum at 206 clusters, and then starts increasing, suggesting that larger models are overtrained.

Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs. $$$$$ The preceding qualitative discussion provides some indication of what aspects of distributional relationships may be discovered by clustering.
Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs. $$$$$ When the scale parameter 0 is close to zero, the similarity is almost irrelevant.
Hatzivassiloglou and MeKeown (1993) clustered adjectives into semantic classes, and Pereira et al (1993) clustered nouns on their appearance in verb-object pairs. $$$$$ In addition to predictive power evaluations of the kind we have already carried out, it might be worth comparing automatically-derived clusters with human judgements in a suitable experimental setting.

Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. $$$$$ In this corpus, the chosen nouns appear as direct object heads of a total of 2147 distinct verbs, so each noun is represented by a density over the 2147 verbs.
Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. $$$$$ Thus this test evaluates how well the models reconstruct missing data in the the weighted average Eneiv., fr,D(11/3) where f, is the relative frequency of n in the test set.
Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. $$$$$ The corpus used in our first experiment was derived from newswire text automatically parsed by Hindle's parser Fidditch (Hindle, 1993).
Of course, this is one of many different possible similarity measures which could have been used (Pereira et al (1993)), including ones that do not depend on additional labels. $$$$$ Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data.

So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). $$$$$ At this point we need to specify the clustering model in more detail.
So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). $$$$$ Figure 1 shows the four words most similar to each cluster centroid, and the corresponding wordcentroid KL distances, for the four clusters resulting from the first two cluster splits.
So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). $$$$$ The preceding qualitative discussion provides some indication of what aspects of distributional relationships may be discovered by clustering.
So far we have used a weighted string edit distance matcher and experimented with di erent substitution weights including ones based on measures of statistical similarity between words such as the one described by Pereira et al (1993). $$$$$ It turns out that this will also be the values of p(v lc) that minimize the average distortion between the asymmetric cluster model and the data.

Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. $$$$$ Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership.
Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. $$$$$ Figure 2 shows the four closest nouns to the centroid of each of a set of hierarchical clusters derived from verb-object pairs involving the 1000 most frequent nouns in the June 1991 electronic version of Grolier's Encyclopedia (10 mil
Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. $$$$$ Figure 3 plots the unweighted average relative entropy, in bits, of several test sets to asymmetD(4,112571), ric clustered models of different sizes, given by W&quot;,'T L_anEArs where Aft is the set of direct objects in the test set and tn is the relative frequency distribution of verbs taking n as direct object in the test set.
Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. $$$$$ The new noun test set is intended to test whether clusters based on the 1000 most frequent nouns are useful classifiers for the selectional properties of nouns in general.

In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ We have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ In particular, we would like to find a set of clusters C such that each conditional distribution p0(v) can be approximately decomposed as where p(c1n) is the membership probability of n in c and pc(v) = p(vIc) is v's conditional probability given by the centroid distribution for cluster c. The above decomposition can be written in a more symmetric form as assuming that p(n) and P(n) coincide.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ That split is somewhat less sharp, possibly because not enough distinguishing contexts occur in the corpus.

While previous cognitively-motivated computational frameworks required structured input (e.g. Falkenhainer et al, 1989), the CP method adapts distributional clustering (Pereira et al, 1993), a standard approach applicable to unstructured data. $$$$$ The task consists judging which of two verbs v and v' is more likely to take a given noun n as object, when all occurrences of (v, n) in the training set were deliberately deleted.
While previous cognitively-motivated computational frameworks required structured input (e.g. Falkenhainer et al, 1989), the CP method adapts distributional clustering (Pereira et al, 1993), a standard approach applicable to unstructured data. $$$$$ The resulting training set was used to build a sequence of cluster models as before.

There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ Ideally, there is just one split at that critical value, but for practical performance and numerical accuracy reasons we may have several splits at the new critical point.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ In many cases, the clusters can be thought of as encoding coarse sense distinctions.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ All our experiments involve the asymmetric model described in the previous section.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ For any given 0, we have a current set of leaf clusters corresponding to the current free energy (local) minimum.

Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). $$$$$ As a first experiment, we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb &quot;fire&quot; in one year (1988) of Associated Press newswire.
Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). $$$$$ As a first experiment, we used our method to classify the 64 nouns appearing most frequently as heads of direct objects of the verb &quot;fire&quot; in one year (1988) of Associated Press newswire.
Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). $$$$$ In principle such a symmetric model may be more accurate, but in this paper we will concentrate on asymmetric models in which cluster memberships are associated to just one of the components of the joint distribution and the cluster centroids are specified only by the other component.
Lexical relatedness between terms could be derived either from a thesaurus like WordNet or from raw monolingual corpora via distributional similarity (Pereira et al, 1993). $$$$$ It can be seen that first split separates the objects corresponding to the weaponry sense of &quot;fire&quot; (cluster 1) from the ones corresponding to the personnel action (cluster 2).

Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. $$$$$ From the practical point of view, word classification addresses questions of data sparseness and generalization in statistical language models, particularly models for deciding among alternative analyses proposed by a grammar.
Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. $$$$$ This collection process yielded 1112041 verb-object pairs.
Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. $$$$$ The evaluation described below was performed on the largest data set we have worked with so far, extracted from 44 million words of 1988 Associated Press newswire with the pattern matching techniques mentioned earlier.
Our approach differs from standard word clustering in that the clustering criteria is directly linked to the re ranking objective, whereas previous word clustering approaches (e.g. Brown et al (1992) or Pereira et al (1993)) have typically leveraged distributional similarity. $$$$$ Therefore, if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations, D(p qi) gives the relative weight of evidence in favor of qi.

The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993). $$$$$ In other words, the exceptional cases are those in which predictions based just on the marginal frequencies, which the initial one-cluster model represents, would be consistently wrong.
The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993). $$$$$ Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical &quot;soft&quot; clustering of the data.
The importance of distributional features is well known for named entity recognition and part of speech tagging (Pereira et al, 1993). $$$$$ As the figure shows, the cluster model provides over one bit of information about the selectional properties of the new nouns, but the overtraining effect is even sharper than for the held-out data involving the 1000 clustered nouns.

For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. $$$$$ We have demonstrated that a general divisive clustering procedure for probability distributions can be used to group words according to their participation in particular grammatical relations with other words.
For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. $$$$$ These statistics can thus be seen as a weak form of object labelling analogous to supervision.
For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. $$$$$ For a large enough sample, we may replace the sum over observations in (9) by the average over Af.
For example, Pereira et al (1993) begin with a co occurrence matrix and transform this matrix into a clustering. $$$$$ The analogy with statistical mechanics suggests a deterministic annealing procedure for clustering (Rose et al., 1990), in which the number of clusters is determined through a sequence of phase transitions by continuously increasing the parameter 0 following an annealing schedule.

In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. $$$$$ We can now see that the variation 6 log Z, vanishes for centroid distributions given by (11), since it follows from (10) that The Free Energy Function The combined minimum distortion and maximum entropy optimization is equivalent to the minimization of a single function, the free energy where (D) is the average distortion (5) and H is the cluster membership entropy (6).
In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. $$$$$ We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar.
In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. $$$$$ More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992).
In a similar vein, our model is both similar and distinct in comparison to the soft clustering approaches by Pereira et al (1993) and Korhonenetal. $$$$$ This collection process yielded 1112041 verb-object pairs.

Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns. $$$$$ More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp â€”nD(p II q) (Cover and Thomas, 1991).
Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns. $$$$$ Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data.
Pereira et al (1993) suggested deterministic annealing to cluster verb-argument pairs into classes of verbs and nouns. $$$$$ This joint optimization searches for a saddle point in the distortion-entropy parameters, which is equivalent to minimizing a linear combination of the two known as free energy in statistical mechanics.

 $$$$$ Our problem can be seen as that of learning a joint distribution of pairs from a large sample of pairs.
 $$$$$ Instead, the only information about the objects is the statistics of their joint appearance.
 $$$$$ Since no other information is available, the membership is determined by maximizing the configuration entropy for a fixed average distortion.

Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns. $$$$$ So, far, we have looked at two kinds of measurements of model quality: (i) relative entropy between held-out data and the asymmetric model, and (ii) performance on the task of deciding which of two verbs is more likely to take a given noun as direct object when the data relating one of the verbs to the noun has been withheld from the training data.
Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns. $$$$$ Unsurprisingly, the training set relative entropy decreases monotonically.
Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns. $$$$$ These cluster membership estimates were then used in the asymmetric model and the test set relative entropy calculated as before.
Pereira et al (1993) used clustering to build an unlabeled hierarchy of nouns. $$$$$ In this corpus, the chosen nouns appear as direct object heads of a total of 2147 distinct verbs, so each noun is represented by a density over the 2147 verbs.
