In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ These A* methods can be adapted to the lexicalized case.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ IIS-0085896, by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program, by an NSF Graduate Fellowship to the first author, and by an IBM Faculty Partnership Award to the second author.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.

In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A* parsing (Klein and Manning, 2003). $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.
In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A* parsing (Klein and Manning, 2003). $$$$$ IIS-0085896, by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program, by an NSF Graduate Fellowship to the first author, and by an IBM Faculty Partnership Award to the second author.
In other words, the coarse outside score computed by the algorithm plays the same role as a heuristic in standard A* parsing (Klein and Manning, 2003). $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.

The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. $$$$$ Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.
The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.
The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. $$$$$ If we wish to maintain optimality in a search procedure, the obvious thing to try is A* methods (see for example Russell and Norvig, 1995).
The search algorithm for the best ITG alignment, a best-first chart parsing (Charniak et al., 1998), was augmented with an A* search heuristic of quadratic complexity (Klein and Manning, 2003), resulting in significant reduction in computational complexity. $$$$$ Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.

Tsuruoka and Tsujii (2004) explore the frame work developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. $$$$$ On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.
Tsuruoka and Tsujii (2004) explore the frame work developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. $$$$$ This is the ideal estimate, which we call TRUE.
Tsuruoka and Tsujii (2004) explore the frame work developed in Klein and Manning (2003a), and seek ways to minimize the time required by the heap manipulations necessary in this scheme. $$$$$ This paper is based on work supported by the National Science Foundation (NSF) under Grant No.

 $$$$$ To situate our results, the FOMs used by (Caraballo and Charniak, 1998) require 10K edges to parse 96% of these sentences, while BF requires only 6K edges.
 $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.
 $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.
 $$$$$ First, it substantially reduces the work required to parse a sentence, without sacrificing either the optimality of the answer or the worst-case cubic time bounds on the parser.

 $$$$$ IIS-0085896, by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program, by an NSF Graduate Fellowship to the first author, and by an IBM Faculty Partnership Award to the second author.
 $$$$$ The total cost per sentence includes the time required for two exhaustive PCFG parses, after which the A* search takes only seconds, even for very long sentences.
 $$$$$ Using these estimates, our parser is capable of finding the Viterbi parse of an average-length Penn treebank sentence in a few seconds, processing less than 3% of the edges which would be constructed by an exhaustive parser.

 $$$$$ On average-length Penn treebank sentences, our most detailed estimate reduces the total number of edges processed to less than 3% of that required by exhaustive parsing, and a simpler estimate, which requires less than a minute of precomputation, reduces the work to less than 5%.
 $$$$$ Even when a lexicalized model is not in this factored form, it still admits factored grammar projection bounds; we are currently investigating this case.
 $$$$$ Note that even if we did know P(e|s) exactly, we still would not know whether e occurs in any best parse of s. Nonetheless, good FOMs empirically lead quickly to good parses.

A specific case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve significant speed up using carefully designed heuristic functions. $$$$$ This paper is based on work supported by the National Science Foundation (NSF) under Grant No.
A specific case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve significant speed up using carefully designed heuristic functions. $$$$$ An A* parser is simpler to build than a best-first parser, does less work per edge, and provides both an optimality guarantee and a worst-case cubic time bound.
A specific case of this algorithm is the A* parsing of Klein and Manning (2003) where they achieve significant speed up using carefully designed heuristic functions. $$$$$ This gives the effect that, for larger contexts, the best parses which back the estimates will have less and less to do with the actual contexts (and hence will become increasingly optimistic).

We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probable parts of the chart to be built, improving efficiency while still ensuring the optimal derivation is found. $$$$$ Since this model has a projection-based form, grammar projection methods are easy to apply and especially effective, giving over three orders of magnitude in edge savings.
We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probable parts of the chart to be built, improving efficiency while still ensuring the optimal derivation is found. $$$$$ We present an extension of the classic A* search procedure to tabular PCFG parsing.
We intend to explore other methods for pruning the space and agenda-based parsing, in particular A* parsing (Klein and Manning, 2003), which will allow only the most probable parts of the chart to be built, improving efficiency while still ensuring the optimal derivation is found. $$$$$ Since we are parsing with the Penn treebank covering grammar, almost any (phrasal) non-terminal can be built over almost any span.

In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars $$$$$ The A* formulation provides three benefits.
In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars $$$$$ In A* parsing, we wish to construct priorities which will speed up parsing, yet still guarantee optimality (that the first parse returned is indeed a best parse).

parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). $$$$$ Second, it must be monotonic, meaning that as one builds up a tree, the combined log-probability β + a never increases.
parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). $$$$$ A close approximation to the F estimate can also be computed online especially quickly during parsing.
parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). $$$$$ Unlike best-first and finite-beam methods for achieving this kind of speed-up, an A* method is guaranteed to find the most likely parse, not just an approximation.
parsing algorithm of Klein and Manning (2003c) can be formulated in terms of weighted deduction rules (Felzenszwalb and McAllester, 2007). $$$$$ In that model, the score of a lexicalized tree is the product of the scores of two projections of that tree, one onto unlexicalized phrase structure, and one onto phrasalcategory-free word-to-word dependency structure.

see Klein and Manning (2003c) for details. $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.
see Klein and Manning (2003c) for details. $$$$$ We would like to Joshua Goodman and Dan Melamed for advice and discussion about this work.

If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). $$$$$ We apply A* search to a tabular itembased parser, ordering the parse items based on a combination of their known internal cost of construction and a conservative estimate of their cost of completion (see figure 1).
If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). $$$$$ So, for example, a state like NP→ · CC NP CC NP would become X→ · CC X CC X (see section 3.3 for a description of our grammar encodings).
If the heuristic is consistent, then A* guarantees that whenever an inside item comes off the agenda, its weight is its true Viterbi inside score (Klein and Manning, 2003c). $$$$$ Such an encoding binarizes the grammar, and compacts it.

We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). $$$$$ The use of A* search can dramatically reduce the time required to find a best parse by conservatively estimating the probabilities of parse completions.
We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). $$$$$ IIS-0085896, by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program, by an NSF Graduate Fellowship to the first author, and by an IBM Faculty Partnership Award to the second author.
We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). $$$$$ This approach also dramatically reduces the work done during parsing, though it, too, gives no guarantee that the first parse returned is the actual Viterbi parse (nor does it maintain a worst-case cubic time bound).
We also experimented with the lexicalized parsing model described in Klein and Manning (2003b). $$$$$ Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.

 $$$$$ The total cost per sentence includes the time required for two exhaustive PCFG parses, after which the A* search takes only seconds, even for very long sentences.
 $$$$$ In Klein and Manning (2003), we apply a pair of grammar projection estimates to a lexicalized parsing model of a certain factored form.
 $$$$$ Second, it must be monotonic, meaning that as one builds up a tree, the combined log-probability β + a never increases.

 $$$$$ This is particularly effective when the tree model takes a certain factored form; see Klein and Manning (2003) for details.
 $$$$$ We have described two general ways of constructing admissible A* estimates for PCFG parsing and given several specific estimates.
 $$$$$ We discuss best-first parsing further in section 3.3.

We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). $$$$$ Using these estimates, our parser is capable of finding the Viterbi parse of an average-length Penn treebank sentence in a few seconds, processing less than 3% of the edges which would be constructed by an exhaustive parser.
We used a simple but effective heuristic for these grammars, similar to the FILTER heuristic suggested in Klein and Manning (2003c). $$$$$ We have described two general ways of constructing admissible A* estimates for PCFG parsing and given several specific estimates.

Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. $$$$$ An A* parser is simpler to build than a best-first parser, does less work per edge, and provides both an optimality guarantee and a worst-case cubic time bound.
Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. $$$$$ In addition, the minimum (U) of a set of admissible estimates is still an admissible estimate.
Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. $$$$$ Our parser, which is simpler to implement than an upward-propagating best-first parser, is correct for a wide range of parser control strategies and maintains worst-case cubic time.

The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations. $$$$$ A* search has been proposed and used for speech applications (Goel and Byrne, 1999, Corazza et al., 1994); however, it has been little used, certainly in the recent statistical parsing literature, apparently because of difficulty in conceptualizing and computing effective admissible estimates.
The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations. $$$$$ One involves context summarization, which uses estimates of the sort proposed in Corazza et al. (1994), but considering richer summaries.
The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding transformations. $$$$$ Two primary types of methods for accelerating parse selection have been proposed.

 $$$$$ An A* parser is simpler to build than a best-first parser, does less work per edge, and provides both an optimality guarantee and a worst-case cubic time bound.
 $$$$$ In that model, the score of a lexicalized tree is the product of the scores of two projections of that tree, one onto unlexicalized phrase structure, and one onto phrasalcategory-free word-to-word dependency structure.
 $$$$$ Using these estimates, our parser is capable of finding the Viterbi parse of an average-length Penn treebank sentence in a few seconds, processing less than 3% of the edges which would be constructed by an exhaustive parser.
