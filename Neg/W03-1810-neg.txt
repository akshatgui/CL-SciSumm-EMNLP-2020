McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. $$$$$ The results for identifying verb and particle tokens are reported in table 1, both with and without the ANLT phrasal list (ANLT phr).
McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. $$$$$ The thresholds might be acquired empirically from some training data, such as the compositionality judgements we have used.
McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. $$$$$ Whilst statistics are useful indicators of noncompositionality, there are compositional multiwords which have low values for these statistics, yet are highly non-compositional.
McCarthy et al (2003) also targeted verb-particles for a study on compositionality, and judged compositionality according to the degree of overlap in the N most similar words to the verb particle and head verb ,e.g., to determine compositionality. $$$$$ This relationship is stronger than statistics which have previously been used for filtering candidate multiwords which suggests that it might be better not to filter with statistics before looking at compositionality using an automatic thesaurus.

Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). $$$$$ Lin uses a log-likelihood ratio to filter multiword candidates before using his automatic thesaurus to detect compositionality in multiwords containing 2 or more open class words.
Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). $$$$$ The parser uses information from ANLT such as phrasals in its dictionary.
Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). $$$$$ Not every verb modified by a particle may be a genuine multiword unit, but may instead be a fully compositional verb modified by an adverbial e.g. fly up.
Building on Lin (1998), McCarthy et al (2003) measure the semantic similarity between expressions (verb particles) as a whole and their component words (verb). $$$$$ We discounted any item where any of the judges had put such a &quot;don't know&quot;.

Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. $$$$$ In this first column, we also indicate how many of the top ranked neighbours were used.
Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. $$$$$ This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the EU 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-200134460).
Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. $$$$$ The thesaurus is acquired from the grammatical relations occurring with verbs, both the target phrasals and their simplex counterparts.
Similar to Lin (1999), McCarthy et al (2003) and Fazly and Stevenson (2006), our method makes use of automatically generated thesauri; the technique used to compile the thesauri differs from previous work. $$$$$ We talk about the generation of the goldstandard set of compositionality judgements in section 4.

A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. $$$$$ These two measures correlated well together (T., = -0.51, z = -5.37) and both are significantly correlated (using the Mann Whitney U test) with whether the candidate is found in either WordNet or ANLT, see table 3, although the relationship using the automatic thesaurus is slightly higher.
A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. $$$$$ In their experiments, true positives were typically defined as such according to the annotator scanning the list.
A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. $$$$$ There are also candidates with high values of the statistics, yet they are in the middle range of the compositionality judgements, for example, plod on.
A similar comparison between the ranks according to Latent-SemanticAnalysis (LSA) based features and the ranks of human judges has been made by McCarthy, KellerandCaroll (McCarthy et al, 2003) for verb-particle con st ructions. $$$$$ Whether such an unexpectedly high cooccurrence frequency warrants an entry in the lexicon depends on the type of lexicon being built.

McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. $$$$$ We are grateful to Timothy Baldwin and Colin Bannard for their helpful comments and useful references.
McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. $$$$$ We supply short labels for these for ease of reference. overlap The size of the overlap of the top X phrasal neighbours with the same number of the corresponding simplex verb's neighbours, not including the simplex verb itself.
McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. $$$$$ The intuition is that the neighbours of the simplex verb should be similar to those of the phrasal where the phrasal has a compositional meaning, and that the phrasal neighbours should include phrasal candidates with the same particle.
McCarthy, Keller and Caroll (McCarthy et al,2003) judge compositionality according to the degree of overlap in the set of most similar words to the verb-particle and head verb. $$$$$ This would be a good way to improve phrasal extraction accuracy, particularly where a particle follows a pronoun.

(1993) and Rooth et al (1999) referring to a direct object noun for describing verbs), or used any syn tactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003)). $$$$$ To investigate if the rankings from the 3 judges agreed we employed the Kendall Coefficient of Concordance (W) (Siegel and Castellan, 1988).
(1993) and Rooth et al (1999) referring to a direct object noun for describing verbs), or used any syn tactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003)). $$$$$ We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.
(1993) and Rooth et al (1999) referring to a direct object noun for describing verbs), or used any syn tactic relationship detected by a chunker or a parser (such as Lin (1998) and McCarthy et al (2003)). $$$$$ We investigate the use of an automatically acquired thesaurus for measures designed to indicate the compositionality of candidate multiword verbs, specifically English phrasal verbs identified automatically using a robust parser.

 $$$$$ We demonstrate that there is highly significant agreement on the rank order of these judgements and use the average ranks for each item as a gold-standard to compare various measures aimed at detecting non-compositionality.
 $$$$$ In section 7 we analyse our findings, and conclude (section 8) with directions for future work.
 $$$$$ This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the EU 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-200134460).
 $$$$$ Whilst it is possible to put every single occurrence of a verb and particle combination into a lexicon this is not desirable.

Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. $$$$$ We investigated various measures which compare the nearest neighbours of the phrasal verb to the neighbours of the corresponding simplex verb.
Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. $$$$$ Not every verb modified by a particle may be a genuine multiword unit, but may instead be a fully compositional verb modified by an adverbial e.g. fly up.
Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. $$$$$ In section 6 we show the correlations of our measures with the gold-standard, and compare these to some statistics commonly used for identifying compositional multiwords.
Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate several tests for compositionality including one (simplex score) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts. $$$$$ We are grateful to Timothy Baldwin and Colin Bannard for their helpful comments and useful references.

Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). $$$$$ For these experiments we use data from the ninety million words of the written portion of the British National Corpus parsed with the RASP parser (Briscoe and Carroll, 2002).
Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). $$$$$ For the statistics (commonly used for multiword extraction) the relationship is in the other direction: high values are indicative of a non-compositional reading.
Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). $$$$$ We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard.
Table 5 shows the results of using different similarity measures with the simplex score test and data of McCarthy et al (2003). $$$$$ W is calculated as shown in equation 1 below, where n is the number of items (111 in this case), Ri is the average rank for the ith item and k is the number of raters.

McCarthy et al (2003) determine a continuum of compositionality of VPCs, but do not distinguish the contribution of the individual components. $$$$$ We use one-tailed tests because we predict the direction of the relationship.
McCarthy et al (2003) determine a continuum of compositionality of VPCs, but do not distinguish the contribution of the individual components. $$$$$ We will consider compositionality as a continuous scale and ask human judges to rank multiword candidates along this.
McCarthy et al (2003) determine a continuum of compositionality of VPCs, but do not distinguish the contribution of the individual components. $$$$$ As well as finding noncompositional multiwords, there were also a higher proportion of parser errors that met these criteria.

McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. $$$$$ This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the EU 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-200134460).
McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. $$$$$ Identifying non-compositional phrasals by employing thresholds to force a binary decision is one option.
McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. $$$$$ W ranges between 0 (little agreement) and 1 (full agreement) and bears a linear relationship to the average Spearman Rank-order Correlation Coefficient taken over all possible pairs of the rankings.
McCarthy et al (2003) evaluate the precision of Rasp at identifying VPCs to be 87.6% and the recall to be 49.4%. $$$$$ However, we believe that permitting measurements and evaluation on a continuum of compositionality allows for a more natural exploration of relationships, without imposing an arbitrary cut-off point required only when finally categorising items for a lexicon.

In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. $$$$$ We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.
In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. $$$$$ As well as finding noncompositional multiwords, there were also a higher proportion of parser errors that met these criteria.
In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. $$$$$ We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value <= 0.000001).
In particular, other than the re ported results of McCarthy et al (2003) targeting VPCs vs. all other analyses, we had no a priori sense of RASP? s ability to distinguish VPCs and verb-PPs. $$$$$ This work was supported by the EPSRC-funded RASP project (grant GR/N36493), and the EU 5th Framework project MEANING — Developing Multilingual Web-scale Language Technologies (IST-200134460).

Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003). $$$$$ We only look at tokens in isolation and therefore do not collate evidence to look for syntactic evidence of particle movement as Baldwin and Villavicencio do.
Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003). $$$$$ This would help in determining which candidate phrasals should be treated separately from the simplex for purposes such as selectional preference acquisition and word sense disambiguation.
Also, we ignore the ambiguity between particles and adverbs, which is the principal reason for our evaluation being much higher than that reported by McCarthy et al (2003). $$$$$ This only removed a total of 5 items, leaving a ranking from all 3 judges on 111 candidates.

In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f? 1) attested in the data set of McCarthy et al (2003). $$$$$ From these results we can see that some of the measures from the automatic thesaurus correlate significantly with the human compositionality judgements and that these correlations are slightly stronger than those of any of the statistics used.
In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f? 1) attested in the data set of McCarthy et al (2003). $$$$$ This only removed a total of 5 items, leaving a ranking from all 3 judges on 111 candidates.
In order to get a clearer sense of the impact of selectional preferences on the results, we investigated the relative performance over VPCs of varying semantic compositionality, based on 117 VPCs (f? 1) attested in the data set of McCarthy et al (2003). $$$$$ We also show that whilst the compositionality judgements correlate with some statistics commonly used for extracting multiwords, the relationship is not as strong as that using the automatically constructed thesaurus.

McCarthy et al (2003) provides compositionality judgements from three human judges, which we take the average of and bin into 11 categories (with 0= non-compositional and 10= fully compositional). $$$$$ We have already looked at recognition of verb and particle constructions in the WSJ identified purely on syntactic grounds using the parses provided with the WSJ Penn Treebank 2 (Marcus et al., 1995) as a gold standard.
McCarthy et al (2003) provides compositionality judgements from three human judges, which we take the average of and bin into 11 categories (with 0= non-compositional and 10= fully compositional). $$$$$ We are grateful to Timothy Baldwin and Colin Bannard for their helpful comments and useful references.

The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et als (2003) manually ranked phrasal verbs. $$$$$ In our experiments we asked human judges to rank phrasal verb candidates as to how compositional they are.
The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et als (2003) manually ranked phrasal verbs. $$$$$ We were surprised, and a little disappointed that the straight overlap of neighbours did not give a significant relationship, other than for the overlap of 30 neighbours.
The algorithm is evaluated both on 89 manually ranked MWEs and on McCarthy et als (2003) manually ranked phrasal verbs. $$$$$ W ranges between 0 (little agreement) and 1 (full agreement) and bears a linear relationship to the average Spearman Rank-order Correlation Coefficient taken over all possible pairs of the rankings.

McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. $$$$$ We only look at tokens in isolation and therefore do not collate evidence to look for syntactic evidence of particle movement as Baldwin and Villavicencio do.
McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. $$$$$ Thus the overlap where we reduce neighbours of the phrasal to simplex form compensated for this.
McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. $$$$$ This relationship is stronger than statistics which have previously been used for filtering candidate multiwords which suggests that it might be better not to filter with statistics before looking at compositionality using an automatic thesaurus.
McCarthy et al (2003) suggested that compositional phrasal verbs should have similar neighbours as for their simplex verbs. $$$$$ We also look at the relation between these judgements and appearance of the candidates in gold-standard resources such as WordNet (Miller et al., 1993) or the ANLT lexicon (Grover et al., 1993), on the premise that non-compositional phrases are more likely to be listed as multiwords in man-made resources.

In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . $$$$$ We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value <= 0.000001).
In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . $$$$$ The parser uses information from ANLT such as phrasals in its dictionary.
In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . $$$$$ He evaluated this manually on a sample.
In order to evaluate our algorithm in comparison with previous work, we also tested it on the manual ranking list created by McCarthy et al (2003) . $$$$$ For the statistics (commonly used for multiword extraction) the relationship is in the other direction: high values are indicative of a non-compositional reading.

This result is comparable with or better than most measures reported by McCarthy et al (2003). $$$$$ We change the log-likelihood statistic to add a sign where the joint frequency of particle and verb is smaller than anticipated from that expected.
This result is comparable with or better than most measures reported by McCarthy et al (2003). $$$$$ However, we believe that permitting measurements and evaluation on a continuum of compositionality allows for a more natural exploration of relationships, without imposing an arbitrary cut-off point required only when finally categorising items for a lexicon.
This result is comparable with or better than most measures reported by McCarthy et al (2003). $$$$$ We examine various measures using the nearest neighbours of the phrasal verb, and in some cases the neighbours of the simplex counterpart and show that some of these correlate significantly with human rankings of compositionality on the test set.

 $$$$$ We obtained a W score of 0.594 which gives a X2 score of 196.30 for 110 degrees of freedom which is highly significant (probability of this value <= 0.000001).
 $$$$$ For phrasal candidates at least, it might be worth using evidence from the thesaurus on the unfiltered list.
 $$$$$ We are also exploring the relation between a verb and verb and particle combination (we use the term phrasal verb) using distributional techniques, but our evaluation is somewhat different.
