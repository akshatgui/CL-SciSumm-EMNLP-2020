Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. $$$$$ Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.
Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. $$$$$ There are many conceivable ways of combining the two parsers, including more or less complex ensemble systems and voting schemes, which only perform the integration at parsing time.
Both Hall et al (2007) and Nivre and McDonald (2008) can be seen as methods to combine separately defined models. $$$$$ By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.

 $$$$$ As explained in section 2, both models essentially learn a scoring function s : X —* R, where the domain X is different for the two models.
 $$$$$ The guided models, MaltMST and MSTMalt, behave in a very similar fashion with respect to each other but both outperform their base parser over the entire range of sentence lengths.
 $$$$$ While there is no statistically significant difference between the two base models, they are both outperformed by MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt (p < 0.0005).
 $$$$$ When parsing a new sentence x' with BC, x' is first parsed with model C (this time trained on the entire training set T) to derive GCx', so that the guide features can be extracted also at parsing time.

For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ (2004), who trained classifiers on auxiliary data to guide named entity classifiers.
For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a).
For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.
For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. $$$$$ This naturally leads one to ask: Is it possible to integrate the two models in order to exploit their complementary strengths?

Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ Preliminary experiments suggested that cross-validation with more folds had a negligible impact on the results.
Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc.
Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.
Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. $$$$$ For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007).

We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). $$$$$ Table 2 shows the results, for each language and on average, for the two base models (MST, Malt) and for the two guided models (MSTMlt, MaltMST).
We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). $$$$$ The data for the experiments are training and test sets for all thirteen languages from the CoNLL-X shared task on multilingual dependency parsing with training sets ranging in size from from 29,000 tokens (Slovene) to 1,249,000 tokens (Czech).

A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ In the feature-based integration we simply extend the feature vector for one model, called the base model, with a certain number of features generated by the other model, which we call the guide model in this context.
A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.
A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank.
A technique of parser stacking is employed, which enables a data-driven parser to learn from the output of another parser, in addition to gold standard tree bank annotations (Nivre and McDonald, 2008). $$$$$ They use local training and greedy inference algorithms, but define features over a rich history of parsing decisions.

 $$$$$ Unlike the models presented here, integration takes place only at parsing time, not at learning time, and requires at least three different base parsers.
 $$$$$ Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing.
 $$$$$ The method integrates the two models by allowing the output of one model to define features for the other.

Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ The exact form of the guide features depend on properties of the base model and will be discussed in sections 3.2–3.3 below, but the overall scheme for the feature-based integration can be described as follows.
Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.
Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ Many of these parsers are based on data-driven parsing models, which learn to produce dependency graphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist.
Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. $$$$$ For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007).

Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ Given a set L = 1l1, ... ,l|L|} of arc labels (dependency relations), a dependency graph for an input sentence x = w0, w1, ... , w, (where w0 = ROOT) is a labeled directed graph G = (V, A) consisting of a set of nodes V = 10, 1, ... , n}1 and a set of labeled directed arcs A C_ V xV xL, i.e., if (i, j, l) E A for i, j E V and l E L, then there is an arc from node i to node j with label l in the graph.
Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.
Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ Preliminary experiments suggested that cross-validation with more folds had a negligible impact on the results.
Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. $$$$$ Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme.

 $$$$$ This is a fundamental trade-off that is hard to overcome by tractable means.
 $$$$$ More interestingly, MaltMST outperforms both Malt and MST for arcs up to length 9, which provides evidence that MaltMST has learned specifically to trust the guide features from MST for longer dependencies.
 $$$$$ By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.

Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. $$$$$ For the graph-based model, X is the set of possible dependency arcs (i, j, l); for the transition-based model, X is the set of possible configuration-transition pairs (c, t).
Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. $$$$$ Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.
Although Mate's performance was not significantly better than Berkeley's in our setting, it has the potential to tap richer features and other advantages of dependency parsers (Nivre and McDonald, 2008) to further boost accuracy, which may be difficult in the generative framework of a typical constituent parser. $$$$$ In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other.

Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ Given a set L = 1l1, ... ,l|L|} of arc labels (dependency relations), a dependency graph for an input sentence x = w0, w1, ... , w, (where w0 = ROOT) is a labeled directed graph G = (V, A) consisting of a set of nodes V = 10, 1, ... , n}1 and a set of labeled directed arcs A C_ V xV xL, i.e., if (i, j, l) E A for i, j E V and l E L, then there is an arc from node i to node j with label l in the graph.
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other.
Co-training (Sarkar, 2001) and classifier combination (Nivre and McDonald, 2008) are two technologies for training improved dependency parsers. $$$$$ The specific transition-based model studied in this work is that presented by Nivre et al. (2006), which uses support vector machines to learn transition scores.

Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ MaltParser trains a model to make a single classification decision (choose the next transition) whereas MSTParser trains a model to maximize the global score of correct graphs.
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ The test sets are all standardized to about 5,000 tokens each.
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ In retrospect, this result is not surprising.
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism. $$$$$ This method of combining classifiers is sometimes referred to as classifier stacking.

We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.
We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ The guided models were trained according to the scheme explained in section 3, with two-fold crossvalidation when parsing the training data with the guide parsers.
We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ The reason that accuracy does not improve for dependencies of length greater than 9 is probably that these dependencies are too rare for MaltMST to learn from the guide parser in these situations.
We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnsonand Ural, 2010) or (Zhang et al, 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). $$$$$ To train a guided version BC of base model B with guide model C and training set T, the guided model is trained, not on the original training set T, but on a version of T that has been parsed with the guide model C under a cross-validation scheme (to avoid overlap with training data for C).

Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ This method of combining classifiers is sometimes referred to as classifier stacking.
Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ We call this system MaltParser, or Malt for short, which is also the name of the freely available implementation.3 These models differ primarily with respect to three properties: inference, learning, and feature representation.
Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ (2004), who trained classifiers on auxiliary data to guide named entity classifiers.
Stacked learning has been applied as a system ensemble method in several NLP tasks, such as named entity recognition (Wu et al, 2003) and dependency parsing (Nivre and McDonald, 2008). $$$$$ Many of these parsers are based on data-driven parsing models, which learn to produce dependency graphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist.

More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). $$$$$ Considering the results for parts of speech, as well as those for dependency length and root distance, it is interesting to note that the guided models often improve even in situations where their base parsers are more accurate than their guide models.
More recently, approaches of combining these two parsers achieved even better dependency accuracy (Nivre and McDonald, 2008). $$$$$ Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007).

 $$$$$ One advantage of this representation is that it extends naturally to discontinuous constructions, which arise due to long distance dependencies or in languages where syntactic structure is encoded in morphology rather than in word order.
 $$$$$ Directions for future research include a more detailed analysis of the effect of feature-based integration, as well as the exploration of other strategies for integrating different parsing models.
 $$$$$ The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during training.
 $$$$$ In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph.

In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ The transition-based model, MaltParser, learns a scoring function s(c, t) E R over configurations and transitions.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ We call this system MaltParser, or Malt for short, which is also the name of the freely available implementation.3 These models differ primarily with respect to three properties: inference, learning, and feature representation.
In the previous work, Nivre and McDonald (2008) have integrated MST Parser and Malt Parser by feeding one parser's output as features into the other. $$$$$ MSTParser favors the former at the expense of the latter and MaltParser the opposite.

 $$$$$ This is the topic of the remainder of this paper.
 $$$$$ This again provides evidence that the guided parsers are learning from their guide models.
 $$$$$ While there is no statistically significant difference between the two base models, they are both outperformed by MaltMST (p < 0.0001), which in turn has significantly lower accuracy than MSTMalt (p < 0.0005).
 $$$$$ For more information on the data sets, see Buchholz and Marsi (2006).

Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ The test sets are all standardized to about 5,000 tokens each.
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ These m additional features account for the guide features over the MaltParser output.
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. $$$$$ The test sets are all standardized to about 5,000 tokens each.
