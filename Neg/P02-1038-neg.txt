The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ An important open problem of this approach is the handling of complex features in search.
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ An important open problem of this approach is the handling of complex features in search.
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model.

Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ We show that a baseline statistical machine translation system is significantly improved using this approach.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ We can use numerous additional features that deal with specific problems of the baseline statistical MT system.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ Table 3 shows the resulting normalized model scaling factors.

An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ We can interpret it as an approximation to the Bayes decision rule in Eq.
An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences.
An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999).

MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ The optimization problem has one global optimum and the optimization criterion is convex.
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ Among all possible target sentences, we will choose the sentence with the highest probability:1 The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ 11, we use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972).

Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ The set of considered sentences is computed by an appropriately extended version of the used search algorithm (Och et al., 1999) computing an approximate n-best list of translations.
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999).
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ In addition, it might be promising to optimize the parameters directly with respect to the error rate of the MT system as is suggested in the field of pattern and speech recognition (Juang et al., 1995; Schl¨uter and Ney, 2001).
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ As additional language model, we use a class-based five-gram language model.

For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ We see that adding new features also has an effect on the other model scaling factors.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei).
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ In this paper, we shall use the first three of these features.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ Search is performed using the so-called maximum approximation: Hence, the search space consists of the set of all possible target language sentences eI1 and all possible alignments aJ1 .

The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation.
The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).
The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ 11.
The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).

These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ 6).
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ 6 results if we use the feature function log Pr(eI1|fJ1 ) instead of log Pr(fJ1 |eI1).
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ 1 with the argument that it yields a modular approach.

For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature.
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ It allows a baseline MT system to be extended easily by adding new feature functions.
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations.
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).

 $$$$$ Multiplying each model scaling factor by a constant positive value does not affect the decision rule.
 $$$$$ As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature.
 $$$$$ This approach allows a baseline machine translation system to be extended easily by adding new feature functions.
 $$$$$ • We could use grammatical features that relate certain grammatical dependencies of source and target language.

Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ 6).
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ The set of considered sentences is computed by an appropriately extended version of the used search algorithm (Och et al., 1999) computing an approximate n-best list of translations.
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach.

We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ In addition, we might have the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence.
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ The key elements of this approach are the alignment templates, which are pairs of source and target language phrases together with an alignment between the words within the phrases.
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ Multiplying each model scaling factor by a constant positive value does not affect the decision rule.

This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ In the following, we present the results of this approach.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ We show that a baseline statistical machine translation system is significantly improved using this approach.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ As additional language model, we use a class-based five-gram language model.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ From a theoretical framework of the sourcechannel approach, this approach is hard to justify.

The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ As baseline features, we use a normal word trigram language model and the three component models of the alignment templates.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model.

The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ We can interpret it as an approximation to the Bayes decision rule in Eq.
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature.

The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model.
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei).
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ As this is not possible for the conventional dictionary feature, we use n-best rescoring for this feature.
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables.

Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ Multiplying each model scaling factor by a constant positive value does not affect the decision rule.
Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.
Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ The use of direct maximum entropy translation models for statistical machine translation has been suggested by (Papineni et al., 1997; Papineni et al., 1998).

 $$$$$ Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).
 $$$$$ In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986; Ney, 1995).
 $$$$$ There are two possible interpretations for a statistical MT system structured according to the sourcechannel approach, hence including a model for Pr(ei) and a model for Pr(fi Iei).
 $$$$$ 11.

To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences.
To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ To apply this algorithm, we have to solve various practical problems.
To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms.
To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ For further details, see (Och et al., 1999).

In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ To train the model parameters λM1 of the direct translation model according to Eq.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ An especially well-founded framework for doing this is maximum entropy (Berger et al., 1996).
