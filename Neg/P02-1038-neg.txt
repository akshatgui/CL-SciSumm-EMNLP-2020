The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the translation model instead of one feature function for the whole translation model p(fJ1 |eI1).
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ This approach allows a baseline machine translation system to be extended easily by adding new feature functions.
The approach of optimizing a small number of meta parameters has been applied to machine translation by Och and Ney (2002). $$$$$ 11.

Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ So far, we use the logarithm of the components of a translation model as feature functions.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features.
Each translation rule in the phrase-based translation model has a set number of features that are combined in the log-linear model (Och and Ney, 2002), and our semi-supervised DAE features can also be combined in this model. $$$$$ As soon as we want to use model scaling factors, we can only do this in a theoretically justified way using the second interpretation.

An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ We present results on the VERBMOBIL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster, 1993).
An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ 6).
An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ Yet, we are not limited to train only model scaling factors, but we have many possibilities: This corresponds to a word penalty for each produced target word.
An alternate way to optimize weights over translation features is described in Och and Ney (2002). $$$$$ As additional language model, we use a class-based five-gram language model.

MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ We see that adding new features also has an effect on the other model scaling factors.
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ This approach allows a baseline machine translation system to be extended easily by adding new feature functions.
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ The second row shows the result if we train the model scaling factors.
MERT directly optimizes the evaluation metric under which systems are being evaluated, yielding superior performance (Och, 2003) when compared to a likelihood-based discriminative method (Och and Ney, 2002). $$$$$ • IER (information item error rate): The test sentences are segmented into information items.

Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ We have presented a framework for statistical MT for natural languages, which is more general than the widely used source-channel approach.
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ We see that adding new features also has an effect on the other model scaling factors.
Smoothing the objective function may allow differentiation and standard ML learning techniques (Och and Ney, 2002). $$$$$ Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999).

For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ To train the model parameters λM1 of the direct translation model according to Eq.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ In the following, we present the results of this approach.
For example, the system described in (Koehnet al, 2003) is a widely known one using small number of features in a maximum-entropy (log-linear) model (Och and Ney, 2002). $$$$$ In contrast to their approach, we include a dependence on the hidden variable of the translation model in the direct translation model.

The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ This is a very convenient approach to improve the quality of a baseline system.
The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ As additional language model, we use a class-based five-gram language model.
The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. $$$$$ The second row shows the result if we train the model scaling factors.

These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ This is a very convenient approach to improve the quality of a baseline system.
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ We can interpret it as an approximation to the Bayes decision rule in Eq.
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ This feature and the word penalty feature allow a straightforward integration into the used dynamic programming search algorithm (Och et al., 1999).
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002). $$$$$ They train models for natural language understanding rather than natural language translation.

For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ Yet, the use of this decision rule has various problems: Here, we replaced pˆ�(fJ1 |ei) by pˆ�(ei|fJ1 ).
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ • We could use grammatical features that relate certain grammatical dependencies of source and target language.
For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al, 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). $$$$$ This approach allows a baseline machine translation system to be extended easily by adding new feature functions.

 $$$$$ Therefore, we are able to use statistical alignment models, which have been shown to be a very powerful component for statistical machine translation systems.
 $$$$$ 2 or as an instance of a direct maximum entropy model with feature functions log Pr(ei) and log Pr(fi |ei).
 $$$$$ The first row shows the results using only the four baseline features with λ1 = · · · = λ4 = 1.

Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ We see that adding new features also has an effect on the other model scaling factors.
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ It should be noted that, as was already shown by (Darroch and Ratcliff, 1972), by applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features.
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ In all experiments, we use the following six error criteria: of the target sentence, so that the WER measure alone could be misleading.
Feature function scaling factors λm are optimized based on a maximum likely approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ The use of direct maximum entropy translation models for statistical machine translation has been suggested by (Papineni et al., 1997; Papineni et al., 1998).

We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ We can use numerous additional features that deal with specific problems of the baseline statistical MT system.
We optimized feature weights using the minimum error rate training algorithm (Och and Ney, 2002) on the NIST 2002 test set. $$$$$ Here, we omit a detailed description of modeling, training and search, as this is not relevant for the subsequent exposition.

This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ In addition, we might have the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ In this paper, we shall use the first three of these features.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
This is similar to what Och and Ney (2002) used for their maximum entropy-based statistical machine translation training. $$$$$ In addition, we might have the problem that no single of the reference translations is part of the nbest list because the search algorithm performs pruning, which in principle limits the possible translations that can be produced given a certain input sentence.

The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ An important open problem of this approach is the handling of complex features in search.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ To train the model parameters λM1 of the direct translation model according to Eq.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ Hence, we approximate this sum by sampling the space of all possible sentences by a large set of highly probable sentences.
The training of the model scaling factors as described in (Och and Ney, 2002) was done on N-best lists. $$$$$ Combining various probabilistic models for speech and language modeling has been suggested in (Beyerlein, 1997; Peters and Klakow, 1999).

The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ To simplify the notation, we shall omit in the following the dependence on the hidden variables of the model.
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ Therefore, we use a large variety of different criteria and show that the obtained results improve on most or all of these criteria.
The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleu based minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002). $$$$$ The use of an ‘inverted’ translation model in the unconventional decision rule of Eq.

The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ We are given a source (‘French’) sentence fJ1 = f1, ... , fj, ... , fJ, which is to be translated into a target (‘English’) sentence eI1 = e1, ... , ei, ... , eI.
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ We observe improved error rates for using the word penalty and the class-based language model as additional features.
The feature functions hi are the system models and the weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). $$$$$ Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).

Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ We can use numerous additional features that deal with specific problems of the baseline statistical MT system.
Word order in the translation output relies on how the phrases are reordered based on both language model scores and distortion cost/penalty (Koehn et al, 2003), among all the features utilized in a maximum-entropy (log linear) model (Och and Ney, 2002). $$$$$ An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms.

 $$$$$ An interesting question is to come up with features that allow an efficient handling using conventional dynamic programming search algorithms.
 $$$$$ Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).
 $$$$$ An important open problem of this approach is the handling of complex features in search.
 $$$$$ So far, in machine translation research does not exist one generally accepted criterion for the evaluation of the experimental results.

To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ The set of considered sentences is computed by an appropriately extended version of the used search algorithm (Och et al., 1999) computing an approximate n-best list of translations.
To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ Hence, we change the criterion to allow Rs reference translations es,1, ... , es,Rs for the sentence es: We use this optimization criterion instead of the optimization criterion shown in Eq.
To translate the input documents into English we use phrase-based statistical machine translation systems based on the log-linear formulation of the problem (Och and Ney, 2002). $$$$$ To solve this problem, we define for maximum entropy training each sentence as reference translation that has the minimal number of word errors with respect to any of the reference translations.

In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ We see that the sentence error rates converges after about 4000 iterations.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ We present a framework for statistical machine translation of natural languages based on direct maximum entropy models, which contains the widely used source-channel approach as a special case.
In fact, only recently, log-probability features have been deployed in ME models for statistical machine translation (Och and Ney, 2002). $$$$$ The advantage of the alignment template approach compared to single word-based statistical translation models is that word context and local changes in word order are explicitly considered.
