When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ A word provides a superset of related concepts, called a hypernym.?In the first example, both substitution and dropping happen.
When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ Data nodes contain data and operation nodes execute operations.
When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ Due to space limitations, we cannot provide all the details of the decoder.We calculate the inside probability and out side probability for each node in the decoding tree.
When compared against current state of-the-art methods (Zhu et al, 2010) our model yields significantly simpler output that is both grammatical and meaning preserving. $$$$$ 3).

Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ isdropped.
Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ The evaluation shows that our model achieves better readability scores than a set of baseline systems.
Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ 4.
Zhu et al (2010) also use Wikipedia to learn a sentence simplification model which is able to perform four rewrite operations, namely substitution, reordering, splitting, and deletion. $$$$$ root stores the parse tree of c and also s1 and s2.

In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ 1360
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ is ?WHNP?.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ 2, PWKP contains more than 108k sentence pairs.

We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. $$$$$ We also described anefficient training method with speeding up tech niques for TSM.
We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. $$$$$ 6.2 Translation Assessment.
We evaluated our model on the same dataset used in Zhu et al (2010), an aligned corpus of MainEW and SimpleEW sentences. $$$$$ An expectation maximization (EM) algorithm is used to iteratively train our model.

However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ and the ?Grade?
However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ This helps humans read texts more easilyand faster.
However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ In this paper, we presented a novel large-scale par allel dataset PWKP for sentence simplification.
However, we refrained from doing so as Zhu et al (2010) show that Moses performs poorly, it can not model rewrite operations that split sentences or drop words and in most cases generates output identical. $$$$$ We develop the Training Tree (Fig.

AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ P (s|c) is equal to the inside probability of the root in theTraining Tree.
AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.
AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ We can select the promisingcandidates for the dropping, reordering and map ping operations similarly.
AlignILP is most different from the reference, followed by Zhu et al (2010) and RevILP. $$$$$ TSM: ?An umbrella term is a word.

Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. $$$$$ The sentences from Wikipedia and Simple Wikipedia are considered as ?complex?
Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. $$$$$ The dropping operation further removes unimportant parts of a sentence to make it more concise.
Zhu et al (2010) is the least grammatical model. Finally, RevILP preserves the meaning of the target as well as SimpleEW, whereas Zhu et al yields the most distortions. $$$$$ The binary features, such as SFT and BDFT, are assigned the initial value of 0.5.

Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. $$$$$ We train the language model with SRILM (Stolcke, 2002).
Our results also show that a more general model not restricted to specific rewrite operations like Zhu et al (2010) obtains superior results and has better coverage. $$$$$ We henceforth chose sentence-level TF*IDF to align our dataset.

Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ We re fer readers to Yamada and Knight (2001) for more details.
Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ We proposed TSM, a tree-based translation model for sentence simplification which covers splitting, dropping, reordering and word/phrase substitution integrally for the first time.
Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ ?PE?
Zhu et al (2010), for example, use a tree-based simplification model which uses techniques from statistical machine translation (SMT) with this data set. $$$$$ The word fre quency is counted using the articles from Simple Wikipedia.

This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ When we simplify a complex sentence, we start from the root and greedily select the branchwith the highest outside probability.
This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ We train the language model with SRILM (Stolcke, 2002).
This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ There are two kinds of nodes in the training tree: data nodes in rectangles and operation nodes in circles.
This system can handle sentence splitting operations and the authors use both automatic and human evaluation and show an improvement over the results of Zhu et al (2010) on the same data set, but they have to admit that learning from parallel bi-text is not as efficient as learning from revision histories of the Wiki-pages. $$$$$ We manually adjusted the similarity threshold to obtain a recallvalue as close as possible to 55.8% which was pre viously adopted by Nelken and Shieber (2006).

Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. $$$$$ We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.
Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. $$$$$ In this paper, we consider sentence sim plification as a special form of translation with the complex sentence as the source and the simple sentence as the target.
Our work extends recent work by Zhu et al (2010) that also examines Wikipedia/Simple English Wikipedia as a data-driven, sentence simplification task. $$$$$ An expectation maximization (EM) algorithm is used to iteratively train our model.

Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB.
Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ The first operation is sentence splitting, which wefurther decompose into two subtasks: (i) segmen tation, which decides where and whether to split a sentence and (ii) completion, which makes the new split sentences complete.
Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.
Our approach performs significantly better than two different text compression approaches, including T3, and better than previous approaches on a similar data set (Zhu et al, 2010). $$$$$ The transformation froma complex sentence to a simple sentence is con ducted by applying a sequence of simplification operations.

There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.
There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ 1 N (6)There are still some important issues to be con sidered in future.
There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ in the secondsplit sentence is ?gov nsubj?.6 The direct constituent of ?started?
There has also been work on lexical substitution for simplification, where the aim is to substitute difficult words with simpler synonyms, derived from WordNet or dictionaries (Inui et al, 2003). Zhu et al (2010) examine the use of paired documents in English Wikipedia and Simple Wikipediafor a data-driven approach to the sentence simplification task. $$$$$ The authors are requested to ?use easy words and short sentences?

We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ We initialize the probabilities with the uniform distribution.
We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ The decoding tree does not have mp op erations and there can be more than one sub nodes attached to a single ro res.
We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ using the dump filesin Wikimedia.5 Administration articles were fur ther removed.
We follow Zhu et al (2010) and Coster and Kauchak (2011) in proposing that sentence simplification can be approached as a monolingual machine translation task, where the source and target languages are the same and where the output should be simpler in form from the input but similar in meaning. $$$$$ All the articles from the Simple Wikipedia are used as the training corpus, amounting to about 54 MB.

 $$$$$ However, most of the existing models only consider one of these operations.
 $$$$$ We use ?mteval-v11b.pl?9 as the evaluation tool.
 $$$$$ They have not been used for training.Four baseline systems are compared in our eval uation.

 $$$$$ For the sub stitution operation, we also integrate a trigram language model to make the generated sentences more fluent.
 $$$$$ with ?use?
 $$$$$ The original motivation for sentence simplification is using it as a preprocessor to facili tate parsing or translation tasks (Chandrasekar et al., 1996).
 $$$$$ The original motivation for sentence simplification is using it as a preprocessor to facili tate parsing or translation tasks (Chandrasekar et al., 1996).

While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements. $$$$$ We select the promising candidates using monolingual word mapping as shown in Fig.
While Zhu et al (2010) have demonstrated that their approach outperforms a PBMT approach in terms of Flesch Reading Ease test scores, we are not aware of any studies that evaluate PBMT for sentence simplification with human judgements. $$$$$ We propose a Tree-based Simplification Model (TSM), which, to our knowledge, is the first statistical simplification model covering splitting, dropping, reorderingand substitution integrally.

 $$$$$ SBAR 2 false 0.9165 Table 3: Segmentation Feature Table (SFT) Actually, we do not use the direct constituent of a word in the parse tree.
 $$$$$ For the sub stitution operation, we also integrate a trigram language model to make the generated sentences more fluent.
 $$$$$ TSM gets the best PPL score.
 $$$$$ 4.

Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ However, the original complex sentences (CW) from Normal Wikipedia get a rather high BLEU (0.50), when compared to the simple sentences.
Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ ?PE?
Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ The reordering operationinterchanges the order of the split sentences (Sid dharthan, 2006) or parts in a sentence (Watanabeet al, 2009).
Zhu et al (2010) learn a sentence simplification model which is able to perform four rewrite operations on the parse trees of the input sentences, namely substitution, reordering, splitting, and deletion. $$$$$ 6) similarly to the construction of the training tree.

Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ We also de scribe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia.
Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ (Moses): The same as (SW).
Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ The evaluation shows that our model achieves better readability scores than a set of baseline systems.
Zhu et al (2010) evaluate their system using BLEU and NIST scores, as well as various readability scores that only take into account the output sentence, such as the Flesch Reading Ease test and n-gram language model perplexity. $$$$$ Algorithm 3 calcInsideProb (TrainingTree tt) for each node from level = N to root of tt do if node is a sub node then node.insideProb = P (sub|node); else if node is a mp OR sp node then node.insideProb =Qchild child.insideProb;else node.insideProb =Pchild child.insideProb;end if end for Algorithm 4 calcOutsideProb (TrainingTree tt) for each node from root to level = N of tt do if node is the root then node.outsideProb = 1.0; else if node is a sp res OR mp res node then {COMMENT: father are the fathers of the current node, sibling are the children of father excluding the current node} node.outsideProb = P father father.outsideProb ?Qsibling sibling.insideProb;else if node is a mp node then node.outsideProb = father.outsideProb ? 1.0; else if node is a sp, ro, dp or sub node then node.outsideProb = father.outsideProb ? P (sp or ro or dp or sub|node); end if end for August was the sixth in the ancient Roman calendar statedwhich in 735BC August was the sixth in the old Roman calendar stated in 735BCThe old calendar.
