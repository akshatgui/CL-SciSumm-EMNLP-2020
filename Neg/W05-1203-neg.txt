The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. $$$$$ Next, we try to determine pairs of similar words across the sets corresponding to the same open-class in the two text segments.
The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. $$$$$ Unlike traditional similarity measures based on lexical matching, our metric takes into account the semantic similarity of these words, resulting in a more precise measure of text similarity.
The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. $$$$$ First, the text segments are tokenized, part-ofspeech tagged, and the words are inserted into their corresponding word class sets.
The aggregation strategy proposed by Corley and Mihalcea (2005) has been utilized for extending these word-to-word similarity measures for calculating text-to-text similarities. $$$$$ Starting with each of the two text segments, and for each word in its word class sets, we determine the most similar word from the corresponding set in the other text segment.

Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. $$$$$ The remaining word classes: adjectives, adverbs, and cardinals, are checked for lexical similarity with their counter-parts and included in the corresponding word class set if a match is found.
Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. $$$$$ For the task of paraphrase recognition, incorporating semantic information into the text similarity measure increases the likelihood of recognition significantly over the random baseline and over the lexical matching baseline.
Most similar to our approach are the methods of Islam and Inkpen (2008) and Corley and Mihalcea (2005), who performed a word-to-word similarity alignment; however, they did not operate at the sense level. $$$$$ For the entailment data set, although we do not explicitly check for entailment, the directional similarity computed for textual entailment recognition does improve over the random and lexical matching baselines.

Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text ,e.g., (Corley and Mihalcea, 2005). $$$$$ Future work will consider the investigation of more sophisticated representations of sentence structure, such as first order predicate logic or semantic parse trees, which should allow for the implementation of more effective measures of text semantic similarity.
Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text ,e.g., (Corley and Mihalcea, 2005). $$$$$ While successful to a certain degree, these lexical matching similarity methods fail to identify the semantic similarity of texts.
Although these implications are uncontroversial, their automatic recognition is complex if we rely on models based on lexical distance (or similarity) between hypothesis and text ,e.g., (Corley and Mihalcea, 2005). $$$$$ The sets obtained for the given text segments are illustrated in Figure 1.

 $$$$$ We also evaluate a metric that combines all the similarity measures using a simple average, with results indicated in the Combined row.
 $$$$$ Note that all these metrics are defined between concepts, rather than words, but they can be easily turned into a word-to-word similarity metric by selecting for any given pair of words those two meanings that lead to the highest concept-to-concept similarity.

 $$$$$ Text similarity has been also used for relevance feedback and text classification (Rocchio, 1971), word sense disambiguation (Lesk, 1986), and more recently for extractive summarization (Salton et al., 1997b), and methods for automatic evaluation of machine translation (Papineni et al., 2002) or text summarization (Lin and Hovy, 2003).
 $$$$$ It is based on an algorithm proposed in (Lesk, 1986) as a solution for word sense disambiguation.
 $$$$$ In the experiments reported in this paper, we use the British National Corpus to derive the document frequency counts, but other corpora could be used to the same effect.
 $$$$$ This paper presents a knowledge-based method for measuring the semanticsimilarity of texts.

Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. $$$$$ While there are several methods previously proposed for finding the semantic similarity of words, to our knowledge the application of these word-oriented methods to text similarity has not been yet explored.
Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. $$$$$ We also measure precision, recall and F-measure, calculated with respect to the true values in each of the test data sets.
Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
Corley and Mihalcea (2005) proposed a hybrid method by combining six existing knowledge-based methods. $$$$$ While word frequency does not always constitute a good measure of word importance, the distribution of words across an entire collection can be a good indicator of the specificity of the words.

Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. $$$$$ While the specificity of words is already measured to some extent by their depth in the semantic hierarchy, we are reinforcing this factor with a corpus-based measure of word specificity, based on distributional information learned from large corpora.
Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. $$$$$ For paraphrase identification, we use the bidirectional similarity measure, and determine the similarity with respect to each of the two text segments in turn, and then combine them into a bidirectional similarity metric.
Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
Then, we use this cross-pair similarity with more traditional intra-pair similarities (e.g., (Corley and Mihalcea, 2005)) to define a novel kernel function. $$$$$ We use the WordNet-based implementation of these metrics, as available in the WordNet::Similarity package (Patwardhan et al., 2003).

In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors $$$$$ For each noun (verb) in the set of nouns (verbs) belonging to one of the text segments, we try to identify the noun (verb) in the other text segment that has the highest semantic similarity (maxSim), according to one of the six measures of similarity described in Section 2.1.
In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors $$$$$ In this case, we use a voted perceptron algorithm (Freund and Schapire, 1998)3.
In line with many other researches (e.g., (Corley and Mihalcea, 2005)), we determine these anchors using different similarity or relatedness dec tors $$$$$ We introduce an algorithm that combines the word-to-word similarity metrics into a text-to-text semantic similarity metric, and we show that this method outperforms the simpler lexical matching similarity approach, as measured in a paraphrase identification application.

Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). $$$$$ When learning is used to find the optimal combination of metrics and optimal threshold, the highest accuracy of 71.5% is obtained by combining the similarity metrics and the lexical matching baseline together.
Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). $$$$$ We determine the specificity of a word using the inverse document frequency introduced in (Sparck-Jones, 1972), which is defined as the total number of documents in the corpus, divided by the total number of documents that include that word.
Experimental results lexical similarity siml (T, H) as defined in (Corley and Mihalcea, 2005). $$$$$ For nouns and verbs, we use a measure of semantic similarity based on WordNet, while for the other word classes we apply lexical matching1.

First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. $$$$$ Starting with each of the two text segments, and for each word in its word class sets, we determine the most similar word from the corresponding set in the other text segment.
First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. $$$$$ For the task of paraphrase recognition, incorporating semantic information into the text similarity measure increases the likelihood of recognition significantly over the random baseline and over the lexical matching baseline.
First, as observed in (Corley and Mihalcea, 2005) the lexical-based distance kernel Kl shows an accuracy significantly higher than the random baseline ,i.e. 50%. $$$$$ The only exception to this trend is perhaps the latent semantic analysis (LSA) method (Landauer et al., 1998), which represents an improvement over earlier attempts to use measures of semantic similarity for information retrieval (Voorhees, 1993), (Xu and Croft, 1996).

The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). $$$$$ Unlike traditional similarity measures based on lexical matching, our metric takes into account the semantic similarity of these words, resulting in a more precise measure of text similarity.
The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). $$$$$ In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
The first backup strategy is a straightforward BoW method that we will not present in this paper (see more details in (Corley and Mihalcea, 2005)). $$$$$ This paper presents a knowledge-based method for measuring the semanticsimilarity of texts.

In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. $$$$$ The results obtained in the supervised setting are shown in Tables 4 and 5.
In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. $$$$$ We also evaluate a metric that combines all the similarity measures using a simple average, with results indicated in the Combined row.
In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. $$$$$ The only exception to this trend is perhaps the latent semantic analysis (LSA) method (Landauer et al., 1998), which represents an improvement over earlier attempts to use measures of semantic similarity for information retrieval (Voorhees, 1993), (Xu and Croft, 1996).
In practice, we implement WBOW by using the text similarity measure defined in (Corley and Mihalcea, 2005) as the single feature in the SVM classifier that, as in BOW, learns the threshold on this single feature. $$$$$ We also evaluate a metric that combines all the similarity measures using a simple average, with results indicated in the Combined row.

The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. $$$$$ Tables 2 and 3 show the results obtained in the unsupervised setting, when a text semantic similarity larger than 0.5 was considered to be an indicator of paraphrasing (entailment).
The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. $$$$$ This paper presents a knowledge-based method for measuring the semanticsimilarity of texts.
The system performance reported in (CM05; (Corley and Mihalcea, 2005)), which is among the best we are aware of, is also included for comparison. $$$$$ In this paper, we explore a knowledge-based method for measuring the semantic similarity of texts.

This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). $$$$$ This paper presents a knowledge-based method for measuring the semanticsimilarity of texts.
This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). $$$$$ While word frequency does not always constitute a good measure of word importance, the distribution of words across an entire collection can be a good indicator of the specificity of the words.
This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). $$$$$ For each of the two data sets, we conduct two evaluations, under two different settings: (1) An unsupervised setting, where the decision on what constitutes a paraphrase (entailment) is made using a constant similarity threshold of 0.5 across all experiments; and (2) A supervised setting, where the optimal threshold and weights associated with various similarity metrics are determined through learning on training data.
This simple feature is the lexical similarity between T and H computed using WordNet-based metrics as in (Corley and Mihalcea, 2005). $$$$$ For illustration purposes, we restrict our attention to one measure of word-to-word similarity, the Wu & Palmer metric.

Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. $$$$$ We evaluate the text similarity metric built on top of the various word-to-word metrics introduced in Section 2.1.
Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. $$$$$ While successful to a certain degree, these lexical matching similarity methods fail to identify the semantic similarity of texts.
Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. $$$$$ While there is a large body of previous work focused on finding the semantic similarity of concepts and words, the application of these wordoriented methods to text similarity has not been yet explored.
Otherwise, WordNet (Miller, 1995) similarities (as in (Corley and Mihalcea, 2005)) and different relation between words such as verb entailment and derivational morphology are applied. $$$$$ For illustration purposes, we restrict our attention to one measure of word-to-word similarity, the Wu & Palmer metric.

Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. $$$$$ While there are several methods previously proposed for finding the semantic similarity of words, to our knowledge the application of these word-oriented methods to text similarity has not been yet explored.
Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. $$$$$ Both these figures are competitive with the best results achieved during the PASCAL entailment evaluation (Dagan et al., 2005).
Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. $$$$$ If this similarity measure results in a score greater than 0, then the word is added to the set of similar words for the corresponding word class WSpo32.
Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures. $$$$$ Given two text segments, as shown in Figure 1, we want to determine a score that reflects their semantic similarity.

Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). $$$$$ In this case, we use a voted perceptron algorithm (Freund and Schapire, 1998)3.
Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). $$$$$ In this paper, we introduce a method that combines wordto-word similarity metrics into a text-totext metric, and we show that this method outperforms the traditional text similarity metrics based on lexical matching.
Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). $$$$$ We illustrate the application of the text similarity measure with an example.
Also, we will try different similarity score functions for both the clustering and the anchor approaches, as those surveyed in Corley and Mihalcea (2005). $$$$$ In addition, we also evaluate the measure using the PASCAL corpus (Dagan et al., 2005), consisting of 1,380 test–hypothesis pairs with a directional entailment (580 development pairs and 800 test pairs).

The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed. $$$$$ Text 1 Text 2 maxSim IDF jurors jurors 1.00 5.80 courtroom jurors 0.30 5.23 questionnaire questionnaire 1.00 3.57 groups questionnaire 0.29 0.85 were were 1.00 0.09 taken asked 1.00 0.28 asked asked 1.00 0.45 fill complete 0.86 1.29 out – 0 0.06 40 – 0 1.39 Next, we use equation 7 and determine the semantic similarity of the two text segments with respect to text 1 as 0.6702, and with respect to text 2 as 0.7202.
The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed. $$$$$ One of the earliest applications of text similarity is perhaps the vectorial model in information retrieval, where the document most relevant to an input query is determined by ranking documents in a collection in reversed order of their similarity to the given query (Salton and Lesk, 1971).
The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed. $$$$$ The optimal combination of similarity metrics and optimal threshold are now determined in a learning process performed on the training set.
The semantic similarity formula from (Corley and Mihalcea, 2005) defines the similarity of a pair of documents differently depending on with respect to which text it is computed. $$$$$ For the task of paraphrase recognition, incorporating semantic information into the text similarity measure increases the likelihood of recognition significantly over the random baseline and over the lexical matching baseline.
