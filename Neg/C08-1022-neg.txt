Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. $$$$$ In the longer term, we also envisage mining the information implicit inour training data to create a lexical resource de scribing the statistical tendencies observed.
Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. $$$$$ In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing.
Since the features of machine learned error detectors are often part-of-speech n grams or word word dependencies extracted from parser output (De Felice and Pulman, 2008, for example), it is important to understand how part-of speech taggers and parsers react to particular grammatical errors. $$$$$ However, by far the most fre quent error type for determiners is not confusion between indefinite and definite article, but omitting an article where one is needed.

As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. $$$$$ Chodorow et al (2007) present an approach to preposition error detectionwhich also uses a model based on a maximum entropy classifier trained on a set of contextual fea tures, together with a rule-based filter.
As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. $$$$$ We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. $$$$$ The baseline refers to always choosing the most frequent option, namely of.We can see that our model?s performance com pares favourably to the best results in the literature, although direct comparisons are hard to draw sincedifferent groups train and test on different preposi tion sets and on different types of data (British vs. American English, BNC vs. news reports, and so 2 No word sense disambiguation was performed at this stage.

Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. $$$$$ The classifier can therefore learn to associate a given preposition or determiner to particular contexts, and re liably predict a class when presented with a novel instance of a context for one or the other.
Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. $$$$$ The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context.
Han et al (2006) and De Felice and Pulman (2008) train a maximum entropy classifier. $$$$$ The classifier can be further fine-tuned to acquire more reliable models of use for the two POS.

The best results of 92.15% are reported by De Felice and Pulman (2008). $$$$$ Han et al (2006) use a maxi mum entropy classifier to detect determiner errors, achieving 83% accuracy.
The best results of 92.15% are reported by De Felice and Pulman (2008). $$$$$ A good picture of the model?s errors can be had by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas sifier?s incorrect decision was.
The best results of 92.15% are reported by De Felice and Pulman (2008). $$$$$ There fore, in developing a system for automatic error detection in L2 writing, it seems desirable to focus on these problematic, and very common, parts of speech (POS).This paper gives a brief overview of the prob lems posed by these POS and of related work.

In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ Although in the past there has been some research on determiner choice in L1 for applications such as generation and machine translation output, work to date on automatic error detection in L2 writing hasbeen fairly limited.
In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ Finally, a surprising difference comes from looking at what to is confused with.
In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing.
In the context of automated preposition and determiner error correction in L2 English, De Felice and Pulman (2008) noted that the process is often disrupted by misspellings. $$$$$ Therefore, here, too, it is very hard tocome up with clear-cut rules predicting every pos sible kind of occurrence.

 $$$$$ Rachele De Felice was supported by an AHRC scholar ship for the duration of her studies.
 $$$$$ Determiners pose a somewhat different problem from prepositions as, unlike them, their choice is more dependent on the wider discourse contextthan on individual lexical items.
 $$$$$ Since the two approaches are not tested on the same data this com parison is not conclusive, but we are optimistic that there is a real difference in accuracy since the type of texts used are not dissimilar.
 $$$$$ Although in the past there has been some research on determiner choice in L1 for applications such as generation and machine translation output, work to date on automatic error detection in L2 writing hasbeen fairly limited.

T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ To evaluate the model?s performance on learner data, we use a subsection of the Cambridge Learner Corpus (CLC) 5 . We envisage our model to.
T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ 2.1 Prepositions.
T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ Finally, Gamon etal.
T& amp; C08, De Felice and Pulman (2008) and Gamon et al (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from well formed text and a writer's preposition is compared with the predictions of this model. $$$$$ Finally, a surprising difference comes from looking at what to is confused with.

On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ Semantic word type information is takenfrom WordNet lexicographer classes, 40 broad se mantic categories which all nouns and verbs in WordNet belong to 2 (e.g. ?verb of motion?, ?noun denoting food?), while the POStags are from the Penn Treebank tagset - we note the POS of three words either side of the target word 3 . For each.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ In the longer term, we also envisage mining the information implicit inour training data to create a lexical resource de scribing the statistical tendencies observed.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ 5.1.1 Further discussion To fully assess the model?s performance on the L1data, it is important to consider factors such as performance on individual prepositions, the relation ship between training dataset size and accuracy, and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in dividual prepositions together with the size of their training datasets.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, utilize the training of well-formed texts ((De Felice and Pulman, 2008) and (Tetreault et al, 2010)), learner texts, or both pair wisely (Brockett et al, 2006). $$$$$ The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context.

On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. $$$$$ no WordNet category food, plant Prep modification?
On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. $$$$$ We show that models of use for these parts of speech can be learned with an accuracy of 70.06% and 92.15% respectively on L1 text, and present first results in an error detection task for L2 writing.
On the other hand, supervised models, typically treating error detection/correction as a classification problem, may train on well-formed texts as in the methods by De Felice and Pulman (2008) and Tetreault et al. $$$$$ Furthermore, issues relating to the use of NLP tools with L2 data must be addressed, such as factoring out spelling or other errors in the data, and perhaps training on text types which are more similar to the CLC.

Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. $$$$$ However, in noting both divergences and similarities between the two learners, human and machine, we may be able to derive useful insights into the way the learning processes operate, and what factors could be more or less important for them.
Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. $$$$$ On theother hand, this would create a distorted represen tation of the composition of English, which maynot be what we want in a statistical model of lan guage.
Chodorow et. al (2007), Tetreault and Chodorow (2008), and De Felice and Pulman (2008) train a maximum entropy model and De Felice and Pulman (2007) train a voted perceptron algorithm to correct preposition errors. $$$$$ Preliminary error analysis shows that the modelis successful at identifying cases of misused deter miner, e.g. a for the or vice versa, doing so in overtwo-thirds of cases.

 $$$$$ Conversely, the less frequent prepositions are less of ten suggested as the classifier?s choice.
 $$$$$ In this paper, we present an approach to the automatic identification and correction ofpreposition and determiner errors in non native (L2) English writing.
 $$$$$ As it is hard even for L1 speakers to articulatethe reasons for these differences, it is not surprising that learners find it difficult to master preposi tions.
 $$$$$ The relation be tween a noun and a determiner is less strict than that between a verb or noun and a preposition, the main factor in determiner choice being the specific properties of the noun?s context.
