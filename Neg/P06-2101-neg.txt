The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ When γ is low, the smooth objective might allow us to pass over local minima that could open up at higher γ.
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ We will usually write this simply as L(y) since y∗ is fixed and clear from context.
The system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). $$$$$ Although the components of the translation and language models interact in complex ways, the improvement on Finnish-English may be due in part to the higher weight that minimum risk annealing found for the word penalty.

A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ Thanks to the linearity of expectation, we can easily compute our expected loss in equation (6) by adding up the expected loss on each sentence.
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ It never does significantly worse.

Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ By calling this global line minimization as a subroutine of multidimensional optimization, he was able to minimize (2) well enough to improve over likelihood maximization for training factored machine translation systems.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ It never does significantly worse.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ We used the method described above, employing the Pharaoh decoder at step 2 to generate the 200-best translations according to the current 0.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.

This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ N. A. Smith and J. Eisner.
This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ Different methods can be used to attempt this global, non-convex optimization.
This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score. $$$$$ Accuracy—in dependency parsing, part-of-speech tagging, and other labeling tasks—falls into this class, as do recall, word error rate in ASR, and the crossing-brackets metric in parsing.

In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ It never does significantly worse.

 $$$$$ Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)’s BLEU translation metric (the geometric mean of several precisions).
 $$$$$ A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences.
 $$$$$ We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.
 $$$$$ Och (2003) observed, however, that the piecewiseconstant property could be exploited to characterize the function exhaustively along any line in parameter space, and hence to minimize it globally along that line.

Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ 1996.
Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ K. A. Papineni.
Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ Elidan and Friedman (2005)).
Deterministic Annealing: In this system, in stead of using the regular MERT (Och, 2003) whose training objective is to minimize the one best error, we use the deterministic annealing training procedure described in Smith and Eisner (2006), whose objective is to minimize the expected error (together with the entropy regularization technique). $$$$$ We used 10 iterations of gradient ascent.

Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ Giving this feature a small or negative weight can discount a submodel that is foolishly structured, badly trained, or redundant with the other features.
Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.
Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ We found this to perform significantly better on BLEU evaluation than if we trained with a “linearized” BLEU that summed per-sentence BLEU scores (as used in minimum Bayes risk decoding by Kumar and Byrne (2004)).

The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from oc to −oc, thereby weakening the preference for high entropy.
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and GermanEnglish, as follows.

The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ Although the components of the translation and language models interact in complex ways, the improvement on Finnish-English may be due in part to the higher weight that minimum risk annealing found for the word penalty.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ It never does significantly worse.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ 27.78 47.23 36.78 Min. error 22.52 54.72 36.78 Ann. min. risk 31.16 54.66 36.71 2: dependency accuracy on parsing 200sentence test corpora, after training 10 experts on 1000 senand fitting their weights 200 more.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech.

Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ Adding a scale hyperparameter γ to equation (1), we have the following family of distributions: When γ = 0, all yi,k are equally likely, giving the uniform distribution; when γ = 1, we recover the model in equation (1); and as γ —* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ In pages 493–496.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ 2001.

 $$$$$ Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).
 $$$$$ Rather than minimizing a loss function suited to the task, many systems (especially for language modeling) choose simply to maximize the probability of the gold standard.
 $$$$$ For clearer exposition, we assume below that the total loss over some test corpus is the sum of the losses on individual sentences, although we will revisit that assumption in §5.
 $$$$$ If loss on heldout data begins to increase, we may be starting to overfit.

In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ Keeping this small keeps weights low and entropy high.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ Adding a scale hyperparameter γ to equation (1), we have the following family of distributions: When γ = 0, all yi,k are equally likely, giving the uniform distribution; when γ = 1, we recover the model in equation (1); and as γ —* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ We annealed using our novel expected-BLEU approximation from §5.

Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ If loss on heldout data begins to increase, we may be starting to overfit.
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ The Lagrange multiplier T on entropy is called “temperature” due to a satisfying connection to statistical mechanics.
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ It looks at the loss incurred if we choose the best analysis of each xi according to the model: Since small changes in θ either do not change the best analysis or else push a different analysis to the top, this objective function is piecewise constant, hence not amenable to gradient descent.
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ Final results are reported for a larger, disjoint test set.

This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ Each parser employed 10 experts, each parameterized as a globally normalized loglinear model (Lafferty et al., 2001).
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ Though the log makes it tractable, this remains a 0/1 objective that does not give partial credit to wrong answers, such as imperfect but useful translations.
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ One training criterion directly mimics test conditions.

 $$$$$ We show improvements over previous work on error minimization by minimizing the risk or expected error—a continuous function that can be derived by combining the likelihood with any evaluation metric (§2).
 $$$$$ Final results are reported for a larger, disjoint test set.
 $$$$$ The views expressed are not necessarily endorsed by the sponsors.
 $$$$$ Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8).

An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ (Depending on the task, the elements may be words, bigrams, labeled constituents, etc.)
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ We may add this regularizer to equation (6) or (7).
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ In both cases, we start with 8 to 10 models (the “experts”) already trained on separate training data.
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ We will usually write this simply as L(y) since y∗ is fixed and clear from context.

We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ A feature in the combined model might thus be a log probability from an entire submodel.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ We then used Philip Koehn’s phrase extraction software to merge the GIZA++ alignments and to extract and score the alignment template model’s phrases (Koehn et al., 2003).

In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ Once T is quite cool, it is common in practice to switch to raising γ directly and rapidly (quenching) until some convergence criterion is met (Rao and Rose, 2001).
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ We used 10 iterations of gradient ascent.
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ 1998.

Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ To approximate E[g(A)], where g is any twicedifferentiable function (here g = log), we can approximate g locally by a quadratic, given by the Taylor expansion of g about A’s mean µA = E[A]: Here µA = Pi µai and Q2A = Pi Q2ai, since A is a sum of independent random variables ai (i.e., given the current model parameters 0, our randomized decoder decodes each sentence independently).
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004).
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ K.-U.
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate.
