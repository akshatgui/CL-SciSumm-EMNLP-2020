The system consists of three phases $$$$$ On Slovenian, annealed minimum risk training does show a significant improvement over the other two methods.
The system consists of three phases $$$$$ Also, instead of the entropy constraint, we simply annealed on -y while adding a quadratic regularization term.
The system consists of three phases $$$$$ When rescoring, we assume that we simply wish to combine, in some way, statistics of whole sentences4 to arrive at the overall loss for the corpus.
The system consists of three phases $$$$$ Although the entropy is generally higher at lower values of γ, it varies as the optimization changes θ.

A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ We trained dependency parsers for three different languages: Bulgarian, Dutch, and Slovenian.11 Input sentences to the parser were already tagged for parts of speech.
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ Other work on “generalized probabilistic descent” minimizes a similar objective function but with γ held constant (Katagiri et al., 1998).
A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface. $$$$$ In our experiments, it never performed significantly worse 11For information on these corpora, see the CoNLL-X shared task on multilingual dependency parsing: http: //nextens.uvt.nl/~conll/. than either and in some cases significantly helped.

Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ In Structured Outputs M. Dreyer, D. A. Smith, and N. A. Smith.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ Informally, high temperature or γ < 1 smooths our model during training toward higher-entropy conditional distributions that are not so peaked at the desired analyses y* .
Deterministic Annealing was suggested by Smith and Eisner (2006) where the authors propose to minimize the expected loss or risk. $$$$$ We also discuss regularizing the objective function to prevent overfitting (§4).

This linearization technique has been applied elsewhere when working with BLEU $$$$$ It never does significantly worse.
This linearization technique has been applied elsewhere when working with BLEU $$$$$ Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8).
This linearization technique has been applied elsewhere when working with BLEU $$$$$ Another simple regularization method is to stop cooling before T reaches 0 (cf.
This linearization technique has been applied elsewhere when working with BLEU $$$$$ 1999.

In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ We consider evaluation metrics for natural language tasks from two broadly applicable classes: linear and nonlinear.
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ We then increase γ according to some schedule and optimize θ again.
In the geometric interpolation above, the weight controls the relative veto power of the n-gram approximation and can be tuned using MERT (Och, 2003) or a minimum risk procedure (Smith and Eisner, 2006). $$$$$ Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).

 $$$$$ We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).
 $$$$$ Thanks to the linearity of expectation, we can easily compute our expected loss in equation (6) by adding up the expected loss on each sentence.
 $$$$$ We now discuss how this expectation is computed.
 $$$$$ The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).

Deterministic Annealing $$$$$ 1999.
Deterministic Annealing $$$$$ Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)’s BLEU translation metric (the geometric mean of several precisions).

Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).
Gradient-based techniques require a differentiable objective, and expected sentence BLEU is the most popular choice, beginning with Smith and Eisner (2006). $$$$$ We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.

The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ Different methods can be used to attempt this global, non-convex optimization.
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ Seeking to avoid local minima, deterministic annealing (Rose, 1998) gradually changes the objective function from a convex entropy surface to the more complex risk surface (§3).
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ In particular, a pure unregularized loglinear model such as (5) is really a function of γ·θ, so the optimizer could exactly compensate for increased γ by decreasing the θ vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose γ and θ accordingly: min Ep-Y,e[L(yi,k)] − T · H(pγ,θ) (7) γ,θ In place of a schedule for raising γ, we now use a cooling schedule to lower T from oc to −oc, thereby weakening the preference for high entropy.
The N-best list based expected BLEU tuning (Rosti et al, 2010), similar to the one proposed by Smith and Eisner (2006), was extended to operate on word lattices. $$$$$ When maximizing likelihood, therefore, we will replace y∗i with the min-loss analysis in the hypothesis set; if multiple analyses tie 1Known algorithms are exponential but only in the dimensionality of the feature space (Johnson and Preparata, 1978). for this honor, we follow Charniak and Johnson (2005) in summing their probabilities.2 Maximizing (4) is equivalent to minimizing an upper bound on the expected 0/1 loss Ei(1 − pθ(yi  |xi)).

The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ Another training approach that incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ The distinction is in using a loss function to calculate the required margins.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
The objective function is defined by replacing the n-gram statistics with expected n gram counts and matches as in (Smith and Eisner, 2006), and brevity penalty with a differentiable approximation. $$$$$ We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.

Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ As (C, A) = Ei(ci, ai) is a sum of independent random length-2 vectors, its mean vector and 2 x 2 covariance matrix can be respectively found by summing the means and covariance matrices of the (ci, ai), each exactly computed from the distribution (5) over Ki hypotheses.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ Thanks to the linearity of expectation, we can easily compute our expected loss in equation (6) by adding up the expected loss on each sentence.
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ Maximum likelihood is not sensitive to the starting value of 0 because it has only a global optimum; annealed minimum risk is not sensitive to it either, because initially -y Pz� 0, making equation (6) flat. more random restarts give better performance and a more monotonic learning curve—see Figure 4.)
Similarly, Smith and Eisner (2006) reported test set gains for the related technique of minimum risk annealing, which incorporates a temperature parameter that trades off between the smoothness of the objective and the degree it reflects the underlying piecewise constant error surface. $$$$$ To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization problems (Rose, 1998).

 $$$$$ A linear metric is a sum (or other linear combination) of the loss or gain on individual sentences.
 $$$$$ In of volume 1, pages 1–6.
 $$$$$ Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.

In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ In our experiments, it never performed significantly worse 11For information on these corpora, see the CoNLL-X shared task on multilingual dependency parsing: http: //nextens.uvt.nl/~conll/. than either and in some cases significantly helped.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ Indeed, on Dutch, all three optimization procedures produce indistinguishable results.
In future work, we would like to investigate other objectives with a more direct task loss, such as max margin (Taskar et al, 2004), risk (Smith and Eisner, 2006) or soft max-loss (Gimpel and Smith, 2010), and different regularizers, such as L1-norm for a sparse solution. $$$$$ In particular, our experiments consider log-linear combinations of a relatively small number of features over entire complex structures, such as trees or translations, known in some previous work as products of experts (Hinton, 1999) or logarithmic opinion pools (Smith et al., 2005).

Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution).
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).
Although 1-best systems are not differentiable functions, we can approach their behavior during ERM training by annealing the training objective (Smith and Eisner, 2006). $$$$$ This technique can be used along with annealing or quadratic regularization and can achieve additional accuracy gains, which we report elsewhere (Dreyer et al., 2006).

This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data.
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ A feature in the combined model might thus be a log probability from an entire submodel.
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ We may add this regularizer to equation (6) or (7).
This is consistent with Jansche (2005) and Smith and Eisner (2006), who observed similar improvements when using approximate f-score loss for other problems. $$$$$ We may add this regularizer to equation (6) or (7).

 $$$$$ In particular, the expectation of log BLEU distributes over its N + 1 summands: where Pn is the precision of the n-gram elements in the decoding.7 As is standard in MT research, we take wn = 1/N and N = 4.
 $$$$$ In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).
 $$$$$ Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.
 $$$$$ Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.

An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora.
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ Different methods can be used to attempt this global, non-convex optimization.
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ For our machine translation experiments, we trained phrase-based alignment template models of Finnish-English, French-English, and GermanEnglish, as follows.
An annealed minimum risk approach is presented in (Smith and Eisner, 2006) which outperforms both maximum likelihood and minimum error rate training. $$$$$ 1999.

We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ The only wrinkle is that pθ(y∗i  |xi) may be left undefined by equation (1) if y∗i is not in our set of Ki hypotheses.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood—it produces lower-error systems.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ K. A. Papineni.
We can therefore support the claim of (Smith and Eisner, 2006) that MBR tends to have better generalization capabilities. $$$$$ Finally, we note the connections of minimum risk training to max-margin training and minimum Bayes risk decoding (§7), and recapitulate our results (§8).

In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ Each analysis has a vector of real-valued features (i.e., factors, or experts) denoted fi,k.
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ We mention an alternative way to compute (say) the expected precision C/A: integrate numerically over the joint density of C and A.
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ We thank Sanjeev Khudanpur, Noah Smith, Markus Dreyer, and the reviewers for helpful discussions and comments. and not everywhere differentiable with respect to the model parameters (Figure 1).
In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. $$$$$ One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution).

Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ None of these regularized models beat the best setting of standard deterministic annealing on heldout or test data.)
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ K.-U.
Cunei's built-in optimization code closely follows the approach of (Smith and Eisner, 2006), which minimizes the expectation of the loss function over the distribution of translations present in the n best list. $$$$$ The variance a2d may reflect a prior belief about the potential usefulness of feature d, or may be tuned on heldout data.
