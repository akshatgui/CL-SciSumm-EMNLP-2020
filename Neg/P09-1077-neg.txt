Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ We compute each example’s probability according to each of these language models.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ Intuitively, one would expect that there is some relationship that holds between the words in the two arguments.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ The PDTB contains discourse annotations over the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped.

In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ Using just these four relations allows us to be theory-neutral; while different frameworks (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Knott and Sanders, 1998; Asher and Lascarides, 2003) include different relations of varying specificities, all of them include these four core relations, sometimes under different names.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ These findings indicate an unexpected anomalous effect in the use of synthetic data.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ For implicits, the span in the first sentence is called Arg1 and the span in the following sentence is called Arg2.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.

Here we provide the details of the 9 features, shown as follows $$$$$ We include a feature for the presence or absence of modals in Arg1 and Arg2, features for specific modal words, and their cross-products.
Here we provide the details of the 9 features, shown as follows $$$$$ The General Inquirer has classes for positive and negative polarity, as well as more fine-grained categories such as words related to virtue or vice.
Here we provide the details of the 9 features, shown as follows $$$$$ We also confirm that even within the PDTB, information from annotated explicit relations (Wordpairs-PDTBExpl) is not as helpful as information from annotated implicit relations (Wordpairs-PDTBImpl).
Here we provide the details of the 9 features, shown as follows $$$$$ Our random baseline is the f-score one would achieve by randomly assigning classes in proportion to its true distribution in the test set.

Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ Two verbs are from the same verb class if each of their highest Levin verb class (Levin, 1993) levels (in the LCS Database (Dorr, 2001)) are the same.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ To our knowledge, this is the first study to examine the prevalence of polarity words in the arguments of discourse relations in their natural distributions.

Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits.
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ It is helpful when added as a feature in a standard, instance-by-instance learning model.
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ For our experiments, we use the Penn Discourse Treebank, the largest existing corpus of discourse annotations for both implicit and explicit relations.

Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ Intuitively, one would expect that there is some relationship that holds between the words in the two arguments.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ The language model features were completely useless for distinguishing contingencies from other relations.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.

Table 2 $$$$$ In this resource, each sentiment word is annotated as positive, negative, both, or neutral.
Table 2 $$$$$ Causal and comparison relations, which are most useful for applications, are less frequent.

 $$$$$ Word pairs containing “but” as one of their elements in fact signal the presence of a relation that is not Contrast.
 $$$$$ The least improvement was for distinguishing Expansion versus Other.
 $$$$$ Our random baseline is the f-score one would achieve by randomly assigning classes in proportion to its true distribution in the test set.

Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ Wordpairs-TextRels In this setting, we trained a model on word pairs derived from unannotated text (TextRels corpus).
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ Such features would not be applicable to the analysis of implicit relations that occur intersententially.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ Comparison We expected that polarity features would be especially helpful for identifying Comparison relations.

This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007).
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ They delete the connective and use [Arg1, Arg2] as an example of an implicit relation.
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ Intuitively, one would expect that there is some relationship that holds between the words in the two arguments.

Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ We analyze word pair features used in prior work that were intended to capture such semantic oppositions.
Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits.
Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ Table 1 lists the word pairs with highest information gain for the Contrast vs. Other and Cause vs. Other classification tasks.

Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.
Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ The largest gain was in the Contingency versus Other prediction task.
Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ In our analysis, we focus only on implicit discourse relations and clearly separate these from explicits.

Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations.
Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ Word pairs containing “but” as one of their elements in fact signal the presence of a relation that is not Contrast.
Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ One approach for reducing the number of features follows the hypothesis of semantic relations between words.

(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ Such features might work well and lead to high accuracy results in identifying synthetic implicit relations, but are unlikely to be useful in a realistic setting of actual implicits. the-but s-but the-in of-but for-but but-but in-but was-but it-but to-but that-but the-it* and-and the-the in-in to-the of-and a-of said-but they-but of-in in-and in-of s-and Also note that the only two features predictive of the comparison class (indicated by * in Table 1): the-it and to-it, contain only function words rather than semantically related nonfunction words.
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ In addition, using equal numbers of examples of each type can be misleading because the distribution of relations is known to be skewed, with expansions occurring most frequently.

Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ Wordpairs-PDTBExpl In this case, the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ They use the RST corpus (Carlson et al., 2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988).
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ We ran four binary classification tasks to identify each of the main relations from the rest.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.

However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well $$$$$ We were surprised that the polarity features were helpful for Contingency but not Comparison.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well $$$$$ They fell into oblivion after the 1929 crash.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well $$$$$ Each relation in the PDTB takes two arguments.

An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ Following Marcu and Echihabi (2001), the pair [The government says it has reached most isolated townships by now, but] and [roads are blocked, getting anything but basic food supplies to people remains difficult.] is created as an example of the Cause relation.

Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.

 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
 $$$$$ Our experiments demonstrate that features developed to capture word polarity, verb classes and orientation, as well as some lexical features are strong indicators of the type of discourse relation.
 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.

 $$$$$ If this hypothesis is correct, pairs of words (w1, w2) such that w1 appears in the first sentence and w2 appears in the second sentence would be good features for identifying contrast relations.
 $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
 $$$$$ However, the corpus contains only Cause, Contrast and Norelation, hence we expect the WSJ language models to be more helpful.
